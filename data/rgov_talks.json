[{"Year": 2023, "Speaker": "Gus Lipkin", "Title": "Your Maps Might Be Lying to You", "Abstract": "Mapping international borders can be tricky business. We'll dive into the messy world of inaccurate data sources, choosing base maps, and other layer problems. We'll also talk about why obsessing over perfect data might not be as important as it seems.", "VideoURL": "https://youtu.be/xnxLmld4M-A", "id0": "2023_01", "transcript": "Hi everyone, I'm Gus and I want you all to know, your maps might be lying to you. So normally I like to set the stage, but in order to keep it in theme I'm gonna map the stage instead. Some maps are bad and I wanna walk through some of the reasons on why that can be. Let's start with this world map, which while it's meant to be decorative it's got some pretty glaring issues. Australia, Iceland, and Hawaii are all missing, along with a bunch of other weird mistakes, like the Suez Canal not existing. If we take a look at this map of the United States it also has a few issues. Cape Cod is missing, Long Island seems to be floating away from New York State, the upper peninsula of Michigan is missing entirely. There's something weird going on in the DC capital region, and the Alaskan Aleutian Islands and the Florida Keys are both missing. Now if we take a look at New Zealand, a country that's pretty well known for being left off of world maps, and we go to their government's website, it turns out that they also left themselves off the map. But this one's actually on purpose to make a joke about it, because this is from the 404 page not found, and so it would be 404 New Zealand not found. Not only are some maps bad, but even when they're good, getting the borders just right can be pretty tricky too. That's because the borders can depend on where you are, who you ask, and even if you were to ask the same person, it could depend on when you ask them. If, for example, we were to ask Data Hub how much of Miami is in the United States, you might be a little surprised when they say, not all of Miami is in the United States. And it looks like that's mostly just due to some low polygon resolution. Or maybe we could ask the World Bank where the border between Peru and Ecuador is. And in this case, that gray line you see is the true border, and the blue line is what the World Bank thinks the border is. And so it looks like maybe the border's been shifted to the left a bit, and it's kind of a lower resolution. But you're probably thinking the World Bank seems like a pretty important institution. Why can't they get their borders right? If we hold on to that question for a minute and hope that it's some sort of fluke and try and look at, let's say, natural Earth instead, we can see that Roosevelt Island is outside the United States. And it turns out Data Hub actually has this problem too, where it looks like some extra polygons that could surround the island aren't included. And it also raises the question, how far up a river do you go when trying to map out a country's borders? By now you're probably thinking, well, even though everyone else is confused about where the borders are, surely the US Department of State knows where the borders are. They do their best, but they're not perfect, like with this border between Egypt and Sudan where you don't have two extra small countries tucked in there. But to be fair to the Department of State, this is a disputed border, so it can be helpful to know that both versions of the border exist. If we move south a little bit and look at the border between Sudan and Eritrea, it looks like the Department of State maybe has some extra resolution that the truth map doesn't have, and it looks like it's been shifted maybe up into the left just the tiniest bit. Okay, well, what if we check an easy one, like the Vatican? I mean, they've got walls, the walls are straight lines, and they're hundreds of years old, and we're built in around 850 AD, so that should be more than enough time to really go in and make sure you're getting the borders right. They actually did a pretty good job here, and while that border isn't quite perfect and does go down the middle of a road for a little bit, it's still far better than the other two examples we've seen so far. At this point, you might be thinking, so what if the Department of State gets a few international borders a little bit wrong? Because surely the US Department of State knows where the United States borders are. They don't. This is the border between Mexico and Arizona, with Mexico on the left and Arizona on the right. But to their credit, they did get the 49th parallel with Canada right, probably because it's a relatively straight line. On paper, these may seem like some relatively minor problems, but in reality, they can start up a bit of trouble. You could have property disputes, and this is exactly why people get surveys they want to know where their borders are. It could be problems with border rights or border security, because if you've got border patrols, you want to make sure they're patrolling on the right side of the border if you even know where your border is. So what's going wrong with all these maps? Why are all of the borders in the wrong spot? And the truth is, I haven't been entirely truthful with you guys. I've been comparing everything to the OpenStreetMap base maps. And what made those maps the true map? And who made me the arbiter of what is and isn't the true map? And I also didn't check my projections while I did check my coordinate reference systems. And I'll get into what those are more in just a minute. All that to say that if I were to take the OpenStreetMap borders and plot it against the OpenStreetMap base map, then sure enough, everything lines up and it all looks great. When we go back and compare with the Department of State borders, I do need to give them some credit, because this particular stretch of border is complicated, because the original treaty that defined this border was based on the path of the river. But over time, the river has shifted. So the question becomes, is the modern day border where the river used to be when the treaty was signed, or is it where the river is now, and constantly changing? Besides funky borders, you could also have a few other common problems with your maps. Like I mentioned before, you can have a problem with your projection, which is how you're choosing to represent a 3D spherical planet, sorry flat earthers, on a 2D map. Or it could be a problem with your coordinate reference system, which is how you choose to plot the points on that now 2D projection of the planet. You could also have data problems, which could be some incomplete data, or you could have extra data, maybe in the form of extra features, or all versions of a disputed border, like we saw with Egypt and Sudan. You could also have incorrect or out of date data, some file type or data type mismatches. You could have a different data resolution between two different data sets, or you could even have fake data that map makers put in there so that if someone tries to steal their maps, they can point at this fake location in court and say, look, this isn't a real place, this person stole my map, and you can tell because they have the same fake place too. I actually came across a really great example of some incorrect data the other night, when Nicole and I were walking to dinner and we'd gotten separated from the group. And so we wanted to know where in the world is Travis Nochi. For those of you that don't know, Travis is a co-worker of ours at Lander. And while we were walking, Nicole realized that she actually has Travis in Find My Friends on her phone. And so she pulled open the app. She was really excited to see that Travis was sitting in the middle of the Potomac River. Well, we wouldn't have been too surprised if Travis had made the decision to go for a sudden evening swim. It didn't seem quite right, and we did eventually find him next to the Georgetown University sign. To better illustrate how far off Travis' phone's location was from reality, I've marked his actual location with a red star on the map. Okay, yeah. Maps can have a lot of problems. But now what can I actually do about it? Just like I found my source of truth with the OpenStreetMap base maps, you also need to find your source of truth. Every map starts with a base map, and every base map has four main features. It has a function, which could be street maps, satellite view, could be a hybrid street, satellite, topographic. It has a style, which could be the, if it's full color, maybe it's grayscale, dark map, or it might have labels or it could not have labels. You'll also have different data sources, which could be OpenStreetMap, the government, map box, overture maps, and many more. And lastly, you'll have your coordinate reference system, which frequently is WGS84, but there are tons of other reference systems out there as well. When choosing your base map, it's important to go back and consider those four features. So with your function, what is it you're trying to show? With your style, is there a certain style that helps communicate the point you're trying to make? Or is there a certain way that you want to display your data to make a specific impact? When thinking about your data sources, where does your data come from? Do that, does that data provider have a base map of their own that you can use? And with the coordinate reference system, it's really important to make sure that you're matching your system to the reference system used by your data provider. When it comes to adding data, there's a lot of other things to consider as well. Your data layers could be any data of your own, maybe it's data you've collected, or maybe it could be data that you've gotten from some other provider, like borders, roads, weather patterns, it could be the migratory patterns of Canada geese, or the locations of working McDonald's ice cream machines in the Washington, D.C. area. Your data could also come in a few different formats. You can have raster layers, which are tiled images, like satellite view or weather radar, and these have a specific resolution. And that's why frequently maps have to reload when you zoom in. You could also have vector layers, or chart lines, and polygons, and sometimes images, and these are usually used to communicate borders or paths. And then you could also have UI layers, like markers, pop-ups, or tooltips, some extra way to communicate information to a user, and then a bunch more other kinds of data layers as well. But Gus, you might be thinking, this is complicated, and I really don't care. Yeah, that hurts, but fair enough, you don't always need super accurate locations. Like, you don't need to know where in the McDonald's the ice cream machine is located, because chances are they're not going to let you behind the counter anyways, and you really just need to know where the McDonald's itself is. You could also be looking for population centers in cities, which really only needs to come down to a round of block or so radius before anything more specific just wouldn't be that helpful. You could also be using a map as a visualization where the data itself is what matters, like a chloropleth, which are those colorful polygon maps that are frequently used to illustrate voting, as well as visualizations of a region's watersheds, or maybe something else. It could really be any situation where you're trying to convey some other data using the map as a platform. The important thing is, just be consistent. Make sure you know what projection you're using. Same goes for your coordinate reference system, and make sure both of those match your data sources and your base map sources, because once everything's on the same field, your borders or whatever data you're trying to plot will be in a much better spot than it would be otherwise. While some maps might be bad, all maps are lying to you. When you're making your own, do you know what you're sacrificing to communicate your message? And when you're looking at someone else's, do you know, and are you able to tell what they've sacrificed to be able to communicate theirs? And that, my friends, is how your maps might be lying to you."}, {"Year": 2023, "Speaker": "Aayushi Verma", "Title": "From Data to Collaboration: Connecting Our Researchers with R and Shiny", "Abstract": "We present a case study of how R and Shiny are enabling our company to gain new insights into our research, and connect researchers with each other.", "VideoURL": "https://youtu.be/d1q08yb9W7I", "id0": "2023_02", "transcript": "Hi, my name is Ayushi Verma and I'm a Data Science Fellow at the Institute for Defense Analysis, or IDA, as we call it for sure. So this presentation is a story of how we answered the question of how to connect our researchers with each other, each of who have different skill sets, in order to foster more interdisciplinary collaboration. And we did this using our company's data strategy, along with data management and governance principles, and something called the research taxonomies in order to produce our solution called ITRAE, which I'll be explaining throughout this presentation. And this process helped us answer the question to deliver key insights to our researchers, all powered by our. So first and foremost, what exactly is ITA? We are a nonprofit FFRTC, or Federally Founded Research and Development Center, which means that it is our job to perform unbiased, scientifically rigorous analyses to answer US national security and science policy questions for our government sponsors and business. We have eight research divisions as part of our Systems and Analysis Center, and we have our Science and Technology Policy Institute, which we sort of consider a ninth research division. Our core values are rigorous analysis, trusted expertise, inclusion, collaboration, innovation, and service to the nation. Our research staff perform a lot of research across many broad domains and areas. So the core component of our objective and rigorous analyses is data. We deal with and depend on a lot of data for our research. And this is why ITA is leading a data strategy to establish a data-centric culture and to leverage data to efficiently respond to research questions. So this strategy develops solid and sustainable infrastructures and practices to promote a data-first approach to our work through multiple initiatives. One of the key initiatives of the data strategy has been the development of a set of research taxonomies. As an institution, ITA has a defined set of core competencies which are rooted in our values, which I talked about on the previous slide. So the data strategy grew on these core competencies and looking at the types of work, fields, tools, technologies that they are related to in order to create our research taxonomies. And these taxonomies are a organized hierarchy of keywords that describe the research done at ITA across these eight broad domains. So our research divisions work on lots of projects and these projects have deliverable products like reports or memorandums. And so we can use the metadata for these projects and products and combine them with the research taxonomy terms to quantify and describe our research. And this works through another initiative which is a periodic tagging effort where the authors of the products tag them with relevant terms from the taxonomy such that it describes that particular body of work. And this creates a data set of products, authors and taxonomy terms. We combine this with other data sources like project, author and division data to produce a data set that describes the research we do here at ITA. This research data set brings together a cohesive and quantitative picture of the research done here which is bringing new insights to our research staff and management. So we created an interactive shiny application that visually shows this data. This is the shiny application and we call it ITRE which is short for ITA taxonomical research expertise explorer. I'll walk you through the app soon but first I'll describe how exactly our enabled us to do this. So this is how the magic happens. This is a pipeline showing our process and I walk you through each step of the process. So prior to ITRE our data had been pretty disparate. We had some data being stored in SQLite database files. We had some data on our SQL server. We had some static data being stored in comma separated value or CSV files. And we had some RDS objects. So using the magic of R in our studio we ingested these data from the different data sources. With the packages listed here we cleaned these data and we combined them together in order to produce a single coherent data set. We out-putted this as a RDS object and we passed this to our shiny application which takes in this RDS data set and displays a table and a graph which I'll be walking you through very soon. So this entire pipeline is being hosted or the shiny application is being hosted on our company's internal shiny servers and in order to deploy this pipeline we hosted on a Docker container. And we have periodic cron jobs which are refreshing the data in the pipeline occasionally. So with the data pipeline and shiny's help we are now able to answer five very specific questions about our research. I would like to take a moment to point out this great quote I saw on Google which says that from a drop of data an ocean of wisdom can emerge. Which I think is a great analogy to our data set because it's a relatively simple and minimalistic data set but by combining the previously disparate sources of data we are now able to extract impactful insights that help our researchers and staff. Okay, finally on to the application. So this is our shiny app, itree and it has five tabs at the top each of which is answering a very specific question. So this is the first tab which answers the question of who are the top researchers for a given research area. On the left panel here is the research taxonomy and other data filtering options and on the right is a graph and a table showing the results which is percentage of the authors products that were tagged with a selected term from the taxonomy. It is shiny's reactivity and modularity which are helping us to provide this very simple no fuss interface for our users to interact with this data set. Because on the second tab on the first tab the screenshot cannot fit all the filtering options on the left. Here are the rest of the filtering options. So aside from selecting the research area which is a term from the taxonomy the user is able to select a range of taxonomy terms. So by selecting narrower terms they are able to select products that have been tagged with their selected term as well as all of the terms that fall below it in the hierarchy. If they choose only the selected term then the program will count only products that have been tagged with that term. They can filter the data set by year the product was published in. They can filter by the research division that that product was published in. They can include formal products and formal products for both types of products. They can include authors who are active, who are still active at IDA, authors who are inactive or all authors. They can also select the minimum number of total products for authors which means that they can set the minimum number of total products that an author has published. And this may be useful in order to get a more comprehensive understanding of the percentage being displayed. So as an example if an author has published say 24 products and 22 of them are tagged with the user selected term then that gives an indication that this author is most likely very proficient in that research area. Finally there are some display options as well. The user can rank by most number of products or least number of products. And they can also select the total number of authors to display. And finally they can download the graph as a jpeg or the table as a CSV file for further analysis if they so wish. So this is the second tab which answers the question of what is the research expertise or research profile of an author. The user can select an author say Jane Doe and now they can see Jane Doe's most frequently tagged terms in their products. So this is looking specifically at the percentage of the authors products that have been tagged with some terms. As an example here we can see that Jane Doe has a lot of proficiency in statistical analysis techniques, operational test and evaluation and some other topics. The third tab answers the question of what research does a division produce. So this time instead of looking at the research profile of an author we're looking at the research profile of a division. So this time we select a division and we can see the most frequently used terms that appear in products for that division. So specifically here we are looking at the percentage of a division's products which have been tagged with terms. This is the fourth tab which answers the question of who are the top researchers for a division. This time again we can select a division and we can see which authors have published the most or least a number of products within that division. So specifically here we are looking at the percentage of the division's products with that author. Finally the fifth tab answers the question of how do the divisions produce research for an area. And here specifically again what we're looking at is what percentage of the division's products or tab with selected terms. So here again we can select a research area from the taxonomy and we can see which divisions have produced how many and number of products under that research area. This is a bonus sixth tab which is looking at a free form sandbox exploration of the taxonomies. So if the user wishes to see or explore the hierarchies of the taxonomies they can head over to this tab and just have a look and learn about the relationships between the tax. So this application I tree was launched earlier this year in January and researchers are already using it from many different things like finding subject matter experts across divisions or like a networking tool or to quantify the research done and many more. So this current iteration of I tree is a stepping stone for bringing more insights to our research staff. We still have a lot more work to do on I tree and some of these next steps include integrating more data streams so that we have a more holistic view of the data. We try to automate and streamline these tagging efforts and it would be really cool to visualize these networks and perhaps even recommend researchers who have similar expertise to each other or divisions which have similar expertise as each other. So to conclude this is how we built a data science solution to help our researchers. We use our company's data strategy, we use data management and governance principles, we used our company's research taxonomies and we were able to produce a shiny app called I tree using the magic of our and our shiny. Thank you very much for watching."}, {"Year": 2023, "Speaker": "Irena Papst", "Title": "From Scripts to Pipelines with Targets", "Abstract": "Do you ever find yourself starting with a simple analysis script only to end up wrangling a thousand line behemoth? Are you sick of wasting time re-running long scripts from start to finish, just to make sure everything is up-to-date? Are you haphazardly saving objects to file because they take a long time to generate? There\u2019s got to be a better way! Enter targets, an R package used to build reproducible, efficient, and scalable pipelines. In this talk, I\u2019ll introduce the targets package and share how I\u2019ve used it to streamline my work modelling infectious disease spread at the Public Health Agency of Canada.", "VideoURL": "https://youtu.be/kFnWc0y7Sq4", "id0": "2023_03", "transcript": "Our next speaker got into the craze, and still, every day, plays the wordle. Please welcome Irina. Okay, so my name is Irina. I'm a senior scientist who has wandered her way down from Chile, Canada, to talk to you guys today about targets. Now, when I say pipelining, some people may think this kind of pipelining, but actually the kind of pipelining that I wanna talk about today looks more like this. So we have sort of some objects in those round represented by those circles, and then we have some methods or functions represented by triangles, and there are sort of arrows indicating how things are related. Now, there are many ways to define pipelining. I'm leaving a lot out here, but the way I wanna think about it for the purpose of this talk is that pipelining is the process of writing down a recipe for outputs where all your dependencies are stated explicitly. So we'll get into more what that means in a bit. And you might ask yourself, isn't that just a script? And in some sense, yes, scripts can be thought of as pipelines, but for me, pipelining is so much more than just a script. So in my work, I often get requests like this because as a scientist, most of the work I do is in modeling infectious diseases. I've had a busy few years, unfortunately, but things are better now, thankfully. And sometimes my boss will come up to me and say, hey, there's this disease, it's picking up, can you forecast it for us for the next couple of weeks or can you set up a model? And this is definitely the kind of request that I might tackle with a pipeline, but there are other ones, oh, sorry, that you might be more, maybe you may relate to more, which is, can you clean this data set for the team, or can you make some plots for a presentation next week, or can you make figures and tables for a paper if you're more on the academic side? Those are all actually examples of tasks that I've written pipelines for as well. So as a concrete example, let's start with a very simple script that might tackle that disease request. So we start with, you know, loading our libraries, we might read and clean the data, I'm doing a tremendous amount of data cleaning, as you can see. And then I'm going to simulate, I have some method that makes a forecast from the data, very mysterious. I get out of simulation, and then I plot that forecast. And honestly, this might be just fine for what you need to do. And the point of the talk is not to convince you to always use a pipeline, because that would be silly. The point is that I want to motivate you to maybe look at adding pipelines to your toolkit so that you can be more efficient in your work sometimes. So this is fine times, but then maybe my boss will come back to me and say, awesome, can you add multiple forecasting areas? And I'll be like, okay, so now I'm revisiting my very simple script and I have to make it more complicated. So an obvious way, I know this might turn some people off because for loops are a little taboo in the art community, I might have used L-apply, fine. But just spend your sort of disbelief that I'm doing this. I might just, you know, set up a list of scenarios that are labeled and there's something under the hood there that makes this help me make a forecast. And I'll loop over those scenario labels and pass that into my make forecast function. But this might be a little slow, especially if those simulations take a long time. You know, there could be a bottle that get from lines 11 to 14. So I'm gonna be really smart and I'm going to have parallelization rescue me, right? I could just, you know, a really simple way is to call the do parallel package, set up a parallel processes. And then for some reason I'm using four workers, even though I only have three tasks, a little silly, but anyway, and I'm just gonna kind of loop over those scenarios in parallel. Sure, all of this works, right? There are always gonna be workarounds or other ways of tackling this problem. But what if I only want to run certain scenarios? Maybe for some reason I'm tweaking scenario C because I'm not really happy with how the outputs are looking. If I'm just running my script from top to bottom all the time, I'm being very inefficient. I'm rerunning scenarios A and B, which haven't changed, right? So from a computer standpoint and from a my time standpoint, that's not really a good use of it. What if I'm running into errors for one scenario? Maybe it's not that I'm trying to improve upon the results for scenario C, but I'm literally running into bugs. I'm not gonna want to again, run from top to bottom. Or I step away from a meeting and I have a notoriously poor memory. That's what my friends say, but I like to say that I don't wanna remember things I don't have to remember or I can write down. Maybe I step away from a meeting and I come back to my computer and I think, wait, which one was I working on? Which results are up to date? What's going on here? Of course, for point one, I could edit the script and isolate the scenario that is giving me trouble or I wanna improve upon for point two as well. Or maybe I can go into my little finder or Windows Explorer and look at when the date modified was on some output objects to figure out what's up to date. But the point is that's a lot to manage. I don't wanna be sitting there juggling things that I don't have to juggle that someone who's way smarter than me has written a package for. And also I will mention targets which I'll get to is not the only pipelining tool. This is a very common concept in data science. There's so many tools out there. But a lot of smart people have put work into this. So I don't have to be sitting there finagling with my script. So that's where targets comes in. So this is ripped straight from the user manual, which was my top read of 2022. I love that book. The targets package is a make like pipeline tool for statistics and data science in R. I actually work with dynamical systems models a lot. I mean, not just I do some stats stuff as well in data science. So I would argue it's really a great pipeline tool for any tasks that you can do an R that makes sense to do an R. So for people who know what make is, you may have been sitting there on a loop in your head, like just use make, just use make. What are you doing? Right with your script. And my answer to that is I don't want to. I've used make before, I actually really like make, but I used it more in my past life as an academic as opposed to now as a government scientist. Because in government and sometimes in other corporate settings, lockdown settings, an R package can actually be a lot easier to install and also to share with your colleagues, thanks to things like our end. So sometimes an R package staying within the R ecosystem is really the way to go. There are workarounds for this. You could set up a Docker image, and get your colleagues on board with like command line tools and other things beyond R, but you don't always have to bring a gun to a knife fight if you don't have to. So let's go into back into our little script that we had before that was just fine. And let's just look at this line here. I'm going to translate that to a target in my pipeline. So a target is something that the pipeline makes. Now I'm going to use the targets syntax. It's quite simple actually. So we have to specify the name of the target, which corresponds to the left hand side from our script and that assignment. I use an equals, I know throw some people off, but it's the same as the arrow assignment. Right? And then we have the recipe for the target, which is the right hand side of the equals sign. And it's just wrapped in this tar target statement. That's all. It's really very similar to writing up a script, or rather an element of a script. But you can't define targets everywhere. You can't just put them anywhere. They have to be in a underscore targets file, which is a special file that defines your pipeline. Thankfully, it's really easy to set this up. There are some necessary elements. So the developer of targets has given us a nice little utility. You call the library and then you call use targets, and it makes that file in the main directory of your project. So you can just go in there. It's got a bunch of boilerplate. You can tweak it as you need and go on. So from that, I can make this pipeline. So this is now a underscore targets.r file that defines my pipeline. And there's two main elements. The first one is the setup. So there's, we call the library targets, everything else under will work. And then we source any files that define our functions because targets is very function oriented. So generally, the best practice is to write a function to do the thing that makes your target. But you can, of course, as you see later on, just have naked sort of code in there as well. And then we have to tell it what packages we need. And this is different from normally at the top of a script, we write library package. But what's happening with targets is it's actually calling a R sub process in your session to run your pipeline away from your locally defined variables. So that you don't accidentally have it so that your pipeline only runs when you have certain things in your local environment, right? It's kind of like putting things in a sanitary box before it runs. And it uses caller for that, if you're familiar with that package. So we have to pass sort of those arguments of like, hey, remember to call these packages before you run the pipeline. And then the pipeline, which I know I've made sound really, really like big and cool. It's literally just a list. It's a list of targets, great, simple. And you can see I've included the three elements of our original script. I have reading and cleaning data. I have a target for the simulation and I have a target for the plot. And so why have I bothered to do this? It looks just like a script almost with a little bit of a weird syntax. Well, there's a bit of a free lunch. Now that you're using this package, you've sunk time into it, not fully free, but there's some sort of free things you get out of it. It's really easy to make stuff. So once I have my target names, I can call tar make on a target name and I will get this in the console. It'll tell me I'm starting the target data. I've built it. This is how long it took it and so on. And then at the end, how long did my pipeline take? So you actually have some built-in profiling. If you're getting bottlenecks in your code, you can immediately see which targets are taking longer than others. It's also easy to pull stuff out. Once those targets have been made, you can read them in your interactive session or into a quarto doc or armark down or whatever and just pull them up. And you're getting the object back. So if this is a GG plot, you can then add to it. You can add theming, you can add elements, you can do other things. So that's really nice because it doesn't require you to go back into your script and find, okay, I have to run this thing to get this into my local environment. It's all taking care of for you. So you don't have to do that finagling. Or you can use tar load, which will literally load the target into your local environment under the same name as the target. So this pulling this command will load an object called plot with my plot into my local environment. It's really easy to visualize the pipeline. I love this because with a script, you have to kind of read it and sketch it out yourself if you're not sure how things interact, which if you're working with others and they've maybe started a process, they've started some analysis, they might have to sit down and explain to you how the pieces interact. With this, with this command, tar, this network, it basically just lays it all out for you. So we have our three objects, data, sim and plot, and then we have our two methods. And while I don't think, I don't know that this will work, this is actually interactive. So I don't know why in these slides it's not good, but in the RStudio Viewer or other options, you can actually drag these targets around and the arrows move. So you can kind of see, right now it's hard to see, but plot forecast, the arrow for it actually goes through here to plot. So it's kind of overlaid. It doesn't actually, sim doesn't depend on it. There's no arrow here, but anyway, you can kind of play around with it. And I'll point out this legend here, which is that this dark forest screen means that the targets are up to date. So I actually wrote a pipeline in the background in these slides and I ran it and it actually made all of those targets. So now it's telling me everything's up to date. So nothing's changed. And what I can do is I could maybe modify a function. So it's not just watching the objects. It's watching all the dependencies, including your user defined functions. So if I change how I'm plotting, maybe I change a point color or something. And then I run VisNetwork, targets will detect that change and it will tell me, hey, you updated your plot method. So actually the dependent or the downstream target plot is now out to out of date. So if I were to run Tarmac again, it's not gonna make data, it's not gonna make sim because those are up to date, but it will rebuild plot. So it's skipping the stuff that it already has made and is up to date, which can save a lot of time. Basically, it's only making what we need. And there's much, much more, which I'll have to glaze over here because we don't have time to get into it. But branching is really easy. So if I have sections of my pipeline, like my forecast simulation that I want to repeat for different scenarios, it's really, it's like a couple extra lines in your pipeline to set up and then you're good to go. And with that, going hand in hand, is distributed computing. There's a really nice new package called Crew, written by the same developer, that it used to be with parallelization, used to be done with other packages, but there's now a sort of targets, almost optimized a parallelization package, which is really, really nice. And it literally, you can parallelize your pipeline with one line of code now. And then there's the target types package. So you can imagine someone who's written a package about pipelining, you're doing a lot of the same kinds of tasks, like maybe reading files or something. It can be really useful to have shorthand sort of targets for more complicated things, for common tasks in data science. So that's another package. Now, I've been using this package for a little over a year and I'm completely self-taught in it. So I've made some mistakes and I just want to share some things with you that if you're going to start to use this package may be useful to you. Not everything has to be a target. It's very tempting to kind of piece out every little thing and add it in your pipeline, especially when you're building a big complicated one. But there's a bit of an art. I think the key is to start with broad strokes and then drill down into smaller pieces as you need them. Larger targets, when they're skipped, will give you bigger speedups. So that's an advantage, but smaller targets mean that you have more control over what is skipped and what isn't. So there's a, you know, you have to kind of ask yourself a few questions, mainly, does this make, might this target get updated independently of other targets if so you might want to split it out? Or is this something I'll want to look at, like get my hands on down the line, you might want to split it out as well. Don't be afraid to run the pipeline. So this is kind of a, I don't know if anyone here has had this experience, but when I started coding, I was always afraid of errors because they're scary and they're red and what's going on, right? But I actually learned to embrace errors and similarly in pipelining, you will get errors when you're developing pipelines. And I would always be afraid of running the pipeline because I didn't want to run into errors and I don't really understand this tool and what's going on. But it's actually not too hard to debug. So for instance, when you run a pipeline, if you run into an error, targets will spit out the last error that, it'll give you some context. It'll say, hey, this target sim error, here is the last error and you can actually look at previous errors as well. There's some guidance there. And I'll point out that actually there is a really good debugging guide online. Your temptation may be too. Well, my sim, remember my sim object was created with, I had some data going into a function called make forecast. So you might be tempted to source the function, wrap it in a debug statement or maybe insert a browser statement in there. However, you like to debug, load the data and then actually run the function in your local session. Don't do that, it's inefficient, it works. I did it for a while, but it's once you have functions that take many, many targets as arguments, it becomes unwieldy to do this. So instead, a really nice trick is to insert a browser statement into your function, make forecast, let's say, you don't have to resource it. The pipeline will take care of that for you. And then when you run the target sim, you just insert this argument that's color function equals null, which tells targets, don't use a caller wrapper, just run it in my local session. And then you'll have access to the browser output because when you run, if you were to run, you would try to debug the function or have a browser in there and just run like normal, the caller subprocess won't let you get in there. Like you can't get in there to browse and look around your function. So that's a nice little trick that has saved me a lot of time. Another one is be explicit. So this is actually not, this is an example of a bad target because if I'm expecting my case counts data set to update, I have not told targets to watch that. I've baked in that file path into a target. So if I update the case counts, data will still look like it's up to date because it's not clear that there's a dependency on that file. So instead, you have to first call in the data file as format equals file to tell it, hey, watch this, it's a file so you have to watch it in a special way. And then that the data target depends on that. And then if you update the file, the updates will cascade, right? And finally, I'll just say that I used to be such a static branch or I used to static branch everything. I'll explain what that means. So static branching, what happens is the targets get generated before the pipeline is run. So you have to know what those targets are gonna be. For instance, if I wanna run a simulation for Ontario and for Quebec, I can pass, you know, I can tell the pipeline to set up with those two branches. But, and what's nice about that is you get clearly named targets like Sim underscore ON for Ontario, for example. So I know what things look like, but it's actually much more annoying to set up aggregation points. If I run simulations for two provinces, I might wanna actually just do a bind rows on those tables so that I can plot them in the same GG plot. Dynamic branching, the targets get generated at runtime, which made me very uneasy because they had these cryptic names. So, you know, I don't really know what that is. And I'm always trying to get my grubby little hands on those targets to see what they are when I'm developing. And the thing is though, that because it's kind of concealed for me, there is some automatic sort of stuff that gets happened that happens with aggregation that can be really, really helpful. It becomes a lot easier actually to branch and then re-aggregate. So I know this might not be super clear because I'm not giving you an example, but if you get into this package, definitely don't discount dynamic branching. Now, my final thoughts, you don't always need a pipeline. So if you have a short one-off task, you're not necessarily gonna return to it, maybe just for homework or, I don't know, some small task at work, just maybe write a script. It's fine. I still write scripts occasionally. I know it's hard for me to admit that up here. But if you have a multi-step process, maybe you'll have to run it again. You'll have to return to it. You'll have to pass it off to a colleague. A pipeline can do wonders for you because it's a kind of self-documenting too. Even though you may not always want a pipeline, you can still set yourself up for success. So there are a couple of coding habits that I've adopted over the years, two of which I just want to highlight here. I almost always work so that my file structure is compatible with targets pipelines as well as our packages, because they use the same thing, mainly that you define your functions in the R folder. In general, I almost always work as if I'm going to eventually write a package because I just find it helps me really keep myself organized. You don't have to go that extreme. But one tip is to document your functions as if you're going to package them. And in R, that means using Roxygen, which has a very standardized format. I know how to read it. The function documentation is right above the function definition. It's all in one place. My colleagues know how to read it. And if I do want to package down the line, it's really easy to go and convert that into a package. So now I really hope everyone here is just itching to try to write a pipeline. So if you are new to pipelining, what I would suggest is read the targets manual. Like I said, one of my favorite reads of last year, if not be favorite, it's really well written. It's written by someone who obviously does a lot of data science and knows where the quirks are of their software, and it's wonderful. And what I would suggest is start with a working script, something maybe you've written for a quick task and just try to convert that and see how that goes. And then maybe take something more complicated that might involve branching and try to convert that. If you're already a pipeline user, maybe you've used other tools like make, et cetera, read the targets manual. It's really good. And it's going to kind of, I mean, a lot of that stuff, there's some really good motivation and prefacing for why you should consider pipelining. That might not be new to you. But if you're trying to get a colleague into pipelining that's never done it before, great gateway. And definitely check out branching, crew, and target types which are a bit more advanced use cases, have a bit more advanced use cases. So thank you. I'll mention that my slides are available online here and there are some links and Easter eggs in the notes. If you do want to refer back to it while you're learning targets later today, I hope. So thank you. Bye."}, {"Year": 2023, "Speaker": "Gary Harki", "Title": "Using Open Records Laws to get Data from the Government (and When to Sue)", "Abstract": "Gary will break down how to effectively use open records laws to get data from local, state and federal agencies. He'll talk about the hurdles you encounter and how to overcome them.", "VideoURL": "https://youtu.be/MovCyrOGmZo", "id0": "2023_04", "transcript": "So our next speaker, so I'll make sure you get all the details right, has a 13 year old house plant that was given to him by the mother of a man he wrote a story about who got beaten up in the street by his own lawyer. So everyone please welcome Gary. My name is Gary Harkai. I am the investigations editor for Bloomberg Law. I am not a programmer. I am a journalist and but I often I've worked in local newsrooms and I've joked many times that like I'm usually the person who knows more about data than anybody else in the newsroom. Not so much now we have like great actual data people but in a lot of places it's been a situation where in the land of the blind the one I'd man was king and I was the one I'd man. So what I'm going to walk you through a little bit today is how to request data from the government, how to get it when you might need to sue them. Which which happens not not all that infrequently. So a couple of things to think about before you file a request, the first thing you want to do is just ask for the data. Sometimes sometimes it's online where governments will will just kind of give it to you. You don't always have to go through the process of a formal formally filling out a freedom of information, act request and sending it along. But you know, and also it just sort of helps to have them kind of know a little bit about what you're what you're looking for. And keep in mind that FOIA is a particular process. It's a little bit technical know how a little bit public relations persistence and a little bit like practicing practicing law. So kind of to get us started here. And you probably understand this better than me, but just so we're on the same page documents equal data. If you find, you know, a form online or something that somebody has to fill out, probably there's a data set that includes all the answers to that form. Keep in mind that most often when dealing with government agencies, the first person we're going to talk to is probably going to know less about data than you. It's a public information officer. That depends. Federal agencies are better than state agencies, but a lot of times. You need to kind of remember that the first person is probably going to going to need to talk to somebody else. So, you know, a grizzled reporter friend of mine once once told me that that. You know, being requesting data and using FOIA requests is like practicing law without a license and it really is. And the thing to think about when you're filling out forms or when you're when you're writing your requests and as you go through the process is that to document sort of everything as you go, because you never know when you may need to show that to other government officials or put it before a judge. And you want you want to have clear documentation of your interactions with the agency. So what to ask for. I mentioned already forms anything online that's filled out the database somewhere. Record retention schedules, which can kind of act as a menu for the data that. That exists in various agencies. Contracts with third party vendors. This is a thing of kind of come to realize over time that like a lot of government agencies, they contract out part of their their data work to a third party vendor. You may not know exactly what fields are in a database from that, but you can look online and find out an awful lot about how they keep their data. And then just by by searching contracts and even like this, kind of the, the, the advertising information from those, from those, those vendors. And then also obviously schema or data dictionaries. A lot of times that's good to kind of request, even before you really know what you want to request, just to kind of get a layout of the data and see what it is because it's very hard sometimes. And so, I think that's what I want to do. And then I think there's a lot of things that I think is a journalist. I know what I want. I want to know, you know, I don't know. COVID data. I want to know how many people got vaccines in a specific area or something like that, but I don't know what fields they keep. And if I get that I can figure that out pretty easy. So moving. Moving on. And then know the law that's involved. Know what foil laws apply. So for instance, you know, there's a federal foil all every state has foil laws, some local jurisdictions will have their own peculiarities on how they deal with FOIA. You really kind of want to look at the law as it's constructed in the, the jurisdiction at which you're going to be requesting information. And so for instance, Virginia in particular. If you look at just their basic law, it looks great. It looks like you're going to be able to get any data you want. But what Virginia has done over time is they have put in a lot of different exemptions for different different types of things. And a lot of them are what I would call optional exemptions. So they could say, okay, I'm going to. I as an agency can give you this data or I might decide that, you know, in this case it's not. It's not proper to give it puts it on the agency to decide. And when that happens, often what happens is that the agency decides never to release the data. And the reason is often because they make the rationale that if they release it once, they have set a president and then they're going to have to release it again. So, dig into the, to at least the foil all part of it and look through, look through it see what exemptions you're going to be running into and try to maybe, you know, work with. Sometimes you can get a lawyer to work with you or people that are more experienced and, and, and kind of see what you can work out to kind of at least hedge that off even in your initial request. So the part is to read through the state code or federal code governing your agency. That's not you don't have to read through every piece of it every time but like a lot of times you can find out. Just specific data that is kept just from that that is required to be kept in state code. So that's another good, a good way to kind of both understand the agency that you're, that you're requesting info from and then also what, what they might keep. So writing a request. This is where the practicing law thing really, really applies. You need to tailor your request to the jurisdiction that you're requesting information from again through kind of looking at. Looking through through the law and, but you can also often like keep the keep the language simple. Around the request, it depends upon what agency you're dealing with in terms of exactly how elaborately you want, you want to go. But there's there's resources out there for writing requests for local and state agencies in particular. I would always go to the student press law center letter letter generator. And what this does is it just gives you a format and you say, okay, I'm requesting information from Texas. I want, you know, you kind of write in what your actual the information that you want is. And then it puts all the legal language that that applies to that state based on its FOIA request or foil all in there for you. And then federal agencies, the national freedom of information coalition has a pretty good kind of there's no real good generator anymore. And so I think it's a really great one, but I think it's been died. But the FOIA wiki and, and the national freedom of information coalition have some pretty great, great tools to kind of help you. And then also federal agencies. A lot of times you can go to the website, you may have to fill out, you know, some basic info, but you can just, there's like a portal where you just kind of put your request in the email you a response and it's, it's sometimes much more simple, even state, state agencies. So, and this, this, this, this thing here, this is, this is from the FOIA generator I was talking about and that's just where you, you know, you've got your state there. There's a bunch of other stuff up and down but like basically you just put the information and that you're requesting. But, you know, a good request is going to kind of weigh how specific you want to get because you're, you're basically balancing being so specific that you run into privacy laws and being so broad that you run into the argument that it's going to take too long to get you the information or it's, it's too, it's too big of a request. That shouldn't be as much of an issue with data but it, it often is. So you really kind of have to weigh that based on what you want. You know, you're going to end up occasionally running into HIPAA, which is the health insurance portability and accountability act, which is a nice warm blanket, particularly for state. Government officials to wrap all of their data and and keep it from the public. So, there are obviously like valid reasons for HIPAA but it gets thrown out for literally anything that touches health care, certainly in local and state agencies. So, also this is probably like. Obvious to the people in this room but just to say it ask for a CSC or other machine readable format. If you want to like here and you know. If you want, if you want to see me start ranting in a newsroom like send me a PDF when you have a CSV file like that happens all the time. And it's, it's kind of crazy and it doesn't even like normally stop us because we can. You know, convert it but it's a, it's a common thing that happens. When you're requesting information. So eventually, if you are requesting information you are going to run into somebody like this guy. And what I mean by that is that. The first person you talk to is often a public information officer who understands data less than I do and doesn't really know what you're looking for. Federal agencies are better. Obviously they handled this all the time state agencies. I sometimes think it's like willful ignorance that they want somebody that is a wall between you and the data that you're trying to get. So what you usually end up doing. Again, hopefully you've, you've, you've already kind of asked for the information informally. And then, you know, I often would send the request and then just follow it up with the phone call and be like, Hey, can we get your data guy on the phone and like this settled this pretty quickly. And it's probably some of you in this room that I want to talk to. But that's, that's like a really. Important part of it is getting to the person who knows how the data is actually stored in caps. I had a case one time where the state police in West Virginia, I was trying to get the concealed weapons permits for the state. I just wanted to see who had concealed weapons permits. There was no law preventing us from getting that. But the state police claim that they could not give us this information and it wasn't that they didn't want to give us the information. It was that it was in a system so old that they did not know how to extract it. So, and it was literally this was like 10 years ago, but it was like from the 80s that they were had all of this on. And I went around and around with them. Another reporter got involved. We eventually had to call like the governor's press secretary and get her involved. And essentially what we had to do was get from them the software and hardware they were using. And then we took that and we put it on a listserv of journalists at investigative reporters and editors, called the nine car listserv. And somebody on there, you know, we explained what was going on was immediately outraged and wrote exactly what we needed to do to get the data from that system, which we then took back to the state police and said, here's how to do your job. And they did it. So that the and that actually never even resulted in a story in the end, but like we got the data at it. Great. And then it was not infrequently where you're like, fight for data and then like well there wasn't the story there that I thought. So after filing your request. You want to keep track of the deadlines for the agency to respond to you. Working days for federal that's around the time for for every for other agencies. And then that's really important is memorialize all of your communications with agencies about your request. I often like if I have what I would consider a bad actor. An agency that I think is just like trying to avoid giving me anything. I will, I have just refused to even get on the phone with them I want everything in writing and everything in email. And then I want to make sure that I have everything memorialized. So keep that in mind as you kind of write these and if they call you, I would immediately when the call is over. Write out an email, send it back to them, say, Hey, this is what we talked about on the phone. This is what we said if you disagree, let me know. But this is what I my recollection from the conversation. So keep your keep your correspondence altogether don't lose track of it. Put it in a separate folder either physical or digital so that you know where everything is. Now when the answer is no. The first thing you want to do is appeal. Appealing allows you it preserves your right to sue. So you don't have to like worry about what you can sue them later if you decide to. You also kind of want to study the response. Look and see if there is a privacy. Exemption, generally, they require you to show your work or to show how you're releasing information and how and how the information would shed light. On the government and that's sort of like the onus of on you to to prove that. And then you also have foreseeable harm, which is sort of on the government to determine why releasing this information may be harmful to the public individuals, whoever. And then also, you know, it's, it's not hard to find a lawyer that will like pay attention to a good for a request. The freedom of the reporters committee for freedom of the press is good with federal boys. And then the Virginia Coalition for Open Government is great in Virginia to at least help you work out like what exemptions they're citing and what what they might mean. And generally every state has some organization like that. And a thing to keep in mind is that lawyers can recoup fees from these. So it's not the hardest thing in the world to get a lawyer to take one of these cases because if they win, then they can recoup fees from the government and you don't end up paying anything. So then this is my final slide. When to super information. You know, you're again, lawyers can recoup fees. So getting them to talk about it and getting say your newsroom, which may or may not have a whole lot of resources to get to a lawyer isn't always as hard as you would think. I know people think local news and journalism is not in the greatest place and in some ways it's not but also like, it's not that hard to find to kind of pursue some of these things even even today. You know, if an agent so so when to sue is really kind of like weighing a couple of factors. And I've been involved in like, I had three of my requests end up in a boy a suit and been involved in a couple of others through through Bloomberg. And it's really kind of weighing, weighing a couple of things is the agency, you know, routinely not responding to information. I think that's what's happening. I start looking for a really good for your request that I think we can win on to sue them, because it's some point. You end up just sort of in a place with some of these agencies where they're just not. They're just not responsive to FOIA. They make the calculation that a lot of ways get filed and then nobody follows up on them. And that yes, in fact, I will follow up on this and track it down. Then that changes the equation for them. Also, it can just speed up the process, particularly with federal for is like there are agencies. We're suing one agency now that has a backlog of for is because of COVID and return of for is filed during coded, but they haven't increased their FOIA staff, and they keep making the argument that we just have so many ways we can't do anything it's going to take, you know, two years. And we want the information and we know if we sued, then that will put it in a different track. And then obviously look at the exemptions that are cited and whether or not they apply. A lot of times agencies will try to shoehorn an exemption and make it applicable to a case where it just simply doesn't happen. The, the, or it doesn't, doesn't fit. So the story that I have up there is one where actually I sued for information got the information and then five years later wrote the story about that information. So basically I wanted to find out what the, what basically where where police officers were moving in Virginia. I just wanted to see officer movement, like, you know, they're in one department, they're in another department. When did they move, why did they move, and I found out, you know, there's a training database where they kind of have to track all that because they track, you know, who's trained what officers have what training and what department. So I worked with the state agency that had this data, actually got them to, we agreed to like kind of a special, we signed a contract with them, basically saying we wouldn't use this information unless we had it from another source, but to use it as sort of a guide post. Um, signed it. And then the agency started talking to local police departments, who did not want us to get to have that information refuse to give it to us. And we took them to court and one, and a couple years later I finally got a story out of it. Actually after requesting the information again because the first time they gave it to me it was so they didn't give me like a unique identifying field, which was a lesson of my own. On doing these always asked for unique identifier. So that's pretty much my talk. Thank you all and I'll be around if you have any questions."}, {"Year": 2023, "Speaker": "Jon Schwabish", "Title": "What Not to do in Data Visualization: A Walk through the Bad DataViz Hall of Shame", "Abstract": "Prepare to be amused and enlightened as we embark on a comical journey through the quirky world of bad data visualizations. In this light-hearted talk, I\u2019ll showcase some of the most outrageous and baffling data visual blunders that have left audiences scratching their heads. From pie charts that vie you everything to bar charts that distort and mislead, you\u2019ll see it all. I mix the comical with the serious to unveil visual missteps in the data world. Amidst the 3D exploding charts, you&#39;ll also glean valuable lessons on what not to do when crafting data visualizations. Join me for a rollicking exploration of data gone wrong and leave with a smile and a newfound appreciation for the importance of clarity and accuracy in our data-driven endeavors.", "VideoURL": "https://youtu.be/KluzR75S6U0", "id0": "2023_05", "transcript": "Do we have any baseball fans here? All right. Our next speaker is an umpire in the McLean Little League. Please welcome John. Good morning everyone. Thanks for having me. Glad to be in this room. I teach at the McDonough School some data visualization here and I never get to go in the big room. I always have to be in the small classroom so I'm glad to be here. And I'm excited for you all and for us to learn about R and data visualization and all the great stuff that you can do with data viz over the next day or and half or so. But I want to talk about some of the bad stuff that we do. So this has just kind of kind of be a fun, fun 20 minutes. So just sit back and relax. You know, one of the things that we learned in data visualization is that there are great ways to communicate data to help people understand information and analysis and data. But there's also bad ways to do it. And so we can start by just recognizing what is that the first rule of data visualization is that there are no rules to data visualization, which is what a lot of us try to say but it's really not true because it really is just one rule for data visualization, which is, anybody know what the one rule is? Yeah, bar chart should start at zero. Okay, so that's really the, that's like the one rule we can all agree on. Now, like, are there other rules to data visualization? Sure, like should pie chart sum to 100% like yeah, they should sum to 100%. Is that a rule? I don't know. I just like, for me, that's just like common sense. I don't really call that a rule. But like, you know, people do stuff like this all the time. They start their bar charts other than zero and then they totally distort the data. This is some congressional candidate in Georgia, but less you think I'm going to be biased today against Republicans. Don't worry, the Democrats do the same thing. Andrew Yang did this to like 22 and a half percent is that not that much bigger than 21%. Only if you start the axis of something other than zero, so you can really, you know, distort all the data. But aside from that, the second rule of data visualization is that there are no rules to data visualization, right? Data is at its core is the intersection of art and science, right? Underlying it is data and yet we have lots of ways that we can add design and we can be creative when you communicate our data in engaging ways to help people understand information, understand analysis, engage them, help them find insights and help find discoveries. But there are also the bad stuff and the bad stuff can be subjective. We don't like certain colors. We don't like a certain layout. We don't like a certain graph. We can also be objective where people are just liars, right? And they're misinforming us and they're deceiving us and they're lying to us. And that's where our responsibility comes in because as people working in data and working in data is, we need to be the responsible party. We need to be able to come out and say, hey, this person is lying to us. Okay, so there's the fun side of this that we'll start with. And then there's a serious side of this that says, hey, we have responsibilities, people who know how to work with data and make data is to do a better job. So I am going to propose that we have a data visualization hall of shame. Okay, and the data visualization hall of shame can be a mixture of these things. And I'm going to suggest that when you get inducted into the data of this hall of shame, you get a particular award. I'm going to call it the tuckty. You're awarded a tuckty when you get into the data of this hall of shame. Not every bad visualization deserves a tuckty. Okay, so I'm going to reserve those to the really egregious ones. From my view, it's really the ones that are misleading and are distorting the data and are lying to us. But you'll see a few in here where I think just because they're so egregiously bad, they just deserve their own tuckty. Now, I'm not going to excuse myself. This is my visualization. From an article I published back in like 2010, 2009, something like that, this 3D line garbage, terribleness, right? Like just the 3D stuff is generally terrible. So so I'm not going to deduct myself, and I'm not going to give myself a tuckty, but still like we all have the skeletons in our closets. Okay, so I'm going to break these into some different categories. So first we're to start with the hall of shame of colors. I don't think I don't think you need an MFA in design to know that this is terrible. I'll also say by the way that I'll have a QR code at the end where you can grab the deck and you can go have as much fun as you want with with with these. Okay, so but you don't need an MFA in design to know this is terrible. Is this egregiously bad? Like no, someone just like picked like as many different colors and and patterns as they could find, but like it's not trying to lie to us. Now this one, this is from the National Weather Service. Okay, they should know better. Okay, this is like life and depth stuff of a major storm in New England. So I'm going to give this one a tough tea, but you know, I don't think they're trying to lie to just like not doing it well, right? They should just listen to Hadley and just do it Hadley says and then we'll all just be better off. I feel like I can say Hadley here in everybody in this room is like, yeah, okay, we got that. All right, we're good. Okay, this this is another example. A lot of great examples coming out of COVID. This is a COVID rates in Washington state, not particularly helpful when the rates go up and you don't change your color palette. So as just as a reference point, Ben Jones created this map of the same data that's in grayscale, but you can see like there is some variation here, right? And you can't really do that if you just make everything red. It doesn't really help. Okay, so the punching bag of data visualization are pie charts. So of course, everybody loves to make one of pie charts because you see garbage like this all the time. Box is a great infinite resource of bad data visualization. People love to make fun of 3D exploding pie charts. They don't sum them more than 100%. These are, of course, great. Actually, this one is a prime example, right? Like, like, yes, pie charts should sum 100%. Is that a rule or just like common sense? I don't know, you decide. But like more fundamentally than that, the labels should match the slices and the pie charts, but even more fundamentally than that, right? The answer to the question of what was the best part of the Super Bowl is not yes or no, it's just not. Okay, so more fundamentally, this is not a pie chart, but still part to whole and I just love, let's just look at what's going on in this shirt. We have 70% polyester, 30% PU, whatever that is, 56% polyester, 44% viscous, which little concerning, and then 100% polyester. So I don't know what is in this shirt, but just be careful when you put it on. This is my personal favorite pie chart, okay? And it's not because it's not because an exploding pie chart, it's not because it sums some more than 100%. It's not because it shows change over time, all of which are very strange decisions. My favorite part of this pie chart is the data source, because the data source is listed as the margin of error, which is just delightful that you would do that. Okay, here's another one, another COVID one. Now this is kind of like an exploding pie chart. This is showing different change in rates across different disease types. Notice COVID-19, the 4.12% contagion rate, it's always important to read the fine print, especially when it's put in a big red box. Let's take a look. The numbers for COVID-19 remain a rough estimate, because the long incubation period means that we still have no idea how many people have been infected. So why are we making graphs when we have no idea what's going on? So this one gets a tough thing, in my opinion. All right. So we start with pies, we can go to pairs of pies. This is a great one. This is showing, okay, so pie charts of the distribution of people in this university, male, female, international, and URM, whatever that is, but like, you know, I like the non-binary moving beyond the binary, but I still think international students do have a gender. I'll give you a second to try to understand this pair of pie charts. I'm going to give you five seconds to just try to read this pair of pie chart. So, let's take a look at the picture. Don't break your brain. Okay, if you're having trouble, I'll draw it out for you. So the orange slice on the left is the green slice on the right, the green slice on the left is the yellow slice on the right, the blue slice is matched, and then the yellow slice is the orange slice. So don't hurt yourself, I'll keep clicking, but like just pairs of pies. And then of course, if you're going to do pairs, might as well just do more and more pie charts, right? And just give everybody a time series and series of pie charts, right? So again, just pie charts are fun to make fun of because they are pie charts, but I can get into why I think you can make pie charts, but we're not going to get there. Okay, the inverted axis. So this is probably the most famous and best example of the inverted axis. So for those of you who, well, I don't know, well, some people may not have been born when this came out at this point. Like I feel that's how old I'm getting at this point. This is a fairly famous example from the South China Morning Post of using an inverted axis. So this is showing deaths in Iraq. The lighter red color are civilian deaths, the darker red color are coalition deaths from the military. And what we have is data going from 2003 to 2011. And then the positive axis is actually flipped here. So it goes from zero at the top to 1500 at the bottom. And so this is just a great graphic, right? Because you've got this kind of rounded bar chart, it's got the red color looks like dripping blood, it's bloody toll at the top. It's a great graphic from the data vis design perspective, not from the actual content, right? Just a great way to evoke emotions with data. And then someone down the hall a couple years later, from the person who created this someone down the hall created trying another similar type of visualization. And they tried to show gun deaths in Florida, right? And again, it's using this inverted axis starting in 1990 going to 2012 and zero at the top, but 1000 at the bottom. And of course, here, when you look at this, it totally changes our perspective, the data right here, when we look at this, I'll just, I'll just ask, what do you think is encoding the data when you just look at it immediately, what is encoding the data? The line, right? Yeah. So why does this not work? Right? Because it's that black, that dark black line that outlines the red area. And there's the axis at the bottom rather than at the top, all these things make us think that the data is actually the line, but it's not, right? It's actually, if you flip this whole thing around, it's actually, this is the pattern. And so these inverted axes still going from 1990, now going up from zero to 1000. These inverted axes are a great way to distort and mislead people. Now this person, of course, in the old days when Twitter was a thing, the author of this piece just got destroyed on Twitter, but she was trying, right? She was just trying something different. But then you see people trying this to misinform. This is from Pike County and Kentucky during the early couple months of the pandemic, right? Wow, COVID infections in our county are going down. Well, yeah, sure, if you flip the axis, right? But like, what the hell? It's actually going up because when you go up from zero to 40, so they updated this, if you notice, I'm going to go back, notice it's from zero to 35 because the endpoint here is June eighth. And people were like, what the hell? What are you doing? People there like the next week, they're like, Oh, yeah, it's so June 11th. They were like, Oh, yeah, we should probably like actually make this, make this true, right? Okay. So lots of ways to distort with this one gets this one definitely gets a toughy. Okay. And then there's a whole class of data visualizations that get their own space in the Hall of Shame that just don't understand how numbers work. Okay. So for this one, like, let's just all agree that 336 is not greater than 336. It's not greater than 336. And it's not greater than 336. Like, that's not how numbers work. Okay. Also, like 13% is not greater than 25%. And 28% is not zero. Like, what the hell, man? This is not how numbers work. Okay. This is, this is from the, this is from the economic times, like this is a fairly reputable thing. Now, this is this is very early. Remember, early on the days we're all concerned about is the, is the, is the, is the virus like on our, our cereal boxes, right? Like this was a thing. Remember, like wiping down cereal boxes at the beginning. So they looked at how long does it remain viable on certain services? Cardboard, the first bar there, 24 hours, plastic and stainless steel, three days. I don't know about you, but you can't put days and hours on the same axis. It's just not right. Like, this is what the bars, the bars go all the way out here, people. We can't just be making stuff up and putting things on the same axis. That one to me gets a toughy. And then there's labels, labels are just like, this is just unbelievable. Now, I don't think they're trying to mislead here. I think they just like, let's just make a map because everybody loves maps and let's just label everything on the map. I'll give this a top D only because it just hurts my like artistic sensibilities. It's just painful to look at. This one, I don't think it's a toughy, but I just, I just still can't get my head around what's going on with the y axis here. Like zero, 11, 20 to 33. You're like, okay, I get it. You're going up by 11s. That's kind of a strange to Oh, 43. Nope. Okay. 50. Okay. So I don't know what's going on in this one. Right. Okay. This is one of my also one of my favorites. We've talked about 3D a little bit already. 3D is is clearly distorts the data. Look at the certification bar. This is from the USDA Department of Agriculture loves 3D. I don't know what's going on there. Something in the water over there. They love 3D. They love clip art. Take a look at that certification bar. It's labeled 1-8-3-3. And yet no part of that bar actually touches the 1800 grid line. Right. We all know this. This is the distortion that's caused by 3D. That's not my favorite part of this graph. My favorite part of this graph is the y axis label over on the side. The y axis label is millions slash billions. So it's a choose your own adventure of administrative costs. How much do you want to spend? You've probably seen this one. This one was from a couple of months ago showing uncertainty, showing error bars is great. Let's zoom in on these error bars and see how much error is actually shown here. Oh wait. It's just the letter T. This paper is actually retracted because yeah, because of the letter T. Okay. That could certainly get a toughy. And then there was, they're just telefunny, man. I mean, if you're going to make a Venn diagram or something that even looks like a Venn diagram, like seriously, just be very careful. Right. Like you don't want to work for this company. Okay. I'll let you enjoy this one. Like we all know we should be careful with our use of icons because of how we represent people. But like if you're not showing decapitations, maybe don't decapitate your icons. Okay. This is one of my favorites. Okay. This is a real old one. All right. I'm going to show you what the segments are in a second. But first, first, I'll ask you instead of I'll just ask you, what's the first thing you observe about this? I'm going to give you the title top 10 drunkest cities in America. What's the first thing you observe here? How many cities are there? There are nine cities on the graph of the top 10 drunkest cities in America. Now let's look at what we're going to plot on this graph. So the first segment, the green segment is the bottom of the percent of excessive drinkers in each city. That makes sense. Sounds good. The red segment is the percent of alcohol involved driving deaths in each city. That makes sense. That seems like a metric. Let's get the yellow segment is the number of bars and restaurants. And if that was an enough meaning for you, let's layer on top of that the median household in Gama. So, okay. So there you go. Like the best labeled graph ever. Okay. And then there are people who are just they're just lying to us, man. And again, this is our responsibility, right? So this is from the Council of Economic Advisors under the Trump administration at the very sort of towards the beginning of the pandemic. You could see what's happening. And you could see what's happening in the number of infections around the world. And you know that the data collection is irregular at that point. And so what do they do? They fit some weird cubic model of just the time series to show. Don't worry. It's just going to go down in a couple of weeks. Don't worry about it. So these guys are just liars. They get a big red tough these far as I'm concerned for this one. I don't know who this who this person is, but the tweet was or the TikTok was if you want to know why people have lost faith in faith and capitalism, this might help. This is showing rental prices increasing by 160% between 1985 and the end of 2023 as compared to median household income, then it only increased by 40%. Yeah, sure. If you only inflate one of them, you get this totally different time series, right? But when you use the inflation adjustment on both series, they don't look that different. And thanks to Philip Pump at the post, you actually did a nice piece on this, right? So this guy gets a red talking, right? And so again, this is our responsibility. One more. This one just infuriates me. This is from the Florida standard news media organization in Florida talking about children in the Medicaid program who are receiving puberty care or gender affirming care. The article says, I'll just read this aloud from 2017 to 2021. There was an increase in children as young as 16 who received irreversible surgical procedures in the Medicaid population between 17 and 20, only three children underwent such procedures, but in 2021, 2021 alone, the number was 12 children, which represents a 1100% increase. Now, I'm happy to argue with you about whether we should provide gender affirming care to 12 children in the fourth most populous state in the country. I'm happy to go to bat about that. But more fundamentally, I don't know about you, but when I do some simple math, three to 12 is not an 1100% increase. It's a 300% increase, which is also it's 12 kids. Okay. So for me, this guy gets the biggest red tuffity. I put him on the plaque in the front of the hall of shame, because this is just lying and this is trying to harm kids. And if you don't agree with me, that's fine. I'll go to bat for that later. All right. So I'm going to wrap up here. With the last piece of the hall of shame, which is to take visualizations and to just manipulate them. And so I'm going to draw on Sharpie gate. Because Sharpie gate, as we all know, is where you just take a Sharpie marker and you just, you know, you just draw on some stuff, right? And you just take that drawing and you just make up whatever you want. You just start drawing on things. And so, you know, Sharpie gate, of course, was when Trump drew on the map of the hurricane, right? But I just want you to know that this was not the first or the only time that someone has taken a graph and taken a Sharpie marker to it. Okay. It's not the first time. I mean, Napoleon, Napoleon tried to make clear that his army wasn't so decimated in the war with Russia. Hillary Clinton tried to undo or not undo, but tried to make the map look not so blue just by taking a Sharpie marker to it. Playfair tried to make the balance, the trade balance not look so, you know, so negative just by taking a Sharpie marker to it. And remember that one guy in the snow map who actually died, but he wasn't near the Broad Street pump because I don't know if you know this story, but the dude up there who died not near the Broad Street pump, he liked the flavor of the water and the Broad Street pump. So he had his house servant go down and get that water. And that's how he got collar. And that's how he died. His family was like, Hey, there were a whole bunch of people over here who died too. So just like the Sharpie marker, you could just do that. So there are lots of ways to distort the data. And I would just encourage you to be careful. It is our responsibilities, people who are working with data, people who are visualizing data is our responsibility in particular to call out those that are misleading that are lying and that are providing disinformation to folks. So QR code, if you want the slides and all the links, thanks so much. Enjoy the rest of the conference."}, {"Year": 2023, "Speaker": "Abigail Haddad", "Title": "What Job Is This, Anyway?: Using LLMs to Classify USAJobs Data Scientist Listings", "Abstract": "Navigating the federal job market begins with finding appropriate job listings. But for data professionals, discrepancies often arise between the content of the listing - that is, the duties of the job - and either the job title or the occupational code, making this step more difficult. In this presentation, I discuss using a Large Language Model (LLM) to generate new job titles for listings in occupational code 1560, Data Science. I'll show examples of listings with mismatches between the official job title and the one generated by GPT-3.5 and discuss the potential uses of this for applicants and agencies. I'll also highlight the advantages of using Marvin, a library that lets you use LLMs to solve Natural Language Processing problems by just writing documentation rather than code.", "VideoURL": "https://youtu.be/Wu4vtPMBKLM", "id0": "2023_06", "transcript": "Our next speaker, her last mile transportation is a non-electric scooter. Everyone, please welcome Abigail. So the talk I'm going to give today is fairly technical. We're going to talk about classification, about classifying text. We're going to talk about language models, and I'm going to try to get you to use a library called Mervin for if you are doing certain types of natural language processing tasks with LMs. But really, why I'm here today and why I'm talking to you and this group about this is that I've now been around the federal government for 17 years now. And I've been a job seeker and I've also been on the other end of it, so trying to hire, trying to bring people in, trying to bring technical people in. There's a lap that's really non-optimal about a federal hiring process. So if you apply to use a job, there are a lot of issues. And again, having experiences from both sides, there's a lot that's broken that's really not a technical issue. We make laws in certain ways, there's policies and regulations. There is really dispersed authorities where in order to change something, you really have to talk to every single person doing hiring. We can't fix any of that today. Okay, like that, that is all that is all off the table. There's a very small piece that I want to talk to you about that I think really is a data problem. And that is making our job announcements more legible. So there's a bunch of text, it describes things, we'll sort of go through that. And what I'm going to talk about is how we can use text analysis, specifically LMs, to generate more accurate job titles that really describe the duties that the agency wrote in the USA job description. Oh, no. Oh my goodness, guys, this is like embarrassing. I clearly if not, I'm not up to date. Okay. So, okay. So I've had this experience, you may have had this experience, you go to USA jobs, you search for something, I search for data scientists, and they get a whole variety of jobs. They're not really what I'm looking for. Okay. And we can argue about like, what is data science? And that can be really theoretical and use something I see something else. But the important part here is it's a problem when people looking for jobs who aren't able to find the actual kinds of jobs they're looking for. Okay. So I again, I look for data science and I find jobs that are more data analysts or this intelligence analyst. They're looking for somebody to look into work in Tableau all day, or they're software developer jobs that are the skills for, or they're really leadership jobs, which are really interesting in some ways, but not really what I'm looking for. And so that's sort of the first barrier. The first way that people interact with this website and interact with federal hiring is they look for a job and if we make it hard for them, then we're really erecting barriers that don't need to be there. Okay. So we're going to talk about test classification and how we can use large language models. And a big piece of this is helping job seekers, right? But another piece of it is internally, there's all these analytics we want to do. Okay. So we want to know things like time to hire. We might want to know things like which types of jobs it's actually hard to hire for. You can announce them and it winds up being failed. So you don't actually hire somebody. And so the more information we actually have about how to classify these jobs, the better we're in a position to actually analyze them internally. Okay. So this is what I want to do, right? I want to go to chat to the T. I want to say like, just show me like actual data science job postings. And that's not exactly what we do here. We work with APIs, but essentially like, this is what I want to do is like, hey, we have this tool is really good at text analysis. Can I just get it to do it for me? Okay. So quick detour, we're going to talk a little about classification problems. Okay. So this is this is the really standard way we sort of explain this is with ham and spam. So when you get an email to your inbox, behind the scenes, your email client is doing some work. And what it's doing is trying to figure out, is this something I should actually send your inbox or just something that's spam? And it's doing that with a lot of features. But one of those sets of features is just the text and email. So based on previous emails that have been sent, based on the kind of kinds of feedback you give. So like every time you report spam, Gmail, it updates this model. But this is just a really basic, you know, two category classification model. And ideally, we want to do some assessments and I'll talk a little about that. So this is actually like, this is a really, this would be a really bad model. So we have four, just having nine plus four, 70 divided by all of this. So it's about 94% are correctly getting classified. For spam, that would be really bad. There's other kinds of problems that are harder. That would actually be a really good model. So ideally, at the end of any classification problem, we would have something like this, or we would actually be able to assess how well our model is doing. So that's USA jobs listening. You may be familiar with this. It's a jobs listening. You can see there's a lot of information to start with, like there's a summary, you can see the agency and the salary, and then you can see this duty section. And in the next slide, we'll look at more data that one of what one of those looks like. And that's the text I'm working with for the actual classification. So this is about to be this audience participation part. When we look at this, this is an example of duty's text section. And we can see there's a bunch of pipeline and your constructing things, your automating things, your eliminating manual participation, all those kind of stuff, your maintaining databases. What is this job guys? Give me throughout something. What would you call this job? Data engineer, thank you Will. And I know Will, I did not actually tell him to say that ahead of time. He really just got it right. This is a data engineer job. This is a classic data engineer job. Okay, it's officially labeled scientist. And this is bad for a couple reasons. This is bad because I look for data scientists and it wastes a little my time. But it's really bad because if you're an actual data engineer, we want you to be able to look up data engineer and find this position. Also, this position is actually misclassified. Okay, so we'll talk a little about this. There are these four digit occupational codes, which are pretty clearly defined by the Office of Personnel Management. And data engineer jobs are really not in 1560, which is the data science position. They're really, they're 2210s. This is or they're 1560s. There's something else. I don't really care about this as much. But this is something that the photograph kind of does care about a little bit. And this is another thing that you could use this model for would be detecting which positions are misclassified internal. Okay, we can't overcome text ambiguity problems. If you have basically no information in your duties section, then my model isn't going to be any better at classifying it than a personal reading it. So to sort of again, there's problems being fixed with hiring, as far as they can't fix with hiring. If this is your duty section, I don't know what my doesn't really matter. My model is going to say it's not going to be very good, right? I have no idea looking at this job, what it actually is. Okay, so my kind of minimum of viral projects, workflow, I pulled a bunch of data scientists job listing. So by data scientists, we mean it is categorized in occupational code 1560. There's a fairly new data scientist like patient code. It doesn't mean that title is data scientist, but it means it's broadly supposed to be a data science job. So I pulled those from the API, there's these great two USA jobs API's that you should use if you're interested in this. One of his current jobs, one of his previous jobs, his old jobs. This is basically 2022 2022 to present. I dropped a handful of them that had no duty list or said go to this other website to find the duties that's not doing that. And then I used a library called Marvin to get structured output from GPT. So really, all I wanted back was a job title. I didn't want an explanation. I wanted a job title. And so with Marvin, I was able to get that really easily. And then I had some analysis and iteration. Okay, so I'm in our conference, I'm telling you why you should use a Python library. I recognize this is slightly problematic. But why I think you should use this is because you don't actually have to write any Python code. Okay, this is not like nine lines code, none of it's really Python. There's one line to call it. And the real work that's happening here is actually this is doc string. So it's this given duties generate a specific job title based on the content, not based on any titles contained in the text. And this is the comment, but this library is actually going to use that and how it creates the prompt to send to GPT. That's really the work that's happening here in the rest of the classes here from this only matter. But this is the library that does it's no other library. And it's pretty easily is pretty easy to call Python code from R. And again, like nine lines, like definitely worth it. It means I don't have to write pumps that say things like, please only give me the job title and all cats, I got them to any of that. I know what I'm going to get back is just a really clean job title. Okay, so what do we actually get from this? We can get a view of the variety of roles that are getting advertised as 1560s. We can get a prototype job like this is a wrap up once again, though we can sort of give to somebody like this is how you can search. We can get a way to highlight jobs that might be labeled challenging to hire for or misclassified. I'll talk a little about that. But for instance, if my job labeling system says like this is a chief data science officer and you're hiring you know, just 13 like that could be really hard. So we can flag some things that way. And again, a variable that might be predictive for research and analysis. This isn't like this is the answer. It's not like chat, she could use brilliant, it gives us the best answer, but it gives us an answer. I think that answer can be really useful. Okay, so I'll show you some graphs. We're going to look at overall difference in top titles between my classifier and the official job titles. We'll look at the that's in job titles that never appear in the official job listings, but do appear in mine and we'll do some spot checks. So here we go. Comparing the top 10 job titles of my LLM. So that's on the left, the things that the GPT generated versus the official titles. So the big thing you'll see is that data analyst is number two on the titles that my model generated and it doesn't appear at all top 10. And those are really the jobs that are like you're working Excel, you know, maybe you're making dashboards, and maybe those jobs actually do involve coding. Maybe the duties section is not accurate. But if again, if this is what the model is seeing, this is what we can expect our applicants to be seen also. And you can see here again, also there's that she data officer, she gets scientists. Those are jobs that aren't necessarily labeled that way and I'm not actually called that. But the scope of the duties there is pretty big, like these look like leadership positions, which first of all is interesting because that's appealing to people. People maybe want those jobs and so if we're not advertising that way, we're missing out, but also the possible mismatch between grade or salary and between the scope of duties. Okay, so these are job titles which appear in the LLM generated titles, but never appear in the official job titles. So some of these are not that useful. Some of these are things somebody might want to search for. So like geospatial analyst, and I'm a geospatial person, I probably like to be able to look for that. If I'm again data engineer appears there software engineer, if it's really a software engineer job, it's probably misclassified, but it might be interdisciplinary, it might be listed under both. So yeah, some of these are just genuinely interesting job titles that contain information and that somebody might really want to look for. Okay, so some big title discrepancies. This is titled data scientist, but my model set was data analyst. And you can see like it mentions macros, mentions pivot tables, it doesn't really mention programming. Okay, so that's in this match. Technical program manager, this looks really interesting. Like I think a lot of people would really be interested in this job. If I had to go leadership, you're probably in reports, it's not really a data scientist job. Okay, but what's better than ad hoc assessment is systematic assessment, right? Like I can show you like this one worked out well, I liked this, but really what we want to be doing here ideally is actually getting something like what I showed you for that confusion matrix earlier, what we want to be saying is how well does this actually do relative to the current titling system. Okay, so how this would work if we actually wanted to do this is ideally we would get somebody with expert knowledge to look at these duty sections, and then they would label them and they would say this looks like this kind of job. And then it would be a little complicated how we could actually compare because what we probably want to do is some kind of comparison of semantic meaning. Like there's so many job titles here that like realistically maybe data engineers straightforward, but what makes something a data science specialist job versus a data science manager job. So we'd have to do some work, we have to make some other choices, but ideally that's really where this would go is we do some real systematic assessments of what these jobs actually contain. And I'll say breathe because I do a few more minutes. This isn't exactly I'm calling this a classification problem. It's not exactly a classification problem. So typically for classification problems, we have a list of preset categories. So like for instance, ham and spam, there can be more than two. This is more it's not like a generative task. So, and that's not something really pre-LOMs. You wouldn't really have had a task exactly like this, but you could have done a few things either you would have had multi-class classification, where I could have written out 10 different job titles and it was sort of into buckets, or I could have done something more unstructured like I could have clustered my job as by how similar they were. I'm not really doing either of those. So it's not exactly classification. We could make it into a more standard classification problem where we could write 10 different job titles and then other and we get the LOM to do that. I tried it. It didn't really work very well. I think part of what's valuable about this is seeing the real range of jobs. And so it's being able to say this is a geospatial job, even if there's only two of them. We still want to be able to do that. And we're not really going to come with categories ahead of time. And that also makes doing those systematic assessment challenging, but it's still doable and I think it's still ultimately worth it. Okay, so I tried a lot of other things too. I tried names entity recognition. So names entity recognition is another classic natural language processing problem. And so what I was interested in was that where the names of software tools and programming languages in each of these, because I think that gives you a lot of information about a job listing. Like if the job listing says HABLO Power BI, Python, R, Stata, C++, I wish I were making this up, I am not. Then this is a job listing is pretty confused about what they're looking for. If it says Python, R, Docker, Git, like cool. Okay, like we're good here. And if it says again, you know, if it says sort of excel power BI tableau, that's probably a data analyst job. And so they probably do know what they're looking for. It's just it's not really a data science job. So I think that that's really another way of looking at job listings. And it works pretty well. So again, Marvin really helps with that. I was able to write a prompts, which asked specifically for tools and software and it gave me a really nice dictionary. So it gave me just a really, really nice format. Anyway, that I struggled more to get by myself. So that worked well. Grouping by category. So saying, tell me if this is a management job, a data analyst job, again, trying to do a more standard classification problem with preset categories, isn't work as well. I think maybe there's some promise there. I couldn't really get it to work. And then comparing actual titles and generated titles. So like, tell me, is the title you generated, you know, the LN made, how different is that from the actual official job listing that just really didn't work. It just, I think maybe it needed more context about what I meant by different. I wasn't able to get it. Okay, so what's next? My hope is, I know a lot of folks here look for the federal government, like you're interested in hiring, or if you're a job seeker who is interested in doing just some of this analytically, my stuff is on GitHub. So if you go to my GitHub profile, then like the first thing is you can see the slides and the code that generated all of this. So if you want to use this in some capacity, like I'd love to talk to you, like this is very doable. It's very easy to get started. Like we really can go very quickly from your data to some answers, just like immediately. Also, if you have a text classification problem, I think you should, or for that matter, another type of NLP problem. I think you should think about large language models generally as a way of going again really quickly from your problem to some data to some answers. And then you can again assess, like it might work, it might not, but it's so fast to get started. And LLMs are this sort of like pre-trained and everything models where there's so much data they've been trained on that they might be good at your problem. So LLMs generally and also Marvin specifically, so that you don't have to fight with the LLM to be like, give me a dictionary, please, that is outputted in this way, please just give me this, right? With Marvin, you're not fighting with the LLM to do that. Okay, and yeah, that's all. Come over with me afterwards for questions, or if you want to use my code. Thank you."}, {"Year": 2023, "Speaker": "Jared Lander", "Title": "Mapping Big Data", "Abstract": "Maps are one of the best forms of data visualization that readily understood while conveying a considerable amount of information. With the modern web, interactive, pannable, zoomable maps---known as slippy maps---have become the norm. Thanks to packages like {leaflet} it has never been easier to generate these maps. However, they don't scale well out of the box. We'll look at different methods for dealing with large data to make high performance maps.", "VideoURL": "https://youtu.be/ZZU53Tl8xms", "id0": "2023_07", "transcript": "So I'm here to introduce the next speaker. And typically I will try to find a bunch of embarrassing things to say about him and make fun of him and whatever, because that's what we usually do with one another because we've been close friends. But I won't. I just want to introduce him as a really swell guy. Although I will say that he has made me do some really bizarre things for this conference and for the one in New York. But I will not go into details. But seriously, I love participating in this. I, you know, I asked Jared to, I've been going to the art conference in New York for a long time and having run the meet up here for a while. I'm like, let's do this in DC. So, I guess, you know, I kind of nudged them and nudged them until he gave up one day and said, alright, fine Mark, leave me alone. We'll do this. So really, it just gives me great pleasure to introduce my good friend and host for this event, Jared later. Alright. So, today, we're going to be talking about making big maps. And by that I mean maps with a lot of information in them, not just big maps you can put on the wall. And then later, you may remember me from previous talks, such as using our for GIS and mapping. Now this talk I gave was both about making maps and about big computations and I love doing the big computations. I'm not right. Oh no, I'm sorry to hopefully hope that worked in the zoom that's so much better. Thank you. Hope that picked up on the zoom. Should I start over. So, I love doing big computations. I love taking a six hour geospatial query making it run in three seconds. That's great. But today we're going to be talking about making maps in that previous talk. I did talk about making maps, including how to triangulate solutions in world. So, this is not wordle. It's worldle. I can't pronounce it. And basically, it is the map version of wordle where they show you an outline of a country and you have six guesses to figure out what country it is. And one time, they gave me some uninhabited island in the middle of the South Pacific Ocean. And I did use triangulation for my previous guesses in order to find Boovet Island, which I never heard of. So today's focus will be on maps. I am a big fan of maps, particularly, TalaMee's world map. I mean, that's a great map. This is based on TalaMee's description of the world. And it's not accurate at all. But we're going to be talking about mapping large data sets, taking a lot of data and displaying it on a map, which is particularly important for a number of my clients. So we are going to be looking today at leaflet. We're not going to be looking at static maps. We're just going to look at leaflet. They're web-based, generated in JavaScript, so they're interactive. They're zoomable. They're pan-able. And the technical term for that is slippy. It's a slippy map. So big is relative. What is big? I mean, big 10 years ago is not big today. Big for a laptop is not big for a server. So before we even get to maps, let's talk about files. And let's talk about file sizes. And for this section, we're just going to talk about point data dots on a map. That's all we're going to look at right now. Okay. So here's a data set. I loaded it up. And I saved as an SF object. And this only has 325,000 rows. That's not really big by any measure for regular data. Let's see how it looks in different file formats, some common geospatial formats. The easiest is a CSV, one record per row. And it's nicely laid out. You have all your information, all your other columns. You have a column for latitude and a column for longitude. And yes, I'm focusing just on latitude and longitude data. It's very simple, but it's really only good for point data. Yes, you could jam a polygon in there, but we're not getting into that. Perhaps more common in the geospatial realm is geo JSON data. It's JSON, but for geo. It's very verbose. Every column name, which are now fields is repeated for every record. So you're getting a lot of extra information there, whatever tough people call a chart junk, but it's data junk. A lot in there. So it's very verbose. Similarly, we have KML, which looks a lot like XML, which is also verbose, a lot of repetition, a lot of a lot of junk in there. And I'm not even going to show you shape files. They're not real files. It's like five files and you just read in one file, but all the other four come with it. It's crazy. So let's look at how big are these files. You can see the CSV is a smallest at 35 megabytes. Right. Geo JSON is 107 megabytes and KML is 144 megabytes for 300,000 rows of data. These are incredibly huge. So the CSV is the smallest, but it's only good for point data. Geo JSON is better for other data, but it's much bigger. And this is not large data by regular standards. But there's a new, there's a new file format. There's a new one that's going to come to our rescue and be better than all of these, hopefully. Geo Parquet. It's Parquet, but for geo. If you don't know Parquet, it is a columnar data format that saves the disk. It's compressed. It's fast. And it's related to arrow, like geo arrow also. Now arrow is in memory. Parquet is on disk. And when you look at this, the size of this file is 16 megabytes. It's half the size of CSV, a tenth the size of KML. And it's much faster to read and write also. So that's really nice. But how do we create these files? We need to somehow make these files unless they're just magically handed to you from the census website, which doesn't come in Parquet. Maybe it does now. And if it's easy with SF, you could use right SF to write most of them. You can write to CSV, you can write to GeoJSON, you can write to KML, you can write to shape files, which I'm not going to do. And if you want to use Parquet, you could use the geo arrow package, which is not on cran yet. And you could use right Parquet from that package and will write to a special geo Parquet file. If you're stuck in the command line, you can use OGR to OGR or GDAW to convert between different formats. So, today we're supposed to be talking about putting this big data in these files into maps. So we're going to start with points on a map. I just want to draw dots on the map. That's all I want to do right now. So we're going to use leaflet. And we are going to call leaflet, which is like GigiPOD initiates the map. Then we're going to say add tiles to put these background tiles on there, then add circles to actually put each lat long combination. And yes, you do need to think is it lat long or is it long lat, and whichever way you do it is wrong. GIS cannot agree which way it goes. Now see here, I sampled the data for just about 4000 rows, because of regular leaflet. Once you get past about 4000 points on the map, you're pretty much hitting its limits, give or take, depending on your machine. And doesn't matter the machine that generates it is the machine that loads it. Because all those 4000 points have to be written into an HTML file as SVG, which is essentially like XML. And then each point has to be rendered on the screen. So we pretty much maxed out and 4000 points isn't big. So we're going to leave points behind for a minute. We'll come back to points later and see ways to deal with it. Because first I want to talk about polygons. These are shapes with area. So easy polygons to think about is the outline of country borders. You think that's easy getting all the different countries as a data set, knowing where the borders are. It's not easy to get clean data. It's really hard. And in fact this data set from our natural earth has 177 rows. But the UN says there's 190 plus countries. What happened to them here? I don't know. That's not the point of today's talk. We just want to plot them. So I'm going to just plot the first 50 maps. First 50 countries. That's because even though there's only 50 polygons, when you're plotting it, you don't have 50 polygons. You have every single vertice in that polygon. Vertice vertex. If every single vertex in that polygon. So for just 50 countries for this. So so resolution map that's over 4,700 vertices. So that's 4,700 dots and needs to draw connect the lines and fill in the area. That's hard. If we wanted to do all 177 countries represented, that would be over 10,000 points at needs to draw. And we already know that leaflet is not going to handle that. Okay. Now of course with coastlines, you have the coastline paradox, which I don't think is really a paradox. It's not paradoxical. The more you zoom in, the more curvy you can get. All right. So base leaflet is not going to cut it. Fortunately, there are leaflet extensions like leaf geo. Leaf geo uses web geo to draw the maps. Web geo is a high speed JavaScript plotting library. It can plot data. It can plot network graphs. It can make bar charts. It can make scatter plots, whatever. There's a number of packages for working with web geo. But specifically, we will use leaf geo to make our maps. And we are going to return to the points. Here, instead of saying add circles, we are going to say add geo points. And now on here, I'm plotting 250,000 dots on the map in an interactive map. This side show, if you come here, you can zoom in, you can click on points, you can pan around with 250,000 points being displayed. Now I did 250,000 points because I'm cramming a bunch of maps into the self-contained slide show. If I was just making one map, I could definitely get to over 400,000 points, which is a big improvement over base leaflet. It renders really quickly. But now the thing is, you still have to encode 400,000 points into the HTML file, still being written in. And that can really get big. I was working on a project for a client. I made 10 different maps using leaf GL, each one showing a few hundred thousand points. The HTML file was 1.5 gigabytes. It got a little out of hand. So, you got to think about what you're doing. We'll come back to points in a little bit. We'll see more ways to deal with points. But I want to go revisit polygons using leaf GL. Once again, instead of doing add polygons, I'm doing add GL polygons. Now, a little caveat where you have add polygons from leaflet, it can handle multi polygons, which are multiple polygons on a single record. Leaf GL can only handle regular polygons. So you need to cast your multi polygon into a regular polygon, which just breaks it up into multiple rows, and then call leaf GL. And you can see now I'm plotting all 177 rows of country data with no problem. Okay. It was New Zealand on the map. Yes, it is. Good. I always want to make sure for that. Now, another nice feature about using GL polygons is that it's so much easier to color code. With regular leaflet, I need to get a color ramp and then figure out which countries map to which colors with leaf GL, I just say fill color equals some column name, and I put the column name in quotes or if it's all day, and it's done. So that's nice, simple, easy. Now with leaf GL, it's harder to make a legend in leaflet, but making legends in leaflet is hard anyway, so how much harder is it? Now, I promised you would come back to the points data. I want to instead of showing the points data in raw format, I want to summarize them. To do that, I'm going to use hex bins. I'm going to break up the world into little hexagons and count how many points falls into each hexagon and color code, according to that point. Thankfully, this is easy using a package called H3. H3 is an open source piece of software written by Uber. And what it does, it breaks the world into a bunch of hexagons. I'm going to use a package that I have 15 different levels of zoom. The more you zoomed in, the smaller the hexagon is, when you're zoomed out, the bigger the hexagon is. There are multiple packages in R for interacting with H3. I'm using H3 JSR, which calls the JavaScript bindings for H3 through the V8 package. Then what I do is I use point to sell to convert any geo point, any lat long, into a deterministic hexagon on the or for a given level of zoom. Then I use regular d plier to count the number of points which fall in each of these hexagons. Then I want to convert these hexagons back into a plotable polygon. So I say sell to polygon and then I have polygons again, which I could go use in my maps. So nice simple calculation, a few lines of code does the counting for me. I know how many points occurred inside of each polygon. Then you make a map, all I'm changing now is the dataset to use add gl polygons. I give it my hexagons and then I color code based on the log scale. That's so easy because instead of putting the column in quotes, I do till day log of the column and it will take the log scale of my data and make it nice and easy to plot. We've essentially made a heat map on a map. That's pretty cool. Great way to visualize. Now another idea that we kicked around was showing different types of objects at different zoom levels. Maybe we want to have the summarized hexagons when you're zoomed out. And you want to have points when you're zoomed in. So you make your map, you start with leaflet, you add your tiles, you add your polygons, you add your points. Then you use group options to say this layer, this group, the polygon group for zoom levels one through three, and the points group for zoom levels four through 15. So when you zoomed out it shows the polygons, when you zoomed in it shows the points. Right now I'm not showing you the map yet I'm just saving it to an object. On the next page, we'll look at it at zoom level three and a zoom level four. The only difference between these two maps that use the map code I just created. One is zoomed in at level three, one is at level four, and between them it switches the points. Now I'm showing them side by side in reality, actually when you come to this talk later you could actually zoom in on one and watch it change. That's really great right thing okay well that only shows data it's not going to blow things up. But it does. Because even though you're only rendering one type of geometry at a time, you still have to write all the data to the HTML. So now I've written all of my 300,000 points the HTML, and I've written all 17,000 polygons, each of which has six vertices. So I've still blown up my map and in fact for this talk I'm showing the map twice so I've done that all twice. Right, this presentation is pretty large. Remember that 1.5 gigabyte HTML file you don't want to do that to yourself. So this was like the state of the art for a while but okay let's use shiny to stream in data. But then I was directed to something called PM tiles by Tim Applehans. I've done well I asked for a lot of Twitter driven development. I would tweet at Tim he would make a new feature I tweeted him you make new features. It's awesome. He's really helped out a lot. I've dreamt of only streaming points to the map that are in the viewport that would be visible on the map. So whatever you're zoomed in on just show that data. But how do you get the data to the viewport in the first place. Hell, how do you even know which points fall in the viewport I was thinking like we'll talk to a database and do like a geospatial join around the bounding box. It was bad and how do you even prevent it from choking when you're zoomed out when you'd be showing everything. Luckily that's all solved. So first you need to create a special TM files a PM tiles file to do that use to be canoe. This can switch between different file formats and I'm doing here I'm calling to be canoe dash Z 15 says allow it to zoom up to level 15. And cluster points at a higher zoom. So that means if I'm zoomed out. If I'm zoomed in let's start there might be 50 points at a certain zoom level when you zoom out there are 10 points and you zoom out further there are three points they're like an amalgamation of the lower points. I'm writing it to locations.pm tiles and I'm using my locations full.geo Jason as the spots layer. And you can have multiple layers in this one pm tiles file. Next, you need to serve the file. And you can use an S3 bucket like AWS or Azure or digital ocean or back place. Or you can run your own file server like engine X. The thing is. You need to allow cross origin resource sharing course. And according to the manual the documentation it's easy to do. I have not found it easy. Not at all. And what this means is you need to allow a file being served up by a web server on one domain to be viewed by another web server, maybe coming from your laptop, maybe coming from like the serving server. And it's not easy to do. So pm tiles is really cool. Now, normally what I would do is I package a web server like engine X and a Docker container and use that, but I couldn't do that for this talk. So I tried using Amazon, but it wasn't easy to do. So I'm using a pre can data set. I'm actually using the example from the documentation showing you breweries in Germany. And you do this by using the leaf m package leaf em. And in there, there's a function called add pm points. But it's not on crann is the development version from github. Okay. And you do that. And then you pass it the URL for the pm tiles, which could be an s3 bucket or a web server. And it plots all the points that you can see. Now, if we were to zoom in on this map. As you zoom in, these black dots break apart into more dots and as you zoom in it breaks into more. So any given level, it's streaming in from the web server, just a certain number of points based on what's visible if I were to pan to the left, and only certain part of Germany was visible. So I'm using the red dots. It's streaming in data as it's being used live. This is it. This is the Holy Grail for me. As long as I can spin up a web server adjacent to my web server, my, my map server. It's there. It's like having your own tile server for vector tiles. It's really fantastic. So now that we've had our ultimate goal, what are the key takeaways, where everything we've done. So now matters. CSV is only good for point data. And it's the smallest. But geo Json is way more common, but it's huge. But geo Parquet is actually I said, Zsv smaller. CSV is the smallest of the common formats geo Parquet is even smaller than CSV and it's faster and it's easier, but it's not plain text. It's a binary files. You want to open up an a text editor. Good luck. So I'm never going to talk about shape files. For the most part, you want to use leaf geo beyond a few thousand points leaf that were freak out. So you're going to use leaf geo to utilize web geo is the back end. You're going to use functions like agile points or agile polygons. And this allows you to plot hundreds of thousands of points. So this is pretty much your panacea. This is what you're good with. So you want to do summarization. Been your data. Use H3. It creates hexagons for you. These are deterministic at level three zoom. These are hexagons all pat out across the earth. You don't need to make them up as you go. They are done. So they are fast to compute. It's like a lookup table. My, I have this lat long. It doesn't look up to H3. It's amazing. So thanks uber for that. But if you have really big data, you're going to use PM tiles only loads of points that you need serves them remotely. It's pretty fast. Now it's relatively new, but there's already support and leaf M and you can write your own custom JavaScript if you really need to PM tiles for big data is the way to go. So thank you very much."}, {"Year": 2023, "Speaker": "Soubhik Barari", "Title": "LocalView: Scaling up the Analytics of Local Politics with R", "Abstract": "Shobek, a computational social scientist at the National Opinion Research Center, presents \"Local View,\" a project mapping local policy-making activities across the U.S. through public meeting databases. He argues that while national politics dominate headlines, local politics in venues like town halls significantly impact issues such as COVID policy enforcement, election administration, gun violence, and LGBTQ rights. \"Local View\" utilizes live-streamed public meetings to gather data, which is then analyzed for trends in political discourse and correlated with local demographics and events. The project, which uses a combination of Python, R, and various analytical tools, provides insights into how political topics are discussed at the local level, revealing differences in governance styles between Democratic and Republican areas. The tool is accessible to the public for exploration and research, available on localview.net.", "VideoURL": "https://youtu.be/PHUAQ4pyRmY", "id0": "2023_08", "transcript": "Our next speaker, in addition to being a data scientist, has been in a band for 10 years and is about to release his sixth full-length EP. Please welcome Shobek. Thanks so much, Jared, for the introduction. So like Jared said, let's see if I can advance these. Okay. Yeah, so like Jared said, my name is Shobek. It's a fun way to pronunciation guide because you need to remember a little bit about me. So I'm a computational social scientist. My day job, I work at the National Opinion Research Center, one of the oldest social research organizations in the country. And I get to work on fun problems at the intersection of data science and public opinion. We work with partner organizations ranging from CNN to the CDC. Outside of my day job, I'm a political scientist by training and I do a lot of research and writing on politics beyond what I do. And today I'm gonna be telling you about one particular project in that portfolio of things. I've been thinking about, and just to set the stage for what I'm gonna be talking to you about today, American politics, when we think about American politics, we tend to think of the things that are happening in the four square mile radius outside of this auditorium, right? Like the big political showdowns in Congress, deal making in the White House. But I wanna make the case to you that much more of American politics is happening in your town hall. Okay. And not just in your town hall, but in particular in public meetings that are hosted at your town hall by your local elected officials. Public meetings are perhaps one of the only venues in our political system where you, the constituents of politics, can directly speak to your elected officials and shape policy with your elected officials, right? It's not just empty dialogue, right? For instance, speaking of the CDC, so we know that obviously the CDC was providing a lot of the policy guidance around dealing with the COVID pandemic. But the implementation and the enforcement of that policy was very much happening at the local levels, often resulting in sort of contentious politics, protests, complaints and backlash from constituents. This is from a public meeting in Boston during COVID times. We know that election administration and kind of claims about election administration were a big part of the kind of policy discourse around the 2020 elections. But election administration, again, happens mostly at the county and local levels. And that's where these claims are mostly being made and also resulting in one of my favorite public comments of all time. So this gentleman here showed up to his town meeting to oppose the renewal of a contract with the Dominion Voting Systems, the kind of systems that were in contention. And he like ran way past his time limit. And when he was told that he was out of time, he said to the local official, no, you are out of time, young man. So I'm very thankful for this issue, for that. And sort of a similar thing with other issues like gun violence and LGBTQ rights. We think of these as national political issues and indeed they are on many dimensions. But with the partisan gridlock that we're experiencing in Congress, local politics is increasingly the venue for a lot of these issues. On gun violence, there are actually a lot of local policy levers restricting gun vendor permits and licensees on transgender rights, for example. There are cities and towns that are doing things like banning conversion therapy. So there are a lot of things that can be done at the local level and you as a resident can show up and actually make these things happen. Okay. So the issue is that you're hearing less about it. And a reason for this is that you probably know what it is. The local news ecosystems in our country are facing a lot of challenges from consolidation to understaffing, to underfunding. Once we're able to reliably hear about what was happening in your town hall, that is less and less true now. And that's really the prime motivation for what I'm gonna be talking to you about today which is this project called local view. And what it is, it's a database of public meetings to actually map and literally map and track this policy making activity at the local level in the US. And so we're able to look at all of these different places, all of these different localities in the US and we're able to actually literally look at the speech in public meetings across all of these places over time. And we can see things like climate change, you know, religious freedom, business ownership, you know, LGBTQ rights, gun violence, all of these things come up in public meetings and we're able to track who talks about them, how they talk about them and we're able to map this over time. You don't just have to take my word for it because as of this morning, you can actually play with this data yourself if you go to localview.net. You can not only play with this data, but you can do things like run correlations with key things like population size, household income and you can download this data as well. And of course you can track trends over time if you'll see here you can track things like mentions of COVID over time. So I really encourage you to do that. So to kind of go behind the scenes here of how we built this thing. And so we being kind of my collaborator on this project Tyler Simcoe, he's a political scientist, he's awesome, you should check out his work. And if you want more details about what I'm about to show you, we've actually published this database in Nature Scientific Data. You can kind of read more details about what I'm about to show you. But there's kind of five steps to this process of how we built this database. So we start with the universe, right? We start with a sampling frame of all municipalities in the US, which we get from the census. And really the whole reason that this is possible is that many municipalities, if not most municipalities actually live stream their meetings on YouTube. And so YouTube is the main source of these meetings. And we go through all of these places and we create these queries of the place name and all the possible, useful governments that could belong in that place. And we actually went through, we had this horrible Excel spreadsheet that we went through and actually manually mapped places to their YouTube channels. And once we did that, we were able to actually go through and download the videos for all the public meetings from those channels. The transcripts, which YouTube often does automatically, but also like a lot of city quirks will have the unfortunate job of having to transcribe the meetings in real time. And we're also able to get things like metadata, right? Like you know what this is, right? It's like the description of the video. There's also descriptions for the channel. These are all very useful things to know. And they're useful to know to help us identify the place and identify the type of government, which is important, and also identify when the meeting took place. They are not all, you know, the date of the video is uploaded is often not the same as when the meeting took place. Okay, so getting kind of into the weeds there, but the reason why this is important is that we can now, you can now take this database and you can merge it with all sorts of things like, you know, population density, which you can get from the ACS. Again, you can merge our database with all of these fun things, population density, for example, census demographic data, election outcomes at the local level, and COVID counts, which are available at the local level. And you can do all sorts of things, all of which you can do on our dashboard now, by the way, things like descriptive analysis of the text, the speech that is being used at these meetings, correlational analyses and overtime analyses as well. And just to kind of peek into the tech stack that we're using here, this is blasamis, but we are actually using a little bit of Python, and particularly the Google APIs to actually pull from YouTube. My fun is actually a little bit better for that than R. And we're using this package called Web VTT, which allows us to pull the transcripts and kind of structure the transcripts and get timestamps for words that are being set in real time. Okay, so these are all kind of friendly characters that we all know and love. So Tidy Census is this fantastic package that actually allows us to get the sampling frame of all places. I wanna give a big shout out to PowerJoin, which besides having a really cool hex sticker, was very crucial for us to turn a thousand line script into like a 20 line script of multiple joins happening in sequence, as we're trying to find for each video, what's the place that it matches to? So I highly recommend you check out PowerJoin, if you haven't already. You probably know these other two characters, Arrow and Shiny, Arrow, so all of our data as of a couple of days ago is stored in parquet format. And our entire dashboard is built in Shiny, as you may have guessed. Couple of other characters that you may also know and love or not love. DuckDB is actually the way that we're able to not turn this project into like a five server, three AWS instance, sprawling behemoth that no one wants to work on. It's all on one server and there's no server. There's really enough, there's no layer between the parquet files and the dashboard, thanks to DuckDB. Our app is hosted on Shiny apps, thank you, post it. And the underlying data that you can actually access and download right now, if you want, is hosted at the Harvard Dataverse project. It's a repository for academic data. We'll try to recommend that you check out as well. Okay, just to show you how easy it is to analyze this data, all you have to do is you have to create an account on the dataverse, on the Harvard Dataverse. You get your token, you install two packages if you don't have them already. And voila, you can just read in a slice of the data, the parquet, the data in parquet format and the repository as a data frame, we're free. Okay. So what can local view actually tell us about local politics? Why is this database useful? So we've been doing research with this database for a couple of years now and I just wanna share a couple of things that we consistently find. Some of these are not necessarily path breaking. But one, issue attention changes over time and context. And this is sort of obvious, right? So if we recall, my gift was showing mentions of COVID over time and indeed mentions of COVID unsurprisingly have decreased over time. And before I get a Tufti award, there's another version of this plot that has percentage of meetings over time. So you know that I'm not trying to fool you. You can look at percentage of meetings over time, again, unsurprisingly, attention to COVID as a percentage of all meetings over time is also, also declined. But something that might surprise you is that attention to COVID itself is actually correlated with a household income, which is sort of funny because in a lot of place, the epicenter of COVID in the local level was not wealthy municipalities in the US. So that's kind of interesting, that's kind of surprising. If we look at abortion, so abortion's kind of an interesting issue. It's an example of kind of bursty attention to a mostly sort of national or state level policy topic. This first verse happened after all of the kind of big state bans that abortion, a kind of total bans that had happened in the South. And this other peak here is after the DOP's decision, which you may have guessed. And so you got these like bursts of attention to abortion. And you can see here that these mentions at the place level are actually negatively correlated with household income, median household income in that place. Okay, and to take a third issue, if you look at attention to policing, this is an issue that's only been growing in attention, at least somewhat steadily over time. And it doesn't seem to be that discernible of a correlation with household income. There's a slight positive correlation with kind of how wealthy municipality is and how much attention is paid to police. I encourage you to query these on your own, because you can actually see, excuse me, specific examples of mentions of these topics in these various places. Okay, so to kind of have a more system level view of what's going on in local politics and to kind of think about the politics part of this. So this is, I'm gonna show you one analysis that we did a couple of years ago. And basically, it take away is that democratic cities and Republican towns govern differently, which you probably already know, but I'm gonna show it to you in a kind of nice way using our data. So what I did is if you're all familiar with topic models, I fit a topic model to our data. Okay, so topic model, did all the topic model stuff, label the topics, validate the topics. I took all of our towns, all of the places, all the governments in our sample, and I looked at the distribution of topics that are discussed in those places, in those public meetings. So what you're gonna see here are a bunch of dots that are places, and what you're gonna see is a sort of principal component grid of those topics. And the reason to do this principal component analysis is to see which topics are correlated with each other. Okay, so a town that's over here on the far right corner on the time that's over on the far left upper corner, they're very different in the topics that they that are brought up in meetings. Okay, and then I'm also gonna show you the topics that kind of explain that difference between a town on the far right and the town on the in the far left. And just to complicate things further, I'm gonna actually color each of these places by their Republican vote share, their average Republican vote share over this sort of 10 year period. And so what you see is a couple of things. So first, we can see the topics that happen in like the different topics that really explain the spread in issue attention and local politics. There are places that talk a lot about housing, a lot about kind of neighbors and transit and biking. They're very different from the places that spend a lot more time talking about cars, motor vehicles that talk about the roads, that talk about, you know, unemployment. Okay, so those are those are very different kind of sets of places. And you can see here that the most democratic cities in our sample Philadelphia, you know, Austin, these like big kind of metro hubs, they just talk about different things than like the smaller places do, right? Social services, attention to elections, community, these things kind of show up more in democratic big cities than they do in these smaller places that tend to be more about kind of making sure that the trains are running on time, the streets are cleaned sort of different style of governance than you see in these big cities that focus a lot more on redistributive policy. Okay, there's a great quote that New York City, the governance style in New York City is closer to Congress than it is to, you know, your suburban hometown. I think this plot does a good job of sort of showing that. Okay, and also if you want to read the paper behind this figure, you can do so at that URL. Okay, so that's basically it. What's next for me is, or rather for you, is one, you're gonna go home and you're gonna start your own project with local view tonight. You're gonna read our research that uses this data. You're gonna chat with me and like all my tweets on the platform that is formerly noticed Twitter. And also a new project is underway, which is called District View. Some of my collaborators are actually in the audience here and you're gonna stay tuned for this and keep an eye out for that. So that's all I got. Thanks so much."}, {"Year": 2023, "Speaker": "David Shor", "Title": "Data, Surveys, and US Politics", "Abstract": "A walk through of the state of the art of Data Science and US politics.", "VideoURL": "https://youtu.be/irxmR7ZJamI", "id0": "2023_09", "transcript": "So our next speaker has, he was one of the first people ever to post on the R-Stand online help list user group list. Please, and if you don't know R-Stand, you should all know R-Stand. It's great for bazing computations. So please welcome another David. Come on down, David. All right. Can you all hear me? Fantastic. All right. Yeah. Okay. I've had trouble with likes in the past. So all right. Hey, everyone. I'm going to talk about data science and politics. Everybody loves politics. So I just need to do the mouse thing. Okay. Fantastic. No, no, I can also arrive. No, I need a question. Simple man. Okay. All right. So first, who are we? So I, one of the co-founders of Blue Rose Research, we're a roughly 35 person shop founded by alumni of the Obama campaign. I have like a big wall of text up there, but I'd say that our main thing is that we were a data science focused firm trying to elect Democrats. We work with, you know, I'd say most of the major spenders in the Democratic Party, we have over 100 clients in more than three countries. And our big thing is that we do very large numbers of surveys in order to help politicians, in order to test content and help politicians figure out, you know, what to say. I'll go more into the details, but just to talk about some of the highlights, you know, we've done literally thousands of experiments on millions of people. We've helped allocate hundreds of millions of dollars. And we've predicted we, you know, help provide races and predictions across multiple cycles for nearly every election in the country. Okay. So just what do, how do you actually do data science? Why is data science relevant for politics? You know, I like to break things down in kind of a boring, unromantic way, which is that a lot of what political campaigns try to do is answer the questions of what do I say, what's happening, and how do I actually spend my money? How that translates, you know, on a day to day basis, can take a lot of forms, you know, when you're deciding what to say, that could mean what, what ad should I put in the air, what talking points should I feed to the press, you know, what policies should I base my campaign on? And we try to, you know, build tools to help people answer those questions. When it comes to what's happening, there's both, how is the world around us changing and how is public opinion changing in response to events? And there's also mechanically, okay, I have $500 million. How do I spend it across Senate races in order to maximize my expected Senate seats? And then when it comes to how to spend your money, it turns out, you know, buying ads, which unfortunately is what most of TV political campaigns are, very complicated. And so just to talk a little bit about what the, you know, what we actually do on a day to day basis, I would say that big picture, we survey really large numbers of people. We join that information to a voter file, generally political campaigns in the US, though not in other countries have access to a big, creepy voter file of every person in the country and, you know, what elections they voted in and various things like that. And then we score, you know, we fit models on the survey data, we score every person in the country, and then we, you know, do a bunch of analysis on the basis of that. So, you know, why is this hard? So the first thing is that politics has a lot of structure. And you know what that means in practice is that, you know, what makes people choose who to vote for is really a function of a really complicated interplay of factors like religiosity, education, socioeconomic status. And you know, they're both the relative weight of those things can change over time, it can change over space. There are some parts of the country where much think there were a lot of these factors are more important than others and the relative importance can change dramatically in response to world events. And then, you know, a lot of the things that we're trying to measure or a lot of the things that drive voter behavior are things that are psychometric quantities, which in some ways don't even exist. You know, I think the hardest thing about this, the central problem in our field is that survey takers are just really, really weird. And I'll talk about this more later. You know, when I first started working in this space a bit over a decade ago, something like one in 10 people would answer phone surveys, that number is now down to one in 200 or 300. And so, you know, at this point, the sheer act of answering a survey is a political act. I think my favorite statistic about, like, if you look at people who answer phone surveys, even among the people who say they definitely will not vote, something like 80% of those people vote, just because, you know, we're talking, you have to just be incredibly interested in politics in order to get people to answer these things. And they're also weirder in a bunch of other ways. You know, it turns out, you know, for example, that Jewish men are much more likely to answer surveys than other groups, which is unsurprising to me personally. You know, but there's a whole host of other things like that that make this hard. And then I think the other thing that's hard is, you know, we fit all of these models. But I think in a lot of traditional business analytics, you know, you're just trying to maximize, you know, the AUC of your model, and you can say, well, you know, we could make everything a lot more complicated, but then that would only increase the AUC of our models by like 0.01. So we're just going to save the tech debt. But what sucks about what we do is, you know, we have this very spatially auto correlated error, which is a fancy way of saying that we roll all of our stuff up to make decisions. And so, you know, if there's a hypothetically of Quirk and southern Western Virginia, where 70% of people are still registered Democrats, even though Biden got 20% of the vote, that's actually, you know, true. And West Virginia's third congressional district, if that that one edge case, you know, less than 0.1% of the country lives there. But if it gives you a wrong forecast and you end up overspending in a district by millions of dollars, that's really bad. And so we have to end up making our models quite a bit more complicated than we otherwise would be. And then I think the other thing that sucks, you know, just in the same vein is that, you know, we really try to measure two things at our shop. One is changes in public opinion over time. And the other is changes in how people change their mind in response to seeing ads or arguments or content. And both of these things are incredibly small. You know, the thing I like to say about this is that in 2012, Barack Obama got about 52% of the vote. And in 2016, Hillary Clinton got 51.1% of the two party vote. So that's a 0.9% change over four years. You know, it made a big difference. So measuring small effects is in our case is important, which is a shame because measuring small effects is hard. All right. So how we actually deal with this, don't worry, I'm not going to actually read all of these all these things up here, but folks want to. The first thing that we just survey really large numbers of people. We did something like 15 million interviews in the 2022 cycle and we're on track to do substantially more now. I think that we are one of the largest purchasers by volume of online survey data in the world, which I guess is cool. But that, as I think any statistician could tell you that, you know, it's that you can't actually solve all your problems by just surveying really large numbers of people. That's not a solution to bias. And so really big thing is that we work with a really, we work with a lot of different panels to collect information from people who normally don't take surveys. My favorite example is one of our biggest panels. The way they get people is people are playing candy crush, they run out of lives and then it's like, oh, would you like to take a survey in order to go? And you know, that does get to this trade off, which I haven't seen too many survey people talk about, which is people always talk about, you know, data quality in surveys, getting high quality survey takers. And the problem that we found is that all of the really high quality panels are filled with people who are really weird. And so all of the shittier panels, you know, kind of filled with the people who aren't paying attention. Those people are a lot more normal, but then the data is less reliable. And so that creates a lot of hard trade offs. You know, the other thing we do is that we just use a lot more data. You know, we join to a lot more things. We have things like administrative data. You know, traditional surveys only control for things like age and race and gender. And we control for literally dozens of different attitudinal characteristics because our whole philosophy is given that survey takers are weird, our really only hope out is to just measure exactly how weird they are so that we can try to adjust for that. You know, the other thing I'll say just to talk a little bit is, I mean, I don't think too many people here work in surveys, but we don't use any survey weights. I think I remember reading as a teenager, you know, Andrew Gellman's would always say, you know, you shouldn't use weights. You should just put everything into this one big model. And I always compare that to, you know, when you're young, you read the communist manifesto and you're like, Oh my God, this is beautiful. This is so exciting. And then you spend the next 10 years of your life trudging through Dask capital and you're like, Oh God, it's actually a lot harder than I thought it would be. But we've really tried to make the dream, you know, Andrew Gellman's dream of fitting big multi-level models work. And I think we've made some progress. And then, you know, the other big thing, and I think that there's probably some broader, you know, some broader insight beyond politics and surveys, is that we do really extensive back testing and QC on all of our stuff. You know, every single time we make any change to, you know, our, at this point, very complicated process, we see how it would have affected all of our forecasts and, you know, hundreds of past races that we pulled. And you know, we end up finding that lots of seemingly minor changes that we do go back and break everything. And so it's a good, it's a good thing to go and do that. So now I'm going to go through some graphs, which is fun. So because the whole thing is because we have this whole, this, you know, whole workflow that allows us to score individual people, we can end up answering a lot of questions about politics that would be pretty hard. So otherwise, so this is an example where what we're showing, you know, in every state, generally speaking, elections are different from year to year. And so, you know, one example is that Democrats did worse in Florida in 2022 than they did in 2020. Even, you know, even then 2020 was a fairly bad year, but it got worse. And so it's a natural question that people argue about, which is like how much of that was changes and who voted versus people actually changing their mind. And in Florida, you know, we could see these two bars. It was roughly even split of both. But it turns out that, you know, what was what really varied a lot by state? You know, for example, in Pennsylvania, turnout was actually somewhat worse for Democrats, but people just really loved John betterman. And he just was in and that's, and he was able to overcome that. And so that's the kind of thing that normally would be very hard to answer. But, you know, with modeling, you can go and do that. You know, this is some more plots. This is just showing a reason why surveys have been a lot harder. Basically one of the biggest drivers of answering surveys is, you know, this laden socioeconomic status. People generally speaking who are more educated really like taking surveys. It's kind of a form of self-expression for them. And what the reason why, you know, that's now a problem is that in the past eight years, or I guess in the past 10 years, there's been a really big realignment on education lines where people who are very educated have become a lot more liberal and people who are less educated have become a lot more conservative. And you know, you can see that with these, you know, county scatter plots. And so then you could say, all right, well, great, let's just adjust for education. But it turns out that actually education is just symbolic of a whole host of differences that represent socioeconomic status. This is showing social trust. If you ask people, do you believe that people can generally be trusted? It turns out 70% of people say no, which is fun and probably somewhat concerning. But what you could also see from a measurement perspective is like in 2016, you know, the college whites, well, the college whites who answered that they trust the people around them or the non-college whites that did swung toward Clinton, the ones that didn't swung toward Trump. And this was a big problem because in 2016, the non-college whites who trusted the people around them were the only people who answered bone surveys, which is why the polls were wrong in 2016. Just to go through some more graphs, just to get it politics being complicated, this is just showing, you know, two-way, you know, support for the presidential candidate in 2016 by race, attitudes toward the Bible and education, and then also showing ideology. And so you can see a bunch of things here that are, you know, that are interesting, the non-additivity of these things. You know, the first is generally speaking, being more religious generally corresponds with being more Republican among white people. But among African Americans, religious African Americans are actually somewhat more democratic than less religious ones. And you know, the reason for that is the complicated interplay of history. Another thing that's interesting is, you know, when people talk about partisanship and ideology, they kind of use them as synonyms for each other. But it turned, you know, this just kind of highlights, you know, that they're really not the same thing, that, you know, if you look at ideology with it by, you know, race and education, sorry, by education and religiosity, it's really quite similar for African Americans and for white people, even though the partisanship is really quite different. And so that's kind of a big part of the job is kind of moving between all of these things that seem similar, but actually end up differing in a lot of very important circumstances. So when you actually go and do all this work, you know, you end up with more important election forecasts, which is important for outcomes. You know, we actually end up helping people, you know, doing all this work allows us to help people understand what policies are popular and which ones aren't. I'm just going to talk about some parts of this graph, because it's funny. So the basic idea here, we've pulled hundreds of different policies. So every single dot is a policy that, you know, various people in the Democratic Party have proposed. And so the x-axis is the support among the overall population. The y-axis is the support among 18 to 29 year olds. You know, I just want to pull out. So basically, the things that are below the line are things that are disproportionately unpopular with young people, the things above the line are popular with young people. And so you can see young people, they like student loan forgiveness. They like renewable energy. But then I think the funny one that's labeled China technology is actually banning TikTok. Young people don't want to do that, which I think is funny. But it also helps us know, you know, what to tell the press. So, you know, here we have a histogram of all of the different talking points that people have tested. We have several thousand in our library. It's available for free for everyone in the Democratic Party who wants it. And you know, what you could see is that, you know, the best talking points really do a lot better than the worst talking points. And so it is actually important, you know, to rank these things and try to say more of the good things and not the bad things. And just to close up, just about, you know, some of the big picture challenges, you know, I think the biggest thing about in terms of how I think about my job is that politics is just a lot more decentralized than it used to be. And I think, you know, there are some aspects of this that are obvious, but others that I think are only clear if you're a practitioner. You know, if you go back to the 2012 campaign, you know, they're really, they used to really be that like, there was like David Axelrod and David Axelrod had a deputy named David Simas who was brilliant. And he just kind of, the two of them and together were just like, all right, this is what the campaign is going to be. And, you know, they came up with these like five, like five proof points that were on the wall. It was really creepy. Like you, once you saw the proof points and you put on TV, you just heard them everywhere, you know, because they went and they were able to kind of control the media environment. You know, now because the basically the Democratic Party was the Obama campaign. Due to a variety of social and regulatory changes, one fun one is, you know, Citizens United made it so that you can kind of donate infinite amounts to these like non-party groups that's led to this really giant transfer of staff and power away from these centralized campaigns toward now this decentralized network of literally thousands of different groups, which don't talk to each other. And in many cases are not legally allowed to talk to each other and also don't like each other, as you may have heard about on the news. And so how do you get all of those different people to speak with one coherent message? That's a hard problem. But I think that I'm hopeful that, you know, technology can help. And then I think, you know, the other thing, and this is something that's probably more clear to people's lived experiences is just that we used to live in a world where most people got their news from the nightly news, maybe some people watched CNN. I guess people still read newspapers, but there weren't really that many of them. Now we live in a world where people really legitimately spent a get large amounts of their news from things like TikTok. You know, the White House now has to care about individual TikTok influencers in a way that just wasn't true before. And so there's just mechanically the problem of if you have tens of thousands of people producing content that people are going to see, how do you measure that content? How do you create an accountability structure to keep to push people toward making better content? How do you respond to stuff? It's all very hard. So, right, that's the talk. Thanks for listening to all of this. And yeah. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you."}, {"Year": 2023, "Speaker": "Alex Gold", "Title": "Learn to Love Logging", "Abstract": "Good logging practice makes the software development parts of Data Science easier and more fun. Learn about how to add logging to your apps, projects, and reports.", "VideoURL": "https://youtu.be/WB1Ob_2ccG8", "id0": "2023_10", "transcript": "Our next speaker is a successful PhD, economics PhD dropout. One of his favorite hobbies is convincing other people not to get a PhD. So if you're thinking about getting one, please go up to a method, and say, Hey, I'm thinking about getting a PhD and let them have a chat with you. Everyone, please welcome Alex. Seriously, I will do my best to convince you. So if you're thinking about it, just talk to me. I'm here again this year to try and convince you to care about boring operational stuff. So I'm going to spend the next 20 minutes to teach you to learn to love logging. If you want this presentation, it's not that interesting presentation. If you wanted, it's on my website, Alex K. Gold dot space. So you can, you can find it there. Just a little about me. I lead the solutions engineering team at positive, formerly our studio. And what what my team does is we help our customers get sort of installed up and running deployed configured with posits pro products. And so part of that often is helping those people think about how do they take an app that they've developed locally, they develop their desktop and get it out and share it. How do they get things into into production. And so one of the things that we think about is this, this person, particularly like being on the other end of the phone from this person who's yelling at you because like the shiny app is down or like they're looking at your plots and the numbers are wrong. Or, you know, the dashboard isn't working for me. I hit the slider and everything crashed. And like I think we've probably mostly all been in in that moment and that sort of moment moment of. Or. And the reality is that like this kind of failure is sort of inevitable right you want to have good practices around testing your code around version control around. You know, controlling what goes into it good pipelines and that sort of thing. But this kind of thing is going to happen. Right. And so the point of logging, which is, again, I'm going to try and convince you to care about. Is that logging is not really about avoiding failure, although it can help while you're while you're developing an app. What it's really about is it's about letting you figure out what happened after a failure it's about sort of forensic analysis of being able to go back and be like, Oh, I understand why this happens like when the, when the dude is yelling at you on the phone, you can be like, no, I don't know what's wrong, but I know I can figure it out. That's, that's really what the important thing is in that moment. So a little bit about logging, particularly for data science. If, if you were not at a data science conference, we were at like a JavaScript conference, like a web dev conference. And we were talking about logging, right. You'd mostly be talking at operational characteristics. We'd be talking about things like, you know, does the button do what we want the button to do do do results return in a sort of reasonable amount of time. Does the app crash when you try and use it? Right. That's what sort of normal logging is about as a data scientist, you actually, you have to care about that. We'll get to that in a minute. So you actually have another level that you have to care about, which is correctness, right? You have to care are the numbers that are showing up correct and fundamentally, because so much of data science is about discovering new things and learning things and going into uncharted territory like you're not going to have something to, to check your correctness against there isn't a given answer that you can just check. So yeah, I got the right answer. And so so much of logging for correctness purposes really means doing process checks right does the process work. The way I think it does and so I'm going to introduce a spicy take here, which is that you should always use literate programming always like full stop. Well, don't up there, give me, give me a fist. I love it. Right. Always use literate programs. So what, what is literate programming. Litterate programming is a term coined by Donald new he's a very, very famous computer scientist, won the Turing prize like the Nobel Prize of computer science. So like this is not just me saying this. And, and he coined this term called literate programming, which is interspersing code pros and output in one, one thing right and of course like we are here at an our conference so you all are going like, of course I know we're talking about like our mark down we're talking about Jupiter. Right. And so this is this is sort of my, my, you know, encouragement is that like this is the way right, we should not be writing our scripts and doing things we care about in them because once you've done them. It's gone forever. You get no evidence of what just happened. If you do it in a literate programming format right if you're running just a job and you run it a littering literate programming format. You do a smart thing of like printing some useful things into the console, and it gets messed up later you can reconstruct what what happened and that's really useful so like, you know, the way the way a project might look if you're doing this right is like you have your directory with your project. You have the like document you actually care about as a as a quarto or an arm mark down. That's fine, not.r. And then you have your functions documented with with Roxagan just like if you were doing it for a targets pipeline. And you have your your test that, you know, stuff as well that you should you should throw in there. So that's that's sort of big picture right just do the job in a literate programming format because then you get to keep the output, and you can also put some, you know, comments about like, oh, this should look like this. And if it doesn't, that's a, that's a good hint. A couple thoughts on what you should log into those literate programming formats. One thing I'll recommend is the point blank package if you've never used it for it's a really cool package helps you create sort of a data validation workflow. I'm not going to spend a lot of time on tape, but it's like really cool. Strong recommend you can have it there's a lot of automated stuff it can do to validate that your data is what you think it is and you can also sort of build your own validators quite quite easily it's a very cool package. And I can work in a lot of different kind of pipeline mechanisms. There are two things in particular I'm going to encourage you to log into that literate programming format, whatever, whatever you're using the first is your joins. And I know this is really like, this is a lot of data. Stata's joins are amazing, because it gives you statistics on how many records joined and how many didn't and how that works and so, you know, but you can do this yourself right you can like do the math. If you go to join two data sets, you can look at how many unique IDs are in data set one you can look at how many unique IDs are in data set to, and you should have an expectation. So, how do you do that join up exactly how many rows are going to be that right you should, it's not like there's no magic there right you can say like oh there are this many unique IDs here they match uniquely to this data set, we should have a one to one match or I know there aren't there are going to be duplicates, I should have some sense of how many so ideally do the math and just figure out how many there should be now that that is a pain in the butt. So, I usually don't do the math, but what you can at least do is you can document in your in your in your literate programming format right how many rows that I start out within each data set, and how many rows that I end up with in the join data set because that gives you some really good unique data set, and you look back later right very often you're expecting your joints be unique so you have you know 1000 rows over here 10,000 rows over here and you're expecting 1000 and you end up with 1000 and 10. That's usually a really good hint that something weird has happened, so at least document how many rows are in the sort of resulting data set. So, you know, you're expecting a lot of duplicate IDs right if you're if you're expecting that data set should join uniquely just just check are there any duplicate IDs right these are really easy checks that if you throw into that script all of a sudden you you know and you go back later you go oh there were a bunch of duplicate IDs here and I expected none. That's a really powerful signal. I always check as a matter of course is recodes and what I mean by that is like when you have you know a certain number of categorical variables and you're you know taking months and making them quarters when you're taking the entire rainbow and making it. I don't know how you split up a rainbow but like three chunks of a rainbow right or or a numeric values and chunking right. The simplest thing you do is just cross tab your values before and after right and do they look the way you think they should look. It's again very simple thing to check but I can't tell you how many times this has saved my butt to just dump this into my my core to document. So, especially any that don't recode at all right that's a really good signal, especially if you're running a job over time right you get new data in. And like somebody's going to secretly add a new value that you don't expect to that variable being able to catch that is is really important and can really sort of you know save the day later on or at least help you reconstruct what what happened. So, I gotta agree that I honestly don't have a lot to say about but you should pay attention to is machine learning models. If you've got a machine learning model if you're running it on new data, you got to check your model drift and quality. There are a ton of proprietary frameworks to do this. I if you want an open source framework vetiver is great. It is particularly great for sort of relatively small collections of experiments. So, you're going to have a proprietary solution to do this. Cool, so that's those and thoughts on logging for like data science purposes. Now let's talk about like logging for logging sake and I mean this in terms of the operational qualities right does the does the app run in a reasonable way. And a time does it crash. Those sorts of things. And so the, you know what what a lot of people do here is they just print print stuff right that works it's not bad. And so what I want to talk about is just that you can up your game. And so, I want to talk about it is just that you can up your game to do better with like very little additional work like that's that's what I'm going to try and convince you so there's a great package called log for R. It's inspired by a package called log for JS, which is a big JavaScript logging package. It's maintained by my colleague Aaron Jacobs at at posit. And it's just a really elegant sort of logging framework that makes it super easy to add logging to a script that you otherwise otherwise might just dump prints print statements into. Now there is a little bit of a mental model to have around log it's not hard, but when you're logging using a logging framework you don't just print a statement instead what you do is you give a logging level. Right. So logging comes these are the ones that log for our uses different packages have different levels. Or there are five levels and they sort of go. Up down. That's hard to say in terms of level severity right so they go from debug to fatal right and so debug is the kind of thing that like just you care about. Like oh right did the thing run correctly right it's like function internals you're going to log as debug statements, all the way up to fatal right like app is crashing here are my parting messages to the world goodbye cruel world like that's that's fatal right. And so what you can do is you can you can in your code and I'll show you this in a second. You declare this statement that I'm making is this level of severity. And then when you're running your code right you can provide usually as an in an environment variable. What level do I want this time to log it so when you're when you're developing locally you can run it all the way on debug mode so every little bit of documentation prints into your console. And then when you're running it in production you probably want to choose something a little more severe so that not every little bit of information gets gets dumped into into your console. So, actually I'm going to stick with. Yeah. Okay, so I'm going to talk about a little bit of code here does this have a. I don't know what that button does. I thought maybe it was a laser pointer but it's not. This it's that great okay we've got a laser pointer. Okay, so this is some code that it's a shiny app that includes some logging and so it's it's really easy to add this login code to your app right the main thing you need to add relative to just some print statements is up at the top. You've got this little. Can people see the laser pointer. Okay, you got a little like I am instantiating a log. Great. That's going to remain throughout my session it's going to print stuff out. And then I can I just have these log statements so the in for log for our it doesn't split the function names right you could think of it. Like you could alternatively design this back to take an arguments but these are actually the function names. So you have a log for our info. You provide the log object and then this is just like what is going into the log. I have another info. What's going into the log. Here I have a try catch block and if an error happens. I dump a lot of stuff into the log right and that's that's really a good practice because again. A huge point of the, the, the logging here is forensics right is later on we want to be able to come back and forensically examine. What might have happened that was bad so I'm going to show you this. Looks like the Wi-Fi is not being my friend so I'm going to just do it live. Here we go. Can we see my studio yes we can. Okay. So let's run this app. I'm going to clear off my console here. Run this app. Okay, this is this is a little shiny app. This is taking the Palmer penguins data set, which is just a data set of like information about penguins. And it's, it's letting me, it has some selectors here right I can select a minimum bill length I can select sex and species and it computes just the average mass of the penguins in that in that little. You know subset of the data and you can see down here right I got my app started. Told me that and it told me the, the, you know, that I hit the compute button because I set it to do that so you know competition requested cool. Now, when let's say let's say I do something that I intentionally designed this to crash but like oh crash my app right and what you can see is that then that try catch block on both and I get all of this stuff dumped out right and I get dumped out here. The values that I input here that yielded a bad outcome right and I could do some digging like why did this cause a bad outcome it turns out there are no penguins with a bill length of 60 or more so this is trying to compute an average on a set of size zero which doesn't work right so this. This now I have the information out I know well I got a not a number error right that's a big hint and then I can actually like go in and forensic my way around around these actual values, which can be a really helpful way to. To be able to do this the other thing I'll point out right is that relative to just printing print statements where you have to with a print statement you've got to like. Get in there every single little bit of information has to go into the actual print statement right that's different here, because the law this is all auto appended by the log right it's giving me the info it's telling me that this is an info and it's giving me a timestamp here and you can add other things. When I when you invoked the logger at the top right this invoking the logger you can add more stuff right you can give it a custom format that prints a ton of information in there, including like session information user information all kinds of great stuff that then you get for free in each line of the law. I'm going to switch back to presentation. There we go. We're going to close our studio. Yes, don't say never save your workspace image never save your workspace image. Okay, other things that you can do that are useful is what you saw there is it with logging to the console right just going to my our console that's useful. Because then I can print it out if you're on positive connect it just goes right into the logs that you get there on on connect which is nice. But there are other places you might want to log for example you might want to go to the sys log this is very common when you're using Docker that a lot of dockerized deployments are the law you set up logging so that it's the sys log that sort of gets sucked out of the container. Or if your organization has a logging framework that they use a log aggregation like Splunk or data dog or some of these things very often you're going to ship those logs right off to to an HTTP location there is also a file appenders you can also put things in a file many logging systems just like watch a file. The other option here is to do structured logging again if you're shipping it off to to another system. Excuse me. That's a nice way to be able to do it usually that would be a JSON blob and you just sort of put the login there and then you have other systems that would consume that JSON blob and do something with it. Cool. So that's that's what I have to say about logging so as a data scientist you unfortunately have to care about two kinds of logs. I'm sorry, you have to care about operational logging. I recommend using log for R. You got a very brief tour through using it as you can tell that did not take very long it's like very simple to use it's barely more complicated than print statements, but it gives you a much richer set of information a much richer configuration to be able to use. And then you have to check for correctness, which of course is the the task of of literate programming. I will, if you thought this was even moderately interesting take them out really. This is a book that I'm working on called DevOps for data science, it is available at d o four ds.com. It will be manuscript will be final up here like this week or next week. Thank you. Yes. So there will be print copies at some point, but it will be available online forever. So, if you're interested the first whole section of this book is about things that this is chapter four logging. Right. There, there's a lot of stuff in here that sort of advice for data scientists to make your apps and projects more sort of like welcoming from a DevOps perspective make them work more smoothly things like logging, all stuff around CI CD deployments package management, all that kind of like that I love and people think is boring, wrong, super cool. Then the rest of the book is really about sort of IT stuff so there's some stuff about how to set up servers, how to use Docker, and then some stuff at the end on why if you work in a big organization, they probably aren't going to be very happy with you doing this yourself and how you can maybe work with enterprise IT admin folks to make this work better. Thank you since I'm here at the Lander conference I want to give a special shout out to Gus Lipkin. One of the first readers of this book and gave me a bunch of really useful feedback so thank you Gus. It's way better than when he read it. It was bad then. Anyway, thanks for tuning in and yeah go log some stuff. Thanks. ."}, {"Year": 2023, "Speaker": "Melissa Albino Hegeman", "Title": "It Works on My Machine (Reproducibility in R for Small Teams)", "Abstract": "Working collaboratively in R can be a lot of fun, but it can also be tricky to get started. A combination of GitHub, renv, and custom packages can help improve reproducibility, reduce stress, and lighten everyone's workload. I've made mistakes and hit roadblocks when implementing these tools within a team. But I've also learned a lot along the way. I'll share my experiences and tips so you can avoid the same mistakes and start on the right foot.", "VideoURL": "https://youtu.be/9yVbk6Z6qls", "id0": "2023_11", "transcript": "Our next speaker once had to be rescued in the middle of the Pacific Ocean when the dive boat she was on sank. Yes, she's here with us now so clearly she survived Melissa. So yeah, it works on my machine reproducibility and are for small teams. So some of you if you're Microsoft shops that work probably recognize the sticker from teams. It's my favorite one to use when I just can't deal with all the problems anymore. And I think it says a lot that it's stuck imagery in the team's chat. So a few disclaimers before I get started. So I'm here for New York State, DC, the Department of Environmental Conservation, but these opinions are my own and not reflect any agency policy. And also everything but that first image was generated with Adobe Firefly. So some of them are a little weird and that's why. Okay, so about me. I am not a computer programmer. I am not a software engineer. I don't do statistics. I'm a marine biologist. And that's a pretty cool job title. It starts a lot of conversations and people have a lot of ideas about what it means to be a marine biologist and what a marine biologist does. So, you know, people think we work at SeaWorld or out swimming with dolphins or. On a research vessel out in the ocean for, you know, a week at a time. And that is Adobe's rendition of a of a newer research vessel. It's okay. But there are many people who do that work as a marine biologist, but I'm not one of them. I get super seasick. So that is not the work for me. So what I ended up doing is working with fisheries data. So this means mostly catch an effort like who's fishing where when how and how much they're catching. And I can do this all safely from my cube on land where I don't have to worry about severe emotion sickness. But what it means that I do is the same thing most of you probably do a lot of data cleaning, getting the data that people need it running my own analysis. And then writing lots of reports. And I use our for most things I do at work these days. Obviously, I'm using it to automate everything. There's so much to end up doing. And if you have to do it more than three times, you might as well automate it. And I hate doing a petted work anyway. So that's worked out really great. But most recently. I've been enjoying using it to generate all my reports. Often we have to run thousands of additional individualized reports at a time. Where they're drawing from multiple data sources and the content of those reports is very specific to individual. Certain sections might apply to one individual, but not another. And I don't want to have to separate those people out first. And this has been like a huge success. And say, a lot of work, everybody loves it. But I'm actually going to talk about that today, that today. What I want to talk about is what happens when you have that success. You've created this thing. You've proven your concept. It's in production and now what do you have to do to keep it going. Before we get into it though, the purposes of this talk. A small team is around 10 people or less. And I'm going to add the caveat that they have limited to no experience with our. Most many people are trained in something else and find that they have to learn. You know, how to code to do their jobs effectively. So they're coming from. You know, biology ecology chemistry backgrounds, but they have to, they have to learn these tools in order to to really succeed at their job. And often there's no enterprise tools. There's no IT resources to devote to this. It's no enterprise R or studio and any of the tools that go with it. It's all local in your machine. Maybe you have a shared file server somewhere. All right. So you've done all this stuff. You made this new thing. You're really proud of it. It's in production. Now you have to share that, that workload amongst your team, because you can't be the sole purpose, the sole purpose and who maintains this forever. It's a terrible feeling being bottleneck when everybody's waiting on you to get to something done. Or, you know, if you come back from vacation full of dread, because you don't know what happened while you were gone. So what happens after you implement this big change, who's responsible for the maintenance and who's responsible for making new features. So, after a lot of googling, I found that the consensus was I should be using our projects. I should be using GitHub. I should be creating my own packages. And there's also a lot of the information out there about our and our end. I don't even know how to pronounce that. They should have made that easier to say. And of course, after reading all this information, no one seemed to have any problems with any of this on the Internet. And spoiler alert alert. I did. Nothing worked the first time the 10th time. I did all the things. I sent my projects off to coworkers to try to run and it was a huge failure. It took several several tries through to get to get anything to work. You know, we saw all the classic errors. There's no packages. There's no data. They can't find your file path. And you don't have the right permissions. And when you're working with, you know, a team that. Is newer isn't as experienced in our that can also be hugely frustrating and really prevent them from kind of buying into this new. These new set of tools. Mostly people who had experience with our Python could debug in real time and get it to work, but you shouldn't have to. It should be seamless. All right. So I'm going to go through all these, those things I just mentioned. The problems I had with them. Some approaches I took to try to mitigate those. I also decided I was going to learn quarter for this presentation. So that just an extra challenge to throw in there. So our projects. This I thought was like a base level of knowledge that anyone who was using our head for to. And that did not turn out to be the case. Most issues we had with our projects were people not using projects as they were intended. So people didn't understand that it was more than just a folder that it had all this special information that helped. Are fine things and keep track of your history and all these bonus things. And I would find staff. In the worst case scenario just opening scripts from several different projects, trying to run them in no project and everything was failing. Because our couldn't find anything. Or they would bring in scripts from other places to their project and again, our couldn't find everything. So it would fail and they would get frustrated. So, you know, unsurprisingly. You know, they don't they weren't feeling good about what they were doing, you know. With the team. And the only way around that, unfortunately, is to just, you know, train your people better and making sure there's onboarding in process and continued training so that. They know what you know. And as I mentioned, you know, relative paths are great. And for ad hoc analysis, sometimes in our project is all you need to be able to run it quickly on somebody else's machine. If it's something that you're never going to do again. You know, next we started using GitHub and GitHub repositories. This had a significant learning curve and. The workflow involved in GitHub remembering to. You know, fork a project check for a sync your project with the main project or sit your sync your repo with the main repo to make sure you're getting changes was very alien to people who aren't coming from that type of workflow normally. So we would run into lots of problems. Just with behaviors and remembering to do this. So again, things wouldn't work. And every time something doesn't work, people get discouraged. And that's definitely not what you want on your team. However, once once people got that that workflow down, it was the easiest way to get code onto some piece of machine. And the most important piece of it is when they fork a repo, they can do whatever they want to the repo without ruining everything. And it's so important for people to have room to mess up, colossally and have it be okay. Otherwise, people aren't going to be willing to help you maintain things and build new features going forward. Excuse me. So after after that happened, I decided it was time that we make our own package for our office. And this was really intimidating. But. And it still has the same problems of keeping everything in sync. You'll hear me say that in every slide, having the same thing for every person all the time is been really tricky when you don't have your larger scale IT resources help you out. And then who's going to maintain this package forever because once you've come dependent on it, if it breaks, are you going to go back to Excel? I hope not. And it also allows me to assign more work to other people. So perhaps before I was only one who could do some analysis because I knew I had to look here, here and here and filter it this way to get the right data set. But now when we put it in a package, everybody can do that. So everyone's treating the data the same. They're starting with the with apples apples. And I'm sure I'm sure a lot of you have the same experience at 90% of what you do is the exact same thing everywhere. So if you can make that easier. That's a huge time saver and reduces your own errors as well. Lastly, we're trying out our our end and. And we've had constant problems with package versions on people's machines, depending on when someone is hired, they might have a different version of our hundreds of different versions of packages combinations on everybody's machine. You know, we're not supposed to install all that ourselves. IT is supposed to do that, but you can't tell it what to do. And you end up with a situation where everybody's just confused and nothing works. So we've picked a few projects to start trying this out at and this has been the hardest thing about R that I think I've ever done getting this toward properly. It's this might be our own system. I'm not really sure, but it's so slow to boot up like if I asked someone to run this project, they clone it from GitHub by the time I'm on the website. And then our studio is done. Installing all the parts of that are in the lock file. It could be an hour and my projects aren't that big. We're not big data. And that's pretty frustrating. Oftentimes. Staff will have to do something else. Well, it's running and have to shut it down. And it's just a bad feeling again when you're trying to get new people on board and to feel successful and want to help you move forward with things. And then also range issues where staff were updating the lock file themselves, which defeats the entire purpose. They should be using the packages that I developed the project with, but then they were switching out and just overwriting their lock file so things weren't working again. So this is still a work in progress. But it has helped me write better code in the sense that I'm almost done with the project when there's so much influx. It's not really helpful when it's just me working on it anyway. And it's also forced me to minimize the amount of dependencies I use. So there's less room for mistakes if you're pulling less packages. So my main takeaways when you're working on this small team that doesn't have a budget or support and you're trying to move forward and do new and innovative things in our or even other software platforms. Is that you can't ignore documentation training and being consistent about it. What you say while you're talking to someone, they're not going to remember it has to be written down somewhere. There has to be something they can fall back on if you're not there. You also need to simplify everything whenever you can. As you're working on projects, you might try things and like the remnants of them are still there. And those things might not be necessary anymore, but they're still causing problems. So you need to get rid of all that stuff. That's not important and keep everything focused on the actual point of the project. And lastly, it's important to avoid scope creep in your projects. I might start a project with this one analysis in mind, but someone asked me a slightly different question, but I can cheat by going back to this analysis. And then I keep building on it and things get messy. And you just again run into more problems. It's harder to break it away when it's time to, you know, run an analysis that you need in 15 minutes. So what's our next step after this? So I'm recently experimenting with containers. I normally have always thought of like things like Docker and Podman as being kind of a large IT issue that I would need a lot of help to roll out. But honestly, working this has been just as frustrating as RN. So why not? And all the code I've been talking about previously to the slide has been code that I've wrote that I want other people to run and maintain. But I also would like other members of the team to start writing their own code. And I think a container is a safer, more friendly way for them to do that where everything that they need is set and there's just a lot of things that they need to do. And then there's less ways that they can run into mistakes. I didn't put this on the slide, but I'm just going to throw it out there anyway. Of all the problems we've had, we have never ever had a problem with a shiny app run locally. I don't know why, but we've been running some apps for years and there's never been an issue, whereas other projects that are less complicated seem to constantly have hit snacks. I don't know why, but perhaps I should be writing more things than Chinese. So that's all. Thank you for listening. All my slides are on my GitHub. It's M. Hegeman and the repose 2023 underscore ARGA. You can also find me on LinkedIn. It's Melissa Elbino Hegeman. I'm the only one there. Yeah, and I look forward to talking to you guys later and hearing about your own struggles working with your, your coworkers and small teams. Thank you."}, {"Year": 2023, "Speaker": "Dusty Turner", "Title": "World Leaders, Military Service, and Their Propensity for War", "Abstract": "This presentation delves into the question of whether leaders with military experience are more likely to lead their countries into war. Utilizing the Leader Experience and Attribute Descriptions (LEAD) dataset, which comprises personal lives and experiences of over 2,000 state leaders from 1875\u20132004, the study examines various factors related to leaders' propensity for conflict initiation. Drawing upon a multitude of sources, including academic literature, obituaries, military archives, and more, the data covers aspects of leaders' childhoods, educations, personal lives, and pre-leadership occupations. The research employs a purposeful selection model-building technique, starting with univariable analyses and progressing through several multivariable models to assess the influence of different attributes on war initiation. Interspersed with anecdotal illustrations and personal backgrounds of leaders, the study aims to provide a comprehensive understanding of the intricate relationship between a leader's military experience and their inclination towards war. The presented findings do not represent the official views of the Army but contribute significantly to the broader discourse on civil-military relations and war dynamics.", "VideoURL": "https://youtu.be/NRgoAXETtKI", "id0": "2023_12", "transcript": "So our next speaker, not yet a PhD dropout, was actually voted by all this PhD classmates as a person they would most want to be stuck with on a stranded desert island. Yes, please welcome dusty. This isn't my focus on what I'm studying, but it is a kind of a side project I did when I first got there working in the consulting lab. I should note the fun fact whatever Jared said about how my PhD cohort. They voted me is the one that they would most like to be stranded on a desert island with. I was super, super happy that they voted to be that you know I'm thinking like Oh what a nice popularity contest. I'm stuck one on one with somebody like on a desert island for I don't know years like they don't want to be with me how nice but I, I found out it's mostly because you know I have this army background and they think that I would help them survive the longest so not quite the compliment that I thought it was. And speaking of that I have the results in a pie chart. Nonetheless. I'm going to win several tough these today and this fly is going to join me several tough these because I have a number of pie charts. Some graphs not well explained so we'll critique it all all at once. And I feel like I can't look this way I have to look that way. I'll probably be looking this way the most maybe I'll change the side of the mic. Well I talk. Maybe. How about that okay. I was also flattered that they thought I would be the first person to die among the cohort. But it's mostly because I'm about twice the age of most of the people that I'm with. They're closer and aged to my my children than they are to me. So it's certainly an eye opener when that happens. I'm also got one vote for most likely to be arrested. So I don't really know what to make of that I'm the red one here. You know I could get a Dundee for confusing me for not a Dundee. A tough. Office fans I'm glad the generation hasn't passed enough that we don't know what the office is. But maybe the most concerning is that I have quite a few votes for most likely to get a non statistic job. I probably doesn't bode too well with the rest of my talk that we have going on that they don't have that much confidence in me so with that being said, I'm going to talk about a little bit of military leadership and world leadership. So we all know that these four people up here are presidents but what else do they have in common. Good guess they all have military service. We know grant from his finger in the Civil War. Kennedy was in the Navy President Nixon was also a Navy and Bush the first spent some time in the Army Air Corps. And so we're going to try to answer through some analysis is does the military service of world leaders influence them when they're in charge, and they have the choice of whether to involve a nation in a militarized dispute. Now, I'm bringing up a very touchy subject I recognize that I were a uniform I'm in the Army. What I'm talking about today has nothing to do with me being in the Army this is not reflection army policy nothing so I don't want to end up on the New York Times or Bloomberg, or anything like that because of the results I I present here. What you should be thinking is, how did you get involved in doing this research so I should explain for a second before I jump into it all that yes I am at a PhD program at Baylor. I started my time in the Army as an engineer officer, about the 10 year point I transitioned into doing operations research. It has led me to degrees at Ohio State and also to teaching at West Point. I'm scheduled to go back to West Point after the PhD program that is the reason I'm at Baylor right now doing this. So, there you have it Oh, I also don't have I didn't use the liquid. I use G. G map. But the reason I use G. G map is because one of my advisors on my dissertation I don't know if you know, Dr. kale. He's the author of G. G map. He's quite a talented statistician and in our coder Google him sometime he's a really impressive man. Speaking of impressive people, Dr. Sir to dance my other advisor. I've known him from being in the military and having served with him in different locations. He is a local wayco celebrity as he's participating on dancing with the wayco stars. So, he rivals chip gains maybe for a wayco fame. So, Dr. Campbell approach this fiscal consulting department at Baylor and he's a political scientist and he's asking us this question to help inform some of his research is what we just talked about our leaders with military experience, like, what impact does that have on, you know, whether or not their countries are in military is dispute. I'm going to go through this process, give conclusions and then I'm going to hedge a little bit. So just have that in the back of your mind as I go through all this. And I think brought with him. When he was asking us to do this research was the lead data set. As you can see, we stands for leader experience and attribute descriptions, essentially, dating back to the 1800s. And so, I think that's a very important question about every world leader from information about their family, their work life, their political background, the type of country that they're in, and the nature of their military service if they hadn't. And so, that is our, our variable of interest of whether or not their country was involved in a armed conflict or military dispute is the official term, while they were in office. They could have taken over that office. At the time of the dispute like they could have already been in the dispute and that would count for them. So this isn't necessarily causal. So that, like I'm saying, should be in the back of everyone's mind. And then talking to you about this, I'm assuming several things. One of those is that you have a little statistics background. I'm going to talk to you as if you understand things like log odds, odds ratios, p values, things like that. Secondly, I'm going to talk to you as if you would have a little our background I assume everyone does or is at least very passionate about that or we wouldn't be here. And then lastly, you might not have showed up here. Interested, but maybe you're interested in the, you know, the role of question, whether these people service impacted any of their, you know, their decisions. When they were presents to the United States, and I should just note now I'm showing only US presidents, but this is at large, throughout world leaders of the entire globe. So back to the main point, let's actually start talking at the data. So tough to award number one. What I'm showing here, I guess number five, and this is the fifth chart is a stack bar chart using Baylor colors, the sick and bears of each factor relating to their military background. So, you know, if you're in a military background, you're not in combat or not, yes or no. And the percentage of those individuals who were involved in the military dispute versus those who didn't. So you can see if you didn't, you because the green bar gets larger. You are more likely to have been in a military dispute when you were the leader of the country. According to this we're going to look at the interactions bone behind all this in a little bit. And then we're going to look at the other side of the career and also military non combat. And some of you who have done some model building exercises before you're probably already jumping up and down saying, Oh, these things are late so much there's interactions. You're right. And I'm spilling water on myself so I'm just going to set this down here. So I don't do that again. I got excited. So, I'm going to take a look at that as a part of this, and we will do that but before we do, let's look at some of the factors that might influence this. One of those might be the years in office, or the years in politics. And I'm going to look at some of the factors that are going to appear and I've also taken the log because to the trained eye you can tell this is already skewed data and even the log is skewed. But those who. Let's see, let's look at years in office. The median years in office. The more likely it appears that they get their their country happens to be in a military dispute. We can see the opposite of that is true with years in politics. And also, when it comes to age, the older you are it appears on average, the more likely you are to get your company company country and a military. Your company's not military dispute, but some might say that's the case around here. Back to these Baylor bar charts. Here's some more factors, you can see a couple things. One, if you have a creative side to you like this is part of your part of arts before you got into politics before you came into world leader. You were more likely to be involved in a military dispute less likely if you were considered an illegitimate child. Medicine, you can see about 5050 and I'll let you just kind of ingest the rest of that while I go on to the next. Now I'm only showing you six and eight nine factors. There was about 70 or 80 factors that we considered in this. And speaking of, of how to consider all of these. In a statistics 101 class, you've probably done some model building exercises like forward selection or backwards selection. There's inherent problems with all of these because what happens when you do multiple hypothesis tests you get into all sorts of false positives and all the things that's kind of the downside of some of these factors. I'm going to talk about Dr. Sturtivant who I mentioned before wrote a book called applied logistic regression where he introduces purposeful selection that tries its best to get around that and what purposeful selection does is it is a seven step process where you look like a different model, you're looking at the same as you look at the same model. You can actually look at the same model, you look at the same model and you look at the same model, and you look at the same model, and you do a ton of steps, some of them iteratively and you eventually get a final model and you look at interactions and you you've tried to get all the intricacies. I'm not going to go through every step here but I'm going to focus maybe on three of them here as we go through the rest of this process. I did, because it wasn't really built out in any package, was I built my own function to look univariately at every factor versus whether or not the leader was in military dispute during their time. And I did this in the following way. I built a function that takes factor, and then it builds a formula, CW, and it was the indicator variable that would create this formula, and then do our standard logistic regression that most of us are probably familiar with. And then it did a lot of cleaning of the data, and then it would output a little table here with, you know, if the factor was age, you could see the age has a slightly positive influence with a very small p-value. So, univariately, we conclude that age is an impact. Maybe the older you are, the less you care about the rest of your life, I don't know what to make of that. But when we look at this for all the factors of interest, I just mapped over this, it took about two minutes, which is frustratingly slow for 70 factors. We end up with this data. And you can see univariately, a lot of factors are significant. So what we ultimately do is we'll take all these into our first preliminary effects model where we want to start testing interactions and iteratively removing insignificant factors. We draw a line eventually. It looks like I drew the line right about here at 0.1 p-value, and we didn't consider the rest of those for the rest of this analysis. I'm not gonna go through each step because it was quite tedious. But earlier I alluded that there might be some sort of relationship. Like what if somebody was in combat and a career in the military, those two might have some sort of like multiplicative impact, or what if somebody was in combat, but not the military career. When you build a model in R, you can create all these indicator variables, but I wanted to be a little more specific about it. So I created my own variable that combined these three, handling all of these factors together. And as terrible and as mind-numbing as it is, I labeled them 0, 0, 0, 1, 0. Like it's awful, but that's why I have this little chart here to help us understand what I did. And you'll notice that these are mutually exclusive. You can't have been in combat and been in the military, but not went to combat. Does that make sense? That's why those don't overlap. When we slowly get down to the final model, we'll get to the punchline, where we can see the impacts of the variable adventures. And it does appear as such that each one of these, with the experience of 101, which you know what that means, I'll pull up a chart in a second, of 101, you do have a slightly positive effect on whether or not your country will be in a military dispute while that person is a leader. Now, I've done my best to help us see these impacts in the next chart. So don't award me upfront with the tough D. Oh, that's not it. That's the, sorry, I'm ahead of myself. This is Jared's favorite co-flaut, early subversion of it, where we can look at the uncertainty on each of these and you can see the wide range we have on each one of our factors. But the more interesting one I wanna talk about is this. What I've done here is I've split, I've faceted it out the types of experience that all of our presidents have had. And the top left are just the presidents who've got no military experience whatsoever. And the top right, these are the presidents that were both in combat and had an entire career in the military. These are the presidents who were in combat but did not have a career in the military. And these are the presidents who were in the military but didn't end up in combat but also didn't serve an entire career. All right. Now the second thing to look at is the green or the blue dot for each one of these people is their coefficient or the prediction of whether or not they would have taken their country to war with all their other factors. So we showed some before, like were they a doctor, what was their background? This is their prediction. And then the green dot is once you add in the coefficient, the impact of their military experience that they have. So you can see some of these presidents went from Truman, oh not so much, Truman, he definitely let our country in war, right? So he's over here. The reason there's no line with these individuals is because they were not, that's the baseline factor. There was no, like addition as part of the intercept. So we can see that these factors do have some type of influence on aggregate over whether or not their country would be at war. Or not, I don't wanna say at war, but it would be in a militarized conflict while they were in charge. What else is interesting? We might be interested in some of the outliers, the logistic DX package is a great package for bringing up a number of outliers such as Delta Beta hat. And just to give an idea on what this might be, Beta hat is the coefficient for that individual, for the factor of interest. The idea is if they were removed from the data set, what would the change be to Beta hat? That's the idea behind most of these metrics. And terrible to look at, I know it looks like a constellation we could all, you know, imagine that this is what the night sky looks like. I bring to, you know, your attention to some of the people that we looked at before the initial ones, they don't seem to be outliers. There were a few who I believe every indicator said that they wouldn't go to war, but their country was. Or every indicator said they would be at war and their country wasn't. So that's why we have some outliers like that. Other things to note, what I don't wanna do is end up, like I said, a New York Times, it's why Jared usually asked me, or those who are in the military when they present to be in uniform. I said, Jared, I can't be here. A guy in the army talking about how we might all be a potential warmonger is that is not at all what I'm saying at all. So, and I didn't want it to look as if an army guy was saying it. What I am saying though, is as we look through the data, there were certainly some things you could debate about. Like what is a military or militarized dispute? Like what is a conflict? So I didn't assemble the data. I'm not throwing my hands up, but what I am saying is some things are up for debate on how we would categorize some of these factors. Other things is there's certainly, we can't draw causality of this. We can't say that because a president or a world leader is has a military experience that they are more likely to have their country at war. Because oftentimes one might think, if we are at war, we might want to elect a president with some type of military experience so that they have some sort of intuition about the proper way to employ it, or that we wouldn't want to do it, or they might be more hesitant about the way it's used. So let's not call causality out of this. And I only showed US presidents on here. Notice, I should point out that of the presidents through in the data set, every single one of them except for Garfield was categorized as having been a wartime president. So that probably says a little bit more about our country than it does about the presidents in charge of it that we just happen to be at conflict a lot. I appreciate that. I was afraid this might last. I might run out of slides, but it looks like I didn't. If you have questions, I'm happy to talk about it. Jared, thanks for the opportunity. I appreciate it. Thank you all."}, {"Year": 2023, "Speaker": "Gwynn Gebeyehu", "Title": "The R Project Sprint", "Abstract": "R is maintained by a group of 20 volunteers called the R Core Team who are responsible for maintaining and developing R. Without these volunteers, R could cease to exist. The purpose of the R Project Sprint was to encourage collaboration between novice and experienced R developers. This talk will give an overview of the sprint, including roles of the R Core Team, patches submitted throughout the duration of the sprint, and outline future work for this project.", "VideoURL": "https://youtu.be/_pb5Xa-x85E", "id0": "2023_13", "transcript": "All right. Our next speaker spent 15 years living in New Zealand. The birthplace of our. So everyone, please welcome when. I'm excited to be here and I'm super excited to talk about the art project sprint. Does anyone know what the art project sprint is? No. Okay. Good. I'm so glad you came to my top. I can tell you what the art project sprint is. So let me just start by asking does anyone have any idea how old are is. Oh, in the back. 20 years. Right. Yeah. Well, what was it? It was, it was, it was. It was launched, I guess, in the year 2020. So it is about 20 years old. But the coding, any guesses on how old the coding might be. Yes. I know it's probably there. Well, you're very close. You're very close. But the people who started coding and are. So I happen to know. Ross, who was one of the people who invented our. They started going to our in 1987. So this is a really old language. Right. It's been around for a really, really long time. And in fact, this is the email that Rossy, HECCA sent from Auckland from the Zealand. Where he's saying, you know, this is the R core team. Right. So, so it's been around a really long time. And of course, that means what? That the R core team is guess what? It's getting old. It's getting old. It's a whole bummer. The R core team is getting old. And why should we hear the R core team is getting old? Because everything that we've done that we've talked about today is based on what these 20 people in the world who are in charge of our core and all of our programming language. And they're getting old. So this is what the problem that the R project sprint addresses and the our contributors working group is also addressing is how do we transfer all of this huge amount of knowledge that they've been acquiring for 40 years into people who might we expect to live longer. Right. So this is the issue that we're addressing. Okay. So what does base are look like? Well, this is something that Paul mural did. Paul mural is one of our core team members and he did this in JavaScript. And let me see if I can get this to point any guesses. Let's see. Oh, no, that's not what I wanted to do. But anyway, let's see. Okay. So here we have all of these things. This is the, this is actually the link to the slide, but this is the library and the source code and all this documentation and tests and packages. And that stuff there. So this is all of base are but before we go into talking a little bit about the our project. I just wanted to introduce you to the R core team. The first people, these are the people that invented it the University of Auckland in the 90s actually going back to 1987. Robert gentlemen and Ross, he, but Martin, we can really think because he's the one that convinced them to make it open source. Right. If we didn't have Martin, we wouldn't, it wouldn't have been open source. It would have been something you'd have to purchase. And so, Simon urban at he's someone back there was talking about s to he's the only person to have worked in Bell labs where s was invented. And at the University of Auckland where I was invented and he's in charge of the apple gooey so anytime apple changes their software. He updates are to teach it how to interact with that software. Thomas lovely my PhD supervisor he did surveys and he also is in charge of statistics. This is Martin plumber. He also does statistics. You know, they just work on this, the, some of the stats, Patrick packages and make sure that they're correct. And so I'm saying to the science, he is. He was in charge of coach code completion. So, so anytime you type something into our there's kind of this code completion, and that is how he got access to the our source code and became our 14 member. Calibera, he's kind of the newest R-Quarantine member. Of course, there's more, but he is the one that has this huge server and he spins up a new version of R before they release it to make sure and does all the text to make sure that it's not going to, that's not going to break anything important, right? And then of course, we have more and more people. One of the other people I just alluded to earlier was Paul Mural and he only touches these green things here. He touches grid, he touches graphics and he touches the graphic devices, right? So this to me was a huge, you know, something I was super excited about hearing. The reason for that is because I often feel terrible that I can't do the whole thing. But the truth is that not even people who are members of the R-Quarantine can do the whole thing, right? They just have their little pockets that they deal with. And so that's really wonderful. It's just like such a relief for me. Okay, so what we're going to talk about next, it's like who has things that really bother them about R? Does anyone have anything that really bothers them about R? What's your thing? You don't like the dictionary? Okay, what else? Oh my goodness. What else? Okay, so here's one thing that bothers the hell out of me. Oh, vectorized. You know at the very beginning of your code, you have to have library, library, library, library, library, library, library, library. You can't just have library and in all the names of all the packages that you want to introduce. It just drives me so easy, right? So this was one bug. This could be a bug. You could call it a bug in R. We want to be able to vectorize these, you know, library. We want to put a vector in. We want to put in like load all these packages and then just load it in one thing and it's supposed to have a library, library, library, library, library, library, library, library, library, library, library, library, library, library, library, library, library, it's just a big pain. Okay, so this is one problem that we have. But what do you do if you want to tell people about, you know, a problem that you have in all this is what you do. You have this, there's this big bug villa list. I copy this on Monday. Can you believe that there's there this many things wrong with R? Like, isn't that unbelievable to me? That's unbelievable. But of course, we are part of a completely untraditional community. And so there's a much different way to do that, which is just to put it on social media. And I am still thinking this thing about R. Can you update it? And so that's what this person did here. He's in, uh, where will he? He's in Australia, and he went to the R source code. And he said, I really want Regidit hex hex codes in the R source code. This is where it needs to be updated. Does this bother anyone else? Now, does it bother anyone else or not? You never thought about it. Well, this is someone who thought about it. And he was like, I really want to update it. And this is where it needs to be updated. Okay. So in August, we got together. And guess what we did? We updated the darn code, right? Because what he did, and the R source code is available on the web. I can tell you where we're looking for Winston Chang has it on his, there's a mirror there too. But he said, look, this page line 365 of the R source code, all you need to do is update this. And then what we did here, line 365, we put it in, right? So there you go. But not only do you need to update the source code, what else do you need to update? Any guesses? Documentation, good work. Next, we update the documentation right here, right? Because you saw there was a documentation all the time. But then what do you need to do next after you do the documentation? Last thing, exactly. So then we write the test. So that's our testing here, right? Right? Because this is a story I heard. Paul Mural, he put in something with our, what was I going to say? He updated grid one. And you know what he did? He broke because grid is the basis for GG plot. He broke a thousand down. So you have to be damn careful with your code if you're going to be doing this kind of stuff, right? You don't want to do that. Okay, so here we go. We've updated it. And guess what? We have one happy user in the room. I am so happy it took you three days and you updated my bug in R. And this is so wonderful, right? How cool is that? I have never seen advanced in software based on a social media post, but this is though this is like my very first case study and I'm just so happy about it. Okay, so other bugs that we dealt with. And of course here we have here before we do that, let me just talk about my friend, Amadou. So this is Amadou from Sanagal. He was one of the members of the R project sprint. This is Cerenji Carr and this is you can see Shannon, Hilebi something. Anyway, this is why Jerry doesn't introduce last name. He just says, anyway, so this is Amadou and he's been winning his first patch, right? So if you fix R, you should have been a patch and he just he just submitted his first patch. Isn't that wonderful, right? So other things that we wanted to update, this is another one, alpha mask. So what does the alpha do? We're with the alpha value for a GD plot who remembers. Oh, right? And so here we have we have opacity, Apple updated their software. So the courts is now now you can include opacity and we wrote coding for this. Here we go. You are seeing the very first for our print, our thing that has the opacity in the courts right here live on screen. How exciting is that? Thank you. Thank you. So it's right there. And then here's some other the other work that a whole bunch of people did, which is the translation dashboard. So let me show you the translations dashboard, which is really, really amazing. Here they are translating R into like 24 different languages so that it's not only people that speak English. So you don't have this extra barrier of learning, right? Here's the web late. Let me show you and I think I'm gonna come on, web late. Right. And so here, this is how much we have updated, right? So obviously, we're not doing very well in Albanian, but our French and our German is doing well. Any of you who speak any other languages, if you want to participate in this, you absolutely can. So here we have Turkish, here we have Polish, here we have Norwegian, and then, you know, this is how much is translated versus how many people actually speak the language in the world. This is how many translators we have. And so they spent a lot of time working on translating this. And in fact, one of the other things that they're working on is going to be translating translating more of the documentation, right? Because some of the documentation that's been there. Well, the other thing I wanted to tell you about is here we all are, oops, I'm gonna go back to the slides. Not what I wanted to show you. Here. Oh, and let me go back to the slide. Okay. And then here we go here. And here's a picture of all of us. I hate having my picture taken. So I'm heading back there with my last one. Here is Heather Turner, who organized it all. This is Luke Tierney. He's also an art court team member. I'm sure you could like pick out the art court team members, right? I think we're about as well like a bi-mortal distribution underneath. And then here's Thomas Lumley and all of us. Let me see if I can slide on with you, who is submitting the bug, I keep over here in the corner there. So there we are. And if you want to participate in updating our and keeping it up today, and if you want to learn C, which is one of the R's learning C, we spent a lot of time learning C together as a group before we got together, you can join this R contributors working group, which is on MeetUp, MeetUp, and you can email Heather Turner. You can get a bugzilla account. You can post on social media about your complaints. And your issues. And if you're lucky, then someone might hear you. So anyway, thank you very much for coming to my TED Talk. Let's make sure we can keep our lives kind of indefinitely. We don't want to do fine with some of these people that know everything. So anyway, talk to you later."}, {"Year": 2023, "Speaker": "Zach Terner", "Title": "Preparing for the Future: How Climate Change May Affect Food Growth", "Abstract": "In this study we examined how climate change may affect food growth in the coming years. To do so, we conducted a historical analysis to understand what weather patterns over the years 2000-2019 led to the largest errors in crop simulations. We focused on simulated and harvested amounts of winter wheat 104 in France, and used historical weather data from the NASA POWER API, as well as information on soil depth, to explain errors in crop simulations. We took a functional data analysis (FDA) approach using the refund package in R to build a longitudinal mixed effects regression model with functional covariates. The results showed that specific changes in weather patterns at different times of the year can explain a large proportion (65% or more) of the errors in crop simulation. This analysis was part of a larger project meant to understand and anticipate how climate change may affect food security, possibly leading to violent conflict.", "VideoURL": "https://youtu.be/tMTJUEAoa7c", "id0": "2023_14", "transcript": "our next and final speaker for the day. No pressure on knocking it out of the park to leave people on a good note. He has an evil twin lurking around somewhere. Everyone, please welcome the good twin, Zach. My name is Zach. I work at MITRE. I'm going to be talking about preparing for the future, how climate change can affect food growth. So this is part of an old project called ORAC, which is anticipatory understanding our resilient agriculture to climate. This project is pretty wide scope in nature. It had about, I think, 12 people on it. In general, the idea is climate change can mean to changes in food growth, food growth, like those changes clean to food insecurity, and then food insecurity creates a violent conflict. So the whole project itself was basically how do we understand what kind of changes climate change might cause that lead to global conflict? This included a variety of things, including identifying wheat fields using remote sensing, predicting crop production under various climate scenarios, modeling grain distribution to different areas in regions and locations, suggesting courses in action to improve food insecurity, and it actually resulted in a nice dashboard, which was great, and you could visualize all these things, and it was really cool. I'm only going to talk about a little bit of that today, because this isn't a conference about OR, and that's the statistics portion of it. So it's just predicting crop production under various climate scenarios, and determining what weather patterns are associated with increased crop simulation error in France. So as Jira said this morning, this is like an R and friends group. So today our friends are Python and our friends are GIS. So we used Wofast, which is a crop growth model. It stands for, I think, World Food Studies. You can Google it online. It's open source. And that was wrapped in Python. And the idea was we want to simulate crop production in France. Compare that to ground truth. Understand where did the Wofast model get it wrong, and why did the model get it wrong? And basically associate changes in climate to changes in food production. So here's a little bit of detail on PCSC and Wofast. Like this just generates crop production different geographic areas. Models run for each grid cell. You get the crop yield, you multiply it by the overall area of the region. You get total crop for that region. Pretty straightforward. For weather data, we use the NASA Power API. It's also built into PCSC and used Python. And this gives us a whole bunch of weather inputs, temperature, rain, sunlight, I think humidity, dew point, other things, which we all fed into the crop simulation model to generate crop production output. This only gives us historical data. And that's all we're going to talk about today, because this was just a historical analysis on how weather could affect food growth. The idea though was to use this for the future. And so Mark's name was a weather production option we identified as giving like decent reliable future weather estimates. And that also would give a lot of variables for PCSC. So this is just for the general project, we do have like an idea of future weather production software that could be used. So this is really the more technical portion of the talk. And that's like the next few slides. But the idea was we want to understand how does the world cost projects crop production trends and like can be used reliably into the future. The way we assessed like how reliable is the future is understandably to use it for the past. And so we ended up building a regression model, actually a few different regression models that like basically build out a relationship between historical weather patterns over 20 years in France and the world cost crop production model. This regression model is built at two different levels of geography. France has broken into regions, which there are like 13 or 14 regions and about 93 departments. And so we have analysis for both at the region level and the department level. And basically to identify what kind of weather patterns are associated with all kinds of errors. Okay, so to recap our data sources, we have weather from the NASA power API. We have crop production from PCSC and Wilkost. We use winter wheat 104 that was part of a preliminary analysis to try to figure out like which wheat strain could be reliably used for this analysis. Winter wheat 104 was a good option. We have ground truth data from France, which is really critical for this whole study. We use soil depth because understandably how much wheat grows and how reliably it grows depends a lot on the soil and the land in which you plant it. And we use a 20 year analysis. So this is 2019, I'm admitting 2016. We omit 2016 because it's just really an outlier year. And one of the themes of this talk is going to be like how do outliers change things. In 2016, I think basically all of the France crop production, crop projection models got it wrong in terms of how much wheat was produced. So we emit 2016 from everything you see going forward. But the analysis itself is still 19 years, which feels pretty good. Okay. To build this regression model, we used functional data analysis. And if you're not like stat savvy, you maybe haven't heard of it. And if you are a stat savvy, you still maybe haven't heard of it. It's a very niche area. But in functional data analysis, your data themselves represent by curves instead of like points. A classic example in this plot is temperature data. So in this plot on the left, you have raw temperature data at 35 Canadian weather stations. And just the averages are plotted. But the goal of this graph is to show you that temperature itself is continuous. But it's not like someone's height or weight, which are continuing to basically stay the same or can be summarized in a scalar value. Temperature itself is a function. And what we're interested in is understanding what about these temperature functions and what about the sunlight functions and their precipitation functions. What kind of changes in yearly weather are associated with changes in crop production? So this analysis is going to have functional data in it. It'll be a little bit technical here. This is the most technical slide. I apologize for the integrals. I just thought they were helpful. But so in functional data analysis, either your response variable, your predictor variable, or both are functions. So here, our response variable is just percent error. For every year we did, we simulated how much crop is being produced. We compared it to the ground truth. We computed a percent error. We have that for all 19 years. We have that for every single region, every single department in France. This is a longitudinal model. And so we use data from over 20 years, scalar, percent error. Our functions here are temperature, precipitation, and sunlight, since those are the three main ingredients that go into cruising crops and agriculture. And this is using the LPA-far function and the refund package in R. So a nice shout out to the refund package. It's regression for functional data. They actually make this a lot easier than it really should be because it's complicated math. And at the bottom, for those of you who are interested in this is the regression formula. Alpha i is a different intercept for every different region. Basically, it says we want to account for the fact that different regions in France are a little different. And so for each of those, we'll fit one model, we'll fit them each different intercepts. And we use temperature, precipitation, and irradiance, which is sunlight, for every year in every region. And we compute percent error and kind of see how do these two correlate or not correlate, but how are they associated with each other. So this was an interesting finding we thought, which was at the regional level, so at the level of 13 regions in France, if you regress the error and crop production on just those three covariates, you get a pretty decent model. You are just an R squared is 0.68, which I know there's an R squared talk coming up. Basically, 68% of the error can be explained by changes in main and sunlight, which I thought was really interesting because it's saying that the simulation models themselves are pretty good, except when the way they changes them. And so what changes and whether are causing our crop production models to be wrong, or to be off more than we thought, and those are changes in main and sunlight. AIC also lowers better. The main and sunlight model for this regional model had was, that was the lowest for any and sunlight. And so just the key takeaway is that main and sunlight are important factors to consider when understanding low-cost error. The other thing to note about the slide is you can't really see it, but this image was made in GIS, GIS is our friend. The color is on the slide behind France, really behind Spain and the other countries. It was an indicate soil depth, and so pink is very low soil depth. So we excluded those three regions in red, which had a lot of low soil depth regions. Those also aren't really large producers of wheat, and so it wasn't so critical to include them. But excluding those from the analysis and also the island really improved the results. In general, this kind of modeling is a little bit sensitive to outliers, and I'll show that in more detail soon. But this by itself, I thought was actually pretty good. I was not expecting an R squared of 68, or 0.68. We also wanted to validate this at the department level to kind of see what does this look like on a more granular scale. And so this was a three-step process versus we excluded the departments from the regions that we excluded also to kind of keep the apples to apples-ish. We removed two more, which had really bad outliers. And then we also removed a bunch more, which were also bad outliers. And I'm not advocating as like, removal of outliers as a data analysis method. That would be insincere and wrong, but I do want to say sometimes it's okay to remove outliers kind of as an exploratory phase or to gain understanding of the data set. That's what we did here. So you can see this is pretty much the same regression minus, I guess, one department here at the regional level, but now it's at the department level. At the department level, we have a lot more granular data. We have more regions with outliers. And this map itself, the results are not as good. Like 12% R squared is nowhere near as good as 0.68. Because we're using a mixed effects model, we can do some fun outlier comparisons. And what's going on here is that you got, we have at least two outliers, two outlying departments that are really screwing everything up. Like everything else, the residuals are pretty good. It means zero. They're outliers aren't so bad. But here, everything is just kind of way out there. So out of curiosity, we kind of remove those, solid, how good would look without those. And looked a little bit better, honestly, but it still was really not so great. Now it's rain temperature in sunlight that have the best R squared, but 0.16 is still nowhere near as good as 0.68. So we keep going. And now we have the 71 departments over 19 years. And you can see once we took out those two, there is still a bunch of outlier years up here. Those also kind of cause a bunch of issues to do this analysis. Again, we maybe would have taken out just the individual years that were problematic for those departments. But we just took out all the departments because that was easier to do in a data sense. And now everything looks beautiful with the exception of the fact that we took out maybe a third of the departments in France. But at least the results look better. And so I'm not, I don't want to say like, or remove outliers, everything gets better. I do want to say remove outliers. And maybe you'll get understanding of what's going on in these other places and identifying outliers is important. I think the regional level regression was actually pretty solid 0.68, taking out low soil depth. This is much more cherry picky. And cherry picking is like really not healthy enough, something we should advocate for. But it's kind of good to know that there are just these few departments in France that are really messing everything up. And when you look at the big bread basket regions, which is what we're interested in, like in particular on the map, the darker regions produce more wheat. And you can see like we're including most of the darker regions on the map. Then we actually get a model that really explains what errors are happening in the crop production simulation. Okay. And when you look at the residuals here, they certainly look really good. And this is what things should look like things centered around zero, no terrible outliers every which way. So, like in short, we wanted to see could well-class be used to predict future crop production trends. And we showed that rain and sunlight are the key weather factors, at least in this model, in this regression of explaining historical error error in the crop production. One other thing about functional data analysis, which I didn't include in these slides for a number of reasons is you can actually see like when in the year things are bad or not things are bad. But not only is like if you do a traditional regression, you really take average sunlight, take average rain, take average temperature and just use those. When you include the whole data from the whole year, you can say like, okay, from January to August, we care more about temperature. And in the spring, we care about we're more about changes in rain. So it really helps narrow down the analysis in terms of what times during the year and what weather factors during the year are more responsible for causing issues in crop production. Okay. And so if you were to do this kind of crop production modeling again, you need a representative crop strain model. We use winter wheat 104. We used good weather data from NASA Power API. The soil depth map helps us figure out what regions and France don't really grow wheat and which ones we not really care so much about because we're more just in the bread baskets and you need a valid simulator which we had in PCSC a little fast and also marks him moving forward. Actually, the last talk and that's five minutes early. So this was funded by MITRE's independent research and development program. I have an email address. I also have a Twitter account. You're welcome. And I have a LinkedIn which is under my name. You're welcome to reach out for any of those. If you're interested in any of those things. Thank you guys."}, {"Year": 2022, "Speaker": "Shirley Han", "Title": "Using Quarto to Write and Update Reports", "Abstract": "Are you responsible for producing a report that uses the same analyses from one iteration to the next? Are you spending your time manually updating tables, figures, and report text to reflect changes in data? This session will focus on how you can use Quarto, the next generation of RMarkdown, to write and update reports. Learn how Quarto can help facilitate your workflow.", "VideoURL": "https://www.youtube.com/watch?v=GnV9H0z5UzI", "id0": "2022_01", "transcript": "Our first speaker learned two very important life skills during her PhD program. How to drive a stick shift and how to siphon gas of a hose. I'm a little worried about that one. Please everyone, please welcome Shirley. Today, I'm going to talk to you guys about writing and updating reports in Cordo. My name is Shirley Hahn. I'm an Assistant Director at the Science and Technology Policy Institute. If you went to the workshop yesterday, my colleague Will Done spoke there and he has another presentation tomorrow as well. All right, so let's just jump in. So just a quick poll. How many individuals here have heard of Cordo? Okay, great. Well, I'm not sure how helpful this product will be, but for those who might be new, I'm hoping to introduce a new tool for you and a new way of thinking about how to facilitate report writing or any kind of documents for your work. Okay, so a few things to know about Cordo. It is the next generation of R Markdown. So how many individuals have worked with R Markdown before? Okay, that's fantastic. So if you're already working with R Markdown and you haven't switched over to Cordo, this might be the next logical best thing, just because a lot of the features are there to make things a little bit more user-friendly. And it's pretty. Okay, so first things first. We can create reproducible documents in Cordo for a variety of different formats, so including HTML, PDFs, Word docs. And you can also do things like presentations and books and blogs and websites and so forth. So you don't have to use Cordo just for documents. You can use it for a variety of other presentations and formats as well. And for this particular talk, I'm going to be speaking about how we can use Cordo for recurring documents. So things that you're going to be doing either over and over again or on an annual basis, something where you don't want to have to manually update. Because if you have done that before, you know it is such a huge pain, I would say. And it's quite time consuming. All right, so a few things to introduce you to about Cordo. If you haven't seen it before, there's actually two interfaces. So let's see if this works. Can you see my? Let's see if I have a pointer. I'm not really sure I do. Okay, so first things first is it is a .qmd file extension. So if you're already working in R Markdown or R, you can actually just change the file extension to .qmd and it will automatically convert it to a Cordo document. So pretty easy things there. You can just do that manually. There's actually two interfaces in Cordo. One on the source side, which is what's presented here. You use Markdown syntax. You write how you normally would. You put in line codes there, your chunks and so forth. And this probably looks pretty standard for anybody who has worked with Markdown. For those who haven't and would like to still jump into Cordo, there's a visual editor. So this is much more user friendly. If you want to begin and you're not quite sure how to make things in a tile, or bold or put in a header or how to insert a figure properly using code, you can go to the visual editor and you can start there. You can see that there are some in the red highlighted boxes. You have formatting options for a bold, a tile, hyperlinks. You can choose the heading style that you want. You can insert different types of bullets and numbers and lists and so forth. You can also look at the inserts tab and you have many options for tables and figures. So if you have something that you want to go ahead and put in, you can go through here and drag and select the things that you want to add into your document or report. So much easier for people who might be new to Cordo or to Markdown, you can learn from here and you can go back and forth between the visual and the source editor to see what the code that Cordo put in for you looks like so you can learn both ways. Okay, so today I'm actually going to use one of the evaluations that we did for an example on how Cordo can be used to help us facilitate this report writing. So, to be my organization was asked to do an evaluation of the National Institutes of Health Transformative Research Award anonymized review process. So there's a lot of acronyms there. So this is a something that's been happening since 2009 and they wanted to change up the grant review process. So in the regular grant review process, the individuals, your institutions, your collaborators are all of that information is made public to the reviewers themselves so that they can assess both you as a PI as well as the proposal that you are you're proposing here. They decided to try and see if an anonymized review process in which all of those informations are withheld from the reviewers might be a more fair and unbiased way to do the grant review and hopefully lead to increases in both applicant as well as awardee diversity. So, Stippy, we were asked to do this during the first year of the implementation in FY 2021. The report is just made public. Earlier on this year, we provided the second year report just to the sponsor a few weeks ago and hopefully that will be up soon as well. But overall, it was a 200 page report. It was very, very data heavy. Most of the report has lots of tables and figures and the text is very, very heavy. So, and we were contracted to do this once a year for the first three years. So, FY 21 all the way through FY 23. All right, before I kind of jump into why we're going to use Cordo aside from the fact that it's a 200 page report that we have to update annually. I also want to point out that just like many other reports, right, this uses multiple data sources. The review process on this right hand figure is actually a three phase process where each phase has different reviewers. They're asked to assess the application in different parts of the application. And so, what we're going to see here is we conducted a variety of surveys of applicants of the administrative staff of different kinds of reviewers for editorial board as well as technical reviewers and asked them what their process was for applicants if they were able to anonymize their application correctly for everybody else. Were they able to actually review and judge the proposal based on an anonymized application? We also have demographic information from administrative records given to us by NIH. So, we have lots of data coming in, many different sources, and we're trying to provide a cohesive story with all of this. All right. So, I'm going to give one, one, two and a half sentence example. So, this is the first result from our applicant survey. And what we see here is we have, you know, simple things like a response rate. We have 105 of 176 applicants that were listed as the PI, the percent of those, and 22 of the 119 co-applicants completed the survey and some additional information about who responded and so forth. Everything that's in a red box will have to be updated for the next year or the year after, right? There's, that's a lot of things in a red box for just two and a half sentences. And before, I think most of us are doing this manually. So, we run our analysis, we run our code, and then somebody that gracious person on our team has the task of you go in and you find where all of these places are and you update all of these numbers manually. So, that's not great, right? It's, one, it's a lot of numbers to update. It's very easy to make a mistake. You might forget halfway, well, did I update this number? Did I update all of the numbers in this sentence? There's just a lot of chances for error. It's also very time consuming. Again, this was a 200 page report. You're talking about over 50% of this has to be updated every year. And it's also really difficult to maintain internal consistency of the document if multiple people are trying to update different parts of the report. All right, so what if there is a better way and there is. So, how can you create a reproducible, updateable report? So, are you already doing some of these, which I would imagine most of most of you guys are, you're already doing all of your data cleaning and wrangling in some form of portal supported language for this presentation. I'm using R. You already have your functions for your data analyses and you're already creating tables and figures in R or you already know how to do that. And you are already using R Markdown. The next logical step might be to move on to Cordo. All right, so starting a dark Cordo document. If you're familiar with this, I'll breeze through a little bit of this since most of you have already experienced with R Markdown. We have our YAML header. We can indicate what format we want. And then we use Markdown syntax here. In this particular document, I am reading all of my functions, all my data, clean data, and so forth, so that it is already ready for use for me to call in my report writing. So, we're going to use this simple sentence as an example of how to do this in Cordo. Again, a lot of this might look familiar if you're already familiar with Markdown. So, here we have the sentence 86 of 128 applicants listed at the contact PI completed the survey and of these 86 combined responses, 70 were new and 60 were repeat. So, what we're going to do in R here is we're going to read our survey data in. The survey is formatted in a way such that each respondent is one row of data. We're going to take the number of rows for that data frame that we have. The answer would be 86 in R. And then we're actually going to turn this into words. I was very, very happy to find that there were things already out there that was doing this. So, we're going to use the English package and the English function to take that 86, that numeric number and turn it into words. It is actually still a numeric number. Even after you use the English function, that is why we also have to turn it into characters. And then we're going to capitalize that first one to get 86. So, lots of ways to make sure that we can implement all of this so that we don't have to write anything out or manually update. All right. So, not everything we are calculating, right? Some things we need to hard code in because those are just the parameters for those years. So, in this instance, we have a total of 128 applicants. We didn't know that from the survey. That was just the number that was given to us from the sponsors. We're hard coding that in as the total number of applicants. We also have to update the years of the reports. So, we're in this particular example, we're using FY 22, and the previous year as referring to the FY 2021. So, looking at the second red box here, the percent of respondents to this survey, we would do our normal calculations in R with the number of survey responses over the total applicants. We would get something like 0.672, right? In our outputs, and then we would, we can use paste and pretty it up for the actual report writing, put in the parentheses, simplify it to one decimal place, put in the percentage signs, and so forth. All right. Other simple calculations. So, all of this should be pretty easy. You probably already have all of this information as you're writing your data analysis anyways. Now, we're just kind of putting in together, linking things together so that you can insert them into the report where you need them to. So, here we have the number of new applicants, and that's just going to be the number of survey responses minus the number of repeat applicants, where repeat applicants was a subset of individuals who answered its particular way for a survey question, and then the number of repeat applicants is just the number of rows there. All right. So, what does this look like in actual Cordo? So, first, we can put in the headers. So, things match the way that we want them to for results, a subheader for response rate, and you can see that these are, we have the inline R codes that are the exact same as what I had presented in the previous slides, but this will generate exactly this, these two sentences. So, it looks like quite a lot of gobble group, if you're looking at the portal part itself, but I think most of us already have a lot of this coded into our data analysis, right? It's just putting it into inline codes so that it is next to the words that you already have, or that you want to use. All right. A few other special things to point out. Again, some of these already exist in our markdown. Cordo makes it slightly easier because it also now allows for cross referencing. So, adding footnotes. So, in this particular instance, we have referencing a webinar that NIH hosted to answer questions from applicants, and we want to put in a footnote where that webinar is available for hyperlink. So, using the format of how to put in a footnote there. Also, to put in figure captions. So, down at the bottom, we see we have our figure caption, the location of the figure itself in a folder I called images, and then the last little bit there, that entire thing in curly brackets, the hashtag figure for noting that it is actually a figure, and the last little bit web program officer, what we have there is the ID tag for that particular figure so that you can reference it in your report anytime that you need to, which is what you see up here when you say at fig, again referencing that you're talking about a figure that you just created, and which figure you're talking about. And the nice thing to know about this is, notice that your figure actually comes after your cross referencing. So, it doesn't have to occur in a, yeah, it doesn't have to occur before when you actually cross reference it in a report. All right, so this is what it looks like. We have our footnote automatically show up there. It will automatically number itself in the sequential order, so you don't have to worry about that. Same with your figures as well as tables, it will sequentially order itself, and then we can also see that the cross reference shows up, and it's also hyperlinked so that it can automatically take you to the figure or table that you're referring to. And lastly, for footnotes, even though it's not shown on this slide, it will show up at whatever format that you're working with. So at the end of an HTML page, or the bottom of a word or PDF page, so it is already formatted for all of those. Okay, and so that was a lot, but I think a lot of it probably already looks familiar. So when does writing a report in Porto make sense? I have an example here of the NSF Science and Engineering Indicators reports. You can see here that it's a biennial report every two years. Something like this would be the perfect example of when you want to use something like Porto to reproduce your documents, right? You are working with data that are pretty much the same. Your data structures are getting read the same way. It's the same type of data that you're working with. It's just being updated. Your report is not that one-time report. Again, this example here is a four-time report since 2012. And you find yourself using a lot of the same checks that you've already used in your previous documents. If you're copying and pasting a lot, there's a better way to do things. So if you're finding yourself going in and manually updating things like tables and figures and those types of things, it might be better to put the upfront time in the beginning and go ahead and take the codes that you've already written and put it into Porto so that all you have to do at the end is click render and then you have your document out in front of you. And the other time that you might be interested in using this is not only if you have a complete document that you're ready to make or create, but let's say you are working with data that are being collected and your deadline, unfortunately, is super, super close to when your data is being done collected and you want to have some semblance of a report by that time. So as your data is still being collected or finalized, you can go ahead and do some of these things, right? You know probably the type of text that you want to put in and you know where that information is going to come from. You can go ahead and create a document and run whatever data that you have so that when you have your final clean data, you can go ahead and hit that render and have a pretty much finalized document or report at the tip of your hands. Okay and with that, I think I will stop. Thank you so much. If you have any questions, please feel free to find me or email me at my email address and I hope this helps you think about whether you're able to do anything in Porto to facilitate your workload. Thank you."}, {"Year": 2022, "Speaker": "David Meza", "Title": "The RStudio Ecosystem as a Critical Part of NASA Analytics Capabilities", "Abstract": "This session will describe how NASA People Analytics incorporates the RStudio/Posit ecosystem into our analytic workflow. We will explore data ingestion, API creation with plumber, MLOPS with Vetiver,  apps on Connect and Tableau extensions created with R.", "VideoURL": "https://www.youtube.com/watch?v=2LDOKPw6EKk", "id0": "2022_02", "transcript": "When he was three years old, his older siblings forgot him at the park. And when they came back to get him, they found him walking home at a stoplight surrounded by a pack of dogs that were not let him cross the street. I assume that since he's here, he survived. So everyone, please welcome David. Thank you. I appreciate the time to be here today and talk to you guys about the R studio ecosystems, the critical part of NASA's analytical capabilities. And my apologies to R studio. I guess I should change that to the positive, but I've already had these slides done. So I left them alone. So let me ask really quick, how many are from a government agency. So you might relate to some of this. There's not going to be any code in this is more about the journey of how we try to create our analytical architecture and the things that I've had to go through. So first thing so hi. Usually I want to just make sure that everybody understands, you know, we're here just to have a good time. We're here to just understand what we can do within the analytical community. One of the things I do want to tell you about this presentation, this is not a presentation about our versus Python. So I'm going to announce this talk everybody's called great we're going to hear why are better than Python. This is not that kind of a talk. It is how we can use our end Python together to be able to do this within the architecture because I have a team that's made up of both our end Python developers. So, so who am I and when do I do very long sentence, you know, paragraph here basically on the head of analytics or human capital at NASA. When I first came up to headquarters back in 2019. They asked me to take a look at how we can improve our analytical architecture, make it more modern, because we had a lot of things going wrong with this within the business side of NASA. So money, you know, NASA gets a good chunk of money, you know, not as much as a lot of agencies, but more than some. Most of that money rightfully so goes to things like supporting the international space station, climate science, you know, getting a Ryan up and around the moon and back and making sure that we can get to March eventually on the business side of things where I'm at. Where I've grown up working within NASA within the OCIO now human capital I've done some work in knowledge management, you know, you're thinking about OCFO the procurement. We don't get as much money so we kind of have to do with what we have and make things work as best we can. So when I first started this journey server years ago, my laptop on desktop at that time was my development environment. We didn't have cloud resources we didn't have anything to do. And our data pipeline of course because of that was not automated. The automation was me going every morning and clicking run the script in order to run the data run the cat to do the calculations and then send that somewhere so that it could be do some more. Calculations and then back to the presentation layer and that presentation layer was basically again my laptop. I would go from meeting to meeting going here. This is what your figures look like. You can go to the next meeting here. This is what it looks like. It was really very, very. This heartening I guess at the time. Just to try to do all of this and go we've got to do better. We've got so many modern technologies out there. Why can't we do better in here. Again, it came down to resources. So what can we do to really make this work. So some of the other things we were missing of course, lack of access to authoritative data sources. Many times here's here's how the conversation would go. We're not going to do our need access to the last 10 years of people data. Great. Which data do you need? I mean, I don't know. I don't know what data you have. Can you just give me the data. Oh, no, I need to know what data you need in order for me to give it to you. I said, okay, do you have a data dictionary. What's that? Okay. All right, let's see what we could do. So over the years we finally started to work through these people. I've gotten to understand the data governance process. Understand what the data owner is what a data what the different data roles are. Got them to give us a data dictionary so that we could start getting enough. Finally, finally, this was exciting. A couple of weeks, a couple of months ago in a meeting. The data, the person that oversees all the data, IT in human capital and the data. Anything that David and his team want they can get. No questions asked. I said, thank you. So that finally was able to get that kind of a data. And for those of you who have that trouble know how hard that can be. That was the good news. The not so good news is on still working firewall issues across some of the different organizations to make sure I can even get access to that data. And of course, because of that, we didn't have any data lake or any way to store our AI ML curated data such ready for analytics. It was still grabbing the data from a SQL server or some other type of database putting it into our laptop. Now we're starting to do it more on cloud resources and doing the transformation doing it. We're slowly working through that back in 2019 with the well, actually 2020 now I guess when with the start of COVID. We realized that we needed to be able to provide data and that kind of gave us an impetus and a little bit of funding to start developing what we call the enterprise data platform. And I'll show you some architecture here that we're working through that I'm trying to promote to OCIO to say this is kind of the things that we need. What can you develop for us if we go through that. And of course through this there is no AI MO DevOps and no presentation capabilities other than my laptop and going around for meeting the meeting. Click share made a lot of each year. And when we finally started using that to be able to click into a board rather than having to show it on my laptop. So what do we need. Well, what I need is a collaborative development environment that supports both are and Python because I've got some computer scientists that are turning to data scientists that started off learning Python I've got some in for IOSychologists in life sciences who utilize are. So we've got to make sure they can talk together. And one thing that I really like about quarter to to Shirley's co presentation is the fact that you can do both are in Python in there and actually share those presentations of documents by freezing. So you can do both are in the code. So when you send it to somebody that you send your our code to somebody working on Python. It doesn't try to run their code if they don't have our on there. They can do their Python code freeze it and send it back to you. And it really works well on collaborating across teams like that. We also need a modern data pipeline. I can actually do a script put it up there on a job and let it run automatically and set it through that. We need the data lake of course for those a ml curated data sets. The more cloud resources and platform. Those are the things that was asking for. I didn't think it was a whole lot. But it's taken me several years to try to get to this point. So here's kind of what we're looking for our future state. So you can see here. We're looking here to try to get some kind of a ml dev sec ops operations here. So we've asked for. In this case we've got a they're developed what they called app that which is an application data platform, which allows us to utilize various containers and which containers in the Kubernetes cluster to be able to do all of this with with configuration management with the cloud. So it's starting to get a little bit of each is that we can share our code and go across that. The biggest thing here was this area right here. Opening up the firewalls between our authoritative data source. Our data lake and utilizing our tools in our studio. I'll direct for some no code, low code type things and then putting that through some ml dev ops with better as well as key flow over to our presentation layers, utilizing both Tableau or actually Tableau power bi now and the studio connect system with all the various capabilities there. All of that has given us a freedom to be able to utilize many different tools. I'm not locked into one thing. So those who know code can work in our and Python in SQL and some other things, those who rather not work with code can work with Alteryx. We're taking a look right now at data fusion, which is a Google on GCP as a data prep type thing. There's also Tableau prep and some other tools, but there's definitely a lot of capabilities. But what I really, but I think it's really important is this bottom layer right here. When you talk into your organization, you have to have that R and D layer. We were so far behind the technology stack. And because we weren't looking at it over time that I made it really difficult to get caught up. So I've tried to get make sure that we understand we've got a set aside funding for some type of research and development to understand and learn how that technology is changing over time and how we may be able to use some of that. Otherwise, we'll come, we'll be here five years from now trying to re up our technology and taking a long time again because we didn't prepare for what's out there and what's new. So again, going back to the development platform. Now, what I have is the R studio pro or server pro out there that allows me to do many, many things again across a different platform. So, we can do the ID, we can do Jupiter notebooks Jupiter lab Python are I don't think what I have on there is visual code, which is also now part of the R studio system to be able to do that and allows my, my developers to create so many different things in various different platform and web architecture, not only do we have the shiny, but we can do stream lead flash plotly. And now with the new creation of the shiny for Python. And what is that web. I forget the name of that now that that allows you to put that actual shiny application on somebody's web or browser send send them the shiny app without having to put on the connect server. I forget that web technology name from the acronym right now and I apologize for that is my 60 years should be on a third I forget something sometimes. So what we do now what we have with our data pipeline working through here. So now I'm able to get to my authoritative data sources and extract that data utilizing some type of container some type of script, something now that I can automate either on the, the connect server, which allows me to put up their script and set a job or through a container and a community's cluster to allow me to do the same thing. But that all allows me to take that raw data. Do some transformation within our and or Python and turn that into what we call trusted data. Now that data is taking everything we've had we've cleaned it up. We've taken our raw data cleaned it up put it in the format we need to be able to say this is the, this is what we're going to use for analysis. It's been clean it's been formatted is ready to go. Then we can start doing some modeling statistical analysis create our pipelines create everything else we need for the analysis and turn that into curated data sets that curated then becomes what we utilize to share by create by taking the curated data set I can now put a API on top of that using plumber or fast API within again within the that our studio ecosystem to allow other organizations to share our curated data set that's been clean. It's been removed them any kind of sensitive information or any PII and they can we can now share that out to other organizations again through our presentation layer. Yeah, this is still kind of new for us we're still trying to develop this with the ML ops. So I just took basically the information straight off of the website of what better or does, but it allows us to really start utilizing our, our development of our model whether we do it in a nutrition model or time to hire model, or we're taking a look at the engagement of our employees. We can now use better to create those models to keep track them to monitor them to update them again both in our end or Python. Either way, however, however our developers create them and then make them accessible and we can make them accessible through many different common or many different techniques here again whether we're allowing somebody to connect to that model through their development environment or creating an API, an plumber or some or fast API again to allow us to be able to showcase some of those things so so really starting to open up what we can do. And really for minimal cost compared to some of the other things that we're trying to do. And we're talking right now, you know, a couple million dollars we've been able to utilize the set all over environment up, which may sound like a lot, but when you're really thinking about the what the organization spend. It's really helping the business side of the institutional side develop something with minimal cost and able to maintain this very easily. So what does that allow us to do well. As we deliver our product we've got a high volume of request but ad hoc and recurrent analysis that we do. We get I get congressional calls that that from my superior saying Congress wants this we need it now so those things happen all the time so we need to be able to move quickly, get that data through that. Or we have things that we're constantly doing from our metrics from our DEI or data component from a demographics component. All of that we have to take an eye and look at how we're going to develop these things this environment allows us to work very rapidly. It allows us to think about what we're doing and repeat and reproduce the analysis we're doing because we're able to share across different different platforms, as well as across different developers. So when we deliver our products, we didn't have something like this which is more of a dash landing page. This particular landing page is a tableau landing page that actually connects to. The connect server that allows us to show our shiny applications or other applications that we have on there. It also actually leads to a shiny a landing page on the connect server. So we have to be trying to mimic this on the connect service and you have two entry points to be able to do that. This is basically broken down to the different categories. And we try to we try to make sure we curate these things so they're similar and look and feel as well as what's available to the end users. So the end users will make go to one of our dashboards or one of our products. They're going to have a place where they can give us feedback. Tell us what's good, what's wrong, what we need to do or ask for requests. They can watch a tutorial how to use the dashboard, as well as the main the important thing is look at the data dictionaries and documentation of how that analysis was done. Because too many times we get our reports and go out there and people make up their own conclusions. We need to make sure they understand the assumptions that they understand what's going on. They understand how the data was pulled where you can actually generalize this data against. Maybe it's only for a particular population not for the entire population. So we, we try to have all that information there so people can utilize this. And again, this makes it repeatable reproducible. So year after year after year we're comparing the same data across that timeframe rather than different data, which we've done in the past many times using laptops. You know, one person would create an analysis on one laptop another person on another laptop and to go with the numbers don't match up why. And we spent hours and hours trying to figure out, well, you didn't do this on when you can when you transform this data say we did. And that's why we're getting this this way we're all using the same curated data set. We are talking about the same type of data. So that helps there. So what's really cool and what I really like again I talked a little bit about plumber API plumber tab blowing and fast API. We have again within the team there's a group of both data scientists and data analysts and these data scientists and data analysts to do different things. The data analysts primarily work on business intelligence side things on the analytical kind of levels they're more looking at the descriptive and maybe some of the diagnostic type capabilities that they're doing. So they're not really interested in actually working with the algorithms developing the algorithms and doing the models and of course the data scientists are more looking across the entire platform but also the predictive and the prescriptive side of the analytics. So they're creating these models these visualizations these different capabilities that we can turn into an all a tableau extension via the API. And through tableau you can load that into the tableau analysis so now we've got both the combination of tableau visualization with some R capabilities tableau is great, but there's some limitations to it. You can't send data back and forth. So I can't update anything if I want to so through through some of these extensions we can help with some of that. So this is some of the products we've created just from our survey data taking a look at our employee engagement or federal employee viewpoint surveys on how we do this and for those of you who in the government and know that we've actually taken our. So now we're able to do this and get the data much quicker and get the results back out to our managers in a month rather than several months to the use to the low and be. So, but this is a great technology we started working with this problem about two three months to three months ago and really starting to develop it I've got some of my data scientists try to develop models that we can then post on some of these tableau sites here. But because I mentioned tableau can't do everything. The connects over really comes into play here where we can do a lot of different things on the top left hand side over here this is more of an application that walks somebody through we're able to showcase you know i've got a i've got a. Actuals that i'm looking for for my hiring how did I do you know across my plan to that that plan actually come to fruition so through a series of questions they can answer these questions it'll run through the model and show them how they're hiring plan so actuals came out or other information but but because this is a presentation component that allows you to do many different things and just dashboard the bottom left over here is a roadmap created within the connect server that's interactive and makes it makes it very easy to to see what we're working on when we're working on the network. So, I'm going to go ahead and talk about the next slide. And again, Connectable to anybody within the network top right is a book. As Shirley was talking about with portob are marked down before that but for now you can create books present this presentation is actually a quarter presentation that i'm working on here. Yeah, For example, and finally of course if we have metrics and developments down here that models will create and generate and run on a daily basis and show the information. So that entire ecosystem just really helps us understand our pipeline a lot easier. We can work through that pipeline to send the data from the authoritative raw data all the way through the presentation layer. And I should have started with it. This isn't a sales pitch, you know, for our studio or anything. It's really more about what are the capabilities you have. This is what I had to work with. I have these these capabilities in house. I've been able to add to them an interest part of the architecture. We still have other things within that architecture. We have, of course, Kubeflow. We have orchids that we're looking at, which is another pipeline. We've got all Tourex. So there's many different things that make this up. But this is a critical component that answered quite a few pieces for what we were trying to do from the data development to the data flow to the actual looking at the presentation layer and the data. And the dev in the dev ops. So it's a lot of good things that are capable there. And again, with relatively inexpensive capabilities to do that allows your organizations to work together across very different mediums. So lastly, I want to thank you. I look forward to connecting with any of you to feel free to reach out to me on LinkedIn. But I do want to leave you with one cool fact. Several years ago was at JPL before curiosity landed on March. And I was talking to some of the developers there of the Rover and they told me this story where they had asked NASA management that they could put a JPL logo on the Rover and management said no, nothing but a NASA sticker can go on there no JPL logo. So the engineers being engineers saying we're not going to take that. So what they did is on one of the wheels there in Morse code. It's a J a P and an L. So every time it runs on the Martian surface, it leaves the Morse code JPL on the Martian surface. So they got their JPL up to kudos to them for figuring out how to do that without showing up anybody but it's a I thought it was a pretty good idea what they did and show some of the things we can do at NASA. Thank you. I appreciate your time."}, {"Year": 2022, "Speaker": "Refael Lav", "Title": "Global Workforce Modeling at USAID", "Abstract": "USAID is a unique organization that operates across countries and supports areas from education to humanitarian aid. This session would focus on how USAID is now using data, a set of analytical tooling, using R, in their process of modeling the demand for workforce across the world and Washington. We would cover the business need, the technical architecture, the R involved, and the impact on the workforce.", "VideoURL": "https://www.youtube.com/watch?v=XZtgBfMxyvQ", "id0": "2022_03", "transcript": "This is his third, fourth time speaking at this conference. Spoken a lot of them. I think he missed one year. I don't know why. We'll get to the bottom of that later why he missed one year. But he hasn't taken a vacation in three years. Well, he hasn't taken more than a one-day vacation in three years. His first vacation in three years starts tomorrow. He's going to, yeah, but he's going to miss half the conference tomorrow. Good for him. Good for him. Let's give him applause for that. Yeah. But he promised to watch it virtually from the road. So everyone, please welcome Raffi. Good morning, everybody. My name is a Raffi Love. I am a lead data scientist with Deloitte, GPS Federal. We do doing a little bit also, studying local and a commercial, but mostly focusing on a advanced analytics in the federal space, crossing over all agencies. Essentially, what I'm going to talk about this morning, you're not going to learn anything new. This is really about how to take, how we took essentially something that all of you know, a little bit of clustering, a little bit of modeling, a little bit of a markdown, and really put it together to create a utility. This is actually not something that sits on your laptop, not something that only you know or end up in a PowerPoint for a decision, nothing wrong with the decision. But how do you actually impact the process? How do you impact an entire agency? How do you impact the lives of 1600 families using tools that you already know, that you already have in your positions, and doing it with tools that you know already. So, let's start by really talking about, stop moving. All right. Let's start with a little bit of a kind of like setting the stage. So, what are we trying to do? Again, I'm not representing the agency over here. This is really talk about R. But in a few words, what you are saying is trying to do, is to utilize capabilities, utilize data to make informed decision on where there is a demand for their foreign service officers, for their individual around the world, not as necessarily as a response to a crisis. So, that's like you know, Ukraine, for example, not as that, but as a general course of business, as a where they're distributed all around the world. How do I know what kind of like a piece officers do I need? Health officers and where? Can I really use a capability with the data? And then align them to the right place. Couple of terminologies that I want to make sure that we are aware of here. So, emission is a location. There is also regional, which is essentially an area. So, emission is, let's say, Laos. A backstop is a particular function within the mission, or a particular function of a job series. You can see this on the right over here, engineering, environment, education, democracy. And finally, the point over here is that we have a lot of different mechanisms. FSO, FSCS, I'm going to use that, but essentially it's foreign service officers. There's other mechanism of hiring for a position, a civil service, somebody who's permanent over there, and there's temporary as well. What's interesting about this, and you can see this at the table on the bottom right, is that we are talking over here about numbers that are being controlled by Congress. So, it's not like they can go and hire whatever they want. And essentially, there is also a limit. So, I can come and calculate that I need 10,000 people. It doesn't matter because Congress only allowed us to do 16, for example, actual 2019 was 1675. And by the way, you can see this number in 1980 over here. This was a direct result of the work that you're about to see in a second, the fact that we went up. So, again, your R tools are valuable. It's about utility. It's not just about it's cool. So, what do we do? We did this. Now, I don't expect this is kind of like the process that's not the point. But the point is that every item over here was controlled by R. So, let's start speaking for a couple of minutes around the data. So, what do we have over here in order to support our data? We have anywhere from headcount information. And by the way, the numbers that you see over here, the visualizations, those are not real numbers. That's not the point and not real titles. I changed most of them. But that's how the tables looks like. So, we have information about people. We have information about grades. We have information if the supervisor is not supervised. We have information historically where they were. So, I know who was where, at what position, and theoretically what they touched. And how much of money, and we'll talk about money in a second, they touch and what kind of information did they know. I know historical, right? So, we spent $100 million on democracy in Afghanistan. I also know information around the future. Like, what do I want to accomplish? Essentially, we want to do, this is all forward looking. What do we want to do next year? So, we have information. So, think about tables and PDFs, right? Extracting those. Not crazy sophisticated. This is not deep learning. But, not deep learning is sophisticated. It's really just regression, but whatever. We essentially have all this information to collect, as well as we have specific information about local. We'll touch on the data call in a second. That's probably one of the coolest, most impacted things that we've done over there. Essentially, how do I collect something that is local? What happened on the ground? So, let's talk for a few minutes about model approach and how that impacts the capability. And all of this, again, give me another minute and then we'll get to the R stuff. So, we're starting with a different concept. I want you to understand the concept of, you know, we have historical information, right? I want to do a prediction. I need historical information. I know how much, when, where, who, right? I have also the level of effort by implementing partner. So, that's the data call. It's all local. A dollar in Afghanistan is not the same as dollar in Greenland. It's also, if I write a check, it's a lot easier than I have to manage that, you know, dollar by dollar. So, how do I make sure that I account for the fact that there's, it matters, the level of effort matters? How do I translate that all to something I can predict? So, that's really the key over here, measurement of work. It's unit list. You knew unit that we developed for USAID. It's unit list unit. It's essentially measured that, you know, you need 400 measurement of work for a particular location. And then any sort of adjustment that we might need. It looks like this. And this is where we're going to start getting into how do you drive utility from your R capability. We're really starting from the best based model, which we have the MOW. This is where I can kind of take all the information and kind of predict that. We get to the next level, which is I know what's, I kind of starting to think about everything is local. What's the information that I know about the local? How many PhD in economics I have in a particular country that I can use to help me in economics versus I have no one and I need to bring them from the United States or I need to outsource that to somebody who doesn't know much. I'm following up. So MOW does that's model will get to the extra boost in a second and others. The base with adjustments, that's the data call markdown that is a survey that is a analysis of network. The global strategy, for example, a development priority, climate, DIA, as well as some sort of limitation. There's some rules that we have to adjust for. And then any sort of mission level calculation. And by mission level calculation is like our president shows up in a particular mission, you know, this preparation, this work that needs to be done. That does not account in anything. That's just like, you know, on top of that's a top of everything. But from a measurement of work, we need to account for that. So that is where we start making, taking all the information in the process that you saw a second ago that you don't have to remember. That's where you actually make those adjustments. And finally, okay, let's say that we predicted that you need 5,000 people. How do I go back and say, I only have 1650. I only have 1850, right, as we saw in the first slide. How do I go about that? That's when we design and this is essentially it's kind of long, but again, within your capability. So this is where you have an if-else statement, a looping. And essentially, we are accounting for the marginal benefit of the next position. And you start from the top and you start going all the way down with adjustment so they can we can decide that a particular mission, a particular area will be fully funded, for example, fully staffed. So you can adjust for that. Again, this is not super rocket science, but you can actually take and design a waterfall in your regular R, wrap it up and we'll talk in a second about how we wrapped it. And to kind of making a decision, hey, our model says that you need 10 people, but according to the waterfall, only 5 will be available considering the fact that we have limitation from Congress. All right, so what does R has to do with that? We'll start from the model. So essentially, what you see over here is that we essential, that's the left side, right? We talk about MOW and the design. This is where we started talking about actual model. So we have a model that was trained on historical data and we kind of generated the last year and compared to what we actually have. And each mission was and each backstop was modeled. And what's really interesting over here is, as you can see in the visual, this is where we got a lot of utility. This is where we actually had the buy-in. On the right side, you see the 45 degree angle. Actual versus predicted. So this is what's we, this is the, this is Gigi plot and this is the Wall Street Journal a type of visualization. So the colors and everything. But this is Gigi plot. And this is where we had like the head of HR, the head of the agency is looking at that. They're like, okay, I get it. This is where I have like actual is a little bit higher, but this is where, and we'll show in a second how we actually show the output in the same kind of matter. So visualization matter. We really had two models that, and this is again, nothing over here. Everybody is sitting over here probably can do this. This is not about something that only we know. It's really about putting the pieces together. So we have a mixed model over here that really does the most of the heavy lifting, what we call the base. And then we have the extra boost and really look at the variables. That really looks at the importance of the different variables and the different mechanism and really what we need to account for. And the way that it looks like is that we really have an aggregation of the both. It's really combined. And this is where again, you need to think about it. It's not just about I have a model, I deploy it, I'm done. It is about how do you take the different components and really put them together in a way that actually makes a difference in a way that actually produce something that people can use. It's actually valuable. And in this particular case, you can see that this is a particular a backstop. And the different variable importance and the mixed model that produce kind of how many do I need in this particular backstop. I removed the name of the backstop over here, but again, the numbers are not real. So that's okay. And what comes out of that one is I need two backstop officers in Ethiopia for this particular work for next year. So and that's how we visualize that. And you can see that again, we have the on the left side over here, we have the model prediction for a particular backstop, which one is over, which one is under. And again, this is very easy to explain. 45 degree angle, everybody understand. This is not a UC and precision and that kind of thing. On the right side, you can see we can do the same thing on the on the mission level. We also accounting for the clustering of the different funding mechanism. So we want to make sure that you stay within your cluster. There's a reason why a particular beer or a particular mission is doing the way that they do. There's obviously long history of to why. So taking that into account, none of this, again, none of this is specifically interested by itself. Although the clustering is actually very interesting because that's something that the response that we got is that, hey, this is exactly what I thought about, but I never actually was able to put it together. And this is where the K-meen is actually came in very nicely. The final couple more points. One, this is when we started to look at the positions. So how do we communicate that? One communication is you can see that the fact that there are positions out there in a particular bureau, it's rolling. The mission is changing over time. And you can see, for example, the different types of backstop that are in color coded. You can see that some of them went all, and it's from, again, left to right, positions rolling over from this type of work to this type of work. When you explain that in that kind of way, we're taking into account the fact that old positions are no longer part of the picture, but the new positions are, even though it's the same backstop and it's the same location, the same mission, really we need to take that into account that it's only the ones at the end that really matter. The ones that are open from like 20 years ago, 30 years ago, that's not what the mission is doing today. So visualization matter. And finally, we spoke about a court over here before. I am, this is still in Markdown. You know, we can fight about this later. But in Markdown over here, what's really nice about this is that we did the global data call, all missions around the world respond to this. And then you can really take the right information that you need out of this one. You can do the cross-tabulation. You actually have a report that our clients can submit to their leadership. We actually have an understanding, as you can see on the right, of what is important and what is not and in what level by the mission. And then incorporate that into the model as we discussed a few slides ago in the process. So Markdown was actually a really key over here in selling this. And essentially, that's something that you can attach to an email and send it across without individuals having to learn how to operate the dashboard or download anything. So as a communication tool, that was key over here. One thing that I want to mention over here for the sake of time is the, okay, how do we actually deploy this? So the interesting part over here is that we actually have a platform. So how do I communicate with R to a platform that really is based on other technology? In this particular case, our solution was R to PY, which is essentially a wrapper. It's a Python wrapper for R that you can essentially run underneath and Python is kind of running on top because the platform isn't Python. So it is essentially not an API-based couple. My last talk over here was about how do you really utilize API properly? And really, this is the future. But in this particular case, because everything is on-prem, on-platform, this is one solution. If you learn anything from today, this is probably it. It's actually very powerful that you can run, develop all your work in R, develop all your functions in R, all your model in R. And you actually interact with that through Python because the operation is in Python. The final piece over here is, and we can't go without Shiny. So we then, all of that is then wrapped into a really pretty shiny so that we can communicate that to the users, to the team that actually need to utilize that. And you can see that you have the actual numbers. You can have in the middle, at the bottom, the bottom left over here, you can see the blue and the green, the blue and the red. That's optimal versus actual. Remember the waterfall we talked about? I can show you, you need 10, but you're only going to get five. And then we can also show you kind of the distribution. You can get the data out through the tables. And what's really nice about it is that you can actually change parameters, and you can actually change and interact with that. And this is something that was new to them, which is the ability to run scenarios. That is really interesting. Now, eventually this Shiny went into an actual platform that took the Shiny and copied that into a different platform, but without the Shiny, we will not be able to communicate. I will use my last 30 second, essentially saying, going back four years ago with my first talk, this was from there. And this is really the point, again, the same point. I just realized at the end, when I finished this presentation prep, I realized that I'm really talking about the same thing, the different scenario, which is essentially the model is part of a process. You solve enough little point over here, and you will get some result at the end. It's not about finding the most sophisticated model, it's about finding utility from what you need, and really understand what is the data that you control, what is the data that you need to generate, and where do you go from there in order to create a solution that created utility for your client. Thank you very much."}, {"Year": 2022, "Speaker": "Asmae Toumi", "Title": "Using Data and R to Deliver Evidence-Based Care for the Opioid Crisis", "Abstract": "The deaths from the U.S. opioid epidemic have reached a new record, totaling 108,000 in 2021 according to the CDC. The number of drug overdose has quadrupled since 1999. Curbing this unrelenting crisis is at the heart of many interventions by the government, public health experts, providers and community activists. PursueCare is a company offering comprehensive care for substance use disorders and other mental health conditions through telehealth technology and in-person treatments. Asmae Toumi, the director of analytics and research at PursueCare, will talk about how data and R/RStudio\u2019s public and professional tools are being used to deliver evidence-based care and monitor outcomes.", "VideoURL": "https://www.youtube.com/watch?v=Ear-MfqjANI", "id0": "2022_04", "transcript": "She's a repeat speaker both at R.Gove and at N.Y.R. and she's apparently disaffected of her current job because she wants a different job. Her dream job would be a Panda caretaker by day but she had to learn Python to do. And then a at-home restaurateur at night. So pandas and restaurants. We don't know what type of cuisine it is, you need to tell us. Moroccan, I like that, you know, you know that. So everyone, please welcome Asma. So well, hello everyone. Thank you so much for breathing the cold and being here. Asma Chimi and the director of analytics and research at Pursuit Care. Today, I wanna talk to you about how we're leveraging open source and professional art tools to deliver evidence-based care for our patients. So Jared mentioned I am a repeat speaker. My first talk was discussing our experience as a startup and I'm excited to talk to you guys about the progress that we've made. We're now in our third year of operation. So first I'd like to begin the talk by discussing the crisis we're facing. So for over two decades now, we've faced worsening rates of mental illness, which have had devastating medical, social and economic consequences at the individual and societal level. From 1999 to 2019, nearly half a million people died from an overdose involving any opioid, which includes prescriptions and illicit drugs. The number of drug overdose deaths has quadrupled since 1999. Just to put it into perspective, that's about 136 people dying every day or about a person every five minutes. So this crisis has only gotten worse during the ongoing COVID-19 pandemic after an already catastrophic increase in 2020 deaths have risen to a new record in 2021. Mortality data from the national vital statistics system show that we've gone from a new record last April of 75,000 deaths in a time spent of a year to 108,000 in the last year. Newer provisional data, which isn't shown here, suggests that most states have faced an additional increase of about 6% nationwide. Due to reporting lags, this actually might be an underestimate. So this rise in overdose deaths can be seen across the country with some states experiencing as much as 50, 60, 70% increase in deaths. And it continues to worsen in many areas. Only a handful have had decreases. And given where we're gathered today, or close to it, Maryland, Maryland is on track to have its second or third year of steady decreases. And not sure if anyone in the crowd is involved with that work, but I know firsthand how difficult it is and they should be proud of their efforts. Unfortunately, most states are not in this boat, as we can see, and the reasons for these steep increases are complex and very in nature from region to region. It's true that the pandemic has exacerbated many inequalities across racial, ethnic, and socioeconomic class. It's also worsened access to care as many in-person clinics closed or decreased operations. Some clinics never truly bounced back, leading to even further fragmented care, even in this environment of loose end restrictions. One very important factor behind this worsening crisis is the sheer inadequate number of resources available to patients. So in a recent 2022 study, it was found that among 3,200 counties in America, 78% had no in-person treatment program. 29% did not have any clinicians that can dispense life-saving medications for people struggling with addiction. So we know people are seeking care, but simply they cannot find it in a way that is convenient, destigmatized, or affordable. Maybe I should keep going with those slides. A little hard to do, but... Okay, folks, I'm gonna pull an expert. I'm gonna hear a second matter, you hear? So the first year we had the... Sorry, I don't even grow up with the pandemic. The first year we had CPR, which is a post-doc, so are you filming? Something like that, how will the audience maybe have their own stuff and mask their own self-takers? Oh, they will. And so maybe when you see that in the moment, it's just hot, dry, and you hear it. And if I was on their slide, then you could just do the same thing on your side. Sorry to run. Go ahead, project. Awesome. Thank you. So an added difficulty to all of this is the access to these life-saving medications that we just spoke about, which continues to be difficult, really indiscriminately of person across sex, age, race, and ethnicity. However, these disparities, according to a recent study, are most severe for folks that are black, Hispanic. This is hardly surprising, as we know, these populations face tremendous barriers to care due to systemic racism, fragmented care, insufficient medical staff, and under-resourced areas, and additional stigma, of course, related to their use of drug. So I work at Pursuit Care, and at the heart of what we do is trying to eliminate every barrier that we can to care. This is why we leverage telehealth technology to meet patients wherever they are. It allows us to meet patients who don't have adequate local resources and who need treatment really fast. We also offer in-person care in New Jersey and Pennsylvania, and we're proud to have local staff there that have intimate knowledge of the dynamics of the crisis on a local level, which allows us to make good on our promise to deliver culturally competent care as much as we can. So in-person and virtually, we're able to treat all substance use and use of the disorders, like opioid use disorder, alcohol use disorder, and other severe mental illnesses. The flexibility of telehealth not only allows for same-day treatment, but it also allows us to implement functionality through our app that we've developed that keeps our patients engaged in their treatment and their recovery. Currently, our platform allows for medication-assisted treatment, which is one with providers, psychiatric care, individual and group therapy, FDA-approved digital therapeutics, and we also have our own pharmacy. Our goals in the data science team is to, broadly speaking, help our organization take better care of our patients. And we have a couple goals that guide us through that. One of them being better understanding our patients. So that begins with detecting trends with the lens of where they are, how they seek care, and once they're in our care, are they meaning their milestones? This involves the continual monitoring of their outcomes for the purposes of implementing procedural changes and personalized treatments. We also monitor the outcomes of those process changes and try to detect any new emerging trends from those. So how do we accomplish this? Well, it starts with a robust internal tooling. So we're bootstrapped. We're a small team of two-day engineers, me and a data analyst, and we spend a large amount of time configuring, and reconfiguring and optimizing our stack for efficiency and automation. So being in a startup in its third year now, we're cautious of right sizing our tools to reduce overhead costs and use our team in the most efficient and productive way possible. A significant pain point in healthcare is just the explosion of vendors that you will have to deal with, which service us in different ways. We have a vendor for electronic health record. We have a vendor for lab results. We have a vendor for our insurance claims and financial data. And the challenge here is marrying all those data sources together to better understand our practice and our patients. So we invest heavily in the maintenance of our data warehouse, which houses almost all of our vendors' data, and is augmented also by publicly available data, summarized from the American Census Bureau and the CDC. So we can enable the data science team, but also our internal and external stakeholders to understand how social determinants of health are impacting our patients, which then helps us to implement more personalized treatments and more culturally competent procedural changes. So as we've said, we continue to invest a lot into our data warehouse. And it was important for us to have control over it and not rely on vendors own data, which sometimes did provision us with reading database access. But I believe we've saved enormous time and proactively avoided many pit falls by having our own, namely to always have this always on monitoring surveillance, reporting and error detection. And we've also cut down significantly on confusion over what certain fields mean coming from the myriad of data sources from our vendors. So with that in mind, we've designed our own grammar or vocabulary for naming data fields. And this allowed really our entire team to better communicate and better collaborate. And this has had trickle down effects from the data warehouse to our internal packages, which also uses the same naming conventions. And it allows us to be faster at writing code for everyday data wrangling tasks. So speaking of internal packages, we've taken the time to develop and refine sort of our internal stack beginning with ETL processes, DB connections and queries. And you one that we've developed this here is data validation. We were very heavily inspired by the point blank package, which helps us do regular data quality assessments. And finally, we're continuing to refine our data manipulation and data visualization internal packages for common repeated steps that we do. And so all of that internal tooling helps us with better understanding of our data. And so all of that internal tooling helps us with better understanding our patients, the teching trends and being on top of any data quality issues that arise. Now, in terms of monitoring our outcomes. And so we're really excited to share that we check on and develop models for on a regular basis. So both the data science team and our internal stakeholders work together to establish these. And they involve engagement, meaning engagement with our care and our app retention and care. How long do they stay engaged with us. And so, you know, it's just a kind of a different kind of a resource or lack thereof. And adherence to the treatment plan as set out by their expert writers. While some outcomes are, you know, derived by simple counting and grouping, the majority do you involve some additional lifts due to the nature of the data and the complexity of the analyses. So, you know, the first tidy models, BRS and some causal inference packages. So how do we actually deliver these analyses to the stakeholders in a way that balances ease of use maximum insight but without overwhelming the viewer. And we found that an approach of carefully curated reports, spanning non overlapping facets of an analysis is successful for us. And we found that these two serve us very well for our purpose. I was very intrigued by Shirley's talk on quarter but I'm not quite ready to make that jump. Maybe when we have a more multilingual team but for now we are 100% an our shop. And so we make heavy use of our markdown for several advantages that I'm sure the audience is very well aware of, but we do make heavy use of it for our more static reports. And for simulations and modeling work, we tend to make use of shiny for a more dynamic experience. And we love using our markdown because its flexibility is great in organizing chunks by the purpose that they serve. I know personally for firsthand that some of my earlier reports were not the best constructed so having that flexibility to change it refine it all in a reproducible and version controlled way is really powerful. And the flexibility of the outputs have allowed us to quickly iterate over reports based on stakeholder feedback. And the portability is great too as we know it can be published on a publishing platform. It can be a PowerPoint document so it really spans many, many different ways to publish your results. And so recent and really exciting addition to our workflow is the use of the pins package and admittedly I've been kind of sleeping on this for too long but for those who are unfamiliar it's an awesome package designed to share and publish data models and most are objects by the way, and I'm not sure if I'm going to go ahead and talk about that to a board. This is really perfect for our use case because we often need to reuse data and track changes in those data sets and running historical point in time analyses. And so we have a lot of data that we have to use in time data in Excel or shared drive, which is a win in my book. And so that's all good but how do we actually get our results from our local machines on to the hands of our stakeholders. And so it's really clear for us in the beginning that our studio connect now known as positive connect made a lot of sense given that our entire workflows were built in our with a Microsoft Azure back end, but we've been impressed over time with its flexibility and ease of use for all our staff. And this is really reflected in our usage data that we can take a look into and pause it connect and I'm so proud by the way the staff, no matter what technical savviness level. Use it and give us feedback. So I think this is really what it's all about. For me, my role is to make this a truly data driven and evidence based company. And I feel like these tools truly allow me and our team to do that. So with that being said, thank you. I appreciate your time. If you have any questions, please hit me up email Twitter. Thanks."}, {"Year": 2022, "Speaker": "Jared Lander", "Title": "Deploying R in a Secure Environment", "Abstract": "Installing the data science stack, including the RStudio suite of products, poses it's own challenges even in ordinary environments, but this gets significantly more difficult in locked down and airgapped environments. We have successfully installed tools in a variety of environments in different states of isolation across industry and government. This talk will go over lessons learned in setting up even the most locked down servers.", "VideoURL": "https://www.youtube.com/watch?v=VXvw1zEQ1xw", "id0": "2022_05", "transcript": "introduce our next speaker who is slightly less attractive, less funny, all this stuff in me that I've said a couple times. And I first met our next speaker in 2017 in Boston at a sports analytics conference after everyone knows what I'm talking about, but I'm not going to say his name yet. But after the speaker gave his talk, I waited patiently after like a flood of people went to ask him questions and get his autograph and all these things to ask my own question. And it turned out like six hours later, all of a sudden, we were still talking and having a conversation. And I bring that up kind of just to talk about the type of person that he is, the personality. I think anyone who's gotten to know him at all has come to know him as just a really genuine caring person, whether it deals with a little bit of over enthusiasm for the R language, which actually isn't possible. And being willing to help each other out, build the community, whether it's with our or in life or with this family, he just keeps revealing himself to be a very special person. And he's certainly a very special friend who's been a lot to me over the last five or six years that we are here. So I'm not going to talk any longer. I'm just going to ask everybody's favorite R guy, Jared Lander, to come to the stage. Well, thank you, everyone. Now, I'm very excited to talk to you today about deploying R and friends into a secure environment. So we're going to talk a little bit about what that means, what you need to consider, and like how you might go about it. So first, what are we actually talking about? What do we talk about when we talk about deploying R in a secure environment? It means mostly getting in the hands of people who want to use it R and Python and maybe Julia. And they're associated packages, like you want Deep Liar and GGPot. But you're not just going to use R, you want a code editor to work with it. You might want RStudio or Jupiter or VS Code. And if you use RStudio or sorry, Posit Workbench, then you get all three of them right in there. But now you also want your add-on packages. So you might need a package manager. Now, this is not a sales pitch for RStudio, but since we use R tools and data science tools, it's going to be a lot of RStudio in here. You need a package manager. You might use RStudio package manager or you might use R defactory. And you can host cran packages, pai, pai packages. You could host Git packages. And you probably might also in a secure environment want Linux packages for either a mirror of apt or R-hell. Please use apt. It's a little easier for data science. A lot easier. And if you're publishing, you want a publishing platform like Connect to get your shiny apps up or your markdown or quartar documents or Jupiter or whatever. And there's a lot of tools we can talk about. We will focus on RStudio, Posit, but the idea is the same for all the tools we might be putting up there. All right. So I'll talk about a little bit of other tools a little bit, but the idea is the same. So what do we deploy these tools on? Where are they going? There's a few options you might have. You might do on-prem bare metal. And if you get bare metal, you're really lucky. You're going to get raw performance. And that's really awesome. Little rare, though, not so many people get bare metal anymore. Little more common is an on-premises VM. This has a lot of nice advantages. If you're ever afraid of going to screw it up, you can just make a backup of it and then like, hey, go break it. It's fine. Well, not fine. But it's better off if you make an image. Plus, you can resize. You want more RAM. You want more processors. You got it. Also common is a Cloud VM. And let's be honest, that mostly means AWS and Azure, maybe means GCP. I would love to see some people use DigitalOcean. I would love to see that get more traction, but you might use a Cloud VM. Or you might be installing into a Kubernetes cluster. It's so hot right now. Usually, it's a managed Kubernetes cluster. EKS or Azure Kubernetes, or GCP or DigitalOcean or LinNote or PaperSpace or whatever cluster you want to use. So we know we're going to deploy in one of these four types of spots. So let me have to ask, how locked down are they? How accessible are they? This means different things to different people. So there's a few different options. Maybe you have your tool exposed to the public internet. Anyone of the URL can access this tool. That's a setup we have seen. But you better make sure you have really good authentication if you're doing that, because it's open to the public. A lot more common is that you have to be inside your network, the left side of this image. You have to be either physically in the office, or you have to be VPNed in to even reach the tool. And you see this a lot. It's never a good VPN like WireGuard though. It's always some commercial VPN that just had a backdoor disclosed the previous week that hasn't been patched. But we deal. I wanted to put an image of NordVPN, but I didn't want to be that snarky. But you need to have some sort of access to the network to even see the tool. But the key takeaway here is that the machine you're installing on has outbound internet access. And that's really important as opposed to our next environment, which is air gapped. And this is two types of air gapped. This is a type of air gapped environment where the right side has no outbound internet connection. But it can talk to a different network that does have an internet connection. And that will be very important. And that's opposed to this version of air gapping, where the right side network has no internet and no access to any network that does have internet. It is completely isolated. And we're going to focus on these air gaps in areas today. So who is doing this? What types of organizations are air gapping or various levels of security? Pretty much everyone these days. Banks, hedge funds, insurance companies have been doing it a little more a little longer than others. But pharmaceuticals and hospitals really care about this, manufacturing, even consumer goods are doing this now. They're taking it seriously. No one wants to be breached. And of course, the government is doing this. They care about security. So how do these organizations know that they can trust us as an outsider to do this? You know, who are we? So for industry, and this is like private and public companies, they might put us through private background checks like Sterling. They might make us take drug tests. They'll put us through their training, like their cybersecurity training, training, their insider training, training, training, training. You get the idea. I might put us through a bunch of different training programs to make sure we're up to their snuff. Now government, they have their own background checks, various levels of clearance. Public trust for most agencies, but if you're doing intelligence or defense, secret, top secret as such. And it's not a hierarchy. Just because you have top secret doesn't mean you have public trust. You would need both. Same forms, different timeframes, but you need to go through it process twice. And of course, their own training, which is remarkably similar to the industry training of different cybersecurity and side of threat training. They're all getting from the same vendors, it seems. So now that they trust us, how do we access these machines? How do we do our job? Well, if you have the first type of air gap network where it can communicate to another network a little bit easier, we can SSH in through a into their network, the exposed network, and through a bastion box, a jump box, get into the air gap network and work of it in there. Maybe it is a virtual desktop and a browser, various ways to get in. But if you have the more air gap network, I think that's the technical term more air gap. We can't SSH in. We need physical access to the network, often meaning we're sitting in a cold room with no windows. And we certainly can't use our own equipment like this person is. We'd have to be using equipment from that company. All right, so we're inside the network, it's time to deploy. The biggest thing I want everyone to remember from this is that we're going to use Docker for everything. Everything goes inside Docker. Everything's better. So for our stack that we're talking about, we're going to talk about Workbench, Connect, Package Manager, Postgres, and Nginx. We find that gets you really far along the way. So we need to build a Docker image for each of these pieces of software separately. So for Workbench, we need to install our maybe Python, the basic packages we want everyone to have, Linux system libraries very important, and of course Workbench itself. I need to mount volumes for the home directory so users can save their work somewhere. And for configuration files, then you need to configure the authentication provider connection to databases, connection to metadata databases, a default R package repo, and what port you're going to listen to, and maybe your Linux repo. For Connect, largely similar, need R and Python, basic packages, system libraries, you need to install Connect itself, and you need to mount the volume for where the Connect content is going to be stored. That needs to live on disk and be persisted somewhere. And of course, you need to configure very similar to before authentication. We're going to get into that. Connection to databases, metadata databases, what URL you're going to serve it on. And you need to set up within, you don't need to, you should, in the good ideal world, set up an email server. You don't always have that option though. A lot of places like, nope, no email for you, which you can live without. Then for package manager, R and Python, notice a pattern here, system libraries, package manager, and then you need to find a way to have it listen on a certain port. If you're using Git packages, you need to find a way to authenticate against those Git packages, either SSH or HTTPS. Maybe you have Git on the secure side. There's all these considerations. But you need to tell package manager how to get there. Now Postgres, what are we using Postgres for? This is not Postgres for data for the data scientists to use. All of our software we've talked about so far, use Postgres as a metadata storage. So you need a Postgres container for each of those pieces of software. So you need to install Postgres in the container, you need to mount the storage directory, and you need to configure it mainly the listening port. You can have one container for all three pieces of software, but it's better to have a separate container for each just to keep things really separate in case you want to blow something up. It happens a lot. It's like unplugging and re-plugging. And then you need some way to route traffic around to all of these pieces of software. So you install EngineX and you configure it to know what URLs are going to what services, how to deal security escalation. If someone says HTTP, you want to bump that up to HTTPS. But oh no, you're using HTTPS and now you need SSL certificates. Ever try using a self-signed cert? It's really painful. So you need some way to deal with all that. So a lot of things need to be configured. But let's say we'll get to that in a second. Now that we have our images built, we're good to go, right? You need to do vulnerability scans. You need to make sure the containers are secure. If we're handing this to a customer, we want to make sure it's trustworthy. So you're going to scan every single image, and you often do this with open VAST or N-Map. There's various ways, but there's probably the two most common ways we would scan an image. And then you're going to know about CVEs, the common vulnerabilities and exposures. Some of them we can't do anything about. There's a CV in Postgres, we can't patch Postgres. But we need to make sure the client is very aware of any vulnerabilities that might be in there. And now if we can change things, which is ideal, it's time to harden the images. You need to make sure these things are as locked down as possible. So you're going to remove any unused services. If you're not using service, why is it in the image? You're going to close any unused ports. And then you want to avoid packages that make internet requests. They're sneaky. The Gert package, which is a Git interface at installation time, tries to reach out to apt and install a Linux package. It was not already there. That can really wreck your day. Other packages like Arrow, try to install, reach out to internet and install Arrow. Luckily, if Arrow, if you use a prebuilt binary, you can install it right in the system and you're okay. But Torch. Torch tries to install libraries at low time when you say library of Torch. So you need to do it the first time, get an error, and then point it to local libraries, which hopefully are already in your image. So you need to have a back and forth about, hey, what packages are you going to use? Because if they say the one used Torch after the fact, you're rebuilding that image. So a lot of things to think about. So we have our images. They've been scanned. They've been hardened. We're happy. How do you get them onto the AirGap network? Well, the first network is where the AirGap network has access to an internet-facing network. That's easier. We get into the internet-facing network, and then we via a jump box or a bash gym, either SCP or R sync the images over to the other side. Then the other side, you could Docker run or Docker compose preferably. Ideally, the secure side has a Docker registry over there. But then you still have to get the Docker images into that registry, but that's someone else's problem. Right? But somehow you're getting the images over via SCP or R sync. But what do you do if you have the AirGap network where there is no outside connection? You turn to the sneaker net. This literally entails putting images on a hard drive or a CD or a DVD or real-to-real tape. Yes, I've seen that. I've seen people who've seen that. I've never seen it, but someone in this room I know has seen it. And then you physically move that over into the secure area, plug it in after telling the right people and move it over into that secure network. You're literally putting things on hard drives and moving them. Does that sound efficient? No. Is it your only option? Yes. Well, you could use a DVD, but you know. All right. Then what about getting your R packages and Python packages over there? Well, actually, a bit of a misnomer. At least package manager can only do R packages in AirGap mode right now. Theoretically, PiPy is coming soon. You would download the packages on the internet network, move them over either by sneaker net or RCP just like before. Okay, great. We have all the software over there. But we're talking about various types of commercial software, whether it's RStudio or Artifactory or GitLab. You need to activate licenses. You need to tell you have a right to use the software. Well, you can do an offline activation, which involves copying and pasting a bunch of code into a terminal and a website. Might not always work. Some of these pieces of software have license servers where the software you have can communicate to a license server on the internet facing network, and that communicates to the software vendor. Cool. If you can't do that, some vendors not all offer activation keys. So there's different things you might need to do. Great. Let's say we got our software activated. We now need to make sure our user connections are secure. Then, and for this, we need to work very closely with IT. First, you might lock out people who are not on the network. It's like network access security. That takes care of the general population. But then we need to properly authenticate, maybe a two-factor authentication, and we need to deal with SSL certs. It's a headache because you need a certificate authority because you don't want to self-sign. You need to get the certificates onto the end-user machines. That's not our problem though. So let's talk about authentication. You're most likely going to authenticate with Active Directory, SAML, Octa, or OAuth. If the client can't provide that on their secure network, we could package that up in its own Docker container to ship with all the other software to get it up there. So there's various ways you can do that. But if you're using a client off, you need to work really closely with them getting the right config because the number one problem with deploying software on secure or less secure networks is authentication. It always and even maintaining stuff that's always the biggest headache is authentication. So get it right because it's a closely guarded secret that IT doesn't want you to have. So you have to develop a good relationship and make sure you have a good experience with them. So then you need to secure your connections to the database that the users will be pulling data from because the data is really valuable. And this comes down mostly to best practices. You want your username, your password, other things, stored inside environment variables, and you never want them in your scripts. Or better yet, you can use a service account for the database connection and set up a system-wide ODBC.ini file. And that means all the users are using the same user account, which is good when you're deploying content. So imagine a user uses Kerberos, our third option, to forward their connect login information to the database. Sounds great. Sounds more secure, right? When that employee leaves the company and you shut down their account, what happens to their content? That content no longer works. So it's a good idea to have a service account which Connect will use to hit the database. Now you have this beautiful shiny app or a flash app or a plumber API. You need to make sure that is secure. So again, network access, make sure people can't get to network, and authentication. Make sure your publishing platform, whether that's Connect or any other product, has proper authentication that goes back to working with IT and getting all the configuration for active directory or SAM or whatever set up correctly. So we flew through a lot of things there. And the key considerations that I want you to take away from this talk, things I want you to focus on are the type of environment. Again, whether it was bare metal, on-prem VM, a cloud VM, or Kubernetes, that's going to make a difference in how you're going to approach this. Then how locked down is the environment? Is it open to the internet? Do you have to be in the network, or is it one of our air gap scenarios? Because that's definitely going to change your approach, especially the type of air gap. Then how are you going to move the files across? We talked about SSH tunnels and sneaker nets. Those are your best options, your only options, maybe. You got to figure out what you're going to do. And it's better to plan that in advance and know what you're doing rather than finding it out along the way, especially if you have deadlines to hit. Then you're going to have to deal with your HTTPS. And notice I keep bringing that up. It really pains me. You have to deal with your SSL certs. Maybe you need to use CIRP bot. Plan ahead for this stuff. And then if you're going to use HTTPS, your customer needs to have not IP addresses, but URLs. You need to have that planned in advance. All these little things that you sort of add up that you need to keep adding in. It's not enough that the network secure. You need to keep adding in all these little things. Then you have authentication. Again, this is the hardest part of anything we ever do with customers. Getting authentication right. And that is not insurmountable. That has nothing to do with being a secure network. Well, it is secure. It has to be by your securing it by putting authentication on there. But that's any type of network. You need to make sure you get that right because that could make or break the whole installation. And if you do all of that in the right order, you'll be all set. So thank you."}, {"Year": 2022, "Speaker": "Danielle Larese & Selen Stromgren", "Title": "Finding Waldo- Transformation of Unwieldy Data into Infor with R", "Abstract": "Transforming data into information requires use of versatile and accessible tools such as R-Studio and is a key step to support decision-making via accurate trend identification and fact finding. This talk will demonstrate applications of R to perform data analysis and visualization on various data sets at FDA\u2019s Office of Regulatory Affairs and discuss the current focus at the agency on building a workforce proficient in data science tools such as R.", "VideoURL": "https://www.youtube.com/watch?v=8INPWIkhIpk", "id0": "2022_06", "transcript": "First of the two joint speakers loves reading so much. She's a member of four book clubs. How you keep up with that? Let's find out. Our other speaker loves the Flintstones and her favorite character is Betty Rubble. So everyone, please welcome Danielle and Celine to the stage. Hello, hi everyone. Thanks for the opportunity to share our experience of using our and data science techniques in government. My name is Danielle Lariz and I will be presenting with my colleague, Celine Stromgren. The title of our presentation is Finding Waldo, our supported transformation of unwieldy data into information. We work at the US Food and Drug Administration, FDA Office of Regulatory Affairs. And I also wanted to say that Celine and I are both chemists. So we have scientific backgrounds and definitely not data science. But here we go. My second disclaimer is that everything we present represents the views of the speakers and should not be construed to represent FDA's views or policies. The Office of Regulatory Affairs or ORA is the second largest group by the number of full-time employees within the FDA as we can see from the left chart. Only the center for drugs is bigger. I think there should be back. Is your mic out? Okay. Right, the middle chart here shows the breakdown of ORA staff into infactional, investigational, and scientific and lab. The split is about 80% inspectional and 20% labs. We think of ORA is the place in FDA where the rubber hits the road because the Office is comprised of people who are out traveling to firms, doing inspections, gathering samples, and analyzing those samples. In this presentation, we're going to focus on the Laboratory and Scientific Network of ORA, which has roughly 900 staff, and which includes 12 laboratory locations around the contiguous United States plus Puerto Rico. So this is who we're talking about, these regulatory labs and their scientific enterprise. So I'm happy to report that there is our in FDA. In fact, there are two R's, including drug administration when you spell it out. Anyway, RStudio is available to all of FDA staff, because it's an excellent tool that allows sophisticated data trending, analysis organization, visualization, and automation of workflows, such as our Chinese interfaces. We found that R could be popular, even for those not previously trained in programming, due to its open access structure, ease of use via availability of packages, the availability of the online community support, and the existence of specific use groups and communities, even at the agency to foster collaboration, cross use of applications, and joint troubleshooting. But there's a shift in the professional landscape at the agency with an emphasis on hiring in multidisciplinary areas. So titles like Chemist, Microbiologist, and Engineer are kind of becoming things of the past, and getting replaced by data scientists, pharmaceutical engineers, and artificial intelligence architects. There's also a new community launched by the FDA's Office of Digital Transformation, called Data Forward, that uses R as a platform to train a pilot cohort in data science techniques. So how do data science and R mesh with ORA? ORA scope intersects with all FDA regulated centers and regulated products, which means that ORA labs work with all these product types listed. Since these product types are so diverse, we get that's two statistics at the bottom. The FDA regulated products account for about 20 cents of every dollar spent by US consumers, and account for about $2.7 trillion in annual consumption. So this helps convey the vast swath of consumer products that ORA interacts with. But we also want to be clear that ORA does not write or develop any policies or regulations. Our mission takes place post-market that is after products are being marketed to the public. So this sounds like an awful lot of work, but let's think about what data it generates since we're here to talk about data science. So ORA has a vast amount of data that relates directly to the agency's public health mission. And there's a bunch of examples, such as facility inspection reports, enforcement activities, outbreak and adverse events in the reports thereof, laboratory results, sample collections, and it goes on. In addition to this data that's directly generated from our public health mission, ORA has operational data, such as training records or internal proficiency records and regulatory lab sample turnaround times. But like many fine people before us, we find ourselves with all this data, but with issues as well. So most of our data is available, but not accessible in the way that we would want it to be. So most data, especially the type of data that's acquired over time and under changing parameters, exists in a format that does not lend itself to easy extraction of key information and trends. And further human brains are not wired to reliably build cumulative understanding of a data set acquired over time. The last data point gets the most attention if there's a need to make a particular decision at a given time. In this issue leads to the impulse to re-acquire new data rather than leveraging the existing data that you have. So sometimes it'll feel easier to re-acquire new data to address a specific need rather than to distill the existing data in an effective way to get more robust and historical information. But the point here is that re-acquiring is in most case not the best option. And often for government agencies, this can manifest as contracting and hiring third party contractors to do this work for us. So further, a reliance on outside expertise of contractors extends from the activation barrier of not having a data proficient workforce. So within the agency, there's an urgent need to become proficient in tools such as R which could in turn help us deploy these data science techniques to master our existing data. And a data-driven approach could transform the historical practice of policy making based on anecdotal and qualitative information into a much more robust approach where objective and wholesome evaluations can illuminate the right actions and decisions. So I'm gonna turn over to Flynn to discuss some case studies that we've done. Thank you Danielle. Okay. Now we would like to illustrate a use case for a systematic data science approach. Our example involves data in the form of laboratory exercises acquired over it. Each exercise is basically an application of a scientific method by each laboratory in our network to detect unknown analytes in samples that represent FDA regulated products. So this could be pesticides in fresh produce, for instance. These exercises show that the labs are able to run scientific methods proficiently to obtain accurate results. There are several exercises provided to the lab network per year and the results of each exercise are captured in memos distributed to senior managers who have to of course keep an eye on quality operations of the laboratories. In this format, the recipient understanding of data is unsurprisingly disjointed, distracted, high level, case specific and certainly not memorable. If there's an interesting trend or revelation, aka Waldo hiding in the data, it will almost always be missed. In the early days, memos were being mailed to the recipients. As email became routine way of doing business, the memos started being transmitted as attachments to emails that hit the rather cluttered inboxes of the managers at random days and times of the year. As can be expected, mere digitization did not make the data more accessible nor noticeable to the managers. Via our pilot projects, we wanted to explore a way to look at the data in an aggregate format. One powerful visual that the user can interact with to see patterns and trends over years, across labs and through different exercises. For our pilot project, we concentrated on five years of data that represented 100,000 data points in 300 separate reports. This slide shows how the data in PDF files were transformed into a matrix format that easily lends itself to further manipulation and visualization through various data science tools. This dashboard is interactive for easy access, transparency, relational observation, and trend analysis. So was there a trend or pattern hiding in our data that became visible when one looked at the totality of information in a structured way? Was there a Waldo hiding in our data? There indeed was a Waldo that we uncovered in our COVID-19 data five years. Trends in reported findings led to an investigation that uncovered a practice of data that caused a reagent to sit too long on the bench, leading to slight evaporation of the solvent, which in turn led to slight overestimation of concentration of the proficiency samples. The workflow was modified and all the labs using the defective workflow attained much tighter standard deviations on their measurements. In this instance, the methodology was fine, but the data was not as easy as the data was fine, but handling during method steps needed to be modified. So we were indeed able to find the Waldo, or at least one of the Waldos that was hiding in this busy data. So what are some other applications of R that we decided to look into? So in addition to systematically looking at big data, we also use visualization in R to assess impact from our scientific activities to inform future investment decisions. Data visualization also helps us understand evolution of our research landscape. Trends from year to year provide insight into laboratory principal investigator activity in research, publication frequency, stratification of projects among different regulated areas, the evolving expertise and proficiency of our workforce, productivity in our organization, and connectedness and collaboration of our laboratory network. Since research impact is a hard to quantify concept, we've come up with some evaluative metrics in order to assess return on investment from our scientific activities. This slide shows the buckets of metrics we use to understand impact and hear the different font sizes represent the amount of output in each impact bucket. These metrics try to go beyond mere bibliometrics and try to get at more nuanced outputs such as whether the research led to a method that was used in a coordinated agency response to an emergency, whether the research stimulated diversification and modernization of our technology platform, whether the research increased our analytical readiness and preparedness to future public health events. This is an example of a core diagram that we've generated using our that packs a lot of information. It shows the relative size of research activity in the various FDA regulated areas and how much output each area of research has produced in our various impact buckets. So at a glance, you can immediately see we're very active in food related research as opposed to pharmaceutical, tobacco and devices. Where do we want to go next with our utilization of our? We would like to advance our mastery of our to the point that we can start using clustering and other predictive relational models for product risk assessment, product profiling, postmarket critical quality factor monitoring of FDA regulated commodities. Being able to leverage such models would help us optimize our method parameters based on analyte matrix scope, perform risk assessment of incoming import items based on country of origin, package features, historical importer information, and identifying and classify contaminants profiles in typically consumed foods. Identifying patterns in our existing databases to inform future decisions will introduce operational efficiencies and allow FDA to focus its limited resources on the products and firms that pose the largest risk to consumer health and safety. To recap, our strategic goal is to expand the use of our at FDA or a to target products, optimize operations and communicate harvested information. We'd like to build a workforce that is proficient in tools such as are so we can take charge of our data. FDA data forward, which was mentioned earlier is part of the FDA life learning, life long learning platform and teaches basics of our two cohorts of students who are FDA employees. And Department of Human Health Services HHS data co lab is another training activity that just got launched. FDA is under Department of Health and Human and Health Services. And finally, we would like to continue mining and retaining historical data in a structured format and expanding our cumulatively increasing knowledge base to help with our investment decisions, operational decisions, risk assessment decisions, and response to emergency decisions. And that is the end of our talk ahead of our time. And that's our contact information. And we'd love to answer any questions you may have during break up. Thank you for your attention."}, {"Year": 2022, "Speaker": "Marck Vaisman", "Title": "Leaping from Your Workstation to Production", "Abstract": "Many (if not most) R users learn R and use it in an interactive session. However, these scripts may require optimization and modularization in order to scale or run in an automated way. This talk explores best practices in R programming allowing you to write cleaner, modular and repeatable code. We discuss functional programming, meta programming and MLOps with R.", "VideoURL": "https://www.youtube.com/watch?v=AEl8O5HlcaI", "id0": "2022_07", "transcript": "I'd known him for many, many, many, many years. Back when his kids were like, their kids kids are in college now that really bothers me. Senior, almost in college. And that really bothers me. That's how long I've known this guy. He often actually introduces me. That's, you know, we're a really good relationship. He's a type of person when I text them some like life news. He'll call me to talk to me about it, which like really super sweet. And I just think that's really speaks to who he is. He'll call you. He doesn't text you back in congratulations. Like he'll call and like, Hey, what's going on? And I really love that about him. So that's my fun fact for him. That he's a sweet guy who like, who cares about you. So with that, please, Mark, come on down. Good afternoon, everybody. My name is Mark Weisman. Thank you, Jared, for the kind introduction. We've known each other for a long time and obviously we try to, you know, make as much fun as we can to each other with being respectful about it. So I am here today to talk to you about a couple of things that I'd love for you to think about as you are going into your our journey. So I mean, I hope probably I think everyone in this room is in our user. I'm sure everyone in this room uses other tools as well. So the idea here really spawned from many, many things and I'll get to that in a second. But the idea here is moving from this, right, from your laptop or your workstation or wherever it is that you're doing your work into a production environment. And I'm using air quotes because production can mean many things. I'd like to do a little quick, just sort of a survey here. So have, who's written, have, who's ever written in our script to be run in a non-interactive way? Raise your hand. Okay, so actually a good number of you. Like, I'm good, good. Did it work the first time as planned? Raise your hand if it did. As may, good for you. Have you ever scheduled in our script to run without human intervention? Okay, all right, so I thought we'd be, I thought we'd be worse, but good. That's as great, actually. Raise your hand if you actually have an our script running in production right now. Okay, very few short hands. So why? Well, we won't get into the why because that's a really complex thing. And I only have 18 minutes left. So we're back. This was 2019. And my walk up song was a, a Klesmer song. And I came and I even did an impromptu horror with Jared, but we'll leave it at that. Who am I? Again, I work for Microsoft. I am a senior cloud solutions architect, been at Microsoft for five years. I was working mostly with federal customers. I switched to a different, different industry. So working with financial customers in the financial services industry. I help customers use Azure for data science, machine learning, analytics, workflows. Right now I'm doing a stint in the Azure machine learning product team, which has been really cool. So I'm working with the PM program management, product management that I don't know what the P really stands for, but it's been really great. And I'm going back into the field at the end of the month, at the end of the year. So again, I teach here at Georgetown. So a couple of my fellow colleagues are here, fellow students. I'm not going to call anyone out. Hi Zeph. Hi Zeph. Hi Zeph. And co-founder of Data Community DC. You know, I used to, like Jared runs the ARMATUP in New York. I used to do that here with Abigida and a bunch of other folks for a long, long time. You know, life gets in the way, move on to other things. We're trying, I'm talking to Alex about maybe restarting and kind of having events. And it's just, we want new blood. We want folks to come and volunteer and maybe help us like rekindle it. Because frankly, like to me, the meetup was sort of what really brought me into the R world. That's where I met Jared. That's where I got into the data science. That's what I learned about Big Data. It's been just phenomenal. So that, the inspiration for today. So I'm doing the work I'm doing right now within the Azure machine product. So interestingly, the work I'm doing within the Azure machine learning product team is working on improving our documentation and examples for R, specifically for R. A lot of customers are asking for it. Anyone here ever use Azure machine learning? Just out of curiosity? No one. Wow, okay. All right, we could talk later. Anyway, the idea is we're just revamping. And I'm actually working on that. So as I was thinking about this, and also over the years of me teaching, I teach a big data class, which you really need to figure out how to interface with systems and stuff like that. That's sort of the inspiration for this talk. So our work has changed. As data scientists, when we talked about data science 10 years ago, we talked about the famous Drew Conway Van Duyagram. Who's, I'm sure you've all seen this before. Yeah? So in this context, really, it was more about perhaps machine learning, like building a model, like figuring out what was going on with your data, that sort of thing. The world has changed. We live in this world now. And as you've seen from the previous talks today, everyone is talking about production. Everyone's talking about Docker. Everyone's talking about a document building. It's a team thing. You don't work in a silo. You may, if you're in a basement somewhere. But you don't, right? I mean, you are part of a team. What tasks within data science you're doing? I mean, that can be a whole day discussion. So I'm not gonna get into it. But the point is, if you wanna live in this world, you have to think differently about your code. Let's not even get into MLOps. This is the conceptual model for MLOps, which is abstracted by a lot of the different platforms. The point is, what I'm gonna talk about today, I think is really, really important because of the things I've said before. Number one, I think it'll make you a better programmer, right? Number two, it just, it will make, again, well, your code will be better, but you will just learn how to think more strategically about the work that you're doing and how it moves on. You don't live in a vacuum. I hopefully not. But the work that you're doing as a data scientist, so let's say you're building a model, right? But that model is a scoring model for some sort of application. Well, that model needs to live somewhere, has to go somewhere and it's part of, it's a cog. At the end of the day, what we do in data science are just different cogs of a big machine. Yeah? Everyone agree with me? Good. But it's really, really important. What is production? Again, air quotes. Anyone know where this is from? Raise your hand if you know what this is. Oh, wow. Okay, I'll leave a few. So this is a famous scene from the Isle of Lucy show, where they're working as factory workers and they, instead of packing chocolates, they're actually eating in its hilarious. So, again, what is production? Production really is different things to different people. So let's look at this, right? And I'm glad many of you have scripts working in production, but let's think back of that scenario where you say, okay, I'm trying to run this and it didn't work the first time. So I have a super, super duper important business stuff, whatever script that I wrote, right? In my on our studio or a visual studio or whatever you use. And then I go and I run it from the command line. And then, you get an error. Yeah? Or maybe you get another error. I mean, these are typical R errors, right? There's nothing sort of glaring about this. I mean, if you use R, you know it's an R error. I kind of go and think of the bucket, but it's not very descriptive, right? If you try to run this into production and some like some orchestra to cause this, it will error and it won't run. So what happened? You don't know. But this is just one aspect of going into production. And again, what is production? Production really is different things to different people. What do you need? So when you think about the code, especially as you're thinking about moving into production, Jared, I think Jared hit, Jared talked about a whole side of production that is just even beyond the scope of what I'm talking about. But production just means running a script in an automated way like without human intervention. And that will run probably somewhere else. And to be able to do that successfully, right? And to avoid errors, you have to sort of think beyond many scopes. So you have to think about logging. You have to think about air handling. So try catch. You have to think about replicable environments. So packages, your libraries, like things that you would package in Docker credentials, logging with interpretable messages, right? Like kind of error messages that make sense. Parametri scripts, which is what I'm gonna focus on in a few minutes. Metadata, right? And then obviously network insecurity, which is part of the whole stack, but I mean, obviously we're not, we're not even gonna go there right now. But you know, really, if you're gonna run our, in our script in sort of a scheduled, triggered, deployed somewhere else in a remote server, you need to think about ways and strategies to write your R code in production. So I talked about a couple of things, but what about the code itself? Right? What do we do about the code? So you have many options here to the rescue. World Cup fans in here? Okay. So there you go. What's the score? Who's playing today? I don't even know. So I'm gonna just very, at a very high level, talk about these two techniques. One, one's called functional programming. Raise your hand if you've heard of that term before. Okay. So most of you have, what about meta programming? Okay. So great. Awesome. And then obviously DRY, which is sort of a mix of both. And it's not really either, but, well, good news. You probably have been doing both of these in R without really knowing that you were doing it. Now, I'm not gonna get into the foundational aspects of what true functional programming is, because there are many computer programming languages that do that. The thing with R is R is sort of, it's not a pure functional language. It's not a pure declarative. Like R is sort of multimodal and you can do all sorts of things. But if you've been using the tighty verse for a good amount of time, which I'm pretty sure ever, I don't know, most everyone in here is, I think. I don't know, raise your hand if you are. Okay. Most of you. The tighty verse just gives you a lot of these tools already in, but I'm gonna just give you a refresher of what these are, just so that, or at least mention what they are. And just so that you understand how to think about your code as you're doing this. So again, two ideas, right? Functional programming and meta programming. And if you go to a Hadley's Advanced R on the website, I forgot to put in the URL, there actually are chapters in this. There's a whole slew of resources. I actually have been reading a really good book. It's called The Brocking Functional Programming. It's a Manning book. The examples are in Scala, which is probably a pure functional language than R is in that sense. But I think from a conceptual standpoint, it's just a really good read. So functional programming, what is functional programming? So it's a combination of things. It's a way of writing code. It's a paradigm, right? I don't think there is functional programming that pure or true functional programming. I'm just gonna keep it at a higher level. Sort of, let's talk about this in the R sense. Some of what we do when we do tighty verse, right? Especially what, like, meta programming, or because R has what's called non-standard evaluation. Like, all of that allows it to do, you can do functional programming much easier, also meta programming much easier. But this comes from, the idea of functional comes from functions, right? From lambda calculus. So the idea that you have a function, some black box in the middle, you give it an input, you get an output. If you give it the same input, you should get the same output. You should not get different outputs, right? We don't wanna do this. You know, we don't wanna do the same thing over and over again and expect different results, right? Good. So here are just three different, kind of quickly. Three ideally pure functions, and I'm using pure in quotes, again, because I think it depends on the context. But, so the first one, adding two integers. Function A, I have the function will add, function A, B, A plus B. The second character, the second one, find the first character, I give it a string, and I substring write s one one, this is actually R code. And then the last one is divide two integers. So the tenets of pure function, so functional programming goes to the idea of using what's called pure functions, which is A function should return a single value. A function should only use its arguments. It shouldn't use anything from your global environment, right? Everything that the function needs has to be passed into the function. And the third is that does not mutate existing values. Again, that's a little bit more complex, but let's look at this for a second. There is one last thing. Code shouldn't lie. Okay. I don't even know where this is from. I guess it's from a Disney movie, but what? Oh, god, well, okay, fine. I just gave myself a way. I've seen bits and pieces of that movie, not never the whole thing. The point, I need to, it's on my watch list. It's on my watch list, but guys, hey, I work, I teach, and I have a family. So my Netflix viewing time is limited. All right, let's look at these functions again. Are these functions lying? Are they the first one? So add function a and b. Now, again, I took these examples from the Grocking book, which came from Scala. Scala is a statically typed language, which means that you actually have to specify the data types going in, the data types going out. We don't do that in R. We should, right? Because that will make us better programmers, because that will ensure that the data that's going into our function is treated the right way, right? And not a character. So if I actually did add, and I said three, and I did, well, R does a bunch of stuff for us, right? Like if I give it a, if I give it a real number and an integer, it will co-ass everything to real, right? Yeah, exactly. So it does coercion for us, like implicitly, we don't have to do it. Other languages don't, we have to do that ourselves. So that's one area where R is like a little tricky, although you can apply these ideas. R does a lot of stuff for us without us really understanding. So it's really important that when you write functions, use error handling and make sure that the types do data type checks when your data, whatever your parameters going into the function are coming in, so that the function does the right way, because if not, you're going to get different results. And obviously, you're going to violate the whole function of programming paradigm. So the first one, possibly, the second one, you know, again, in R, if I do this and I give it a blank string, if I, does it give us back a string, it will. If I give it a blank string, I get an empty string back. In Scala, you don't, you get an error. And the third one here, obviously, if we give it an NA or something, we will get an error, but the point is, the idea is that there is more to meet, there is more than meets the eye, especially in R, because of a lot of the type conversions and so co-ursions and other things that R does for us. So as you're writing your code, make sure you know what your parameters are. Meta programming. The definition from Wikipedia, it says, it's basically a programming technique that lets your data is programs that get converted into other programs. So we would probably do a lot of that without even knowing. If you use, what is it, like DB player, like DB player converts, deep player code to SQL, for example, you can generate YAML, you can generate Docker. So that's, it's all meta programming, but this is also, there's also meta in the sense of like passing parameters into your tidy verbs. I'm not going to get into the whole thing, because that's sort of a bigger thing, but we are doing that. And those two things like functional and, and meta programming, I think R is really flex is really, what's the word? It's great for that. It's great for that. And it allows you to be a lot flexable with your programming, but flexibility also perhaps may introduce error. So you have to find that balance right between flexibility and kind of writing, let's call it production level code, whatever your production might be. It could be like taking it in a USB stick, doing sneaker net, right, walking it to a sensor out in the field that's collecting data. I don't know what production is, but if you know, to each their own. So how do we prepare our code for production? You should, this is, again, this is, this is sort of my opinion, but you should think of like, take your interactive script, right, which typically you write and you say, you know, set working directory or use here or you create a, you know, you create a project which sort of bundles everything for you. But rather than using that, like, parametrized everything that goes into your script, because that way you can call that script externally, right, and send the script. So it's basically, you take a whole level of abstraction up, right? Your script is a function and your script takes some inputs and produces outputs. Ideally, the same inputs will produce the same output. So your whole script should be, you know, have this idea of functional programming. But then you can send metadata to that script, right? And then you can run the same script over. So example, let's say you're running a bootstrap or you're building a bunch of models for, you know, large data sets or many models scenario where you have different, you know, thousands of different groups and you have to build, you're still running a regression on all these different groups. You can scale that out easily. You just send the right, you know, the piece of data, right? And again, I'm not thinking about like, you can do that on your machine, obviously, if it's, if you have enough space and memory and all that. But if you're doing thousands or hundreds of thousands of simulations, you probably are probably going to do this either in a cluster or in a Kubernetes or Docker environment or in an MLOps platform, whatever it is. Here are some packages that will help you with this. The first two are two ones that I've learned about recently, especially as I'm working on updating our documentation. These are just ways that we're going to show you how to take an R script and be able to run that on Azure Machine Learning, for example. So the first one is called carrier and carrier. I still don't fully understand carrier, but it basically takes it. It encapsulates a function with all of its meta. It's not just like taking an object and saving it and serializing. It does a lot more so that you can replicate that elsewhere. OptPARS. Now you can, you know, R has the, it's like it's the ARGS function, which you can pass in command line scripts. Because typically when you run these scripts in some kind of production environment, it's run via, it's using R script, which is the command line execution for R and you pass in parameters, right, to the script. So our OpPARS is based on the Python OpPARS package, which allows you to, you know, how in Python you type in like Python script and then like double dash parameter. You know how you, yeah. So it allows you to do the same thing with an R script. It's actually pretty neat. QS is one I learned about recently. So QS, it's called quick serializing. It's really, instead of using like doing Save RDS, you use the QS because they're going to actually save multiple objects from your workspace into a single RDS file and it does it very, very quickly. Vettever, we heard some talks about Vettever before. I'm still not fully familiar with it. I know it's MLOps-ish related. One thing I read about Vettever, which I thought was really neat, is that Vettever can generate a Docker file for you and it can also generate a plumber script for you. So think of meta programming, right? And then YAML. So you can generate YAML because typically when you run these things in production, you're going to need some sort of YAML script, which is your job description. So you can generate that YAML with R as well. And obviously everything you do in the tight-everse, whether it's tight-everse or the time series stuff, tight-everts and the tight-y models. And I love the idea of the tight-yverse because everything is a data frame or a table. And then you can use the five basic verbs of a table to basically manipulate everything else, which I find it extremely useful and extremely easy. And I have one minute to go. Up-parse. This is an example of up-parse. So what you do is you load the library, you do this and this. And essentially, you can actually define the what, how you're done, because everything is going to come in this text from the command line. So here you can actually, I mean, you could do coercion as dot something, but this is much more flexible because you can actually, it's going to check for type as well. And then what happens is here is this is what single script, it's called data file. And the idea is that as this is, if this is part of my script, I am going to pass it in the path that's going to be right in. Okay, that's just one example. And then carrier, like I said, it sort of encapsulates everything. So here, again, this is using species, right? We train, this is the model. Yeah, we're not doing the splits, whatever. But we're creating the predictor and you're creating this. So it's basically packaging all the stuff up into one environment. To wrap up, just to talk about ML, you know, why all this? Well, because again, I am writing examples for our in production using MLOps, right? And Azure Machine Learning. So, Ms. MLOps is really big. And as you've seen, right, it really takes into consideration all sorts. There are many people involved in MLOps, right? People with different skills and people that are you doing different things. But this is how we, Microsoft, think about MLOps. Again, you have on the left side, like kind of the data part, right? The prototyping, then the middle part is collaboration. And then the last part here on the right-hand side is actually operationalization. How many syllables? Seven, six, five. Anyway, data science lifecycle, I think we've all seen this. And again, you know, kind of the, there has been some talk out in the world about how we are not really supporting our, that's not true. We are. No, I'm serious. We're working on it. That's all I can say. These are some of the references I use for today. So that's the book. I told you, Grocking Functional Programming. It's really good. This is just the website that we have this thing called the MLOps technical paper. Some of the art and production stuff I saw from this guy in some of this talks, there's the link and obviously had these things advanced again. So thank you very much."}, {"Year": 2022, "Speaker": "David Shor", "Title": "Data and American Politics", "Abstract": "How data and data science gets used in American politics.", "VideoURL": "https://www.youtube.com/watch?v=wW1vureAI6o", "id0": "2022_08", "transcript": "I met him seven years ago at a paella party here in DC. Mutual friend, now a mutual friend invited me and who's with along my now wife is like, I started dating, invited us to a paella party where I met an ex-speaker. So it was very nice for two of us tonight. And he's now a New Yorker. So he moved up to New York. So I got, and I still see him more often in DC than I see him in New York. So we had to fix that. And I was gonna tell an embarrassing story about him. It's not embarrassing actually. All right, so you might have noticed this folks. He is a great political scientist and data scientist. Yet the New York Times ran an article about him about a party he was throwing, like he's a nightlife pressario. So everyone, let's ask him questions about that, all right? But his fun fact is that he was, who knows the stand programming language? For Bayesians, raise your hands probably, right? Who uses it? Fewer people, right? But who wants to use it? Everyone wants to use it, right? So he was one of the first 10 people to post on the stand helpless. So he's an early adopter, a true hipster using it before it was cool. Everyone, please welcome David. Hey everyone, thanks for coming to listen to this. I'm gonna talk about data science and US politics and how data science is used by campaigns, at least on the Democratic side. I have to admit, I don't really know what the Republicans do. We don't hang out that much. Good job. But if you know anyone, I'd love to meet them. All right, so I'm just gonna talk about, so I'm the co-founder of Blue Rose Research. I'm gonna talk a little bit about who we are and what we do. We are a roughly 30 person firm. We exclusively work with Democratic campaigns and progressive groups. My co-founder and I were alumni of the 2012 Obama campaign and their analytics team, which was I think really kind of the first campaign that I think they had like a, something like a 50 person analytics team by the end of the election. So our team is an even mix, software engineers, machine learning engineers and client facing analysts. We had over 100 clients this cycle, which included most outside groups in the progressive space and spanned a lot, hundreds of house races, Senate races, governor races, state executive races, which includes things like a secretary of state races in places like Nevada or Arizona, and also lots of state legislative races, which we're incredibly proud of. In terms of what our shop does, and I'll talk more about this later, we focus a lot on forecasting elections, helping groups allocate money, predicting behavior and testing content. In terms of what that entails, we've conducted literally thousands of RCTs on millions of people. I think the exact number is maybe like 14,000 pieces of content on something like 13 to 14 million interviews. You know, we generated problem ballistic forecasts for every house, Senate and governor race in the country, as well as a lot of other things. When, and in doing this, we scored billions of rows per day and also helped allocate hundreds of millions of dollars in spend. So, you know, how does data actually get used in campaigns? I just want to talk through, you know, some of the core questions, I like to joke, it's a boring, unromantic way to think about politics, which is that politics comes down to, what do you say, where do I spend my money? And then how should, well, like, what do I say, what's happening, and then how should I actually spend my money? And so what, what politicians say, and this is like a big part of what we do, can really take a lot of different forms. You know, what policies should your basic campaign on, what ad should you put on the air, what talking points should I circulate to the press? And the reason this is hard is that in politics, if someone sees an ad or hears a speech and changes their mind, we have no way to know that because there are no tracking pixels in people's brains. Yeah, I don't know, I'm sure someone's working on it. And so that means in order to figure out what works and what doesn't work, you really have to do lots of experiments and you have to survey a lot of people in order to figure out what works and what doesn't. You know, prior to, you know, the rise of this kind of experimentation, you know, people would look at engagement, like you could just put a bunch of ads up there and see what people click on or see what people spend the most time on. But unfortunately, the only people who engage with democratic content are highly educated liberals. And if you choose what to tell swing voters on the basis of what excites highly educated liberals, you might have a bad time. So over the course, you know, of the last six years, you know, we've built a large scale experimentation system that's allowed us to test thousands of pieces of content. And when we've done that, we found that roughly one in five advertisements that we tested made people that tested with the intent of persuading people to vote for Democrats actually made people more likely to vote for Republicans, which is bad. And so for a long time, you know, our big theory of change was find those ads and don't show them to people. And that, you know, plays a big part in what we do. Another big, another big part of politics is just really trying to measure changes in public opinion, which is very hard. You know, public polling has really never been super accurate, but it's really been, it's been wrong in really important systematic ways in 2016, in 2018, in 2020, also in 2022, as I think a lot of people here were surprised about, you know, at least with the election results. And 2020 was actually so bad that it was actually the single worst year for polling in something like 40 or 50 years. And if you can't really measure what's going on, then you can't really do anything. And so we've kind of pioneered a new approach to doing polling, which is involved surveying very large numbers of people and doing legitimately sophisticated statistical modeling, which I'm gonna get to in a second. And, you know, so that's just a big part of what we do. In terms of why it matters, like why is it that campaigns have to know, you know, what races are close? Obviously we all just wanna know who wins, but intrinsically campaigns, you know, campaign interventions are not super effective. If you show someone like a hundred ads in the two weeks before the election, you know, that will increase their chance of voting for you by maybe something like 0.8%. And lots of elections are much closer than that. And so it's very valid, but it means that you really have to concentrate your spending in places that are close, and you have to have a good sense of the joint probability distributions of outcomes in order to actually get, you know, the best, get as much out of your money as you can. And then, you know, that just kind of parlays into the next thing of how do you actually spend your money, which is that campaigns, you know, spend an enormous amount of resources, I think total spending in US politics is, was on the order, I think it's on the order of like $8 billion. And most of that goes into TV ads, and it's a very complicated question to figure out exactly what mix of spending across mail and canvassing and television and digital, and across all of these complicated media markets will actually go and get you the most votes. So just to talk about why this is hard, the first thing is that politics is really complicated. You know, voting behavior, what is something I really like about this job is the thing that motivates why people do what they do, even though there's a lot of math, is fundamentally, it's fundamentally something that comes from sociology and history and a bunch of other things, and I'll get to some specifics toward the end of this talk. But basically, why people vote the way they do is really driven by a bunch of complex factors, things like ethnicity or income or education or religion, many of which are difficult to directly measure, and many of which interact with each other in a variety of complicated ways. How important those different factors are, really can vary across space and time, and unfortunately, surveying people is expensive. And so it's important to do a variety of different kinds of pooling. The other thing that makes this really hard is that survey takers are really, really weird. This is, I think, probably the core reason why polls are ever wrong is that it's basically survey non-response bias. If you look at a phone poll today, only something like one in 200 people pick up the phone, and that one in 200 person, like answering a phone survey at this point is now more correlated with whether or not you vote than past administrative vote history, which is just an absolutely wild thing. And so it's really, there used to be this world where you could have a political science grad student need like runs an MRP on like six variables and generates reasonable estimates, and that era is basically gone. At this point, you really need to survey very large numbers of people, you need to have a bunch of proprietary third party data, and you have to do pretty sophisticated modeling in order to actually generate reasonable estimates, at least with the accuracy that's demanded in politics. And then the last reason why it's hard is that our business decisions are very sensitive to auto correlated error. And so what I mean by that is, when we fit models, our basic workflow is that we survey a bunch of people, we fit models and we score the voter file. And if there are certain parts of the country, my favorite example is in 2018, there was a district in Southern West Virginia, West Virginia third, where 80% of the population were registered Democrats, but only I think Joe Biden only got like 30% of the vote there. And you know, there's like a bunch of complicated historical reasons why that's true, but if you just fit some simple linear models, it will tell you you'll overestimate vote share there by something like 20 or 30 points. And then you both look dumb and misallocate millions of dollars, which is bad. All right, so just as I'm gonna talk a little bit more about how politics has a lot of structure, there are a lot of deep interactions in politics as the Bayesians would like to put it. And so what I'm showing here is this is a two-way Clinton vote share and three-way ideology for liberal, moderate and conservative, broken down by race, religiosity and education. Our religiosity question here is a little bit weird, but it's relatively common in the political science literature, where you just ask people their attitudes toward the Bible, where your options are the Bible is the literal word of God, the inspired word of God, or a book of fables written by man. Each all three options are roughly equally common and it correlates a lot better with vote choice than a lot of other things like church attendance. And so a couple of, I could talk about this particular table for a really long time, but there are a couple of super interesting things that pop up. The first thing is if you look among white people, generally speaking, being more secular makes you substantially more democratic. But if you look among African-Americans, obviously the range is much more restricted, but generally speaking, being more religious actually makes you more Republican. And, oh sorry, being more secular makes you more Republican. And I think this really highlights the extent to which all of these relationships that we look at in politics are actually historically contingent. You know that one of the big reasons why this is true is that in the US, among generally white churches tend to be pretty conservative, but in places like the South, African-American churches play a huge role in democratic party organizing and GOTV. And that's one of the big reasons why we see that reversal. Another example of deep interactions I think is interesting is that generally speaking, being more educated being more educated among white people makes you more democratic, but if you already are very religious, then actually being more educated makes you more conservative because it means that you follow politics more closely and you just kind of know, ah, I'm very religious, I'm supposed to be a Republican. And then I think the other thing that's super interesting here is really this interplay where partisanship and ideology seem like very similar concepts, but this table really highlights that they can really diverge in a bunch of similar ways, sorry in a bunch of interesting ways. Like partisanship is clearly not exchangeable by race. Almost every category among African-Americans ranged between 94 and 97% while for white people it was between 15 and 75. But when you look at identifying as liberal, as opposed to voting for a Democrat, then actually these two columns look relatively exchangeable. That being more secular or having a degree doesn't necessarily have the same relationships on partisanship among African-Americans as it does for white people, but it does actually have very, very similar effects when it comes to identifying as liberal. And all of that, I think, you know, I think you could talk about why, but this is I think something that's really interesting about politics is you have all of these like related concepts that differ in important ways and your models need to be quite complex in order to accurately capture them. So this is a different plot, this is from 2012. This is showing the, you know, the Obama campaign forecasting model which me and my co-founder played a big role in polling in versus the Gallup poll. And, you know, one of the things to highlight is that, you know, all of the public opinion is usually quite stable and that most public polls, you know, just because they control for so few things are much more volatile than, you know, than you'd expect. But the thing that's annoying, you know, is that in 20, in 2012, Barack Obama got 52% of the two-party vote. In 2016, Hillary Clinton got 51.1%. And so that was really only a 0.9% change over four years. But obviously that 0.9% really made a big difference. And this is one of the things that really makes what we do very hard, that, you know, in our, at our firm, we try to measure two things, changes it in public opinion over time and also the extent to which treatments and arguments change people's minds. Both of these effects are very small, but it turns out are actually very important. So this is a screenshot of our treatment library where, you know, we've tested where we make available, you know, to a bunch of our clients, a really large battery of different messages that we've tested. And something that you can see here is that some messages really are much more effective than other messages. This is showing the average treatment effect on Senate vote choice among non-voters. And you can see COVID lockdowns forever, very unpopular. If anyone runs for office, don't campaign on that. All right. So just to get through some fun technical details of how things actually work, all our production models are Bayesian or at least approximate Bayes, you know, our traditional modeling and our heterogeneous treatment effect modeling, you know, generally involve millions of rows, thousands of treatments. And also our non-experimental causal inference. It's also also Bayesian as is our time series modeling. But, you know, unfortunately, I mean, Stan was mentioned earlier here, you know, our big thing is that we try to fit the big Bayesian models that people do use Stan for, but we use variational inference to try to approximate it. It's very hard, variational inference is painful, but that's kind of what we do. And this is just showing, you know, our mean, mean squared error, the cycle versus 538, which just kind of goes to show that all of this different work, all of this extra work really does make things better, but that things are still quite hard. So just to talk through, and this is, I believe the last slide, just to talk through some challenges in this field, you know, the first thing is that business, and I suspect that this generalizes outside of politics, but I've only ever worked in politics, so I don't know. The first thing is that business questions from clients usually are not well-defined statistical questions. It'll be things like, what do we do about Georgia? Or, you know, what candidates should we try to nominate? Or, how do we respond to this thing that Trump just did? And, you know, most of the time, that is not something that corresponds exactly to fitting a predictive model or doing randomized controlled trial. But if you focus on really well-defined, easy to measure problems, then you really end up leaving most of your, you limit your relevance and leave most of your impact on the table. Even when things are well-defined, like you have $300 million, then you need to allocate them across, allocate that across media markets to maximize your expected Senate seats. Your answer is still realistically, probably depend on a bunch of parameters that there is no way you can accurately know, or you could do your best. And so, but the most important thing, you know, that at least what I found is that it's really important to be willing to make leaps in order to avoid decision paralysis. Decision paralysis is really bad. The other thing that I've noticed is the decision makers usually lack background in social science and statistics. But the flip side is that they're still usually quite smart and ignoring them as a bad idea. Just to tell, you know, some anecdotes about this, I think back a lot to 2012. In the 2012 campaign, the analytics team was just this incredibly young group of people. I was 20, my boss was maybe like 23 or 24. And there were like these posters on the wall that were like, destroy the consultant, save the world. Cause the consultants, you know, they were these older people, you know, who'd been working in politics for 20 years. They'd been Clinton hands and, you know, I hated them. And I just looking back, you know, 10 years later, I think that probably, even though they didn't know statistics or didn't know, you know, didn't know math, I think like maybe like of all the things that I disagreed with them on, I think looking back, I think they were right on maybe like 80% of the things. And that's because, you know, it turns out wisdom is important, experience is important. It's very hard to get to the top of a high stack, of a, of an organization. It's a high status job. And ignoring what they say, it's like really easy. I feel like I talk to people, data people all the time. And they're just like, oh, we need to have a data person in the room, you know, these people just don't get data. And my experience is that usually, if you're saying something like that, there's something wrong with what you're doing. And you should like analyze, you should, you should think critically. Like I might, I don't know. So, and then the last thing I'm gonna say is that democratic politics is highly decentralized. And yet, you know, you have these hundreds of races and thousands of different groups. They're all theoretically all trying to do the same thing. But in my experience, they don't ever talk to each other. Like you'd really be, like if you, if you can imagine two important, you know, democratic politicians who you imagine have like a weekly or monthly meeting, they almost certainly do not. Coordination is just like a very hard problem in general. And it just means that it's really impossible to have impacted scale. And that's, you know, something that we think quite about, we think quite a bit about. And all right, with that, that's my talk. Thank you so much for listening. I'll end it there."}, {"Year": 2022, "Speaker": "Alex Gold", "Title": "Avoid App Failures Through Code Promotion", "Abstract": "It\u2019s all too easy to write an app or report or add an update and suddenly, cold bead of sweat running down your back, realize everything is broken. In this talk you\u2019ll learn how to think about avoiding this moment with good R promotion practices. You\u2019ll learn about a general framework for code promotion, as well as specific tools you can use to make deployments risk-free and easy.", "VideoURL": "https://www.youtube.com/watch?v=shADSWhoIoo", "id0": "2022_09", "transcript": "Our next speaker is a PhD dropout. I hear a lot of people saying like genius, right? Other people have been like so sad, but people don't know, genius, right? But his life took a turn after he quit his PhD. He became a segue tour guide on the National Mall. And he would like to apologize to all of you for that right now. Please welcome, actually, more so than I've been, one of the organizers of the DC data community groups who's gonna become much more active in the next year, right? Right? Pinky swear? All right, Mark is gonna make you. Yes, we're gonna hold you to it. Oh, I'm actually scared of Tommy for this threat. So with that, please welcome Alex. Great, thanks much Jared for having me. Thanks much everybody for being here. My name is Alex. I'm gonna be talking a little about code promotion in R. That's my Twitter handle if you wanna tweet at me. And just a little about me, I lead the solutions engineering team at the company formerly known as RStudio, now known as Posit. And so my team primarily what we preoccupy ourselves with is like taking stuff to production in both R and Python. So in your brains, we're gonna rewind to like before David's talk, which was great, back to Mark's talk. So like these two talks are really like peas in a pod. So rewind your brain to like, oh, Mark's talk was so good. He taught me how to like do code in production. It was so good. So this talk is not gonna be about so much about R code. It's gonna be sort of about the stuff you do around your R code to get it into production. So I know I have lived this life. You probably have lived this life. You're a very intimidating boss comes to you. They're like, why is the dashboard down? This is not good. The meeting was today where we're supposed to look at the dashboard, the dashboard is down. You know, it's because you made some update, you pushed some change, you did something and you didn't quite check it through beforehand, right? So what we're gonna talk about today is code promotion, which is generally sort of the sequence of things you want to do to put something into production in a way where it's protected, right? And so there are sort of two parts to that. And that is being able to test, validate, and sort of make sure the thing is ready before it goes up. So that's part one. And then part two is making sure it only goes up once you've done those things, right? You have a validation plan and it only goes there once it's validated. So I will say this is a little bit of an advanced maneuver. And so I would suggest you do some other stuff first. So like writing good code is like really helpful. And Mark, I think talked a bunch about that and also had some of these similar resources. Doing some basic project hygiene, right? Using projects, using correctly sub path kind of things. If you know what they forgot to teach you about our course, it's there, using Git is good. And using Rn, probably, would also be great. And I have resources here on all of these there on the slides linked if you want them later. But okay, let's say you've done all that. Now you're like ready to go, right? We're gonna do code promotion. So the first thing you need is three environments. You wanna dev a test and a prod environment, right? This is like pretty, you've probably heard about this before. We're gonna get a little deeper into like how to do it. But like dev test prod, right? You're gonna develop stuff in dev. You're gonna test it in test. And prod is for production. I have this little asterisk, sometimes two, depending on your use case, you can kind of smoosh dev and test together, right? Like those could be one environment where you do your development and your testing. But prod should like definitely be its own place. Okay, so you need three environments. And so how are these environments different, right? Like so one big difference is how easily can you make changes in each of those environments, right? And dev in test, in dev in particular, you wanna be able to iterate quickly. You wanna be able to try stuff out. Prod like no, right? Only things that have been really validated go into prod. Additionally, right? The level of validation sort of goes along with that, right? In dev, you're gonna be trying things out. You're gonna be testing them, things might work. They might break, they might not work. That's fine. It's a sandbox, right? You wanna be able to do that there. Stuff in prod, you wanna have it validated. You validated it both for like, right? For data science, you need to validate it both for the empirical correctness of what you're doing and the code quality, right? That this is gonna stand up to some sort of scrutiny, some sort of rigor. And then the last thing that comes up a lot is sort of the realness of the analysis, right? Are you accessing real data? Or is it some sort of fake data that you have instead because you don't wanna have the real data in your dev environment? This is especially important. If you're working with PII or PHI data, right? You gotta think about if you have a dev, that's great where you can use real data, but it has to be really sandboxed in that case, right? You can't get anything out. If you're doing like writes from what you're doing, are you actually, like you probably don't wanna be like doing your testing and accidentally writing to the like real data, like that's bad? So you need to be able to figure out some way to not write to the real data as you're doing your development. And then the last piece is sort of like, if you're doing things that take a while to run, it can be really annoying to have that happen as you're trying to iterate quickly in your dev environment. So you may wanna like do things like down sampling to be able to iterate quickly and dev in a way that you don't want to find your going into production, obviously. So there's an R package called config. It's one of the like little loved R packages. It's one of my favorites. Actually, I should have brought it. One of my colleagues made a hex sticker for it as a fig on it, which I think is very clever. But it's a great package that lets you sort of differentiate environments in a variety of ways. And we'll get into exactly how that works. But the basic idea is, as usual, there is a YAML file, right? Going to production is just like YAML. It's just YAML all the way down. Just YAML on YAML on YAML. So you take some YAML and you like load the YAML into your R session, and that's how you do config. Great, cool. We'll get a little more into that in a minute. So the second thing you need then is a mechanism. You have your three environments and you need a mechanism to go from one environment to the other, right? And so the broad class of mechanisms, obviously, is something called CI-CD, continuous integration, continuous deployment. You've almost certainly heard of it. Some of you may have heard of it and it sounds extremely intimidating. It is not so bad, I promise. And in particular today, I'm going to be talking a little bit about GitHub Actions, which is one CI-CD tool. I like it because it's really easy to use. If you use GitHub, it's already right there. But there are lots of other options, like GitLab, Azure DevOps, Jenkins, all of those are completely valid. And work quite similarly. So I believe all of them are also YAML. It's YAML all the way down. So how does CI-CD work? You have your CI-CD stuff and it's watching your Git repo, it's watching for things to happen. Those are triggers. When those things get triggered, then it goes off and does something. And we'll get into what exactly that means. But usually, that something is either automated testing or a promotion from one place to another. Those are usually the two things that are going to happen when your CI-CD pipeline is triggered. Okay, now we're just going to show off some of this. So I'm going to go to, and the Wi-Fi has been a little spotty. So YOLO, here we go. This is a document that I've got. I'm going to render it. It is a quarto doc, which I believe have been coming up a lot today, because they're great. Oh, that did something interesting. As I said, YOLO. Let's see if this doc is here. And it is. So this is a little report based on the Palmer Penguins dataset. If any of you know it, it's just a little toy dataset in R. And so what you'll notice is that right up here, this is the dev configuration that I'm using, right? That's just like something I manually put in there. It's not like auto generated. And it uses 166 data points. If any of you are familiar with Palmer Penguins, this is half the dataset, right? So I've down sampled, obviously on a dataset with 300 and whatever, 22 data points, you don't need to down sample, but you get the idea. But this is just on my machine. This is on local host. This is on my laptop. I cannot show this to you, unless I like walk my laptop over to you. So if I want to share this, there are a couple of options. There are many options, right? I'm gonna, today we're gonna use something called Quartopub. If any of you haven't played with it, it is a place where you can host public Quarto documents for free. So that's a pretty cool resource if you didn't know about it. A little bit like Shiny apps, but like with Shiny apps, there's a paid tier. There's no paid tier for Quartopub, at least not yet. And so this is the test version of my app. You can tell because it says it is the test version. And because it has this great slug that includes test in the name. And so that's cool, right? Here I'm using the full data set, 333 data points. And then I've got the prod version, which is exactly the same other than the fact that it was rendered like 30 seconds later and that it's at the prod URL, right? So, and this is actually really important that like test and prod should look really similar, right? You want as few differences as possible between test and prod because very often the reason the dashboard goes down is differences between test and prod. So when you try to do that promotion, everything broke, right? So you want to really minimize those differences as much as possible. Okay, so let's make a change. Let's, let's goops. I don't need to open Zoom. That's not what I'm doing right now. Let's make a change here. Who has a favorite GG plot theme other than minimal since we're using minimal? Black and white, I think you're doing there. All right, so let's check out a new branch. We're gonna call it theme for our new theme. We're going to commit this new theme change to new theme. New theme, theme. New theme, typing in front of a crowd, folks. So I'm gonna commit that. Oh, gotta hit the check mark. New theme, commit and push. Push from terminal. Okay, so now this is getting pushed and this is where we are living on the edge because the Wi-Fi is spotty. Yeah, that's being slow. Okay, so I'll show off what would happen if we got to get, to get. So here is my get repository and I'll keep an eye on it in case that shows up because I would like to show you what it looks like. Nope, no good. Nope, okay. Well, what would happen is I go ahead and make a pull request. And when I made the pull request, this action would kick off, right? So I'm gonna start doing things, which is pretty cool. I'll show you one that rendered already. So this is a job that ran a little while ago. It takes about two minutes to run the job, which is I think pretty good, right? It's longer than I want to show you during a 20 minute talk, but really not very long in the scheme of like, I'm trying to promote this into test. And you can see it does a bunch of different things, right? It sets up the job, it checks out the repository, it installs quarter rate. So like you can think of it starting in totally like a bear or server. I got a set up quarter, I got a set up R, I got to install my R packages. I have to set up something so that it knows where to publish to. And then it's gonna render and publish. And if you, I believe it will tell us here where it's publishing to, right? So this published the test report that we looked at just a minute ago. Let's see if this has recovered. Oh, it did not, it just timed out. Oh well. So, so this process pushed up to my test report. So, how, like, so, okay. So now we've understood what we've done so far, right? We had our local change we made, we changed the theme. We were gonna push that up to the test branch. It was going to do our testing. You would have seen and you would have loved that after the test ran, we could have merged it. And when you merged it, it was going to run another set of GitHub actions to push it to the prod branch, right? So that's like a pretty straightforward set of things. But like, how does it know how to do all that? How is that what happens? That's what we're gonna get into for the last eight minutes. Okay. So first let's look at what's going on here with the config package. Remember I said, like config is just a bunch of yaml. But we wanna use it inside our R code, right? I used it here to down sample. So you can see I've got this config, config get, right? And when I do that, it loads up this config object. And you can see that that is just right, like it's a bunch of variables. It's just a bunch of R entities, right? So it's a list, name list. And so that's pretty useful. So now I can just use that in my code. So for example, here where I'm slicing out part of the data, right, I'm taking half of it, in some cases, full in another. And then I have like the config name and that sort of thing. So how, again, how does it know to do this? Well, I've got my handy yaml file. It's the yaml all the way down. So I've got in the yaml file, there are three definitions of environments. There's default, test and prod, corresponding to dev, test and prod. In config, they use the term default, because that's sort of a fallback, right? If not specified elsewhere. And you can see that in the default, in the dev place, I sample down to a half of the data set. I've also got it configured so that that goes to the test version of the report, right? If I were to go to the test configuration, right? There I go up to a full one, right? All of the sample. And then in the prod version, I add going to the production version of the report. So how does config know which package to pick up? Well, when I do config, config get, right? What this does is it looks at an environment variable. It's in my environment. If you haven't spent a lot of time playing around with environment variables, they're like a really crucial part of how configuring multiple environments work, right? That is how things that are running know where they are is by the environment variables that are set around them. So what I'm gonna do is here I'll manually just to show you, sys.setenv and we'll set the variable it uses by default is rconfig active. I'm gonna set it to be test. So now when I set rconfig active to be test, and I'm gonna run config get again, you can see now I'm picking up the test config, right? You can see my sample fraction has updated to one. And so it's just, right? If we think back about our yaml, it's now selecting this middle block based on that environment variable that I had that is called test. That's how config works. Like that's the whole thing. It loads variables from a yaml file depending on what on the value of an environment variable. It's like super simple, but really powerful. If you have multiple environments that want different things to be true in each environment. One other nice thing you can actually do is inside your config file, you can actually run our code as well. So these are all just like values that I'm typing here, but if there's something you need to do that's a little more complex, you can actually run our code inside your config. I don't recommend to do anything complicated, but sometimes it's useful to do other kinds of things. Like for example, sometimes you wanna have a password that gets passed through, but not appear in your config file. You could do a get the password from an environment variable and pass it right through inside your code. Okay, so that's the config piece of this. That's the dividing up the environments. Now the GitHub part. So by convention in the dot GitHub workflows folder is where my GitHub actions go. And again, it's just yaml, yaml all the way down. And so what you'll see is if you look in this yaml file, it's all the things, right? If you remember here, this is all the big headers, right? Set a quarto, install R, install our package dependencies. These are those steps. They're just defined here, right? That's all this does, it defines those steps. And what it does is it can use other actions that people have predefined for me, right? I can use the actions, check out a repository action. I can use the quarto dev, right? That both GitHub and standalone people can define actions. So the quarto developers have defined a setup quarto action. The Rlib group has defined a setup R action and it actually allows me to provide a variable, right? What version do I want? I can also just run arbitrary code, right? So I think Mark, right? We're talking about running R script, right? I can run R script here, right? Through GitHub actions by just telling it to run that R script. So that's really useful. So that's what this action does. Now, how does it know, right? Again, this is the test one. It knows it's the test one because I just have it set an environment variable here, right? You can define environment variables inside the definitions for your GitHub actions flows. Works really nicely like that. And the last piece is when does this run? So this runs on a pull request and it opens or reopens, which is, right? That's what you would have seen if the Wi-Fi were being a little nicer. You would have seen that when I opened a pull request, the GitHub action checked off right away and that's my testing workflow. This is also a really common workflow if you're doing things like linting your code, right? You can put that in here, doing things like running test suites, really great to put in a pull request triggered action. And then there's another action, right? This one is identical. I will tell you, you can look through if you want to, but the only difference here is that it's using the prod config and it runs actually on a push to the main branch, right? And a completed merge is a push to the main branch, right? So that's actually what it's really going to be triggered on. You can do more complex things like nesting them with variables, but the syntax gets kind of complicated and I didn't want to have to explain the syntax nor actually figure it out for myself. So you can do that, but I just duplicated the same action twice. Just to show you a little bit of what these actions look like. So if you look, for example, this is the rlib actions setup r action. You can see they define all of these various variables and I'll zoom in a little bit, but you can see like they give you that the r version you can use, you can set the number of CPUs, all these kinds of things, you can set, and they just sort of list them, right? In the help for that action. I don't think this pushed at this point, but let's double check. Maybe we can show it real quick. No, it's still not pushing. Oh well, great. So that was mostly what I wanted to show you today. So just to recap a little bit of where we've been. Okay, so the bottom line, use config and environment variables to differentiate your different environments, right? One from the other, use git to manage when things move, right? The trigger is a git branching strategy. And so like I said, back toward the beginning, right? Having good git practices is kind of a prerequisite for this or at least something you should do along with this. Sometimes I see people where this is actually a really good motivation to set up good git practices, right? Like if your git practices only matter because you're like, we're gonna do git. It's like kind of hard to care, but if it's like do good git or else you're gonna break the CICD, that that's a much stronger motivator. Sometimes this can actually really go along nicely if you're trying to get people to use git in a more powerful and intentional way. And then use CICD, GitHub Actions to do promotion mechanics. Since I do work for Posit, I would be remiss if I didn't just say that Posit, I'm so used to saying in our studio, it's very hard to change my language. It's been really difficult. Posit Connect can do parts of this for you, right? Posit Connect can do the thing where it watches the git repo and picks up the changes for you. So you don't have to have a GitHub Action it's personally my favorite feature of Connect is that it can do that part for you. The other thing I'll share is if you thought this was cool and you were interested in this, I am writing a book. It's called DevOps for Data Science. It is drafted, the writing is still rough, but the entire book exists, it's online at d04ds.com. And if you're extra double interested, John Harmon, who maybe is watching online and has been mentioned a number of times, is actually running a book club where some folks are reading it. So if you're interested, you can reach out to John and I'm sure they would be happy to have another couple people in their book club. Thank you very much everybody."}, {"Year": 2022, "Speaker": "Nick Childs", "Title": "Everything is Hacked Already", "Abstract": "The best known security posture is to \"assume breach\" and build your policy from there. The most likely possibility is this assumption is correct. You might be lucky and you could be wrong. Let's take a look at the numbers.", "VideoURL": "https://www.youtube.com/watch?v=fMvTSSSahG8", "id0": "2022_10", "transcript": "This next speaker is actually one of my team members, and I found him on Twitter. He just recently had his one year anniversary of my company. Aside from being an awesome member of our team, he has only been shot at once and blown up twice despite rumors to the contrary, there's been a lot more. Everyone, please welcome Nick. He's right about hiring people. Jared's a great guy to work for. The only problem is he'll take a lot of the work you do and put it in one of the slide decks and take credit for it. So I didn't want to mention to you. And he did miss out on SNCC. And SNCC is a really nice plugin for Docker. Whenever you're building Docker containers and stuff, from the image itself, it'll automatically scan everything for you and kick out the CDEs that it violates. It's a pretty cool tool I can use in my stuff. So anything I talk about here today is for educational purposes only. Don't break things, you're not allowed to break. There's a code with ethical hacking. It's fine if you break it, but if you have permission, it's terrific. You just tell them how to fix it after you break it. If you do not have permission, congratulations. You get to spend time in the federal pen. So just letting you know this stuff is for educational purposes only. And I enjoy speaking in public, but I have a really difficult time not cussing. So I apologize that's the aircraft mechanic in me. If I do, I want to hear a boo. Like let's do it right now, everybody. Boo, come on. Okay, so if I cuss, you do that, you call me out and I'll keep going and I'll have to live with it. So the reason why I put this up there is I had to take several hours and compress it to 20 minutes. And so I have this talk. I know you're not supposed to click on strange links from strangers, but if you want, you can run that through virus toll first if you like. All it is is my GitHub with a PDF of this talk. All the links are there and I just say, if you guys want to go deeper in it, because I want you to, I want to pull you guys into cyber security. I think we need more analytic people in cyber security and that's the whole point of it. So that's where we start out with. Everything is happening. I could yell, we're all going to die, but it makes this point right there. So who am I? Boxwapper, you can find me on Twitter. Also, I just put this talk, it's pinned up there if you want it. I'm an Esquil hacker, Defcon 29 speaker. And so I'm used to a lot of interactions in hacking conferences when you speak. People interrupt you when you're wrong. They prove you wrong. We're supposed to take several shots before we talk for the first time, but I'm not going to do that here. I want to stay coherent. But if you interrupt me or if you have any questions or if I talk too fast, I'm excited. Say, hey, Nick, make stop. And I will go over whatever you want me to go over. I have no issue with that and we all learn something from it. I get to learn my audience better. This is my first time talking about it. So I'm an aeronautical pin tester that came from being an aircraft mechanic. So I came into cyber security. I hacked aircraft. SISTI testing hopeful. Yeah, I failed it. I'll admit I had failed it. 50% pass rate on that test is a tough one. I think I did pretty well, but next time I can knock it out of the park. I do like to watch CDE and MDDs and how a lot of people watch YouTube. I sit and watch the threads. They come out. It's enjoyable to me. See what's coming out. See what's broken. I call myself a server mechanic. I'm still asking for Jared to give me that on my card. If I can pass it out there, and one, I'll take that title gladly. And just recently, I became a Kubernetes Ranglai. If you've ever had to deal with Kubernetes, I more than have to tell myself a Wrangler, especially since I like to call a rancher, one of the things that you can use to adjust it. So if you want to see my talk, I talk about hacking aircraft. That link will be right there for you. And this is my first markdown slide then. So let me get away here. So I want to go over the reported trends, the way you can find these reports, the way you can go over this stuff yourself when you need to. And the vulnerabilities that come up because megacorporations got a corp. They do what they do. We all understand it's that bottom line. It's getting to the point now where it's becoming more and more dangerous as more and more of our lives become online. And we just got to watch out for some of their practices. I'll show you guys a few toys to break the world. This is the demo part. Yeah, I didn't sacrifice any chickens, so it's probably not going to work. And then I want to go over the corrective actions, ways that you can defend yourself and help yourself in your own life. And then I want to introduce y'all to Rita. And I'll explain it more when we get there. So we'll go over some basic definitions because you're going to hear these in the future. What is a hack? The best way, and all of these, all of the CVEs, everything, they always follow by the same thing, the CIA, TRAD, not that CIA, not that TRAD. But it's the confidentiality, integrity, and availability. If any of those are compromised, that's considered a hack. You're done. A DDoS, distributed denial of service attack, that would be a hack because now you lost availability. Just an example. So there's something in the hacker community, nobody wants to be called a skid or a skitty. It's basically a script kitty, a person who uses a script to attack a vulnerability. They are not talented, they just find the stuff online, pull it down, and they just run it and see what happens. It can get you in a lot of trouble, especially if you don't know what you're doing, especially with some of the tools I'm about to show you. So what is a vulnerability? So be talking about that, basic, a weakness in a system that can be exploited. So these are the reported trends, these are the links I wanted to tell you about. The Verizon Data Breach Investigation Report, it is like the touchstone for a lot of blue teamers. Whenever we have to present to our bosses, hey, this is the cybersecurity posture, we need to take this, what we need to defend against, this is where everybody goes. It's written by one of the data analytic nerds that y'all know that I just got to meet Bob Brutus, and it's a beautiful product. It's one of my favorites. We always use that to explain what's going on. The meter, CDE, common vulnerabilities and exposures, a lot of the vulnerability scanners are based off of these word lists. You can download a JSON file, build off the word list, it's the NVD, National Vulnerability Database, it's great, I don't like it, I'll tell you why in a moment. And then of course you have the SISA known vulnerabilities and exposures catalog. Is there anybody in here that works for like state, state level government, or does any contracting for state level government? All right, so the SISA known vulnerabilities exposure catalog, you have to follow that. Everything you do has to follow this. As of a document that came out two years ago, and I'll show you that as well. So the Verizon DBI at Kurzgesat in a nutshell, it's from 2008 to present day, it has private and public orgs, and it uses the vocabulary for event recording and incident sharing. And that's how many incidences they've recorded so far. And that's a big bat, that's this year, by the way, that's this report from this year. And I can show you a little bit of the companies that are involved in this. So out of this, these are the companies that contribute. So it's a very large data set, private and public companies. And this is one of my favorites. I always use the sets from this in order to present what's going on threat model in order to build a threat model and defend against. You know, like air gapping your environments. And I wanted to show this to y'all because I think things are getting a little better. We can improve, and this is gonna fall on y'all in the audience. So this is over time since it first started 2017. The great thing is, you see social engineering's gone down, right? That's a great trend. That means we're training, we're not opening emails, we're not supposed to open, we're not clicking links, we're not supposed to click. The problem I see, you see the basic web application attacks. That's your cross-site scripting. That's your, I literally go to a website, run a few curl commands that if you put commands, and I can get past your web application. So be real careful whenever you're making your shiny apps and stuff. And maybe even have a pen test or go through and run some checks on it. There's a lot of, there's a trend on that because more and more of our lives are going from a, you know, the tangible, you're holding it in your hand or taking a report to your boss's office to creating a web app that people log into and look at. And so because of that, there's more code out there. The code has to get out there faster. We've got a rising trend. And I actually have some zero days for that right now on my laptop, but I'm not gonna share those with you. I'm sorry. So, like I said, the meter CDE's, it's the blue team's favorite, just on files. Load in a voltar bill of scanners nicely. That's how many CDEs as of that date that you see up there, the 27th, that's how many were total recorded. These are all submitted from bug bounties like just individual hackers that put stuff together. It looks amazing on your resume if you're a hacker and you have actually done a published CDE. The thing is, I wanted to present this to you all because if you have, if you're going through the CDEs, the meter CDEs, a lot of these are used in hacking competitions whenever we go to a conference. The CTS consists of multiple CDEs. So we've got the code out there. We tell people how to hack it. And it's all there and people are practicing on these. And these are supposed to be low hanging fruit. As long as you can take care of these in a timely manner, you're gonna be good. I can't, I will never say 100% not hack because people are clever. Now, this is the one I don't like. There's a reason why I don't like this. I used to work for the DoD as an aircraft mechanic and then I became a system admin for about three years. And I wrote a lot of scripts because I'm a lazy coder. So I wrote scripts to do my job for me. They kicked out products. I follow those products, I was able to do everything follow up. Problem is, NIST doesn't say, well, here's the change coming. Here's the change. What do you think of a change? No, they go, here's the new change, enjoy. So I had to rewrite my scripts every month. So yeah, I'm not a fan, but this is a very strong model. A lot of government models use this. They just take the database, they build up the defense for it and actually disable services according to this list. All right, so this one, I really wanted to present to you guys, especially working in state, in order for doing state contracts. So this is the baseline, the known vulnerability exposures. It was established by the binding operational director of 1902, not 1902, but year 19 dash O2. So the reason why that was there was the government said, well, we've got a lot of contractors that have vulnerabilities. We need to set us a baseline. And they did. The baseline literally sets it up to where, if you do not fix these CDEs and they're found in your environment after that date, you can no longer be a contractor. So I wanted to show you guys this, because this is live, this is what it looks like right now. We've got a drop down date of the 19th of this month for multiple CDEs. So if you have this stuff in your environment, it just have a cybersecurity person look at these and they can actually go through and run a scan against it or they could manually do it. But if these aren't taken care of, you can't be in the contract. And you have to do the change management and work everything through for that as well. Got quiet, I gave you bad news, didn't I? So yeah, that's the drop dead date. So this is the one I'm gonna do my best, not to victim blame. It's very hard not to do. When it comes to cybersecurity, we see hospitals and we see everybody else getting taken and then they pay off the bounty on this stuff, not the bounty, sorry, they pay off the ransoms and it just makes things worse because once that goes public, you've got more and more hackers going into it. And I try not to victim blame, but we always try to say, have better practices, do better. You can do better. Just like it happened, I think it's seven years ago where we were one of the major credit companies got hacked, that was a misconfigured Apache web server. Anybody could have found it. It didn't take a skid, a skid could have found it easily. And I think that's who did find it and exploit it and then put it all out there. It's very important to try to put it that way. And I understand that companies have to make money, but they're starting to push more and more into danger zone. So, and the other thing that I've seen lately is with the features and services behind a paywall. A lot of people think farmers aren't too smart, but once they started making them pay monthly bills to unlock features on their trackers, they learned how to hack. They learned how to program. I met a few of them at a Midwest Well West Hackenfest and they basically went around and broke everybody's trackers because John Deere was charging them monthly for it. And now Mercedes is trying to do the same thing, BMW does it. Intel's starting to do it on their servers. I'm like, yeah, you're screwed. You're gonna get hacked after a week. It's not gonna take any time for somebody to unlock that stuff. I also wanna go into the responsible disclosures, a thing. And this is what I'm talking about with the Bug bounties. I've mentioned a few of you one on one. I do talk about this probably too much because I get excited about this stuff. So, Bug bounties is basically, I'm a hacker. I'm at home. I find a vulnerability on the website. I do a write up on what I found. I send you the Bug bounty. Five years ago, people made a lot of money. People could make a living off of it. There were hackers that were literally just living day to day and they did it and they did a great job. I have a friend of mine that you can follow his links and this is a responsible disclosure day because he has already submitted these Bug bounties over 180 days ago and then none of them were approved. He's gotten denial letter after denial letter and they all proof of concept work. So, I'm seeing this trend with a lot of my other hacker friends where they're not getting paid. And so, when you take a lot of people that are on that edge of being an ethical hacker and going, you know, if I was unethical, I could be a lot richer. And then now the companies are saying, now we're not gonna give you that money. We're not gonna pay you. So, I'm just giving you all the warning to, you know, keep an eye on your stuff. I think things are gonna get a little bit worse. Doom, gloom, sorry. So, this is what I'm saying with a responsible disclosure. Companies aren't paying bugs and they're not giving out resources to support ethical hackers anymore. We do have a few websites, those websites, I'm not gonna call them out by name. They have also not been standing in the gap for a lot of the hackers. And so that, for me, those are my friends. It sucks because a lot of them, they just can't make day-to-day work. They're trying to fund other means of working, talented people, but they're not getting paid for the bug bounties. And the companies that they're supposed to be representing, that's supposed to be representing them, are not doing that anymore. So, I also put a link up there by a reputable hacker. This is the guy that I asked, hey, can I release these today? Has enough time lapse? He said, that's fine, and that's what that link is. That's his GitHub. There's a lot of zero days there. It's a lot of nasty. So, I'm not gonna run any DMOs on it because I don't wanna break anybody's stuff. So, the other issue is IoT is fast and cheap. So, when you have the internet of things, I like to call it iOS for certain reasons. You can add internet of, yep. There's a reason for that. It's because people have to release these things as fast as they can to be their competitor. And the problem is you have I-O boards left open, you have them hardware wired to not change passwords. You can't change the passwords. And I put up here the exposed port 515 and 9100 because there's a tool called PREAD, and those are printer ports. And there's a lot of IoT devices that have exposed printer ports. And I was gonna do a scan to show you, but it just took too long. And there was just so many. I did a random scan within map, which is perfectly legal, by the way. You can run a passive scan to see what open ports that are on the internet. And the list got too long for me to actually present and export it to a file. So, that's how many devices are out there with that open port. And with the PREAD, you can actually leverage that vulnerability in your advantage. So, toys to break the world. These are some of my favorite toys. They're not the only ones. And they're pretty cool. You got the USB rubber ducky, the Flipper Zero. This thing, the HackRF, the Wi-Fi pineapple, and the new one, which is the Shenina. This is software that came out 23 days ago, and I'll go over that. Running out of time, love it. So, the USB rubber ducky looks like a duck, it quacks like a duck, it must be a duck. What this is is the small computer on a USB stick. And what it does is it tells your computer, hey, I'm a, it says, I am a keyboard. And then you run a script and it runs scripts as if you're typing on a keyboard. So, you can imagine what that does. It doesn't matter if you have antivirus, it doesn't matter if you have anything else running. It's as if somebody's sitting on your keyboard typing. So, somebody walks by and plugs this in, I'm actually gonna do a demo for you real quick. Cause they have it on the Flipper Zero. They actually have this software there. The way it works is I can just set it up to run a rubber ducky script. I can see. And I plug it in. And it's gonna think, my computer's gonna think it's a keyboard and it's just gonna run it. Or it's supposed to. See, this is why I don't like demo world myself. No, it's all right. It probably is trying to run it on my Linux machine. Yeah, it is. Cause like I said, I had other demos for you. Try it again. Yeah, there it goes. So this is what you see what it's doing. It's running like a keyboard. And then somebody's typing. This can do it via USB and then also do it via Bluetooth. So I can connect to your laptop and run some exploits, maybe to set up a web server on your computer, grab your files if I want to. Runs pretty quick. You can also make it to where it doesn't show anything. This is a demo. So obviously, we're seeing it. But normally we wouldn't see what's going on. I'm fighting the clock here. I'm laughing at myself. Now you ever add more to a thing thinking you don't have enough time that you don't feel enough? Okay, so that's the flipper zero hack our F. This is an RF receiver device. It works with software to find radio. You can receive everything. One thing that I do at home is I like to receive IFF transponder signals from aircraft. And I can track them. Cause as they fly over, I'm an aircraft guy. So I like to watch that. You can also pick up a pager traffic from emergency services. Cause that's not encrypted. It's just encoded. So you just plug in and encode and you can read all the pager traffic. You know how many passwords go across that? It's pretty insane. And they're like, how do I get into this? Oh, this is the password. So, you know, be careful what you're, what you're sending out and what you're doing with that. The Wi-Fi pineapple is a Wi-Fi device. I was going to do a demo, but I don't have time. Your phone is constantly sending out beacons and probes. Now with the Wi-Fi pineapple. So, you know, it sends out a beacon going, well, my home and I at home. And this goes, yeah, you're at home. You're at home. Connect to me. So me as a hacker, if I'm in the middle of that and you connect to me, I now can perform man in the middle attacks. Like give you a fake Google login or whatever. Now I've got your credentials when you log in. So one thing I can tell people in public today, turn your Wi-Fi off when you're not using it. Or I know there's other ways you can set it up to where it'll only turn on Wi-Fi when you're at certain geographical locations. I'm pretty sure Jared turned off his Wi-Fi because he was afraid I was going to do something while I was up here. And this is the new one. This is, I just learned about this. I'm going to try to be quick on it. The Shenina is the automated host exploitation framework. The mission of the project is to fully automate the scanning, vulnerability scanning analysis, the exploitation, using artificial intelligence. It's integrated with Metasploit and InMap for pouring the attacks as well as being integrated in the in-house command and control server for exultration of data from the environment. Now with Shenina, you just run the code, point it at a server. And if any of those CDEs, public CDEs are open, it'll automatically exploit. This is a skinny tool. It came out 23 days ago as a proof of concept. Right now it's a dangerous piece of kit. All right, so I'm going to skip ahead because of where I am, corrective actions. I tell people to silo their personal identity, use different email addresses, one for your financial, one for your paying your home stuff, one for your social media accounts, and then like one for your work, and then another one for your family. One of those gets exploits, you only lose that part of your life. And you don't have to correct everything else. Keep your crypto in your freezer. Those of you that have crypto coins, keep them on a lock, not connected to the internet, but I don't want to, like you can put them on a USB stick, you can put them on a freezer. They have different locks to do that. Keep them cold. Keep stuff on your own hardware if you can. I know that's not always possible. But one thing I have at home is instead of buying a ring doorbell, I built one that whenever somebody comes by, it sends me an email and somebody goes by the door because I don't want somebody else to have that data, I want to keep that data. Cost me $35, $95 with all the hardware that I had to build it with using a Raspberry Pi. I use a password manager. I have a sticker on here that says, my password's got hacked, I had to change the name of my dog. Don't do that. So Rita is a threat intelligent threat and this is what I wanted y'all to see and I wish I'd have gotten to it and talk about it. But it uses your type of math to figure out what a hack looks like in a live beacon environment. And I put the links on my slide because right now it's getting contributed with a bunch of hackers. But I think some data analytics people can really contribute to this project and help out. It's been around for about two years. It's named after John Strand's mom and it was introduced from Black Hills Information Security and it's always going to be open source and available. It's written and go. So that's it."}, {"Year": 2022, "Speaker": "Dusty Turner", "Title": "\"Win\" Your Fantasy Football Auction Draft with Integer Programming and R", "Abstract": "Fantasy football is a widely enjoyed activity where you compete against friends, colleagues, or strangers, to see who's real life football players outperform each other from week to week.  While player performance is largely random, you can improve your chances of winning with deft analysis. In particular, the initial draft at the beginning of the season can get your fantasy football team started on the right track. In this presentation, Dusty will show how you can use the Tidyverse, web scraping, integer programming, and Shiny to start your fantasy football season off right.", "VideoURL": "https://www.youtube.com/watch?v=rKit_jfNi1U", "id0": "2022_11", "transcript": "He is now in a PhD program, but his peers in the program are closer in age to his children than they are to him. And they call him dad. Brutal. Please welcome Dusty. Yes, I am in the army. However, anything and everything I'm talking about today has nothing to do with the army. You can see that the conversation is going to be about fantasy football. I think any colleague that I've ever had passed or present knows that whatever organization I work for, I'm a part of, always has a fantasy football league going on. And that becomes the most important thing that we do in that agency, as my friend Robert reminded me earlier. So I'm excited to talk about this. Very grateful for the opportunity from Jared to do it. Hopefully, even if you're not a fantasy football or football fan, you can get something R-ish out of what we talk about. So as I mentioned, we're going to talk about three major things. The ESPN API to get our data, integer programming, which is how we're going to model and also R. But before we do it, the short side note, I've taken up this hobby of any time I go somewhere, I go for a run, and I usually try to make it in the shape of an R. So this is my DCR. It wasn't actually today. It was when I lived here recently. As you can tell, it's a hobby. I ran in an italics R at West Point. This one looks like my daughter drew it. And this one is in it's off the screen, which makes me kind of sad. That was in New York City. Hopefully, it's not the case for everything that I have. Just briefly, a little bit about my background. I'm a former engineer officer with tourist Iraq and Afghanistan. Spent some time now as an ORSA operations research systems analyst at West Point as a faculty member. And previously at CAA, I'm currently in a PhD program at Baylor. Here's some action shots of me typing on a computer, because that's what it seems like I do most of the time. Hanging up a tank here, an Iraqi tank, which I probably shouldn't have been doing. And here I am trying to pull a Humvee, which wasn't. It was a very successful act. I've got a lot of math-ish educational background, probably too much. Mostly operations research. I went to the Ohio State, which is pretty cool. Go Buckeyes and currently Baylor. My family, who mean the world to me, I show my wife, Jill. I refuse to put the aim in Michigan, as most Buckeyes do. But that's where she's from. And I've got a new family of bears. We're really excited about it. But what I assume about you today is you may or may not know much about football. But that's OK if you don't. You don't need to know much. I'm assuming almost no knowledge of fantasy football. So I'm going to give a little bit of background on how this works. So the math makes sense. And then maybe a little bit about integer programming. It's OK if you don't have much knowledge about that either. And lastly, I assume quite a bit of knowledge of our stats. I'm going to show some code. But even if you don't know exactly what I'm doing, hopefully we can hang on. So let's talk about fantasy football. In summary, for those that aren't familiar, fantasy football, like most fantasy sports, you join a league of people usually that you know and you kind of select players that play in the National Football League. And based off their performance, you gain points for their performance. And every week you play head to head with another player. And if you score more points than your players score more points than who you're playing against, you win for that week. As you can see here, we encourage in the Baylor Stats Department League that I'm in to pick some pretty, you know, mathy names. I'm the armchair quarterback. I beat tidy versus Terry Tibbles. Oh, and I should also note before I go too far, you can see my record there. So I don't really know if you should trust me about winning fantasy football. I'm two and nine. So I probably shouldn't even have mentioned that. Would you participate in this league? Rossers have a certain makeup. As I show here, you have one quarterback, two running backs, two wide receivers, a tight end, another player, a defensive player and a kicker. And you can see here the second win I had the year that I decided to show against the strong law of large victories, which is a fun pun off of the strong. I'm not even going to go there because I'm not doing so well in theory right now in class. Now, to kick off the league, you have to select players. One of the traditional ways of selecting players is a snake trap. And that's where you all take turns in the league, picking someone. But a more fun way to do it, I think, is an auction draft. What happens is someone nominates a player and then everyone has $200 of fake money and whoever's willing to spend the most money on that player gets them. But of course, you know, the obvious cost if you spend a lot of money on a good player, you have less money on other players. And there are you continue bidding on players until your entire roster is filled out for every league. Now, there are a couple strategies as Drew Brees here is thinking about one of them is you can just spin big on a couple players. And that's a technique and it sometimes works well. You can spend $66 a piece and you have $1 left for the rest of your team, which you have three good players. The other option is to try to get bargains throughout the draft, but that's risky as well. Maybe there's a more intelligent solution, which is where I want to introduce and as your program. With it is your programming, what you what we're going to do to try to maximize the expected point production of our future team is we're going to make a few assumptions. One, we're going to assume that the players in the league that we know how many points that they're going to make, there's all sorts of experts out there that are making these projections, so we're going to assume they're correct. Second, we're going to assume that we know the costs that we can get the player at in the auction. So with those two assumptions that we know aren't true, but we're going to we're going to proceed with it with those two assumptions, the goal is to maximize the expected points. And we have a number of constraints that we have to adhere to. One is we have only $200 right. And we also have to build out our roster according to the certain positions. Otherwise, I just get a bunch of quarterbacks because they score more points than anyone else. Another important information that I want to mention before we proceed is I'm going to assume that we don't care about our bench, meaning that we can just get whoever's left at the end for $0, which is fine. And secondly, I'm going to assume that we're indifferent to our defense and our kicker. And usually that's a pretty good assumption because their performance throughout the season is largely arbitrary. So don't waste money on them. OK, getting a little mathy. The goal that we have here, I'm going to try to move my mouse around to point is in the upper left hand part, what we're trying to maximize is the points of player P or QR WT, which is their position person player indicator and the X is a zero one indicator, meaning they're either on my team or not. And this is subject to, you know, if they're a quarterback running back by receiver tight end. Now, if we just didn't have any constraints, we would just take every player and we would, you know, run up a lot of points. But as Lee Corso says every Saturday, not so fast, my friend, we have some constraints that we have to adhere to. And among those are the positional constraints, like how many are on our team? We get one quarterback. So across all the quarterbacks, which I can't even point while I wish I had a really long poll, across all the quarterbacks, we can only have one. It is possible to have up to three running backs. If one of them is this extra position, this flex that we have down here, which is equal to six because those are you know, the running back wide receiver tight ends, you can have up to six, but no more than three running backs, no more than two tight ends. And we also have the last financial constraint, which is that every player on our team has to sum to two hundred dollars or less. And given those constraints, we can proceed forward. But one of the things that I think is more interesting about this is how we actually get the data, right? Because we could just spend a lot of time and create a list of how much we think it's everyone's going to be. But the really fun part is is that ESPN has an API and there's a niche community on Reddit who's done a lot of work to try to like hack into this API. They don't publish any information about it. Hopefully they don't turn it off. But I found about about the API originally, if anyone's got any experience with this by spying on the browser in Chrome, like you can watch all the APIs that it hits up when you load a page. So I found this API, which has this, this base of the URL. And you can see it hitting up the fantasy football data, which is known as Kona player info for some reason. And I've hosted all this code online. I'll give you the link to that later. But the problem I had is when you run this code as is you only get 50 players and there's only more than 50 players that you're interested in, which is where I are relying on the Reddit community. I appreciate those folks. There's this block of code that sends a header in the request saying, hey, send me more than 50 players and I've requested 500 here. Afterwards, it comes in like a not very readable format. You have to turn it into a JSON. That's how you say it. Hopefully I'm saying it right. And then we have to like pick that file apart to get the data. I know there's more eloquent ways to do this. But the way I did it was I just started reaching down into this nested data frame, this nested file, selecting out fantasy points, what season it is, the scoring period ID, which doesn't make much sense right now. But when you see the data in a second, you will. And then do a little bit of cleaning to get the exact row of that player you want. Get the right season. Still filter for the maximum fantasy points for the season because it also gives weekly expectations. There's a lot going on here. So I built a function to do one of them. And I decided to pluck out one player in particular. I know one of Jared's favorite players is Step On Digs, but he's one of the higher predicted fantasy football players that exist. And I think Jared made a credit for him being on the Vikings for a little while. But here's an example of one output, but we create a function to get one output. We can now map that one using per over all the fantasy football players. And we end up with a well, we also need to clean that one up a little bit too. It takes a lot of cleaning because you got to identify their positions and all sorts of things. But once you download and clean all that data, now you have the expected fantasy points for each player. And I show the top 10 from each position. Hopefully these names look familiar and they should look like they're pretty good. I'll also note now that every time I run this code and render the presentation, it goes out and hits the ESPN API and pulls the most recent season predictions. So the reason these are all predicted to do well is because they've already been doing well at the beginning of the season. They had a few other players on here that were that were ranked quite a bit differently. But I think that's that's kind of a feature and not above. Now, let's get down to business of actually modeling in R. There's a number of integer programming or linear programming functions. And us at CAA have used a number of these to do some modeling. Maybe I'll talk about that at the end. But the one I'm most familiar with and I wish I knew some of these others better is LP Sol. Some of these other ones you can use more of like notational syntax, but this one, you have to build out your matrices and all the fun things. So let's talk about how to do a model with LP Sol. First, you got to give it your objective equation. So this F, O, B, J, the objective function, fantasy objective equation that I've got there is a whole bunch of ones and zeros. Oh, no, it's a whole bunch of points. Like how many points does each player have? The second one is the constraint matrix. So what are their positions? Like it's a bunch of zeros and ones like are you a quarterback? Are you a running back? So on and so forth. And the last row is the expected point value, which I kind of overlooked. But ESPN also projects how much you should spend on that player. If you want them on your team, I completely blew past that earlier. I should note that now. And then we build the like is equal to directional vector, which you can see here. Oh, see here. Here. And then I can scroll a little bit and you put, you know, is equal to one is equal to less than or equal to three. And then that. Then I wanted to show show you as well, like what these look like, you know, this is like enough to make your eyes bleed with like code and vector numbers. That's just terrible. But to actually get the solution, you run the LP function and the LP function. You tell it the direction, which we want to maximize points. You know, what's our objective objective function? What is our matrix? You can see each one of the arguments that you provide. And then lastly, you designate that it's all binary because you can't have like a fractional Tom Brady, which as much as I think he's great, he's probably a fractional Tom Brady as is. I hope he doesn't hear that he says that. I think he's great. OK, and this is what our solution looks like. A bunch of ones and zeros. You should notice there's enough ones to fill a fantasy football team. So here's your answer. Go when you're late, right? You can't do much with this. So let's take it back and clean up the results a little bit. Find who actually these ones and zeros belong to and put them on our team. Now, if you're a football fan or a fantasy football fan or anything, I hope that you feel that this would be a pretty darn good team if you were able to get all these people in your league, right? Like this would probably win every week. But as I mentioned before, every time I run the code, it re-updates everything. But the thing I want to mention is when you go into your fantasy draft and here was the mistake I made the first year I did this, I went in, I had my list of players, but all of a sudden I want to spend $18 on Josh Allen, but somebody's willing to go like 50 bucks on him. I'm like, no, the math says $18 and that's it. Like I'm not spending any more than $18. And then that completely messes up like everything. If I spend more than it, I don't have the same like best team. So there's certainly a challenge to overcome. And that's what I want to talk about for a second here is what happens if things don't go as planned. Well, let's see if I can do this live. I don't have this hosted. I need my mouse back. Oh, no, there it is. All right, got some code here. This is dangerous, y'all. Dashboard. Run. It's gonna work. Nice. Well, not nice. Hold on. I don't know why, but sometimes I have to go here and change it like this. And it works the next time. I don't know why. And if it doesn't work like this, I change it back and it works next time. There we go. Look at that. So here's the team that I should get right. It should look familiar. We saw it a second ago. I want Josh Allen dog on it, but I'm unwilling to go over more than $18. So I built this application to go in. And I really wish I was mirrored right now. Somebody else got Josh Allen. My O key doesn't work. There he is, Josh Allen. And now it removes them from the data and reruns my integer program and tells me who the new best most points likely to score points fantasy football team is. And you can keep iterating through this if you lose players. And then as you say, I do get, I don't know. Who's my running back up there? Is that Josh Jacobs? I like all the Josh's, I guess. Well, we're gonna work. There we go. Then he comes off the board and I also provide a little bit of functionality that if I had to. Well, this is going well. There we go. If I had to go over, I can like make adjustments. Like if I went a dollar or two over, I can adjust this. And all of a sudden you see it changing my. My team because I don't have as much money as before. I still lost this year. So I if nothing else, it helps you make a more informed decision when you go into it. But I think that's what we find a lot in our jobs is that we have all the analysis. A lot of times it's not the right answer, but if you didn't even have something to go down that road, you'd just be completely lost. So I'd rather operate with something like this in my back pocket than anything else. And my final final thoughts here. What I would love is if somebody out here was like super passionate as well about this and had time I've got this repo hosted with with everything please for it updated make it better. Maybe we can all win our fantasy leads together like our powers combined super Jared and is like our costume. Jared wore his costume so I had to wear mine as well. So I appreciate it. Thanks everyone for their time. Look forward to chatting with you afterwards. Thank you."}, {"Year": 2022, "Speaker": "Tommy Jones", "Title": "Natural Language Statistics with Latent Dirichlet Allocation", "Abstract": "Because language is one of the most abundant and information rich data sources that exists, Tommy dreams of a world where we can measure ideas and narratives with the same level of scientific rigor that we use to measure the economy, effects of medical interventions, and other quantitative phenomena. In this talk, Tommy will talk about his work with Latent Dirichlet Allocation to make measurement of language more scientific and how you can access the results of his research through a tidyverse adjacent package, tidylda.", "VideoURL": "https://www.youtube.com/watch?v=WN-udmSSscM", "id0": "2022_12", "transcript": "You heard me make some jokes earlier at his expense. I know he's a good sport about this. But he, I don't know how recently, but he cut off the tip of his finger in a cocktail accident. What cocktail were you making? All right, fingerless Tommy, come on down. Jared was giving this away. I thought I would share that. So I recently defended my PhD. And my, all of my dissertation notes are in one of these that I got a couple of years ago. So it's one little personal fact there. All right, so as Jared said, I keep trying to get out of this. So originally when Jared and Nicole approached me and said, would you like to speak at the conference coming up, I said, no, because I'm defending my dissertation and I don't have time to make a new presentation. And a few weeks later, they said, well, would you at least be a backup speaker? And I said, yes, but only on the condition that you get a 20 minute version of my dissertation defense because that is the only presentation I'm going to make this year. And then two weeks ago, they said you're on. So with that, I wanna, I wanna give everyone here my apologies. Instead of an R talk, you're gonna be getting some key points of what it took me to get a PhD in computational statistics. I've tried to keep the math to a minimum. There is an R tie in though. So last year when we did this virtually, I presented on an R package called Tidy LDA. And that's actually, I've been building it in concert with my research. So, in addition to having this package out on CRAN that you can use, this is now sort of the background of the underlying science that went into some of the novel features or default settings for that package. So with that, language is an abundant and information rich data source. It is literally the protocol we use to convey information to each other. And I dream of a world where we can measure ideas and narratives with the same level of scientific rigor that we use to measure economic activity or the effect of a medical intervention. But to do that, we really need two things. So the first is science. And my dissertation was developing some statistical theory for working with languages data, primarily through using latent Dirichlet allocation. I'll explain why in a little bit. And then we also need tools and methods that are intuitive and user friendly because you wanna make it easy for practitioners to do something in a principled way. Not everyone needs to be able to follow the math and re-implement it in code from first principles themselves. So the current generation of language models, you might've heard of them, they're called transformers, like BERT, GPT, there's a few other ones. They work so well, latent Dirichlet allocation, actually quick show of hands, who's heard of or used latent Dirichlet allocation. I'd say maybe about a third of the room here. That model is now 20 years old. It was developed, the paper was published in 2002. So why am I still studying it? Well, there's a couple of limitations to this current batch of pre-trained language models that have really been taking the world by storm since 2017, 2018. First, they're really task-based. They're built for making chatbots, they're built for summarizing documents, tagging parts of speech, et cetera, et cetera. But what they aren't fit for is making inferences on populations from samples, which is obviously a core statistical task. Second, they're almost all deep neural networks. And so for all of the really strong predictive power they bring, the complexity of a deep neural network is makes it effectively a black box when it comes to interpretability. And so I would really like to see text analyses enter the statistical mainstream by focusing on what statisticians do best, not just building better chatbots, although somebody needs to do that too. And so I've taken to calling this natural language statistics to differentiate it from natural language processing, which is this inherently task-based sub-field of artificial intelligence. And so with the goals of natural language statistics in mind, latent Dirichlet allocation has some pretty nice properties. It aids with interpretability and quantification of uncertainty because at its core it is embedding language into a probability space, which is kind of all of how statistics works. Because it is just a Bayesian statistical model, we can lean on established best practices to figure out how to use it. We don't need to reinvent new statistics to figure out what are some guiding principles to getting good models. And one of the most powerful things is that it models a data generating process. And so we can leverage that data generating process to create simulated data that can provide sanity checks for developing new methods and metrics. And I'm going to talk a lot more about that in a moment. So as a review for those that may have used LDA before or for those that it's new, here's a quick intuitive take. So I have a collection of documents. Those documents are full of words in different frequencies. And what LDA does is it splits those documents up into two groups. So you get a collection of topics, which are themselves groupings of words that are co-related. And then you get these simplified documents that are now a collection or distribution of topics that can help you get topical similarity rather than just purely word co-occurrence between these documents. And these are represented by these variables of interests, beta and theta respectively. But because I told you this is a cut down version of a dissertation defense, we're going to get a little mathier, sorry. So here's a more technical take. A researcher is interested in creating theta and beta. And that's like what you analyze at the end of the day. What is the distribution of topics? What are the distributions of words within these topics? But a researcher has three parameters that they need to set up front. One is this thing called eta. It looks sort of like an N, but the tail on its long there at the bottom right. And that tunes this distribution of words over topics. Then you have another one called alpha that is over on the left there. And that tunes the distribution of topics over documents. So there were what are called prior parameters. This is like, what do I think the world looks like? And then I'm going to update that with my data. Then you also need to choose the number of topics. And if anyone has done any reading on people that are using LDA, it's stack overflow is full of people that are like, how do I know what number to choose? Newsflash. Nobody knows. Good luck. Most of the attention in the literature has been on selecting K. The second most attention has been on selecting alpha. But one of the interesting results of my research has been that it seems like eta is actually a much bigger deal than alpha when it comes to tuning what results you get. But consistent with conventional wisdom, it seems like selecting the number of topics is even more important. And we're still not sure how to do that. So I did three studies and I want to summarize the key results here for you really quick. So LDA is what's called a latent variable model. So that that theta and that beta, they're latent. We don't see them in real life. And that's a problem because no ground truth exists against which we can compare our models and what's going on. And then we can compare the data. And then we can compare the data. And then we can compare the data. And we can compare our models and methods. And we can compare them to the data. That's an important problem because. No ground truth exists against which we can compare our models and methods. It's just sort of. Good luck. This seems reasonable. I feel like we can do much better than that. I felt that way for almost 10 years, which is what's up me on this journey. Fortunately, like I said before, LDA is a generative process. So I can use that process to create synthetic data sets where I do know the ground truth. And I can use that to aid in the development of methods and metrics. And so simulation studies like that actually have a really rich history in the statistical literature. And if you read the LDA literature, they're used occasionally. But one of the big limitations to doing simulation studies with LDA is that if you're simulated data is going to be valid. Then you need to be able to create simulations that obey the gross statistical properties of human language. Most notably it's something called zips law and we'll talk about a bit on the next slide. That actually goes to the key scientific contribution that I've made. Being able to create simulated data that conforms to zips law. So now I argue that the use of simulation studies goes from what some other folks in the LDA literature called to a toy studies to this is something that is a statistically rigorous way to study LDA as a model of language. So that is my key thesis here. So I'm going to talk a little bit about that study where I went to establish that. So zips law describes this relationship between the frequency of a word and its rank order. Basically, the most frequent word in any corpus and any language in any context is roughly twice as frequent as the second most frequent word. Is roughly twice as frequent as the third and so on and so on and so on, going down the line to tens or hundreds of thousands of words that are in your corpus. That relationship forms something called a power law, which is an extremely skewed distribution. But it is linear and log log space. So if you'll look at the middle and right hand side graphic. These are more or less straight lines. So that's because the why and the x axis are logs of the the original data. So the bottom line of this graphic is that if this parameter Ada is itself shaped like a power law, then LDA is going to generate word frequencies that are consistent with zips law. But if Ada is not a power law LDA will not generate word frequencies that are consistent with zips law. So that gives us a very important constraint on if I want to simulate data and I want to make it look statistically like human language. This thing has to be has to have this property. But you know I told you before that a lot of the attention the literature has been on this other parameter alpha. The left and center of this have both a power law alpha and a symmetric alpha. But the left most graphic has a flat Ada, and the right or in the center graphic has a power law Ada. So you'll notice that is not linear and log space, but the middle one is the key here being alpha this very important parameter that people have spent a lot of time on seems to be irrelevant when it comes to shaping human language. That doesn't mean the parameter itself as a relevant, but if I want to make something look like human language. It doesn't really add any information here. And then on the right, that was just taking some simulated data in green and calibrating it against a real world data in orange and you can see that at least the passing the eyeball tests there, roughly the same shape. So I want to do examine how different tuning parameters for this data generating process can affect some corpus statistics, and I wanted to see what the effects of miss specifying these parameters on an LDA model were so if I intentionally chose the wrong parameters because in real life you don't know what the right answer is. What is the effect of that. So two different studies. To do that I generated over 4000 synthetic data sets across a range of parameters. And then for the first study I saw how those parameters affected the resulting word frequencies and for the second, I intentionally miss specified for different LDA models and compared it to against a perfectly specified LDA model to see what the effects of miss specifying individual parameters were And I have time for the sake of time to really get into like exactly what these different statistics are that are on the screen. But you know I want to call out sort of the end result here so this is looking at how these parameters affected your corpus properties. First is the magnitude of alpha and I also use the quadratic of it. Basically have no effect on these outcomes the top row you see there's a variable with a gigantic confidence interval. So that's the alpha. And so on the bottom row I just took it out of the regressions and said you do not matter. And you can see that K so your number of topics has a pretty big effect on what you'll see from the your resulting word frequencies and Ada does as well as well as things like the average length of your documents which is basically like a proxy for data size. And so for the miss specification study. I did roughly half of these miss specified models from no matter what parameter I was miss specifying were too big and roughly half were too small. Again, I'm not going to get into what all of these outcome statistics are but I am going to talk about some of the key takeaways here. So one is miss specifying the number of topics seems to have the biggest effect on your models performance. So that thing that everyone worries about that nobody really knows how to set. Yeah, it turns out it's super matters. One thing that was curious is that if I intentionally mess specify Ada to not be a power law but to be flat for many of these performance metrics that actually seem to improve the fit of my model. I don't know why. But you know that's that's fodder for for future study. Consistent with my results on the previous slide miss specifying alpha does not seem to really affect things at all. That was surprising to me. And then, you know, one last thing is that if you look at this one l coherence here which I'm not going to describe something called a coherence metric which you may have heard of if you heard of LDA if not. I'll talk to me about it after the presentation. But you notice if K all the way on the right there is too small or if it's too big. They're both negative. So that's what I'm going to do is I'm going to look at the base line of a perfectly specified model. So what that tells me is if I get it right. I reach some sort of maximum there. So that tells me that, you know, there's some evidence that says this statistic might actually be helpful at selecting the number of topics. Unfortunately, that's a future study so I don't actually know how to do it yet. So, synthesizing four key points and highlighting three. I'm going to show you a few of the other important things that you're going to do in the state of the world. It's a very related data in a principled way to now actually study this model and develop some, you know, regular statistical theory on how to use it. Next, it looks like alpha might be over emphasized in the literature, which is a pretty startling result. For the sake of conservatism, I will just say that there's evidence to revisit. So, I've established we can use simulations in a principled way to study LDA. And so the next two studies that I did were using those primarily to help develop in one case a new metric, well, a new old metric, we'll get into that. And in the other, a slight modification of the model in a way that I think is helpful to practitioners. So, this first is extending a well-known statistic for goodness of fit that's often used in linear regression, called the coefficient of determination or R squared. So, as a quick review, R squared for linear regression, it goes between 0 and 1. It measures the proportion of variance in your outcome variable that was explained by your model. So, if it's 1, that means that your model is explained 100% of the variance in your outcome. It is a ratio of a sum of squares, which is the very bottom point there. It's sort of an important fact to understand the insight that I had in extending this to work for multi-dimensional outcomes, which is what a topic model models. So, the title here says it all. The traditional sum of squares is the sum of squared Euclidean distances. Take my word for it, but if you want to look at the formula, I can point it out to you. And so, I said, well, if it's the sum of squared Euclidean distances in one space, why can't I just extend it to squared Euclidean distances in n space? So that's what I did. And this graphic depicts that in two space, right? So, to get a total sum of squares, you would just add up the squared length of each one of the red lines on the left there. And to get a residual sum of squares, it would just be the squared length of all the black lines on the right graphic. So most notably, this is the exact same R squared from linear regression. It's just used generalizing it to outcomes in n dimensions, like a vector of word frequencies, which is exactly what topic models model. And this isn't limited to topic models, and theory one could apply it to deep learning models of language too. The only note I would give is that this is now interpreted as the proportion of variability in your data explained by your model, not variance, because variance is just a very specific formula that I broke when extending this to n space. So the bottom line is the same R squared we all know and love, all the same caveat supply, but it also means that we've had something that's a little bit more accessible to an audience that might be more familiar with linear regression, maybe coming from the social sciences or from the digital humanities, but may not be themselves quantitative experts like statisticians or computer scientists. Okay, so last study, power law nature of language means that the central limit theorem, which you may or may not have heard of in the stats class, takes a lot longer to take effect. So the practical result of that is that you need much, much, much, much, much larger samples to be able to get something that is representative of your population. So for language, that means I need a lot of data to have something that has decent statistical power. But that actually provides the theoretical justification for the use of these pre-trained language models that have gotten really popular over the last few years. So there's been this paradigm shift in natural language processing. Previously, a researcher like in most other areas would take a data set that was theirs, they'd build one model on it, and then try and do some inference or build a task. And that worked, you know, okay, but it was kind of brittle. And now researchers are starting from these pre-trained language models, they're calling them foundation models, and then fine-tuning the parameters by introducing their own data. But that foundation model can be huge fit on hundreds of gigabytes of text. They are massive. What I did is I extended LDA to enable fine-tuning of these pre-trained, enabled like that pre-trained fine-tuned paradigm for latent Dirichlet allocation. I'm not going to get deep into the math here, but the gist is you're basically using the weighted posterior of your base model as a prior for the model that you want to fine-tune. And this has been implemented in TIDLDA, which, as I said, I presented last year and had some code examples. So I did a simulation study to study this too, to see if the models would converge in probability to the true topics. I see them out of time, so I'm going to wrap it up really quickly. Apologies. But this graphic is a visual of a single model's posterior converging to the true values that were used to generate the data as you introduce more data and you're fine-tuning it. You know, caveat supply under certain conditions. Just takeaways around the left here. But if I can emphasize a couple points, this wasn't just about creating a new tool for LDA. There's theory here. Like I said, Zipslaw justifies using these pre-trained models, particularly for language data instead of just doing your own. And I want to move into a world where we can establish scientific rigor of Brown's, the use of language data. So it's in an R package. I don't have time for the novel features nor am I going to summarize the results. But this is how you can contact me if you'd like."}, {"Year": 2022, "Speaker": "Merav Yuravlivker", "Title": "Talk Data to Me", "Abstract": "Ever feel like you're the only person on your team who \"gets\" data? Have data everywhere, but no insights? Merav Yuravlivker, CEO of Data Society, will share her experiences and best practices for developing a data-driven culture and a common data vocabulary. You'll walk away with an understanding of what it really means to be data-driven and concrete steps that you can take to make a big impact in your organization.", "VideoURL": "https://www.youtube.com/watch?v=qvEG6V5eROg", "id0": "2022_13", "transcript": "She has gone skydiving in New Zealand and Vancouver. What you think about is probably about the same latitude, probably very similar geological features, I'm hoping. Same air, you're flying for the same air, right? All right, everyone, please welcome Marav. Hi everyone, thank you for being here. My name is Maravia Blisker, I'm the CEO and co-founder of Datasys. I think my talk is a little bit the opposite of Tommy. I have like zero math, not technical. So we're finishing out the day with talking about data, which is really what this conference is about. So in order for you to know where I'm coming from, let me just give you a bit of a background. Data society as a company, I helped start back in 2014. In a nutshell, we help professionals use data better. We deliver custom data science training programs to large organizations, Fortune 500 companies, and other large organizations. And we also do some consulting projects as well, kind of come out of that. Mostly to do with building software predictive algorithms. But the reason we really got started is because back in 2014, I did not know data science. I'm actually an educator at heart. I started my career in New York City public schools teaching elementary special education. Still the hardest job I've ever had. And so I was really good at education and I used data to inform the decisions that I was making, but as I was trying to learn more, I couldn't really find anything that spoke to me as a professional. So serendipity happened, I met with a few friends, and we also have the same problem in different fields and decided, hey, if we can't find it, let's build it. And so we ended up building those types of solutions. And we're still here eight years later, so we must be doing something, right? For today, at the teacher, I always like to put my objectives up for anybody who knows. And I need to remember what they are, so I'm gonna look up. We're gonna talk a little bit about professionals and some of the challenges that they're still facing today, some of the trends that have changed from 2018 to now. And then some of the things that you can do to help increase being data driven within your organization. So I know I'm not allowed to take questions from you, but that doesn't stop me from asking you questions. So just out of curiosity, how many of you work with data today? That's like almost everyone. How many of you have colleagues or a manager that don't really know what you do? Okay, also a lot of people, right? So this is something that we actually found to be a really big challenge because the lame jokes that I like to say is if a data scientist finds an insight, but nobody around her understands it, does it matter? Right? And so what we see all the time is you have these really amazing data professionals who wanna have an impact, but they can't convey it appropriately because people don't know what they're talking about, they don't know what they're saying, they don't have the right resources. And so what we've really found is that it's actually a communication gap. There's no common data vocabulary. And that's really one of the big challenges that we solve. And why is it so important? It's because without having data literacy, your organization, your company, it's they're gonna fall behind, right? There's a lot of data out there they need to be able to use that effectively. And I like to show these types of statistics to the executive audiences that I present to you, the people who don't use data on a regular basis, because I want them to understand exactly how important it is. Most, I would say most industries today are still not using data the way that they need to be using it. And it's not because of anybody in this room, you all are doing a great job. It's because everybody else is still having difficulty understanding what that really means. Okay? And these are again, these are some numbers, I think back from 2020, I would say they're still true today, it's not more. A lot of people still feel really overwhelmed with data. We talked to a lot of folks who are buried in Excel spreadsheets or are doing SQL queries all the time because their colleagues don't know how to do them. I see some heads that are nodding, right? They're asked to build these Tableau dashboards to look at one metric that nobody ever needs to look at again. So we're seeing a lot of wasted time, a lot of wasted energy because people are not able to communicate. And we see what the impact can have here. Advanced organizations are more than 20%. What did I write up here? More profitable, right? And so think about these numbers, and I will tell you how you can leverage this later in the talk. So with that, let's turn back time a little bit to 2018. 2018 Black Panther came out. People were eating tide pods. Do you guys remember that? I didn't understand what this was, but it was a thing. It was a thing. Now people are doing other things, like drinking bleach, I don't know, it's the whole thing. And then like Beyonce absolutely crushed it at Coachella. K-pop also became a lot bigger. Harry married Megan, you guys remember that when they were still royal? Yeah, that was like a big deal. And so a lot has changed. And what we were seeing in terms of data trends, right? So first off, there was a lot of data pipelines that were being built, a lot of focus on that. There's also a lot of advances in NLP. Like obviously, Tommy is continuing that trend in 2022. And what we were also seeing is that there was an increased demand on model interpretation. And back in 2018, we were seeing these communication issues. So we decided to do a survey. So we released a survey. It was like, I just put basically a blog post on Medium. I created a survey that was, I don't know, maybe 15 questions or something like that. That was about the plight of the frustrated data scientists, right? Because we kept hearing this story again and again. And here were the results that we saw. So back in 2018, we saw that leadership and other non-data people didn't really understand what data was. 30% of data professionals times were spent on explaining what they do to others. Does this resonate with anyone here? Again, seeing some heads not. And magic. Did you guys know magic existed? It does in data, apparently, because everyone thinks that it is magic to do data science, right? Managers, people who didn't have an understanding of that. And so that was a really big pain point. And so from that, we said, OK, well, how can we empower data professionals to overcome this challenge? So we created a toolkit back then. And what it was is it essentially was a presentation that combined all of these different myths and tried to dispel them in a presentation, 20 to 30-minute talk that anybody could download and put on, had a script, had slides, and had everything else. We got a really positive feedback. And then we went back to regular life. People keep downloading it. Sometimes I'd get emails saying, hey, this was really helpful. And fast forward to 2022. Again, a lot of things have changed, right? We got COVID. We started to see data needs to be everywhere. Big focus on data literacy. How many of you all have heard data literacy of data literacy before? How many of you all have organizations that are like, we're going to be data literate. But then the next five years, we're going to do it. We have a few people who are like that. OK, maybe not. Maybe you want them to be. I don't know. And we decided to rerun the survey, because we said, well, it's been a couple of years. What's changed? The answer is sadly, shockingly little. I wish the results were different. Keep in mind that the sizes are quite different. So I actually put the survey link on one of the first pages. Please take it so that we can get your feedback and your input. And we can incorporate that in here. There are not that many big differences. So when we ask the question, what do you want your manager, your leadership to know? In 2018, it was what data science can and cannot do. And now in 2022, it's really focused more on how data science can impact the company. But they just switched. There's not actually a huge difference. So it sounds like four people kind of understand that data is important, but don't really understand what to do with it. One of the things that, again, is very similar. When we ask this question, do you think your organization is using data effectively? Two thirds of respondents said, no. Just a show of hands, how many of you resonate with the two thirds of respondents that we got here? A lot. OK, take the survey so we can get those responses. So we're still seeing this. Even though, I'm sure you've seen over the past four years, data, everybody's talking about data. Everybody says that they're using data. Everybody's talking. Think that they're using AI, right? But are they really? That's the question that we were trying to answer. So some other trends that we were seeing. What are some of the biggest changes? We're seeing a lot more people are hiring for data-related roles. There's more training opportunities involved. Pretty sure that's why we're still around. And then some of the biggest challenges, production. So we got a lot of comments about putting work into scalable production, which we did not see in 2018. So that was actually one of the bigger changes that we saw. People really want to get production code up and running, right? We're seeing a lot of platforms that are really coming to the forefront to empower that. Getting access to the data that I need. So this is for government conference. We do work with a lot of government agencies. One of the things we see most often when we run some of our programs is that our students can't access the data sets that they need to do the analysis that they want to do. Sometimes it's for a good reason if we're dealing with data that has HIPAA or personal information. Obviously, there need to be protocols in place. But equally, people are being prevented from doing good work because they don't have access to their data. And so again, these are some of the challenges that we see every day that we're working with to overcome. Something else that I thought was interesting, misconceptions. How easy it is to gather data. Have you heard this before? Have you folks heard this? You're talking to a manager about a project, and it takes you three to four weeks to even find the data set and make sure that the data is OK. And then your manager is like, oh, that should have taken a couple hours, right? No. We all know how hard it is in this room to collect data. Machine learning, build as magic, apparently. And this is an interesting comment that I saw a few of. Data science can do anything because Google does anything. Google can do anything. And therefore, we as an organization can do anything that Google does. OK. Again, do they have the infrastructure in place, right? So we're seeing a lot of these challenges that still exist. And we're doing a revamp for our toolkit. But what I want to do today in the last eight and a half minutes that I have with you is talk a little bit about what you can do in order to go back to your office, to your team, to your organization, and start to break down some of those barriers that we see. So what does it really mean to be data-driven? So we created these two axes. This might be familiar to you. Maybe you've seen this before about what it means in practice to be data-driven. So we have data literacy, and we also have data infrastructure. And as we can see, all the way up there in the top right, you're data-driven. If you have solid data infrastructure, people on your team understand what data is. They understand how to ask the right questions. Awesome. You're there. Feel free to close your eyes and just try not to snore for the rest of this talk. Otherwise, you might have some work to do. And we actually have a questionnaire, I think it's 10 or 20 questions that can actually help you identify where you are. Now, what's really interesting, and this is what we do with a lot of folks, is that different teams and different levels have different responses. So some people think, of course we're data-driven. I have access to all the data that I need. I have access to the infrastructure. I can access PODSIT or Python. And others say, no, I have no access to the data. I have no idea what my other team members are doing. And so a lot of times, these answers are different. It can be very segmented. So let's look at the first axis, which is data infrastructure. Again, as you're talking about this with your colleagues or as you're thinking about this for yourself, ask yourself these questions. You have data collection. You have data storage, and you also have data security. So these are the three pillars of data infrastructure. And this can mean different things to different organizations. If you're a health care organization, obviously data security means something completely different than if you're not. And you also have to think about data storage, how is it backed up? A lot of things are in the cloud now. I don't think I know many folks that still do anything on prem. But how is that backed up? How is that backed up? How is that accessible? So these are a few things that are concrete pillars that you can go to your leadership with. Or if you are leadership, you can go to your colleagues with to talk about, where are we on these things? What do we have? On the other axis, we have data literacy. So again, we have these three pillars here. First one is data leadership. One of the biggest factors that we found to be either success or not success when it comes to implementing a larger data-driven policy is, is your leadership bought in? Can they allocate the resources? Can they allocate the time? Do they understand what it means to be data-driven? Make sure your leadership is on board. Data governance is another big thing. How many of your organizations have data governance, have talked about data governance, are implementing data governance? You. Oh, I would get on that. I think that's a really important component, because data governance essentially outlines the guidelines for how to use data responsibly. For example, there's a ton of dark data that exists, because people have a bunch of those Excel spreadsheets downloaded to their computer and not anywhere else. So what happens when they leave and their laptop gets wiped? All of that data goes down the drain. These simple guidelines of where to store your data, how to label your columns, how to handle data appropriately. Minor guidelines that make a really, really big difference. And last but not least, data knowledge, does your staff understand how to ask the right questions of data. One of the biggest things that we see, if you're a data professional, is they get a lot of queries from their colleagues about SQL queries, about questions that are business related, but they're not really expanding on what they really need. And so there are, again, these miscommunications. If your colleagues around you understand how to use data, how to ask questions, they feel empowered, they can actually do some of that work themselves. And then that leaves you to really dig into more in-depth, you know, algorithms and modeling and things like that. Okay. I have four minutes left. I think we're good, actually. How can you encourage data-driven thinking? So we've been doing this for over eight years. We've seen a lot of ways for this to be implemented successfully. And it starts with you all who are in the room. So the first thing that we like to say is to be the role model yourself. Sometimes that sounds really daunting, but you can take one step at a time. So what does that really look like in practice, right? If you are running a project or you have colleagues that are doing an analysis, ask for those metrics and show them what you mean. If you have a prototype there, you can also ask questions about the data, dig into it, do people account for those biases? Where is the data coming from? Are those models valid? Have they been tested with real-world data? Use tools and exercises to get people out of the box. Demonstrate your decision-making during meetings, especially if you're, again, in that leadership, like literally point to the chart that you are using to inform your decision, right? Be very deliberate about that and start to show people what that means to be data-driven and engage in creative thinking to drive innovation. So one of the events that we love to see are hackathons or lunch and learns, and I think I'm eating into my next slide maybe, a little bit. So find ways to showcase the good work that people are doing. At the same time, provide people a safe space. One of the things that we see most often is people are afraid to fail. They're like, I'm gonna do this model and if it doesn't turn out exactly right, I'm not going to be allowed to do anything else. I think everyone in this room understands the first time you run a model, it's not gonna be perfect, right? We have to iterate. Sometimes we need to go back and get more data or we need to redefine the question itself. Be a safe space for other people. Let people explore their ideas, help them talk it through, and provide them a place to fail because that's how they learn from it, right? And that's the best way to do any of these projects, in my opinion, is to be able to really learn from our mistakes. And if you provide that safe space, people are more likely to take that risk themselves. So that's really important to do as well. And last but not least is give recognition. People love to be recognized and one of the biggest drivers that we've seen to help inspire others is showing successful projects within an organization. So let's say you're on a team that did great work. Maybe they implemented a model, they were able to automate some processes. Put on a lunch and learn, put it in a newsletter, do a hackathon around the next stage of it. Shout it from the rooftops because other teams will see that and they'll say, oh, you did this within the restrictions that we have on our networks, you did this within our organization's mission. And I actually think I have a similar problem that we can solve, right? And it's really fun for the people who are doing the hard work, frankly. It's important to give recognition when that good work is done and also when there are improvements that need to be made. So this was a very quick talk. I have one minute left, so I think I did really well on timing. But hopefully you're able to take away some of these action items, maybe pick one that you think is that low hanging fruit that you can go back to your team with next week and just say, hey, let's start using charts, let's start using the dashboards that we've created to actually drive some of those decisions that we're discussing. With that, I'm gonna finish out my talk just to say if you wanna learn more about us, you can go to data society. You can also just email me. I didn't include my last name because nobody would ever email me correctly. So it's just maravatdatasociety.com. All right, thank you all so much."}, {"Year": 2022, "Speaker": "Jake Dyal", "Title": "Building Knowledge Systems: Integrating Expertise via Knowledge Graph Solutions", "Abstract": "How the Certus team helped national security analysts drowning in data by augmenting their data systems with human expertise and automated data engineering solutions that helped them tackle four disparate data sources, over 2 Bn records, and 40+ Terabytes of data while discovering along the way that enhanced knowledge beats rudimentary data and poorly-trained AI.", "VideoURL": "https://www.youtube.com/watch?v=Hl_635RpSPs", "id0": "2022_14", "transcript": "I know for nine years working together of him, he served overseas and some perilous places, does a really cool great way at wrangling people and getting them together and getting stuff done. Very nice person, if I was in a car accident, I'd call him to come rescue me, right? But also, he is a master in jujitsu and he's skilled in many different ways to choke a person and get them in a submission. So please welcome to the stage, by his fake name, Jake. All right, thanks for having me here today. I'm from Sertis Group, we're a data knowledge engineering solutions company. And like Jared said, I've worked in government quite a long time, like 23 years. Last 10 years, I was a government survey and I met my other two founders we came together, trying to provide solutions. We found out there was a lot of data engineering solutions that were relevant to government. So I'm here to talk a little bit about what we did, what we were in government and then what we're doing now, how that relates to you guys. So as far as, oh geez, sorry, as far as geopolitics goes, we think we're in a critical point in history. Data is really ubiquitous and it's really eroding a lot of the national advantage that we've had for quite some time. Jared likes to have this joke with me that I'm like a proto hipster because I've been doing data since before it was cool because I was in the community. But the reality is, it's like, we used to have all of these systems that just gave us data and we didn't get data from the outside. And now everybody wants to get data from, commercial sector, it's producing so many different data products. So defense, national security, leadership says, yeah, we need transformative innovation to help strategic competition. Industry has been disrupted by data products. And most of that new thinking that's coming out is like, hey, give me more data. Well, data's like Mark Twain said, data's like garbage. Just to add more data, it'll be fine. No, I'm here to tell you it won't be fine. You need to have a plan. You need to know what your questions are. You need to know what your concepts are. This have a problem, get data for it syndrome. It's not good. Think through those concepts, think through those questions, turn those concepts into ways to constrain your data. And I'm saying all this and I'm being a little bit preachy, but I've been there at home. Let me tell you, I am a self-professed data addict. I would go and like, this is one of the projects that we did towards the end, you know, get 40 plus terabytes of data, I would get bank secrecy act data, I would get satellite data, a whole bunch of different data sets to integrate as a solution. I would go to a variety of geographic and back commands as part of my job at R&D facility I worked at. To provide them solutions, they usually didn't have data. Now, I would try to find something in commercial land, you know, engineer something that they could utilize, but you know, it's not necessarily the right path to just get more data. You need to come up with something that will organize and constrain that data and help you answer the questions. So in this one problem, I'm gonna tell you about, it's kind of extreme case of way too much data, but it was purposeful. We got over 40 terabytes worth of data, no images, no videos, just text. We bought all of what chain allowances had at the time. Why? Because we're the government and we spend lots of money. You know, we bought all the Bitcoin, Litecoin, Ethereum data, Solana data that I had integrated over three years worth of container shipment data, you know, US Treasury Bank Secrecy Act data, had a bunch of different shipping transactions, cryptocurrency transactions, and lots of variability and structure. We had unstructured structured, the Bank Secrecy Act data had 93 different fields, and each one of those reports was actually an investigation that financial institutions submitted that was really a roll-up of a bunch of different transactions that they looked at for maybe years. And we had like, I was at 90 million of them. So a lot of variability in data over a billion distinct entities. So what we did is we focused on our questions and then we came up with constraints based on those questions. We really focused on effects that we were trying to generate with those law enforcement partners that we had. We were doing money laundering operations at the time. So we looked at sanctions. We know that we wanted to support law enforcement, you know, drug operations, money laundering, and then identifying sanctions related activity. So we really focused in on how to create a structure that would enable that. So this is kind of like a toy version of an ontology that we created, but you see there's kind of variability in the entities and the relationships that were up there. The point of showing this is, this isn't what was in the data. This is what we transformed the data into. And we left data on the floor, like quite a bit of data on the floor because this is the stuff that we needed. So we created, you know, transformation pipelines to create that top level ontology. We didn't want to be too prescriptive. You know, there was a lower level that a lot more changed to it, but that upper level is what we really focused on. And that was able to answer, you know, which sanction envies involved in what commercial behavior, what companies, what shipments. And the relationships were key. If you look up there, you know, I have that variability between, yeah, the person is in this location, but they've transacted with this financial institution. So you can ask different questions and be able to answer, hey, what people are associated with what company? Who shipped these goods to what location and what sanctions program is that connected to? And all of that was available through a graph traversal, different type of query that we ran. So we did that. So what? Well, it enabled more rapid targeting. We worked with 1G, graphic and back command. They had a targeting shop. They would do it manually. They would go through not this data, but some of it, they had some of the data and they would generate a target really every 30 to 45 days. One target. We got it down to every 30 minutes. We trained an analyst on how to do queries in Gremlin. And we said, okay, here's the graph that we created. Go to town. And two weeks ago, I did over 135 targets. And that 30 minutes wasn't, you know, all the queries that he ran, that 30 minutes was like, oh, what is this? I found something, is this a real relationship? Is this not a real relationship? Were there areas in that data engineering process? These are really validating a lot of those relationships. So that wasn't query time. That was his investigation to identify. And then there was a lot of unknown connections that were made. And then government does what the government does, created a program office and try to productize this thing. And we had bought a bunch of commercial software. So created a pipeline, a bunch of people were, they're now doing this for all of defense support to law enforcement. And that's great. But we came up with these ideas and these processes. And we thought, hey, we can do something with this. This has broad applicability. And that's really what I'm here to talk about. It's like, if you go back to the effects, you're trying to generate. And the military is great because, you know, they have all of these orders and these plans and they write down, okay, we're trying to disrupt these people and that's the effect we're trying to generate. But, you know, even in your organization, even in business, you know, they have effects and things they're trying to do, concepts that they want to identify. So I'm gonna show you an example of a commercial application with the same kind of thinking. And we created some software that, you know, demonstrates this thinking as well. So I'll give you a toy problem that a lot of services companies have. You know, is the juice worth the squeeze, right? Hey, I'm working for this person. They're giving me a lot of revenue. I'm doing a lot of invoicing, but, you know, what are my expenses associated with that? How much money did we get for what we did? That's a common, you know, commercial related problem. So we created this toy ontology that you see here. And a lot of ontologies are a little bit more complex, but have more hierarchy, but they don't have to be. You know, they can be really simple as long as you're interrelating your data. And you notice those key differences in the relationship types, you know, services, you know, belongs to, works on, and that's reflected in the data. And, you know, we're taking data from four different systems. You know, we have our notional ADP data, GIRA data, Salesforce data, QuickBooks data, stuff that doesn't normally fit together. You know, we'd have to download or hidden API, something like that to generate. So I'm gonna do and show you a video of what we were able to do here. So you see that same ontology there on the left part of the screen, where my left, your right. That's what we're using to engineer the data. So we had already done the data pipeline. We're gonna show you a query of what we're running against it. So you can see some of the relationships that we created. And zoom in on one of those fields there. Looks like we have a company, Soylint. You know, they had this Charlie Heston Charlton, maybe is what he goes by. Had a project called different green tank. And you see all of the different relationships there, invoicing, expenses, charge to, belongs to. And it's all showing up as the fields there on the right hand part of the screen. And I think we're gonna switch to a different query here in a second. And this is a very engineering heavy solution, engineering heavy visualization. This isn't meant for like that final exploratory data analysis product, but this is just to verify, hey, I did my engineering correctly. So now we're gonna change some of those charge to queries and then put in services related queries so you can append that existing graph and see like, okay, I really did add in those employees, miles and Arnie to that cyberdying systems project that they were working, something associated with terminators. And then, you know, they had bills that we invoiced to them. Okay, so you can see all of the relationships there. Now that I've done that engineering and I've looked and I see, okay, I have all these relationships. Let's get the data, let's download it and let's do something with it. So that's in essence what we did. So after we downloaded that and I'm just gonna zoom in real quick so you can see, now you'll see all of those relationships together in that table. I know you guys probably can't see, but there's invoices, they're connected to the different projects connected to the different customers, all together in one consistent data product. And that consistency is actually really key. If you wanted to be deploying models on, you wanna consistent data product. So going through the analysis, back to the original question and this is the key part, you're answering a question. You know, that original question is, is the juice worth the squeeze? You go to that time spent on each customer table and I did this in Luka in a combination of Luka in Excel just to show there was possible on that flat CSV that we generated. Down on the right of that time spent on customer table, see Glenn Gary, Glenn Ross as, you know, the person that ever the, excuse me, the company that the, they spent the most amount of time providing services to. And if you go to that percentage of revenue from each customer on top. Yeah, that's the biggest revenue for that company, but that client value score, Glenn Gary, Glenn Ross scores all the way kind of towards the back. And the number one is this other company's soil. And the point I'm trying to make is that those relationships and that integration and those questions really matter to how you engineer the data and how you do your data pipelines and how you provide that consistent product that you then deploy your models to. Another way of saying it is you're trying to add context and that context enables the understanding and informs decision making. So this little triangle example here is like showing you, yeah, you get raw data and you try to create information and meaning, but the context of how it relates to that individual or that organization is really the part that you have to implement. So the raw data in this case, it would be three backlit colors, red, yellow, green. If you go to information and meaning like, okay, that's a traffic light. And it's at the southern part of this intersection. Okay, but the context and knowledge related to that is I'm driving at that intersection. I'm driving at that traffic light that's red. I better stop. So you really need to have ways to add in that context and just have a plan for your data. Know what questions you're answering, what effect that you're trying to generate, what's that overarching strategic goal and then ask your data, hey, like what is it that I'm trying to do with you? Maybe the data that you have internal to the organization isn't the right data. That's okay. You can go get other data or you can collect data within your organization. There are surveys, there are ways to go in procured data. Maybe the data you have isn't right for the question. And that's something that you need to come through in your analysis. Data operations is the thing. Data management, data governance, data lifecycle management combined with content management and task management. That's something that you should do. We have an open source ebook that just click the link, go to it. I'll show you how to do it. It's tool agnostic. It's out there. We want people to do it. I was very happy to hear Marav yesterday talk about how data society is doing it and training people. Please implement some of those basic data management skills, data governance skills. But most importantly, turn your questions into concepts and then use those concepts to constrain your data in some kind of engineering format. So you can get a consistent data product that you know will answer your question. And that's really the point I'm trying to make is bringing those questions to your data makes something that's not necessarily like what the data is and then transform the data to it. And if that data isn't the right data source, you'll have some kind of metric of value because the most important thing isn't the data. It's your question. It's the strategic goal. It's the effect that you're trying to implement. And that's pretty much all I had to say."}, {"Year": 2022, "Speaker": "Max Richman & Danya Murali", "Title": "You R an Analytics Engineer!", "Abstract": "A gentle intro to dbt from some folks who have worked extensively in R over the years. It would be a light intro to analytics engineering, testing, and other prerequisites for good production ML in R.", "VideoURL": "https://www.youtube.com/watch?v=3il9ald1lkY", "id0": "2022_15", "transcript": "There's two of them. So one of them spoke at the very first New York car and at the very first R-gov. And he will always speak at the very first conference in every city we put on. Despite him being the reason I had the only time I've ever used data was because of him. I'm still bitter about that. And our other speakers, speaking with him, Madam, at this conference back when you gave a talk, she got a job of him. And she gave a talk the next year. So she's also a repeat speaker. And beyond that, our other speaker, she actually teaches Bollywood in Maryland. So please welcome Max and Danya. Hi, everyone. We are super happy to be here. Thank you, Jared. Thank you, Landar\u00e9 Aladex for inviting us back. I think this is our third year speaking for both of us, which is kind of crazy. Today, we are going to talk to you guys about how you two are an analyst at Jourone. So you're probably asking, who are the two of you? Maybe you're not. His Jared just gave a very nice introduction. But I'll tell you anyways. So like he said, we both like R. We met at this conference. And I only own one shirt, which I am very proud to be wearing today, which I was also wearing in 2015 when I talked about survey analysis. And then I wore it again in 2018 when I talked about talking about survey analysis. And at that talk, I said, I want an infinite recursion photo. So please get this photo, where it's me talking about me talking about me. And I'm just going to keep doing this every single time. Enough about me, Danya. So I don't need that infinite recursion photo. So I do change my outfit every time I speak. But yeah, in this picture over here, when Max was talking about SQL for everyone, that's actually when I met him at this conference. And I was in the market for a new job. And I met him. And I was like, oh my gosh, this sounds like a great place to work. And so rest of history, it's been four years. In 2019, I had the opportunity to speak here, which was great. In 2020, I did it virtually. And now I am super happy to be back in person. And yes, my fun fact is I'm also a Bollywood dance teacher at Zind Academy. So if you ever want to do some Bollywood dance, I'm always down. And then the actual important part that's related to us is that we're also a clean tech data scientist at Arcadia. Hence the song Energy. Yes. With that, who are Arcadia? Who is Arcadia? I wanted to keep with the Arbit, so it works. This is from our website. Arcadia is a fast high growth tech company that was founded here in DC. And our mission is to clean up the energy grid. And we're doing that two main ways. Number one, we are the leading manager of community solar projects, which basically means solar projects that can get installed on farms or roofs. And other people who don't live in those houses, who maybe can't access solar from their roof, are able to get the value and savings of solar while greening the grid. And the other product line is our platform, which is basically taking the software we've built for community solar and licensing that out as a SaaS tool for any data entrepreneur, any energy entrepreneur, basically trying to digitize the electricity grid in many ways that the financial sector has done with stripe and plaid, working with energy data is simple API. With that, do you want to talk about community solar? Yeah. So community solar is the main line of business that Max and I work on. And I love community solar for a multitude of reasons. One of the big ones is that it truly is access to clean energy for everyone. So if you think about solar, I imagine that most of you guys think about having solar panels on your roof. Like this building. Like this building, for example. And sometimes on the White House, depending on the president. But the issue with putting panels on your roof is if you don't own your roof, if your roof has trees near it, if it's not facing the right direction, if your credit score is not high enough, there are a multitude of reasons why solar is not accessible to the general person. However, with community solar, with building farms near communities that allow you to get that solar power from that specific farm, you get access to solar to all people. You pretty much democratize the access to renewable energy. This is also great for working against a lot of the energy injustices that have come up from using fossil fuels for many, many years. And so what we do is primarily focused on the data aspects related to getting people access to the community solar product by getting people onto our platform so we can then service them. This is despite our talking about it, not the main thrust of our presentation. But we wanted to give that background. If you do want community solar, if you live in Maryland, DC or Virginia, it is available, Arcadia.com for more. And that's the end of the sales pitch. What we're really here to talk about is analytics engineering. Raise your hand if you've heard that word before. A couple of hands. All right, so data science is all about Venn diagrams. We have the original Drew Conway diagram where we're hacking and stats and domain knowledge, danger. Like, that's super cool and exciting, right? So here is another kind of starting to emerge Venn diagram in the data science world, which is it's great to follow Jake's presentation about the importance of data engineering. And Don and I work primarily in that big purple circle on data analytics. But what we're here to talk about is how actually that overlapping section, analytics engineering, is really critical for the success of any type of data work, whether it's analytics or machine learning or what have you. So with that, I'd like to tell a story about how us two data scientists discovered that we actually needed analytics engineering for us to build through our jobs. So like I said before, we are an energy tech company. We want to get everyone who can be the power bill onto the community solar program. And therefore we need to acquire customers. So we were like, oh yeah, we're data scientists. Like we can totally do that. We can build predictive models. We can tell marketing that we can use our predictive models to help target certain people in certain ways so we can help them get onto the platform. And we were like ready to go and ready to build this super cool predictive model in R. However, any of you guys who have ever built any sort of model, you might notice that the hardest part of it is actually cleaning your data, acquiring your data, storing your data, cleaning your data, making definitions about what your data actually means in a business context. And then also, once you do all of the modeling, storing the results of your model somewhere that's actually useful, being able to update it and iterate on the model, and then being able to export the results of your model off to some stakeholders. So in our case, this was marketing people. So we needed marketing folks to be able to access the output of our predictive model in some sort of app or something. And we needed to build and enforce data quality and document everything we did. So basically, we were super excited. We were like, yeah, we can build this. And then we were pretty much massive halt and went, wait, we actually don't have the infrastructure to do any of this. And so what we really discovered is we needed to build the foundations of a data ecosystem. We really needed generation storage, orchestration, and transformation of our data. And then using that, we could then create, export, and maintain governance and discovery of our data. Raise your hand if you think you do ETL, meaning extract transforming loading of data. I mean, come on. If you open R, you're loading data in, right? You're doing something. And then you send that data to somebody, right? That's the classic. I mean, maybe you're not writing the code to extract it from some hard to reach place. But you're doing some form of ETL. How many of you do ELT, extract, load, and then transform? Ooh, not as many hands. OK, well, that is kind of the real innovation here. How many of you would say that you probably spend 90% of your time cleaning data, munging data? I'm seeing more hands. All right, it's time we embrace our true identities. Friends, we are data munchers. We are data cleaners. This is nothing to be ashamed of. This is something to embrace. This is something to celebrate. This is something to be intentional about. And so that's what our talk is really about today, is that thinking about what we're doing with the data cleaning process helps everyone downstream. So your data might be generated in Salesforce, or it might be generated in JIRA. We saw great examples from Jake, the different places that data can come from. But then you need to be able to pull it together. And so what we do is we throw that into a data warehouse quickly before we add a lot of information to it. And that allows us to then be intentional about what we want to do with it. So let me tell you a story of how we've evolved at Arcadia. When I started there, which was only about a year before Dania, we just had one primary application that we would pull data from. We would put it in one data store at that time. It was Amazon Redshift. And then we would all hit queries on it to just answer questions. And that worked when we were a team of one, four, two, or three. But we just needed to scale. So we then moved to a concept of more of a data lake, where we're pulling from all of these other systems, especially as the company scales. We sign up for this SAS tool and that SAS tool. We have Stripe data. We have Zendes, whatever, all the different data systems that are producing data that you want to be able to answer questions across. So now we're integrating all that data into a data lake. But still, you can see from all the messy lines that are crossing, now I have to query something that's pulling from this SAS tool and this corporate database and all these things. And it's still messy. So what did we figure out that we needed? We needed a new concept, which I don't love the name of it. But I guess when you have a data lake, you need to organize a data lake and determine Vogue has become Lake House, which sounds pretty pretentious. And I guess it is kind of pretentious. But anyways, that's the term that I'm going to use to talk about how we develop business concepts on top of that data lake. So now you're not having to hit all of the different assets, but you create common definitions from those assets that can be used downstream. So ring this back to our concept. We want to be getting good data, and then we want to use it well. And in doing so, we have different roles to play. So we have a data engineering team that helps us wrangle those data sets, pull them into our Snowflake Data Warehouse, that extract and load step. And then we have our transform tool, which we're using DBT, which I think stands for Data Build Tool. It's basically a command line tool for documenting and automating tests. I'll get more into what that is. And that is what we're really using for our analytics engineering layer. And then we have our downstream analytics and modeling that flows from that. And so what we've realized is we have a role to play as analysts, not just in the analytics downstream, but also in defining the core business concepts. So what are some of the advantages of using a lake house or some sort of transform layer? First of all, when we are developing new business concepts, like Don, you had the example of what is a customer? Like, how do you define what a customer is? Surprisingly hard. You would think it's easy. It's actually very nuanced. And lots of people have different takes on it. So if you want to develop a concept of a customer, but then, oh, you got a new product that just emerged, and we need to also consider them customers when before we didn't call them customers. OK, I'm going to change that. But what's that going to affect downstream and all of the other models and data sets? So the lake house runs tests and allows you to see, oh, OK, if you change the name of this thing or you change the way it's ingesting data, it's going to break these other things down that are being used. So it actually is bringing a kind of engineering rigor to the analytics and documentation that you're doing. The next thing are these configurable tests. So yeah, the code QA is basically like, we can push our code and it will run and create a QA instance of it before we ever push it live into Tableau or any business intelligence tool downstream so that people can actually check it and see the numbers and how they're going to change before they actually change. So then we have the configurable tests, which allow us to be able to see, you know, did it break things downstream? Did this model break or not? And then last but certainly not least is what I'm calling docs out of the box. Because we are putting so much an emphasis on bringing data all the way through, we're giving things good column names, we're writing descriptions, we're putting the tests all in this layer of mostly yaml files. And I'll show you what it looks like in a second that then automatically produces a website that has ERDs, like you can see this diagram that's showing how the tables flow to each other and also has documentation for discovery and people to be able to answer their own questions and understand what's in the data. So very briefly here, this is what it looks like and feels like using DBT. You have VS code, your code editor, you're managing your models on the left-hand side, you're coding them with templating language that basically compiles to SQL. Sorry, I'm still talking about SQL at an R conference, I just can't help myself. And then here on the right in the yaml files where you're doing the documentation. And so when you build your DBT models, it then creates the table and runs all the tests that you specified on it. Okay, so Max likes to not talk about R at R conferences. I'm going to try and bring it back in a little bit. So going back to the original problem statement, what we were asked to do was to build a predictive model that would predict the likelihood of a customer staying a customer, given that they convert based on a bunch of features. And what we really wanted to do was make this a production ML model where the customer, so in our case, let me not use the word customer, the stakeholder, which in our case is marketing, will have the latest version of our model that is learning every time we get new data and every time we get new customers and helps them identify who they should be focusing on. And so the basic first step is the machine learning workflow. So we need a bunch of model inputs. In this case, we were looking at demographics, like a graphic and geographic data. And we were combining that with survival data of passing current customers. And basically survival data meaning how long did they stay? Are they still customers? Did they leave? When did they leave? From there, we did actually use R. We use survival analysis packages in R. The specific ones being RMS survival and survive minor. And doing that, we were able to create this pretty nifty probability of survival model at a selective time period. So essentially at 30 days, we could say this customer or this individual based on their demographic, psychographic, geographic data is whatever percentage likely to still be a customer at 30 days or 90 days or one year, what have you. But what we really wanted to do was take that entire model and throw it into this DBT structure. So you are in a situation where you can continue to get more observations, continue to have your model learn from those new observations, continue to make different predictions and save those different predictions into a production model that you then send off to your stakeholders. So what DBT suggests and the way that we implemented it was with these three different models. One thing that is confusing is with DBT, the language used a lot in analytics engineering is a table is called a model, but in modeling a model is called a model. So those things are confusing. In this particular case, I am talking about tables. So the observations model or table, if you structure it in a way where there is exactly one record for observation that you can make a prediction on, you now have one set of all the observations that you want to apply your actual, your survival model ought to. From there, once you do the actual survival modeling and you build your model, you want it to output into a predictions model or table in this case. And that table, you want it structured where there is exactly one record per prediction, but there might be multiple records per observation, meaning in this case, if you're a customer one, customer one, the first prediction on 10 one, you have a prediction of one year for the same customer one, the second prediction on 10 two is 1.5 years because it learned from more information. That's an example. And then finally, you want your production model. So your stakeholder always wants to know what is the latest version of this model with the most information that has learned the most and that is when you join your observation model that has all of the geo demo, whatever psychographic data for your customer and then the latest prediction. And that latest prediction is what gets pulled in to all of your reports and exports and the entire cycle is interconnected. So a better way to look at this is in visual diagram form. So you have your observations model table. It goes into your actual survival analysis model. The output goes into your predictions model slash table and you pull those two things together to your production model table. And that gets put out as either an export, a data export or report whatever way you want to surface this to your stakeholder. Awesome. So we've been on a journey, Donnie and I at Arcadia on this, there was a data engineer who was like, hey everyone, I heard about this cool tool called DBT. Should we try it? And half of our analysts are on PCs and are more point and click and half of our analysts are on max. We're on max as you can tell. And we're more conversant in the command line. So it was scary, change is scary. And so what ended up being really helpful was finding a community of practice first within our company but also outside of it, talking to people to understand like how this stuff is done. I mean, this our conference is a great example of a community of practice and why we keep coming back here even as our talks are less and less or more tenuously related to our because having people to be able to talk to about this stuff makes it less scary. I also think that it's a team sport. Like this is a thing where we didn't know right away what to do. So we started setting up like an office hour session and working through it. And then last but not least, like expertise is really valuable. So having an aid data engineer who was able to start playing with it and understanding how to actually implement it in our GitHub instance and like that. So we could actually start using it. And then we as the analysts started translating the benefit and explaining that to other analysts and other people of the company of like why we should be doing this and spending the time. But it really wasn't until we hired somebody who had done this at multiple companies who really was able to bring a lot of that together. And so with that, I want to acknowledge a lot of the content here. Comes from Sam Swift, our VP of platform who helped us really think about DBT. He's given similar talks like this at DBT conferences. And I also want to acknowledge and appreciate our colleague, Nino, who did a lot of the modeling work as well and is now a data scientist at YoFundMe. And so yeah, with that, thank you all. Thank you."}, {"Year": 2022, "Speaker": "Molly Huie", "Title": "Democratizing Data: Using Shiny Dashboards to Make Data Accessible to All", "Abstract": "Exploring how Bloomberg Law uses Shiny dashboards to make our proprietary data accessible to everyone in the business unit, not just the data scientists.", "VideoURL": "https://www.youtube.com/watch?v=ZFl445UVIHE", "id0": "2022_16", "transcript": "She's spoken to New York are before and while she's hadn't watched his career so far, she's already planning her second career of being a librarian because already this year she's read over a hundred books and we'll want to do the math on that more than two per week 2.1 use our calculator come on no sorry everyone please welcome Molly. All right friends I am Molly Huey I am the team lead for data legal analytics and business at Bloomberg Law we call ourselves data lab and since the our talk in New York my team has gone from four to twelve so we're doing a lot of things I'm gonna make sure I'm not overdoing the mics here so in having a team that big I do practically none of the coding anymore so my entire value of incoming here and talking to you is about how to manage your stakeholders so please go ahead get out your devices get out your laptop because when Mark was talking yesterday and everybody's like Richard hand and Richard hand I'm like I can be better than that I can do a slido so so either get your QR code or enter go to slido and enter that thing and I've got a little polling question for you because also like max I do survey data I'm wearing my surveys and data analysis shirts which I also wore at New York are because it is my team t-shirt and this is what we do and I wear it all the time so now I have a conference uniform it's great all right this will be up on the next slide too when you see yeah it's still got your it's got your same thing on it okay so who are your stakeholders right when you're doing your data work who are you targeting who's your end user who are the people that are feeding into your work not technical I'm not surprised and let's press it off right because who was who among you was doing data work and giving it back to another data engineer data scientist and then I'm going oh yeah I know exactly what you did that's awesome thanks like that that doesn't happen so it's kind of what tech people think you need and what actual scientists think you need the amount of times that I get asked hey can you just show me how to do this in excel cool people cool I work with almost primarily legal analyst on Bloomberg law and very many of them have said we went to law school so we didn't have to learn about numbers okay I gotcha that's why we call ourselves data counselors as well and also in Max and Daniel's talk when they kept saying dbt over and over again my master's is actually in counseling so to me that means dialectic behavior therapy so we're doing some therapy in a group today so but what I want to come and talk to you about is how we use our and shiny to make all of our survey data accessible to our very non-technical colleagues very very smart but non-technical so I have broken this down into a 3a is approach the probably the majority of the data we use is survey data right so we're working with our legal analyst on the front end to make sure we're asking the right questions because none of my data scientists are attorneys right I can teach them how to craft a really good question so that the data we get back is good but I don't necessarily know the right question to ask right so what's what's going on the marketplace so when we're talking about making data available for everyone accessibility right like we can't tell them to go run a sequel script and just get the answer like we have to make it in a way that's usable we have to provide appropriate training because like I said they went to law school so that they didn't have to do numbers so we have to tell them how to use the numbers how to think about the numbers what makes sense about this to them and then availability and follow up right like this is not something where we can be like here you go see you later have fun bye especially because a lot of this work is being published it's being published on the Bloomberg Law Analysis channel or on the Bloomberg Law product or being picked up by other outside sources and so we want it to be right um so by way of background I'm gonna go go way back machine I started at Bloomberg about seven years ago um and that was right after Bloomberg Law had gotten a qua sorry Bureau of National Affairs or BNA which was a legal content publisher got acquired by Bloomberg to make the Bloomberg Law product and when I started they hired me as the surveys manager and they said oh we've got these surveys we've been doing them for 40 years great but this is 45 pages long and this was the shortest one it was all paper we would sell them individually and all the data that we would collect we would hold on to with the title of this and run it in SPSS and print these paper tables and that's all people would ever get like you want the day to go look at our report thanks right it's terrible and also Nick if you're around here like your cyber security brain is gonna buzz because um when I got there my my new boss was like here's your file cabinet here's all the historical data and I was like oh this is gonna be great paper copies of surveys with PII on them from before I was born like why we were holding on to this it's gone now anyway I digress so enter shiny dashboards here's where we start talking about our um when we very first started doing more legal focused work um we got to design brand new surveys and I was like we are not making these paper copy reports and giving people tables that they can't do anything with we're gonna make them a dashboard and so we start with like here's the index we had a dashboard for each specific project which was fine we could reuse the code we could you know it wasn't terrible from our end but it was terrible from the end user's end because they couldn't remember what the link was to the dashboard they were supposed to use and if they happen to be able to get to this index they couldn't remember the short name we'd made for the dashboard for their project and then like a year later if we're gonna run their same survey again to trend it then we have to figure out how to attach years to it it was it was a disaster so we had the individual surveys i'm gonna add agile as the fourth A of our how to make your data available to everyone process because we had to be real agile about this and when I say agile I do not mean having a scrum master I don't have a scrum master um but we did have start doing sprint reviews and we did start doing um retros especially after newer projects like what went right what went wrong what can we do differently next time um so we're trying to be real agile about it and we're trying to really learn from our stakeholders like what do you want how can we serve you better so off of that index here's what our original survey dashboard look like this is this is a shiny dashboard we use we use Qualtrics as a survey hosting platform we get the qsf and the csv files of that data we ingest it um you know scripting into a postgres database make the tables you know use our and shiny create these dashboards works really well um so you can see we you know we've got this list of things we've got a count we've got a percentage um one of the things that we built in that was super fancy was the you see not the biggest drop down the biggest drop down is survey question the second drop downs are workforce size and organization type these are just the things that people like to filter on the most are you looking at in-house attorneys or law firm attorneys and from what size of organizations so that's sort of sorting and filtering sorry I thought I turned all of this stuff up guess what I'm not doing I'm meeting right now um this allowed people to do a little bit of their own sorting and filtering but they would still have to come to us every time they you know they'd be like hey I looked at law firm only but can you tell me what's happening with just bankruptcy attorneys or just IP lawyers and we're like yeah we can but you can't do that yourself sorry coming later um one of the things we liked about this down here at the bottom in our dashboard we have a copy csv and excel download buttons which is fantastic it's super easy one of our things at Bloomberg is that we are very brand conscious um and when we publish articles we have we call them analysis pieces because we're not central news we're editorial so we can't write news and we can't write stories so we write analysis pieces that get pushed out on a channel look just like news um but for people to know that they're us we have a very specific branding Bloomberg is real weird about having the um y-axis on the left instead of the right of your chart and you only put the things on the top like there and it's got to be the certain colors and the certain fonts so I can't do that in R I can't give people something that they can just download we have a program that runs on terminal so if any of you are finance people and runs Bloomberg terminal um there's an internal program on that called toaster I don't know why they made it toaster but the story I tell myself is that you put in you know already aggregated graphics like this and being like toast you get a brand standard graphic so that's why we made these download buttons so that you can look at the data and the cuts you want and get the get data in a shape that works to go into toaster um we also have a data visualization button on here you can't use it it's not brand standard so you can't download it and do stuff with it but it's so helpful in learning how to tell the story of your data to look at the visual first right like so we're allowing our stakeholders and our legal analysts to be like oh this is the story the data they're telling this these are some things I can drop this is where this is really going and then they just download the data table and make their chart from it so that was the first iteration the next iteration you know can it consistently evolving agile innovative multi-survey dashboard because I was telling you that problem with the index of you know you can't remember where your stuff is we ended up pulling it all into a dashboard so looks a little different we gave it a blue header so that it looked different from the other one um and we've got a whole lot of disclaimer stuff on here like this tells you how to use it and it tells you where to look for things it gives you big red warnings if you're you know you need to think about coming to our team for help with something so what this looks like I started the overview this is the kind of survey explore our piece there's a drop down and you can select which survey you want to look at so people only need the one link they come to the master survey dashboard then they use the drop down to find the survey they want you get an overview a question list and your responses so the overview it tells you when it feels that it tells you how many responses you have overall it will link to our final summary report when we're done with that and the confluence and gira so we're like we've got data on top of data in here and it's great um the other thing we have is we built in survey analysis pieces right I told you analysis pieces equal news but we can't call news so that's that's that um but we're getting so much data and we have so many legal analysts that people can look at stuff and go well I don't know if somebody's written on that before come and find out it's still a little bit of a manual process like if you write an analysis piece you need to go here and click the little teal add button and you can start typing and it'll do a type ahead it'll search the argyra system to say oh you wrote a piece called looking for a preparation edge and reconsidering law school it pulls it up it pulls up the link on the news channel and you can write a little description put your author name in there and all you have to do is hit say and then we've got an entire repository of all the analysis that people have done using survey data which is great um yeah the next problem we had we gave people free rein on these dashboards and they went bananas and the in the the amount of analysis I had to kill because I looked at it and went did you realize you use seven people you can't do that but they don't know because it's you know these are not super technical people so we built in a sample size indicator you know you start with your full sample you've got a nice green box if you start using your sorts and filters and you get down lower than 30 you get orange you get real low you get red so we've we make this really clear when you're not using an appropriate sample of data um it can't be any easier and so and when we train people to use the dashboard we say hey look if you see anything other than green please talk to me first there are ways to talk about this there are specific surveys that we do um every fall we do one on litigation finance and we survey funders and there's really only four or five major companies that are funding litigation in this way so having a sample size of 20 is actually pretty good for them but there's there's ways to talk about it to to represent data accurately um when they when you have a little just a little bit of it all right next enhancement side by sides we we added this compare results button because people were saying as we started doing surveys year over year they were either having to have two instances of the dashboard open or like slip back and forth so we just were able to split the dashboard in half and you can compare results and so you can bring up last years and this year's or you can bring up this year's version twice and look at law firms on one side and in house counsel on the other side it's really nice um the only thing it doesn't do is cross tabs wait for it just wait for it cross tabs um when I gave a very similar talk in New York this summer this cross tab dashboard was not yet live I was like guys this is beta don't tell anybody about it yet it is now live our legal analysts have been trained on it people are using it and it is fantastic um so what we get is you can choose basically your columns and your rows on your cross tab dashboard we've built in a very similar um color coding so that you know people just know if you see yellow orange red you got to come talk to us before you cross thing we've also built in um some guardrails where you can do multi-select questions and single-select questions but you can't do things that have logic ahead of them right so that people know the universe they're talking about um this has given us a lot of good training opportunities um to help people understand the data that we have who has answered which questions and how to talk about it um so I wanted to bring this one upside you could see this big red warning don't make comparisons across groups without talking to the data lab uh because when we put a multi-select question in there people are like oh you did this and this and this and this we can yeah no the same person may have selected multiple of these so again training opportunities to tell people how to use multi-select data single-select data and have it shown in a in a really good way um okay let's see what else do I have on here oh because cross tab's that's sort of so fun we made this so that it also does um what am I looking for this one um central tendency so you can you can get percentages for everything you can also get central tendency on it which is super cool it also has the um all your copy paste download and a column visibility which is really cool because if you decide like I really only want to see the people that are actively seeking new jobs or open to other offers like I don't really care about these people you can hide them from your download which is pretty cool because again if you're taking this and you know dropping it into toaster for a brand standard chart you don't want all that extra data um this is also new we uh i was telling to people earlier when i have another really big presentation on monday to our leadership about like what we're gonna do for the next five quarters and what my team's value is and what we're building this is really helpful so this is looking at the analysis pieces this is using our adobe analytics data and our jure data and combining them and saying okay pieces that we put out like either our team writing it or us helping other legal analysts on other teams writing it but with survey data those pieces with survey data have like probably about double the amount of unique visitors to our site and more average views like those pieces take it out of the park so everybody should be doing that um another thing i don't have visualized but another metric we used was we put uTM codes on the on the end of links in analysis pieces when it's driving people back to the global block product so say we write a story on like attorney well-being right like oh mental health is going down all these things are really hard go back to our you know lawyer well-being page on product we've got a little code on there that can show that they came from that analysis and went to product our pages that we own are five of the top ten on all of the product because they get so many clickbacks from pieces like this so having that data to tell our end stakeholders like writing these like new stories on you know what a lot of attorneys would feel like you're kind of touchy-feely things like are you looking for new jobs are you burned out like how are you sleeping these days people like to read it and it drives people back to platform you know so they may not necessarily buy our product because of it but it's building awareness and it's building brand equity and that's super important um so just quick recap we went from three days to four because agile's important make your data accessible appropriately train your stakeholders on both the data that you're giving them and how you're displaying it and what they need to be doing with it be available for follow-ups you know they should know how to contact you they should know that it's okay to ask questions even ones that they think are stupid i get lots of those and i i tell people it's the most fun because if they have that question other people have that question and then be agile right like learn and listen to your stakeholders be willing to make mistakes and be like yeah we were wrong about this we're going to change it next time and that is it"}, {"Year": 2022, "Speaker": "Kim Ky", "Title": "Evaluating the Minneapolis 2040 Comprehensive Plan with Synthetic Control Method", "Abstract": "With its adoption of the Minneapolis 2040 comprehensive plan, Minneapolis became the first major city in the U.S. to eliminate zoning regulations that ban the construction of duplexes and triplexes. The changes are part of a strategy to expand housing supply, increase housing affordability, and encourage equitable development. As the City implements the Minneapolis 2040 plan and related housing policies, many want to know whether the changes will have the desired effect on housing access and affordability relative to what would have occurred without the 2040 plan. We evaluate the impact of the plan using Synthetic Control Method on various housing-related indicators. We continue to track and monitor these indicators as more data become available. Learn more at: https://minneapolisfed.shinyapps.io/Minneapolis-Indicators/.", "VideoURL": "https://www.youtube.com/watch?v=QQBe5Q56H-c", "id0": "2022_17", "transcript": "Our next speaker has lived on three continents. I want to know what three. So when you come down here to tell us, please welcome Kim. Hello everybody. Thank you for coming back for the second day. My name is Kim Inky. I am a scientist with the Federal Reserve Bank of Minneapolis. I am here today to talk to you about a causal inference method that we used to evaluate the Minneapolis 24E comprehensive plan. A little bit different than the other talks you've seen so far, but I hope it's interesting to you all. Before I go any further, I want to emphasize that the views and opinions expressed today are mine and mine only and not necessarily those of the Federal Reserve Bank of Minneapolis or the Federal Reserve system. So why are we doing this? So this is a partnership between the city of Minneapolis and the Federal Reserve Bank of Minneapolis. In 2018, I believe the city of Minneapolis passed the 24E comprehensive plan that went into effect in late 2019. And so for our purpose, when we talk about post-treatment, pre-treatment, when I said post-treatment, I really mean 2020 and later, and pre-treatment is 2019 and earlier. The goal of the plan was that by 24E, quality housing throughout the city of Minneapolis was affordable and accessible to all Minneapolis residents, regardless of their raised income or anything else. And with the adoption of this plan, Minneapolis became the first major US city to eliminate zoning regulations that ban duplexes and triplexes. Yeah, in what used to be single family detached homes only, zoning. So how did we do this? There are three topic areas of interest. So what the plan was, the adoption of the plan, what we wanted to see was that there would be more housing, more affordable housing, and also more equitable housing. There is no one indicator, metric or measure that could capture all these three topic areas. And so what we ended up doing was to develop 10 different indicators that capture different aspects of these three topic areas. For each of those 10 indicators, we estimate the treatment effect of the plan using the synthetic control method or sometimes I'll refer to it as SEM. The SEM doesn't really, at least with the app, the app package that we use to implement this. It then gives us a way to assess the physical significance. And so we went one step further and developed an all-adapt permutation task to assess the physical significance. So just a way to calculate p-values so that you can say whether or not that treatment effect is significant or not. All right. So before going into talking about the methodology is and how to implement it, why do we even consider synthetic control? Traditionally, what people have done in the past is maybe pick a similar series. So we're talking about Minneapolis. You might be tempted to say, why don't we just compare to St. Paul? It's close enough. It's a very similar housing market. And maybe it's a good control city for Minneapolis. But maybe the population makeup is different. Culturally, it could be different. So there are so many things that could go wrong with just picking one city. And then you can say, why not pick five cities? Maybe look at Chicago or some other cities that are closer may have similar economy in the housing market and all of that. But then you have to decide, do you want to take simple average or weighted average? And if you do weighted average, what weight should you give to each of the city that you pick? And how do you know that they actually follow the same patterns that would give you what we call the counterfactual, which is the, you know, what the outcome would have been had the plan not been implemented? And so with synthetic control, SEM, what it allows us to do is to systematically select the comparison groups. And also gives us their weight. So we don't have to make that subjective judgment, which, you know, could be different depending on who makes the decision right. And then most importantly, because it weighs the control groups to better match the treatment group before the intervention. So this is, you know, 2019 and earlier in our study, it accounts for the effects of the confounders changing over time. So, you know, we only have like one victim founder, I can't think of right now is probably the COVID pandemic. Like, you know, it's just a tiny thing that happens to start at around the same time as the start of the implementation of this plan. But the pandemic affected all still affects every city in the US. And so, yes, maybe the effect, the effects may be a little bit different, a class difference, you're very different, but we can assume that it affects everybody. So on average, it should, it should not, you know, affect our treatment as treatment effect estimation that much. But what about events that may only affect the treatment unit. So in this case, me now. So I don't know if you all have heard, but in summer 2020, there was some the murder of George Floyd, the civil unrest that all started in Minneapolis. So it confounds with, you know, actually, in a little bit, but at this point, we're hoping that, you know, what we find is still valid. That's not really anything we can do like we can distinguish that that effect from the plan. All right. And of course, as a star session, I have to give you the assumptions before going into talking about the methodology. So there are three assumptions that I want to point out here. So when you choose your control units or sometimes I'll refer to them as donor cities. They cannot have similar policy change. And the policy of interest. So in this case, the adoption of the plan cannot affect the outcome in the control cities. And so no spillover effects. So we want, we want it to be clean. And then, you know, as the end what it gives us is it calculate the weight. So off the donor city. So what it really gives us is the weighted average of the donor cities that then make up the control series or like the path of what what manifest could have been had the plan not been implemented. And so one of the assumption is that that counterfactual outcome has can be approximated by a fixed combination of those donor cities. Now, with all of that out of the way, let's talk about what I see. I'm really is I'm giving like very conceptual overview of the methodology. I have a link in the slide that you know links to like much more technical description of the methodology. All of the mathematical mathematic patients that I know you all love, but I decided to skip for today. So first you want to identify predictors of the outcome variables. So in this case, I'm outcome variables. They are kind of them. Right? Like we talked, we, I mentioned we picked on 10 different indicators. And what we didn't want to do is to customize the process or the procedures for each indicator. We wanted it to be general enough that we could apply to all 10 indicators. So we follow the same one. And so then, you know, trying to like stay away from like choosing something that gives us a good outcome versus like something more general enough so that like we're not p hacking our way into like a good outcome. And so in in our project we picked this in these predictors so we have total population homeownership rate median housing cost median household income. And because again, this is still a time series we're thinking I'm developing a data over time. We want to be, you know, we want to take into account past values as well. And so you also don't want to like use every single pre treatment value to like fit your model and so we compromise and use up your like values of the outcome as predictors. And then the next thing is identify potential control units or the donor cities. And so when we mentioned earlier, you know, there are a few assumptions that we have to meet. And so when we picked donor cities, those came into our decisions on like what we like what they need to include as well. And I should know this, but I think I'm in a first has the population of about like between three to 400,000 people, I believe. And so we have a range of population of cities to be included, you know, to like be somewhat similar to Minneapolis. And so we chose 150,000 to 2 million. Those cities cannot have any policy in effect at around the same time. Portland, Oregon. And then like some, a lot of cities in California actually like looked into like similar policy change in terms of zoning. And so we excluded all of them. Dona cities also cannot have any spillover effects of St. Paul. We're not included. We're not including them. We're not including St Paul in our donor city pool. And then the other two is just, you know, with like working with census data is like they have to be a census place so that we can pull the data. And then like the cities have to be like one of the main cities that it can just be like something random. Yeah. And then once we have the predictors and the donor cities, we can calculate the predictor weights. So these are the weights. Yeah. So. These are the ways that go into the next optimization problem to give us the control city weights. So they're not. This is not like the weights that we are using to estimate to get the weighted average of the control cities, but it's important because it goes into that optimization problem. There are two ways that are implemented in the package that we use that I'm going to talk about in a little bit. So one is to give it equal weight. So if we have 10, if we have 10 predictors, each one would get one 10. And then the other one is regression based weights. So this is where all of the data is like the predictors data and outcome data over time. Prete all of the pre treatment data get put into like a regression. And then it's fits out the way for each of the predictors against like all of the technical detail on this is in like the technical appendix that I'll share the link to later. And then finally, so I want you have the predictors predictors weights. You can. Put them into the optimization problem and then say what you want to what it does is it minimizes the root mean squared error. And you try to minimize the root mean square prediction error and site whichever method on whichever set of predictors weight on predictors weights that gives you the least I'm se that. And then the that will give you the final control city weights that then goes into your weighted average to get your synthetic control series on the counterfactual. I'm skipping on I'm skipping over a lot of details. There's so much more detail in the technical index, but yes, so I say so these are kind of like the steps that we can do to implement this method. And then once we get the weights, we can calculate the counterfactual post treatment. And then what we and then next thing is we do the statistical assessing the statistical significance. So calculating p value we all love p value right like it has to be below 0.05. I don't believe that but yeah, but we have to pick a cut off. We actually use point point one in in our project but yeah so to do. I'm running out of time so I'm going to go through this very quickly. So to calculate p value what you want to do is running the same running the same SEM on all your control series and then calculate the MSE post treatment to pre treatment so if the if SEM fits your data well. So this would be like for the control series. There's no there's no plan in effect so they should fit free well. And so the ratio the MSE the post to pre treatment I'm a ratio should be pretty small. But if there is an effect. The MSE ratio should be larger because now right like it fits well in the pre treatment but it does not fit well post treatment. That means there's an impact of the plan. And so then to get to get to the p value. In a way you can think of it as like ranking each of the each of the control series and minneapolis so we have 126 control series that met the criteria I talked about. And then minneapolis so 127 and let's say Minneapolis has the 10th largest. I'm at the ratio of all the control of all the cities that would give us a p value of 10 divided by 127 which is around point. So that's how we that's how we did our statistical significance assessment. So not the implementation. So this is one of the 10 indicators that we study. So this is housing cost button among extremely low income venting household. And so you spend 30% or more of your household income on housing on housing expense and extremely low income. Extremely low income households are households that earn 30% or less of the area median income. Just a little thought to show you like this is the data for minneapolis that we have. And then the package that we use to implement SEM is synth. So synth implements and data control methods for comparative case studies designed to estimate the causal effects of policy interventions and other events of interest. You can do you can just do the install that packages and then like load using library just like any other packages that you use. So I want to give like a little look into the data that go in that and I'm going into the actual synth function. So our unit our time variable is year and then I use our unit variable is city and so that's would be like the place ID. Because we're using census data and then the input data have to be. They have to be unique the roads have to be unique at the time variable and the unit variable. So we have 13 years of data and 127 cities. So the row the number of rows should be 13 times 127. We cannot have missing values at least in for the. Yeah, we cannot have missing values for the outcomes of interest. And then we talked about predictors. So here I have all those four predictors here also go into go into the function later. But before. Oh, it's sorry it got tapped off a little. It was fine. When I looked at it this morning. So I apologize. But before going before running as him on this we want to put the data from like a regular data frame into the data prep object. So data prep function is from the sense package. And so you what we did here is giving it the data so that input data is what we just saw. And then the column names for the predictors column names for the dependent variable. So that's the outcome. Column names for the unit and time variables. The special predictors are the lack values that we talked about earlier. And then it's like below here if you could see it. So you like you give it like the pre treatment period that you want the algorithm to optimize. And then you can use for the optimization. And then also like the ID for like the control and for the for the treatment unit and the control units. And then so, and from that it's one line of code really so you just run it through since and it does all of the magic and then gives you what you see here which is another line of code to show you how the model. How SM with your data. Out of the box chats are nice, but sometimes they don't give you what you want. So, for example, here, I mean, like if you're working in base, right, like you can add stuff to this but who works in base. We all love GG plot. So that's what we do. But a little bit additional code like you can. I use data that table and I'm realizing that a lot of people are the activists on people, but this is there are tables in packs. But I love the GG plot to package. So, you know, extract the weights and then like try and then we can put it into this is not that great, but better than the out of the box chat. And then here's just a little bit of code again and they had that tables in packs, calculating the people you using the methodology that I mentioned earlier, or described earlier. And yeah, and then once we have the, like the series, we can visualize it and then we have the PV you to let us know whether or not there's actually whether or not the difference we see between the counterfactual and the actual outcome is statistically significant. And now we're ready to communicate that to our stakeholders. And I'm very, very happy to go after Molly because we have a shiny app. So, we took everything into shiny app. This is actually public and there like we have been updating it regularly as data come in. And, yeah, and so this is our final product that lives on our website. So now you might be asking what is the effect of the plan. I have not talked about that at all. So the plan went to effect, went into effect in 2020. You know, a lot happened right census data was a mess. So a lot of things happen and it's only, it's still only 2022. So we don't actually have a lot of data to really sufficiently estimate the treatment effect yet. But we kind of see this like we wanted to make this public because, you know, it's a way to monitor how things change over time. And I'm not just the intent indicators. We also have like additional contextual information that our stakeholders are interested in. And then it's a way for us to pre register our methodology so that when we do have the data, we're not engineering it so that it fits the outcome that we want to see. Last slide. And then so this is just the link and that the third one is the technical appendix that I mentioned to all of the math that you want to see. And then here's the that's the link to the slide is live on our pops. And with that, thank you. And top to me if you have any questions."}, {"Year": 2022, "Speaker": "Thomas Bacas", "Title": "Data Science in Legal: The Impt. of Interpretability to Non-Technical Stakeholders", "Abstract": "In this impromptu talk, Thomas Bacchus reflects on his previous role as a data scientist at McGuire Woods LLP, discussing the integration of data science in the legal industry, which is traditionally slow to adopt new technologies. He emphasizes the importance of using visualization as a tool to bridge the gap between technical results and non-technical users. Bacchus shares experiences from his work, such as developing visualization tools for risk assessment and proving objective truths in litigation cases using big data resources. He underscores the role of data scientists as strategic advisors who inform decisions rather than replacing traditional legal practices. Bacchus also addresses the challenges of technology adoption in non-technical fields, stressing the need for trust-building and starting with simple solutions to facilitate broader acceptance and integration of data science practices.", "VideoURL": "https://www.youtube.com/watch?v=Mve_tF4TdYs", "id0": "2022_18", "transcript": "Speaker dropped out last minute. Both of our backup speakers, not here. So Molly, who gave a spoke, talked to us a minute ago, got one of her team members to come straight from the office. So with no slides, please welcome Thomas. Thank you. Well, very excited to be here, everyone. My name is Thomas Bacchus. Like I mentioned, I work with Molly. I'm the Assistant Team Lead of our data and surveys analytics group for Bloomberg Law. So I'm going to speak generally about actually my previous role before joining Bloomberg Law, where I was a data scientist with McGuire Woods LLP, our offices in DC. I like to joke that it made my commute, especially bad, because it's right next to the White House. So pretty much any news that came out would add 10, 15 minutes to my commute in. But I gave a previous talk a few months ago at the ILTa Convention, which is really unfortunate. The title of that talk was, what is a data scientist and how you use them at your firm? And I'm working with the assumption that many of you guys know the answer to that first question. So an important aspect of that event was really talking about leveraging data science and an industry that otherwise wouldn't be especially technical. So in that case, it was legal. Legal's always been a slow adopter to tech. Generally, there's apprehension to engaging new technologies and legal. And myself and the other panelists were talking about how we found great success engaging non-technical people with data science tools. So a few of the techniques that we used in that process was, one, we wanted to always work as strategic advisors and as opposed to working and using the data science mindset to address problems where everything's very technical, mathematics, statistics-based. We tried to deconstruct problems with the context of a non-technical user in mind. So one of the great tools that we used for that was visualization. And that's sort of the great middle ground when you're trying to bridge the gap between technical results and sort of easily digestible results. So I mentioned the general apprehension with legal when we came in. So the group that I helped co-found was called Maguar-Wiz Excel. And essentially, what we were providing at that time was data science consulting with our litigation teams and our in-house enterprise teams. So we worked in two different segments. So my specialization is working directly with our litigators in courtrooms, trying to find ways that we could use data to improve the outcomes of our attorney work. So that manifested in a lot of different ways. Primarily, we tried to use data to set a ground objective truth. So some of the cases that we worked on were large defense cases of insider trading or using health care data, defending doctors. And as opposed to making general claims without having a solid statistical basis, we leveraged our expertise using big data resources. So this could be like trade data. This could be population data or health care data to set that objective ground truth. That was one of the great ways that we were able to find that we weren't replacing the work that our attorneys were doing, because otherwise we'd be out of the job, because they wouldn't want us involved in their practice. But more so working in that strategic mindset of providing insight and informing decisions that were already being made. So along the way, we built a series of tools. And then this is an R-conference site, generally used Python. But we did use R as well. So Bredi, your tomatoes, if that's the topic. But we built visualization tools into some of the integrations that we had in mind. So risk is a big aspect of the legal proceedings. And calculating risk is an important part of the work that we do. So when we were building risk calculators for industry groups, we tried to build in simplified visualization at every step of the way. So you could monitor how changes to any given risk calculation would impact the results as you were doing it. So the sensitivity analysis was being performed by the attorneys as they were using the tool. I really wish this was going to be another out for short notice that I had some slides to show you, because it would illustrate that much better. But use your imaginations that there's a very cool web app that exists on the screen here. But yeah, so besides that, we wanted to also turn our focus to the business unit of the law firm, because I don't know what Molly necessarily talked about. But our familiarity with law firms is that they're trending more toward running more efficiently as a business. And for all the data scientists in here, it's a great opportunity for you to look out as opposed to working just solely in tech fields, trying to engage data science as a business practice as well, where using business intelligence and the insights that you can gather to really inform the practice of your business or law firm in this case. So one of the questions that we've gotten for any of the practicing data scientists here, you probably get something at where you work, where it's like we have problem X. So in our case, it was we have associates turning over at X, Y, rate. And our OGC, our Office General Council, was asking the question of like, why does that happen? So I came up with this extensive quantitative solution, calculate turnover, and they sort of turned their nose up at that immediately. But one of the other data scientists on the team built a network diagram, which basically had our associates connected to our partners and the edge nodes for the matters that they were involved in. And what we found in that visualization was that all of the edge cases of our associates ended up turning over. And the sort of qualitative inference that we got from that was that the associates that aren't involved in as many matters with more partners turned over at a much higher rate than those that don't. And using that information, the OGC was able to inform decisions moving forward of like pulling not only similar associates under new matters, but also partners into underutilized associates. And all of that was created, we again, I'm gonna keep defaulting to Python tools. We used Power BI and Django, but all that could have been done with R and R Shiny. So it's one of those great sort of intuitive business case results that was really, really fun to work on. Yeah, so outside of that, one thing that I wish I had known joining what I'll call a sort of underutilized industry was the difficulty of adoption. And this is something that comes up quite a bit if you are working in a business that otherwise doesn't use data at their core. Now, we're finding more and more that business regardless of their focus on data are collecting it. And you really need to become the advocate and the expert in driving the adoption wherever you were. When we founded MWXL at McGuire Woods, we started to engage in teams that weren't actually working with us already. We just had access to their data. And we're trying to build solutions to problems that we knew existed in them even before we got asked. So really we were trying to build a minimal viable product for them and provide that offer alongside the business they already created. And I think that as, especially in the legal industry, the maturity of technology continues to happen. Data scientists especially need to be working as the focus to drive adoption and continue to be advocates for engaging technology wherever you work. So, yeah. What's my time? Am I good? Okay. Well, I can keep talking. Unfortunately, again, I'm gonna turn to my notes and I looked at these on the car right over and I was like, hmm, data science. What does a data scientist makes up about 85% of this? So, yeah. So I'm gonna point to a couple of quotes that are really relevant to what I had mentioned before, which is that data analysis with lawyers piece. So this is a quote from David Burnett in his article entitled The Importance of Data and Data Analysis and Mitigation in law 360. And in that he says, data analysis allows lawyers to bridge the gap from individual to collective proof. Lawyers can use investment market data, sales records, public health records, demographic data, and the like to generate compelling claims and evidence about incredibly broad subjects. In short, big data allows for big litigation. So, sort of what I mentioned before, trying to visualize that in practice is, if you wanna make, I'm gonna point to an example that we had pulled up with one of our litigation teams. So, we had an employee lawsuit defense case and the company that we were representing basically making the claim like, there's no way that this event could occur on random chance twice in this timeframe. And for, do we have any lawyers in here, by the way, before I make disparaging remarks on lawyers? No, okay, excellent. So, lawyers aren't especially good at statistics and generally they don't go into the practice of law to learn math. And I'm sort of looking over at Molly to see if there's gonna be any like, we're losing business, okay. Okay, good. Excellent, yeah. So, we really wanted to try to prove out this conditional probability case, which this is more of a statistics thing than it is in our thing. So, I'm diverging. So, forgive me. But we were trying to calculate this probability of occurring. So, we had this one event that occurred at XYZ timeframe that was especially odd, which theoretically in a vacuum could have occurred by random chance. But the fact that it occurred two and three times after that at the same rate, which was the case of, I think it was an injury, seemed a bit fishy. And essentially what it came down to with the litigators on this case, we ended up getting dismissed was that there was a unnatural precedent for these occurrences to happen within a reasonable timeframe. And essentially how we presented that was for one case to occur, it's one in a million chance. For this case to occur twice, it's one in 10 million. And for one in this case to occur three times in a row in this timeframe, it's one in like a billion or something. And this is another one of those cases where they probably could have gotten their way around that without understanding, but working as the data expert and the statistics expert on the team, we were able to really drive that as an efficient, efficient story and narrative that we were building. Then also along the way, we were working on a sort of brother versus brother lawsuit over shares of the family company. One was trying to buy out the other. The one being bought out was saying that his shares were worth five, 10x, what he had initially valued. And we didn't really have a plan. Well, the litigation team didn't really have a plan on how they wanted to manage this sort of collection of ownership. So we built this really extensive pipeline that was built partially in R. So we, good job everyone here. Parsing and Python sort of built that into our AWS network. We were able to build out this really extensive. And again, these are very hard to visualize. So imagination hats on. Dashboard and visualization tool set that allowed in our court proceedings, one, our client to be able to review ownership details of hundreds and hundreds of family owned properties that were related to the business. But also we were able to go back to the auditors that had been involved in this case that had valued all of these properties, 10, 5, 10x, what had initially been audited. And we were able to show again, going toward that data at the core of ground centralized truth that what the opinion of the auditor had made was not necessarily reflective in the data. And so in our ability to visualize that and build out that tool set, we were able to sell that narrative with much stronger efficacy. So yes, that's one more. And how am I doing on time? Good, got six minutes. I kind of wish I could do questions. Can I answer one question or no? Yes, and? Sure. Yeah. Absolutely. Yeah. Yes, absolutely. So the question was talking about wins with adoption, trying to sort of illustrate what it would look like because everybody runs into challenges of adoption for non-typical people. So the big thing that I would focus on always is trying to build trust. And that starts with the really easy, low hanging fruit work that you do. So I am guilty of this also. My data science reign is always looking to build a really complex solution to problems that could really easily be solved with something simple. And you really have to stop yourself on that conclusion because what you'll find is, as you try to build these fun, complex solutions, people will turn their nose up at that because they don't understand it. But if you build a simple dashboard or a visualization and get your foot in the door, that's a really great path to adoption. And using that as your trust building process. And once you start to have that trust that whatever organization that you're at, it's way easier for you to engage with teams or individuals that otherwise are not familiar with your process. But the data process can be scary to people that don't understand it. So my focus always is try to build trust at the start, do easy things, do the not fun stuff, do the things that are not complex and use that as a building block toward building more fun projects. Again, really wish I had my slide deck. But at my previous talk, I had this value complexity matrix, basically like value complexity x, y. And we have those broken out into steps and it's basically descriptive and, again, I gotta look at my notes, but it's basically descriptive. And at the very top, we have our prescriptive and our predictive analytics. So we start with the simple stuff, which is low value and low complexity. We use that to progress to our higher complexity and higher value work. So yeah, I hope that was helpful. Well, it's a pleasure to be here for everyone. And thanks for the time. Bye."}, {"Year": 2022, "Speaker": "William E. J. Doane", "Title": "Qual: Simple Shiny Qualitative Document Coding", "Abstract": "When working with textual data, having the capability for multiple researchers independently to mark-up the source text is vital. It enables persistent identification of critical information and discussion about agreement and disagreement about what counts as evidence resulting in traceability and transparency. While there are a number of commercial and open source tools available, finding a feature set and cost point for use in isolated networks with rapidly changing resources has been difficult. Qual is an early-stage Shiny app to meet that need.", "VideoURL": "https://www.youtube.com/watch?v=P74_fT1Pohw", "id0": "2022_19", "transcript": "Recently, he has traced his family back. How far back do you think is really far back? How many decades or centuries is far? All right, three centuries, that's far. What else has this far? Well, what's far to people? I can trace back my family 100 years max. Yeah. What about anybody else? How far can we go? 700 years. The year 900. Our next speaker has recently traced his family back to the year 600 CE. That's over 1400 years. So please welcome the geologically astounding. Will. My name's Will Done. I am a research staff member with the Science and Technology Policy Institute. You heard from my colleague Shirley Hahn yesterday morning at nine o'clock and you'll be hearing from another colleague of mine, Tyler Morgan Wall, who's I saw in the audience. There we go, Tyler. Will be the next speaker after the next speaker. Stippy was charted by Congress in the early 90s to support the White House Office of Science and Technology Policy. And so very often we face challenges dealing with qualitative data, such as having vast amounts of information, no time whatsoever to try to process it and needing to work in isolated air-gapped systems. There are commercial tools available to help you with qualitative document coding. I'll talk about what that is in just a sec. But not the kind of things we can do in an air-gapped space, not the kinds of things that make it easy for us to work on quick turnaround. Sometimes the turnaround time from the time we get asked a question to the time we need a response is a matter of hours. So every few months or so for the past five or six years, I've been thinking, well, maybe I could do something that's shiny that would help me with this and that would allow me to run on my local compute cluster in this air-gapped space. I took another shot at it a few months ago and this is where I've come up to. So what do I mean by qualitative document coding? We have textual data, lots of textual data. Might be academic papers, it might be transcripts, it might be automated transcripts from like YouTube videos, anything that can be converted to text. And qualitative coding is basically going through, as we all did in high school and college, go through with your markers of different colors to signal the meaning of different things. We do that and then collect the data about how many times we marked something one way or another. It's called coding. We also collect the margin area, the margin notes. Why did we mark that? Why did we feel that way? This is meant to be an eye chart. The only things that are important for us here are the big bold words. So don't try to parse the rest of that. That's okay, qualitative coding. Two modes with qualitative coding typically employed, inductive where you start with a set of codes and then code your document according to that code set or deductive where you start with your documents and start coding things that are interesting to you and then figure out what the codings and clusterings and hierarchy of those codes are. I wanted a tool that would support both of those modalities. I wanted to be able to support multiple coders because me marking up a paper doesn't mean that what I've marked up is truth. I need other people to also mark it up and then have conversations with them about what we disagree on, what we agree on and so on. I need that coding to be independent. So when I'm coding, I don't wanna know what anybody else has coded things as. I don't want them to influence me and I don't want my codes to influence them. I want us to do it independently and then for us to be able to compare and contrast. We need to usually do this in an iterative way because we won't agree, there won't be 100% ender-radar reliability ever and there won't be 100% on the first pass certainly. So I did what everyone does, right? I wrote an app using Shiny. I started thinking about what the current affordances were in Shiny for something that really felt like an app. This is an about page that's built into the app for people who are coming to it who might not have a background in qualitative coding to be able to ground them in the topics that we're thinking about. The app has a little sign-in screen. I don't do any authentication because I trust people on my air-gapped network. If I don't, they shouldn't be in my compute space. So the user provides their username, signs in. They get forwarded to a projects page that shows the projects they've created up to that point and off to the left, they can create a new project just by giving it a name and providing a description. As I've done here, you click Add, now it's on your projects list. When you select a project from the list, it takes you to the code book for that project if you've already established one. And if you haven't, you can start adding new codes on the left-hand side. When you're adding codes, you can specify foreground and background colors so that when we start highlighting the documents, it's actually highlighted using CSS. And it's designed in such a way that multiple users can make use of it. So other people within my organization can see the projects I've created and the codes I've created so that they can collaborate with me on the coding efforts. There's a really simple, some of these screens are intentionally minimalist at this point. The app, even though I'm working from static screenshots here, the app works right now. It's a proof of concept, it's functional, but it's not pretty. I need to go make the code elegant now and upgrade some of the user experience elements. But the simple document upload where you can use the HTML browse feature to bring in, let me think, text documents, PDF, Word documents, Excel documents, XML documents, you can bring all of those things in, it will extract the text from those documents and turn them into plain text documents for you to be able to work with in the system. Once you've done that, there's then this documents screen and a pull down menu that lists the titles of all of the documents you've brought in. When you select one of those documents, you have a scrollable text area. You can go through, highlight the text just as you would imagine you would. A little box pops up. Right now, I coded my complete own editor as a first pass. It was good, had a few issues that I wasn't happy with. I've been looking at other alternatives. The Riccagato JS library seems to be an interesting one, so I've been playing with an implementation that relies on that. This is what the Riccagato looks like. When you highlight something, it pops up your code book as colored tiles based on your background and foreground colors. You have the option of putting comments there, clicking the codes, or notice the word doctor there. You can type in a novel tag there. So if you're doing some version of deductive coding, you think something should be tagged, but the code doesn't exist for it yet. You can enter a novel tag there to have that element. You have the option of choosing one or more coders that you want to see what they've highlighted. So pull down select highs list in Shiny. You can check multiple items there. When you do that, you get a table of all of the codings that they've done. It's a little too much at the moment. You don't need to see the start and stop points of everything. But when you click an item in the table on the right, it jumps to that part of the document. When you click on a highlight in the left, it highlights that element in the table, a little bit of cross talk working back and forth with these. There's even a reg X based auto coder built into the system. Right now, you give me a tag and a regular expression, click the button at the bottom to auto code, and it will go through all the documents and code the text. You might have noticed back on the authors list. One of the authors is auto code. So there's a special author dedicated to the auto coding efforts. At the moment, the only analysis that's in this as a proof of concept is just a frequency count. But it's pretty straightforward and trivial to start thinking about adding. Additional analyses. Certainly we can talk about document coverage, how much percentage of the document has been coded. We can talk about excerpt extraction, pull out all of the coded elements so that we can then review them as a team, perhaps discuss our agreements and disagreements. We can do named entity recognition based on the data that's in there. Any number of inter coder reliability assessments, we can do standard Kappa evaluation, Scott's pie percent agreement. Anything at this point, the data is simply a table of data that tells me who coded it. What did they coded as in which document starting at what character position ending at what character position. So any kind of statistic that I could derive from that is fair game at this point. As I say, currently the frequency is already done. Trivial then to implement term frequency analysis relative terms. Inverse document frequency analysis or to do some topic modeling on these documents. It's built around that project idea. So different people, different teams, you can come in, you can create a project, you can share that project with other people. You can have multiple coders working simultaneously in real time on whatever your subnet is. On the back end, I've coded it for two database systems right now. One is SQL light. So you can run it as a totally standalone app off the network entirely. The other is as an SQL server. So Microsoft's database offering. You can tie it into your enterprise SQL server. And you can have all of your data stores process there. And much to Jared's chagrin. That is largely the talk for today. So I'll leave you time for the, for the gift announcements. This is also this was the first screen that I created in the application. I'll just give you this as a design principle. Anytime you're creating an app, the first thing on your mind should be how to users feedback to me. All right. I want to have a way for them to tell me what went wrong as soon as they start looking at my app. I don't want feedback to be an afterthought or forgotten entirely. So. I build the feedback screen, have it in there, let people get in touch with me. Right now, this is a proof of, as I say, it's a proof of concept. I've got it hosted on a private GitHub repo. Eventually, I expect I'll make it public and distributed as a package. But if you're interested in it, feel free to use that feedback link. Get in touch with me and I'll be happy to keep you updated about progress. Thank you."}, {"Year": 2022, "Speaker": "Maxine Drake", "Title": "Working with Databases in the Army with the Help of Shiny Applications", "Abstract": "This talk describes a few examples of how Army organizations are managing their data, and we use R to support. First, the talk will cover, in generalities, progress in building databases, cloud environments, and limitations that organizations face. Then, we will discuss examples of Shiny apps indicative of what these organizations use as well as Shiny code that streamlines large applications.", "VideoURL": "https://www.youtube.com/watch?v=BZaVkSmy-Jo", "id0": "2022_20", "transcript": "She is another repeat speaker, has three children, age seven, four, and one. And she still beats all of them. I need to finish that sentence otherwise it's bad. She still beats all of them in Mario Kart. Please welcome Major Max. Today I will be talking about building large shiny apps, some techniques that I use while working in the army. To start with personal introduction, I am an army officer, I've been in the army for 13 years. I started as an engineer, kind of like Dusty over there. Our job as engineers is to both break things and build things. I don't know how that works, but it works somehow. And after that I changed my career field to operations research systems analyst. I see some head nods. I know there are a few of us out there. And my first job, well I studied operations research at George Mason University down the road. And then my first job was at Center for Army Analysis. We've heard that one a couple of times too. What CAA is, is the lead analytical agency, specifically for headquarters department of the army. And CAA focuses primarily on the strategic realm, so that highest conceptual realm of military operations, the escaming, planning, et cetera. The software. After that I moved to Fort Eustace near Williamsburg where I work now at Future Concept Center. And what FCC is, is we conceive of and develop the future concepts and capabilities for the army and the joint force. So we're talking concepts and capabilities from at 20, 30, or 2030, 2040. And what I mean by concepts, I'm talking technology, organizational structure, maybe new personnel specialties. Maybe we'll conceive of a data scientist in the year 2030 as a military occupational specialty. That's the sort of stuff we build. So as a data scientist, that's my job title right now. What do I do? I kind of do it all. And the other horses can probably relate to this. We do data science, data analysis, data engineering, data. We troubleshoot Excel. We automate processes. We take notes when needed. We basically do whatever our organization needs us to do to get data to inform decisions. And what that means is that we develop breadth, but not necessarily a lot of depth in one specific data oriented career field. We use R a lot. R in R studio. R is definitely the most accessible of the powerful data science tools. Given all of our security restrictions in the DoD network. I'll give you some examples of what we do. So we do big mathematical models, COVID-SIR model. We use machine learning algorithms. Did linear programming, like we mentioned yesterday. For example, a network flow linear program for logistics war game. So more than just NFL modeling. And then a lot of process automation. Organizations come to us. We have this repetitive process. Please help us. And another one that I'm working on right now is designing a database schema to help individual an organization organize their data, structure their data, and then we build a fun end for them. Everything that I just listed, almost everything I just listed, we like to build a shiny app as a means to deliver the code so that the customer doesn't have to look at code. Instead, they can just plug some inputs in, click some buttons, beep, boop, beep. They get their output. We also deploy it on our RS Connect. We use a cloud environment. We have a few different cloud environments that we use in the army. And, but as far as the actual product that we hand them, it's a lot of times it's shiny. If I were to show you an army app that I've made, or that my teams have made, it would bore you to tears. And I'd probably get in trouble. So I'm not gonna do that to either of us. Instead, I have my own personal meal planner, shiny app. You're welcome to look at it. It is on the shiny IO. I don't pay them. So I don't know, maybe I'll break it either way. I'll be able to look at it still because I'm running it locally. So before I pull it up, I have three children. Planning meals is a bit of a headache. Two working parents, you all get, yada, yada, yada. You all get it. When you're planning meals, it is a bit of a process. And so like any good coder, I spent countless hours automating a process that would otherwise take me 10 minutes to do. So here's what I did. I have a shiny dashboard here and I have three pages, Choose Meals, Shopping List, and Edit Meal Data. On this first page, I can input my meals for breakfast on Monday. I'm going to have Chia seed pudding, sounds boring. Tuesday, I'll have the heat is for lunch. Thursday, I'll have vegetables. Then I can choose my staples, almond milk and bananas. And I go to my shopping list and I generate my shopping list. Haven't decided what I want to do with this now. It's a work in progress, but you get the idea. The techniques, oh, and one more thing. If I want to edit the meal data, I can do that here. What we're looking at here is an R hands on table. If you haven't used it, it's a pretty handy feature or a pretty handy package, I should say. It gives you a fully editable table, which obviously we would use with discretion because we oftentimes don't want to give our users the option to fully edit a table, but I want to be able to do it. And you can structure it slightly. Something you cannot do in a editable data table in shiny is a dropdown box. You see this? I made a dropdown box in this row of data. So it's pretty handy. And also has the XL user experience, which is kind of nice. Okay. So we have this example. The techniques that I use here that I also use in building large shiny apps in the army are three. There are three of them. The first is shiny modules. The second is reactive functions. And the third is something, it is generating UI and mass by groups of UI using per's map functions. That's a lot, I'll get there. So shiny modules. First of all, what is a shiny module? A shiny module is a pair of functions, a UI function and a server function. They are slightly unique from other functions. Now, all of the benefits of functions that Mark went over yesterday and the benefits of functions that we are already aware of apply to shiny modules. One thing I will emphasize is that shiny modules and modularizing your script into multiple scripts specifically is more conducive to DevOps. If you try to merge, use Git to merge one single long script. It's probably no, it's a real headache when you have multiple people working on one script. Got a lot of merge conflicts. If you have multiple scripts, then I work on module one. Mikayla, you work on module two, dusty module three, then Robert, you just check dusty's work because, I don't know, got a lot of typos in that one. But you get the idea. Great for distributed teams working on something. The other thing about modules is they create a namespace. This is what makes the sets them apart from just regular old functions. A namespace is important because if you have a shiny app with, and multiple inputs have the same name or multiple outputs have the same name, you're gonna have problems. At best, you will get an error. At worst, you will not get an error. And you'll just get some really funky results. So let's look at how I like to structure a shiny app, or I'm using a shiny dashboard here. So the first thing I do, and this is when you're in our studio, you open up your new shiny web app, you select single file and you have your app.r, you delete the sample app that they give you. And then I would copy my shell in here and start working from here and changing the names that I want. So the first thing I do is I source my global script with all my functions, and then I source my individual modules. I have one module per script. That's just my personal preference. I define my header, my sidebar. That's all pretty standard stuff. And then I go down to my body. Notice I have tab items, which gives me the pages on the shiny dashboard on the left-hand side. And then I have my meal staples, I have my three UI functions. If I go down further into the server script, I have my three server functions. Now, just because a server and a UI happen to be in the same module script does not mean that those are a linked pair. What actually links them and gets them to talk to each other in the app is the ID argument. So let's look at edit data server, this first server right here, ID equals edit data tab. I go to up here, edit data UI, the ID equals edit data tab. They match. Therefore, this UI function and server will talk to each other and they will not talk to any other module, no other cross talk. Okay, so that's what I call the parent app. And then the module itself, what I would do with the module is, what I actually do is I have a shell, an empty shell that I just copy and paste every time I wanna start a new module. Cause there's a specific syntax that you gotta follow. Specifically, you have, the first thing you do in either one, each one of the functions is you define your namespace. That's what I'm doing here online three. And then the other thing I'll highlight in the UI function is that you can have multiple individual pieces of UI in your UI function, but you do have to wrap them in some container, a div container or a tag list. So that it comes out of the function as one component rather than lots of individual components. Okay, and then in the server function, notice the syntax here, module server, and then you define your ID there, that defines your namespace. Okay, the next thing I wanna talk about, reactive functions. I'm in a crowdsource something. A is I explain what reactive functions are, and B is I assume you know what reactive functions are and I show you my personal rules of thumb on how I implement them. So A's, right there, and got a couple, B's. Oh, okay, but I wasn't expecting that. Okay, all right, so I ignore reactive value, that function is useless to me. I stick with reactive values. I like how it gives me a list. It looks more like a list and I can reassign it just like it was a list. I don't have to worry about the weird parentheses thing, even though it seems like an object to me, that part just confuses me. So I use reactive values. In this case, I'm assigning ingredients as one reactive value and stables as another reactive value. Notice I'm just reading in files. That's how I use reactive values, is when I just wanna do one simple thing, read in a table, that's the initial value for these items. And then I use the reactive expression. So I use reactive expressions anytime I want to perform some calculation, and that calculation is gonna remain the same for this particular object. So every time I call ingredients category, I want to perform the same calculation, and that calculation includes some other reactive value, some existing reactive value, or an input. So that's when I use reactive expressions. That's this function right here. And then I define my output. Notice this output is not nested in some reactive function. It's generally speaking, it's best to make sure your output stands alone. And I certainly use reactive values to define what the output is. For instance, I'm building my R hands on table, and I'm using a reactive value here, V dollar sign ingredients. And then next one down, I say, okay, I wanna make the category column editable, and then I wanna make it a dropdown box, source the options for the dropdown box from my reactive value category. And in this case, because it's a reactive, categories are reactive expression, because I use that reactive expression function up here, I have to say category, open, close parentheses. If I don't, then it'll give me an error saying something like expecting a vector, but it's actually a reactive expression, something to that effect. You got your missing parentheses, or you've mismused named your reactive value there. And then I use event reactive. Event reactive and observe event are very similar. What distinguishes them is that event reactive ends up giving you an object. Use event reactive when you wanna make a calculation that ends in some reactive object that you wanna use later on in your script. Observe event, use the same syntax basically, but you don't assign it to an object. We use observe event when we want to perform some side action right to a database, right to CSV. We don't actually want a value out of that operation. Okay, so that's kinda how I use those. And I think I captured all of the good ones, left out reactive vowel, but okay. The last thing I wanna cover is generate, oh, I got some time too, okay, good. Generating UI in mass with per. So let's, I'm gonna explain this one by visual example. And then I probably won't have time to go line by line through the code, but I will give you this presentation afterwards, so if you're interested in that, you can certainly look at that, see how I code this, and then talk to me afterwards, of course. Okay, so you see a lot of drop down boxes here. I did not code each one of those individually. If I did, it would be awful. Instead, I have a function that gives me one single drop down box. And that function takes in arguments that will define the parameters for that drop down box, label, options, selected, that sort of thing. And I use per to apply that, I use per to apply a set of arguments, not just one single set of arguments, but a vector or a vector or a list or a data frame of arguments to that function. And those arguments are reactive. So, kind of side-stepping here, what is really common and normal in what we all do in Shiny is we make a drop down box and we make the options reactive. But what I'm describing is something slightly different. This does that, but what I'm describing is that the UI itself is reactive. So, if I check Saturday, because I want to plan for Saturday too, it will give me an entire new column, which is still the same function being just repeated again and again. And I have another function on the backend that reads those inputs, interprets those inputs in series and then collapses them into a data frame. That's all using per the map function and the P map function, because a lot of them require a lot of arguments. And because I have time, I will show you some of the code, but it is a little dense. So, first thing I do is I define my function, I'll kind of go over the whole thing first. First thing I do is I define my function to generate the checkboxes in this case. This is what the example is getting me. You can see that it's just saying, give me a out of this function, I want one column and in that column, I want a checkbox group input, the group of checkboxes. And then I have the arguments are a data frame. So, I want to feed a table into this. Then I have my staples table that has all the staples collapsed into one table with a category column and an item column. And I group split that table by category. So, now I have a list with tables and each table is for unique category. And then with the map function, I feed each that list of tables into my function and the output is something like five or six, that could change depending on what staples is at the time. But right now five or six columns of checkbox groups. So, it's pretty handy. I would recommend using this method when you have to make several pieces of UI, all of which follow a very similar parameter pattern. But if you have UI that every single one of them is very unique, don't bother. And don't confuse this with shiny modules. When I use shiny modules is for, when I have lots of UI and lots of code, I wouldn't necessarily, what I'm describing here with the per function is when I have a smaller piece of UI that I wanna use over and over and over and over again. I know that was a lot. I'm happy to continue the discussion. And my primary reference is mastering shiny, specifically for the reactive functions. He's a great chapter on that and the shiny modules. And then in conclusion, I really appreciate all of your attention. And I really enjoy speaking. So, I look forward to speaking next year. I have a question. And this is the presentation. If you care to look, I published it. Thanks to someone, one of the speakers yesterday, I can't remember, I published it on Cordopub. So it's there and I'll look at it. And that concludes my presentation."}, {"Year": 2022, "Speaker": "Tyler Morgan-Wall", "Title": "all - Building an Entire City in R: Interactive 3D Data Visualization with Rayrender", "Abstract": "We live in a 3D world filled with 3D data: why limit yourself to 2D data visualizations when your data are three dimensional? In this talk, I will show how you can visualize an entire city in R, building a 3D model of every single building in Washington DC directly from open geospatial data sources. Using the rayverse universe of packages and the sf package, we will recreate a Washington Post visualization showing the 3D path of a helicopter descending on the 2020 Black Lives Matter protests in only a few lines of code. Additionally, we will use the rayrender package to visualize the point-of-view of the helicopter itself, and show how you can interactively fly through and visualize massive 3D scenes entirely in your R session.", "VideoURL": "https://www.youtube.com/watch?v=8NV5MxcaWR4", "id0": "2022_21", "transcript": "You may know him from such animated series as Ray rendering and race shading, but he has done, let me show, improv, improv comedy in front of Tony Hill, the Emmy award-winning actor from VEEP and Arrested Development, so I'm sure there wasn't stressful at all. So everyone, please welcome Tyler. Hi everyone, my name is Tyler Morgan Wall. I'm a research staff member at the Institute for Defense Analysis in Alexandria, Virginia, and I'm the developer of race shader, Ray Render, and the Rayverse, a collection of packages that enables you to create beautiful 3D data visualizations all in R. And today I'm going to be talking about how, or showing you how you can build a 3D digital replica of a city completely in R in only a few lines of code. So I'm also going to show you why you should use R to make stunning 3D data visualizations and how you can interactively fly through these scenes without leaving your R session. And finally, I'm going to show you how you can use this ability to help tell data-driven stories within these cities. So first, let me start off with an example of a beautiful, hyper-realistic 3D city rendered entirely on a computer. So this, of course, is a SimCity 2000, released by Maxis in 1993, this overhead isometric city building game, lets you design and build your own metropolis. And as a kid, I love playing this game, although not in the intended way of managing tax rates and building sewer and electrical grids. Being a child, I just use cheat codes to get unlimited funds and do what I actually found fun, which is building and designing cool-looking cities. And what, in my opinion, made a city cool-looking? Well, it was creating towns and cities that interacted with the natural terrain and fun and interesting ways, building a city bisected by multiple rivers or on top of a plateau or within a mountain range or a volcano caldera. And that's because humanity in the environment are intrinsically linked, and SimCity provided Kid Me a very easy canvas to play with those concepts. And in general, representing that intersection of humanity and the environment is one of the goals of cartography, maps of the language of how dirt, water, and nature relate to flesh and blood. And this was one of the goals I wanted to bring to R when I started developing Ray Shader. But I had a problem. Many of Ray Shader's 3D maps looked like the empty, pre-generated SimCity terrain, devoid of life and civilization. And since one of my goals with Ray Shader was to help scientists and journalists tell stories about how our changing environment affects the people within it, I realized I had to find a way to add humanity to Ray Shader's maps. And the utility of this goal was made especially clear to me in June 2020, when Andrew Batran of The Washington Post asked me if Ray Shader could be used to help visualize and tell a story about how the Trump administration then used helicopters against protesters during the Black Lives Matter protests here in DC. This was intrinsically a 3D driven story. They had GPS and altitude data and wanted to visualize the path of the helicopter as it wove through the buildings of downtown DC. But at the time, Ray Shader only supported 3D terrain and basic data like points and lines. Not enough to build a publication worthy visualization of the downtown area so they had to use other tools. But as someone who believes you should be able to use and make any data visualization in R, I knew this was something that I had to fix. So, I did. And here's what you'll learn in this talk. How to build a city from raw data in R in just a few lines of code. But I'll also show you how you can integrate other data into that 3D city. And I'll show you how to create animations and explore your data interactively. But before we dive into 3D cities, I'll introduce the 3D ecosystem provided by the Rayverse and show how you don't need any fancy 3D modeling skills in order to create a stunning 3D data vis. And since most common feedback I get when I give a talk on 3D data visualization is, well, I wish I could make something like that, but I don't work 3D data. I'm also going to make sure to show you some examples of 3D data visualizations made from a variety of different data sets all in R that will hopefully inspire you to try something on your own. Now, there are a few packages out there for 3D data visualization, but I'm going to be talking about the Rayverse, the set of packages I developed that focuses on making high quality 3D renders with realistic lighting. The main package I'll be using in this process is Ray Shader, a package that enables you to transform your data into 3D models. The other Rayverse package I'll be using is Ray Render that allows you to make beautiful path traced 3D data visualizations like it's from Blender, but entirely in R. And all the spatial data manipulation I'll show in this talk has been done with the simple features package, which allows you to import and parse almost any kind of data in R. And finally, or spatial data in R. And finally, I'll be using the elevator package to programmatically fetch elevation data for our 3D maps. So you need to walk before you can run. So before we can build an entire 3D city, let's cover the basics. What exactly goes into a 3D data visualization? What are the different things you need to keep in mind when putting together a 3D data? First, what kind of data can be used for 3D plots? So all you need for a 3D data is our data with at least three numeric components. Here we have European flight path data from the crowd sourced open sky network. And flat path flight path data has altitude, longitude and latitude data, which intuitively map here to x, y and z, and are plotted as 3D paths here. So this is a fairly straightforward and intuitive use of 3D. But you can also you choose to use 3D aesthetics with 2D data. Much like you might choose a line chart over a bar chart purely for aesthetic reasons, you might choose a 3D chart over a 2D counterpart because you can make it more interesting and visually appealing. This is a map of the submarine fiber optic cable network, and there's no actual 3D data present, just latitude and longitude data. The cables rise out of the earth where they start and fall back in where they end. You could easily plot this as 2D paths on a simple world map. But I think the floating cables, subtle shadows and rotating globe really elevate what is otherwise a fairly simple data set into something more tangible and relatable on a human scale like the earth is being wrapped in yarn. So many 2D plots have direct 3D equivalents, but just turning points and lines into spheres and cylinders can lead to mixed results. Data floating in the void can as hard can be hard to interpret if you're just presented with a static image as objects in the foreground can block those in the background. Like we see here in the static 3D rendering of a morphine molecule generated with the arrayed molecule package, but you can solve this problem by animating the 3D data. The movement disambiguates the foreground from the background so that the reader can fully interpret the 3D model. Besides points and lines, the other main type of 3D data visualization is the 3D mesh, a continuous grid of data values. This is where the race shader package comes in. It takes 2D grids of raster data and creates 3D models by transforming that flat matrix data into 3D. This visualization of the underwater volcano in Tonga that erupted earlier this year was generated from a simple 2D matrix of bathymetry data taken from a sonar survey. Check out the code to see how straightforward this is from our matrix to 3D model in just a few lines. This is true of almost all the visualizations you'll see in this talk, the rendering code is usually only a dozen or so lines of our code and most of the work is in cleaning and prepping the data, which it always is. Now, if you're the type of person that obsesses over the tiniest details of your data visualization, great news, 3D data brings an entirely new type of detail to obsess about lighting. If you've ever talked with a photographer, you know that the secret to amazing photographs isn't necessarily having the best camera, but rather ensuring you have adequate and well-designed lighting. And the same is true of 3D rendering. If you only have a single light like we have here in this 3D historical map, the visualization is a bit spooky like a swinging light bulb in a haunted house. But if we look at this visualization of the blooming California poppy fields with multiple distant fill lights, we see the atmosphere has changed completely much brighter and less moody. Even easier than all of that is using an HDR image file for environment-based lighting, which we see here in this visualization of Monterey Bay, California. It lights the scene with a real-world image which results in natural realistic lighting like the object was actually in that environment. And finally, if your data changes over time, you can also introduce animation to your data. Here I've used a Microsoft Connect to capture my movements and load them into R using the R MoCAT package. There is a package for everything. By extracting the individual time slices and rendering each frame separately, I was able to combine all the frames into this animation using the AV package. So now that we know the basics about what goes into a 3D data visualization, let's build our city. So what do we have to do to accomplish this? Well, first we need spatial data. Then we need to load parse and filter that data to only what we need for visualization. Then we need to turn that data into a 3D model and render it in R. So it sounds like a lot, but thanks to the great spatial R ecosystem and the easy 3D rendering capabilities of the ravers, the hardest part by far is just finding all the data you need. So where do we get our data? Well, first thing we need is elevation data to rate our base 3D map and we can get this programmatically using the elevator package for bodies of water and road data. DC's open data portal to download polygon data. You can download polygon data to demarcate the bodies of water and road data to overlay the street network. And most importantly, for our case, DC has geo-referenced 3D building data. So not every city will have all of these data sets available or make it easy to find. But thankfully for our case, DC has a nice web portal for accessing all of this information. So this is all the code that it takes to load our packages and our spatial data, our downloaded data. And thankfully SF, you can see here, makes loading spatial data a breeze. Once we load our data with ST read, we then want to ensure all of the data is transformed. So we load it here and then we transform it to the coordinate system that is specified in the building's data. So that's what that CRS means. It's this coordinate reference system. And then we transform it to that. So everything's being measured from the same point. And so we do this for each one of the objects for the roads and for the water bodies. And now that we've loaded it all, we just need to download the elevation data for our base map and convert it to a matrix to input to race shader. So we'll use the latitude and longitude coordinates from that intersection from the Washington Post. So that's what these are. I just pulled these from Google Maps. And then we will transform that point to the coordinate reference system that was specified in the buildings object. And then we'll expand it by 10 kilometers to make sure we're pulling in all of DC, which will then crop down to just the buildings, the extent of the buildings data set. And then finally, we'll use a race shader's raster to matrix function to convert that raster object to a raster object to a basic R matrix. And then we're done importing all of our data. So to turn this into a 3D map, we'll just run this short race shader script. So this code creates our 3D terrain model by building up layers. So starting with the basic elevation, the color map and height shade, followed by some shadows to get some nice shading on the actual elevation data itself, followed by our overlays for the water and roads. And then we pass these all to plot 3D and get this. So this is a 3D map of all of DC. Not only do we see the shape of the terrain, thanks to the underlying elevation data and hill shade, but we can also see the road networks and waterways, thanks to the overlays. And now all we need to do is add our buildings with a single call to race shaders render multi-polygon Z function, which is the file format that it comes in. Wait a minute or two to generate the model because there's a lot of buildings in DC. And now you have your full 3D digital recreation of every hill, street and building in DC generated directly from spatial data and only a few lines of code. So this scene has over 38 million vertices rendered in real time in your R session. And this map is pretty neat. I mean, we can see a lot of detail here. But while buildings and terrain are cool, it seems a little barren. And that's because we're missing really one important element of our environment, trees. And I'm happy to tell you that not only does race shader now support adding 3D trees to maps, it actually you can add a lot of them. So this is every single tree in DC, 10 million of them detected from 30 gigabytes of LIDAR data covering all 68 square miles of DC rendered within our digital replica. You might think this is just a fun rendering showing off the cool feature of race shader to handle millions of trees. But you can actually start to see public policy impacts in just a single render. By rendering all the foliage in the city, you can actually see how in Northwest DC, over here, we actually have fairly substantially more tree coverage, canopy coverage, than in Northeast DC. And this has real impacts when it comes to health and wellness. Lower canopy coverage is associated with high, higher summer temperatures. And with combined with the gradual warming effects of climate change, actually leads to more health, a heat related health emergencies. So being able to visualize that disparity on a tree by tree basis is I think really really powerful. So you might also notice that the 3D rendering so far have been a little flat, what I've been showing of DC. And that's because we've been using the raw rate of shader output, which is just faster to render. If you want something a little nicer, we can call race shader's render high quality function and get a beautiful path trace visualization with realistic lighting. So using a high quality visualization techniques like path tracing can really enhance the impact of your data vis and increase engagement and interest from your readers. Plus, it just looks really awesome. So you can see the national mall here rendered with buildings and trees derived from data. But in addition to trees and buildings, race shader can also now reference arbitrary geo reference 3D models, which allows you to answer such pressing questions as what if we replace the Washington monument with a movie accurate version of Sorin's dark tower from the Lord of the Rings. So Sorin 2024. Now, I've only showed you rotating views of our map so far. And you might ask how difficult it would be to set up some more complex visualizations flying through 3D space. And how would you even do that programmatically? So unlike in 2D where there's only one way to look at visualization in 3D, you have an infinite number of camera angles, orientations and settings to choose from. So it can be intimidating, especially if you've never worked in 3D before. But I've made it a little less daunting by including a fully interactive real time 3D graphics device built in the Ray render and working out of the box with no dependencies. So if you can use race shader interactively, you can use this 3D Ray render view. So here I've recorded a video of myself flying through DC, visiting various landmarks, parks, and stadiums throughout the city. You can fly around your scene with just your mouse and keyboard and find the best angle and print out your camera info for future use. Or you can interactively save keyframes and generate an animation smoothly flying through your data visualization, like a helicopter flying through a city. That's foreshadowing. So now let's see if we can recreate that Washington Post data visualization in our using race shader and Ray render. So we already have our 3D city model, but we need to import the helicopter flight path data. So how much code do we need to do that? So we need four lines of code. We load the geo JSON flight data using the geo JSON SF package, which includes latitude and altitude and altitude of data for the helicopter flight over its flight. We then again transform the data to the coordinate system of the buildings data set, like we did with the road and waterway data. And then we extract a data frame of X and Y coordinates using the ST coordinates function. And then plot it in our 3D map using Ray Shander's render path function, which we can see plotted over there. And we're all downloading the data. But to fully recreate the Washington Post visualization, there's two things we really need to do. First, we need to edit out all of this helicopter data just to include the only the region of point of interest. So it goes well beyond and well before the actual time period we're interested in. And we also need to gray out the pretty 3D model we have. So we just have so we really emphasize just the flight. So we remove all the great work we did making that look all nice. So after we do all that. Oh, so yeah, this is the code that it takes to extract here. We see here up here we're extracting the data, just a couple lines. And then here we just specify various shades of gray for everything. And that's how we turn our map substantially more boring. So this we end up with this recreation done entirely in our all in all until about 50 lines of code to go from raw data to fully form 3D city. So you can compare it to the original and see we didn't get about 95% there. All that's less for us to do is maybe add some labels and some arrows and do a little post processing to bring it up to the full publication worthy status. But being able to do all of this work in our means that we have a tight link between our data analysis and visualization. So streamlining your workflow and improving reproducibility and having access to the great 3D rendering ecosystem R has to offer means you can produce these stunning 3D visuals without needing to learn any new languages or software suites. And finally, after completing this visualization, I had an idea. You see this past summer at our studio conference, I gave a talk on making roller coasters in R basically putting viewers into the data visualization as a passenger on a 3D thrill ride. So while this overhead view is cool, wouldn't it be even cooler to see the view from the flight of from the helicopters per second perspective itself. So simply by using the helicopters coordinates as the input to the camera position in Ray render, I was able to recreate the view from the helicopter just by swapping out that one line of code. So not not only have we shown a new and interesting perspective on this data, but we've also invented an entirely new type of data visualization, the Star Wars Death Star Trench Run Blot. But in all seriousness, the art ecosystem is one of the best out there when it comes to spatial data and 3D data visualization. If you work with data and perform analyses in an urban environment, being able to create a visualize your results in a high quality 3D digital replica can help aid understanding by your readers by placing your data in a familiar and recognizable environment. And using the our spatial ecosystem means it's easy to integrate multiple disparate sources of data into one combined visualization and do it all in a reproducible way. To me, it feels like a cheat code for mapping. So go forth, build a city, and thank you for listening."}, {"Year": 2022, "Speaker": "Benjy Braun", "Title": "Tech Debt: How to Communicate w/ Non-Technical Stakeholders to Avoid this Mission Risk", "Abstract": "In an attempt to deliver solutions as quickly as possible, non-technical co-workers and customers often butt heads with data scientists. A \u201clet\u2019s get it built as quickly as possible to meet this customer need\u201d mindset may look like an asset, but the \u201ctech debt\u201d\u2013 the backlog created when writing unsustainable, non-reusable code \u2013 created in the process to deliver so quickly is a hidden mission risk. Like racking up debt on your credit card, organizations don\u2019t just have to pay their tech debt but often pay interest in the process \u2013 with sneaky creditors calling debt due when you least expect it. That\u2019s why in this talk, Benjy Braun who is the Chief Architect of the DC-based, tech start-up 202 Group, will explain everything data scientists need to know about tech debt so that their non-technical colleagues and customers will listen. Participants can expect to walk away with a better understanding of what tech debt is and why it\u2019s a business and mission issue \u2013 not just a tech team issue, tips for balancing the need to deliver quickly while scaling sustainability (spoiler: not all tech debt is bad), frameworks for how data scientists can better communicate in support of shared goals, and strategies for paying back the debt you have without sacrificing mission success in the process. The bad news is that tech debt is one of the biggest hidden mission risks issues affecting organizations of all sizes. The good news is that once you make a mindset shift you can accelerate strategies for meeting mission needs sustainably.", "VideoURL": "https://www.youtube.com/watch?v=7U5j6Fr77-Q", "id0": "2022_22", "transcript": "He met his life partner from a chance encounter at a frat house. Everyone please welcome Benji. I'm going to bring you back to fall 2021. You can imagine where you were. I was sitting at my dining room table and I had just gone off a scheduled zoom call. At midnight. And I'm sitting there and before I poured myself a second glass of bourbon. I thought, I thought to myself, what, what got me here? Why, why was I here? Getting off of a scheduled zoom call at midnight. And, um, well, at the time we were, um, my company, I was one of five employees there. And we were trying to drive hard, build our product, get it out into customers, respond to what they need. And through this process of trying to deliver as quickly as possible. My colleague and I who are the only technical people on staff at the time found ourselves up until midnight trying to deliver. And I thought was trying to think of a way of how I could communicate this problem to my non technical coworkers who always thought, well, the client asked for it. Let's get it to them as soon as possible so we can really show value. Um, but over time, obviously, this create a lot of problems. Um, and even though I came up in this world as a data scientist in a consulting type environment where I would do analysis, we would do analysis on a regular basis. And then deliver it and then move on to the next thing. At this point, we were delivering things more in a like a product company. And I wasn't really familiar with this framework, but I found that this was the framework to talk about the problems we were having with my non technical coworkers. Um, I've given this talk before. Um, so and more in like a business context and less of a government policy context. So I've tried to adapt it to that. But really, the lessons are the same regardless of where you're at. Um, so I'm Benji Braun. I'm the chief architect at a company called 202 groups slash luvoyant government solutions. We're going through a bit of a awkward rebrand right now. Um, but I do, I've done a lot of things over the past three years, but when it was just the five of us and just two technical people, I really kind of did everything. Um, and so I have had this experience of building a product and understanding. Um, the problems you have when you build with tech debt, which as I'll explain is inevitable, but I hope that you will learn something from my mistakes so you won't have to make them yourself. Um, so the goals for this talk. I'd like to define what tech that is I've talked to some of you guys about this talk already and some of you are familiar with this concept some of you aren't so I'd like to define it. Then I'd like to talk about a new way of thinking about this that you can talk about with your non technical colleagues that will help, um, help you help help you outline this problem. And then, um, since I went to business school, there's going to be a framework got to have a framework and then we'll, you know, we'll wrap it up. Um, okay, so, so definitions. What is tech debt? Here's like a pretty, um, jargony example. So in software intensive systems, technical debt is a collection of design or implementation constructs that are expedient in the short term, but set up a technical context that can make future changes more costly or impossible. So what you do something now. Later, you have to pay for it. You have to pay that debt back. Right. Okay. Word Cunningham who actually coined the term. This is what he says, a little debt speeds development. So long as it is paid back promptly with a rewrite, the danger occurs when the debt is not repaid. And so this is what was happening. Why we were up till midnight or later really fixing these problems is because in the speed to want to deliver this piece of data or this feature to a client. We were writing bad code. We're doing things that were inefficient and layering on top of that so all our problems stack one on top of the other. We weren't repaying our debt. You know, or a sales product perspective, why can't engineers just shut up and implement those features already. I'm trying to close this sale. I think like a lot of this is the perspective that technical people don't, non technical people don't really get. I think we've heard this in one of the other talks earlier today or yesterday that a lot of people think what we do is magic. And they don't understand what takes what what takes a long time what doesn't take a long time. Why you should do things fast so let's talk about away from that. Someone can care someone who's not technical can care so it's not just about complaining that they can get they can start to care about it. Okay, so I put this in here because I definitely didn't think of myself this way but I've started to think about it, especially as a building a product. And I never thought of myself as a software engineer, but I've started to. And I think that's a good, a good thing. So you can be a software engineer, even if you don't write Java script or PHP or Java you're still writing software you're the code you write those scripts are pieces of software. And you should try to build them in such a way so that you're not acquiring writing it you're not writing it poorly you're doing a but in addition to not writing it poorly you're writing in the most optimal way possible so that it's not rebuilding the wheel every time you do it. Okay, so we we think and I think I've set up this, this this dichotomy between like the tech side of the house and I before I said business but let's call it policy and like the government context between like the technical people in the out and now analysts let's say. They're like they're against each other that the analysts don't understand why the tech people can't just get it done and the tech people don't understand why the analysts keep asking them to take shortcuts and making it work worse. So I think that's a really big problem because your mission or like the business isn't those two competing forces it's those two forces working together so you need to be on the same page to to to to work on it and to to figure this out. And so I think a lot of the time that's been done the first time I tried to talk to my team about tech that the non technical people I thought it was going to be like a really easy framework and people got really defensive with it. Because I didn't propose it from a business standpoint of what they could do to do why this was better for the business, not just like why it was going to make better code because like at the end of the day, why do they care if the codes good or not, if the business outcomes are the same. Okay, so tech that can be both a good thing or a bad thing so tech that could be a problem. In the way that this is like a graph I've seen a few different places but this often happens with software organizations in general, which is that as time goes on productivity goes down, but cost goes up. So like the idea is that, you know, it's takes longer to implement those features it takes longer to do the analysis to build that model. What do you do you hire more people. Okay, so those it takes those people longer and longer to do it. And instead of fixing the problem you're just layering on top of that problem every time you hire new people, it makes the problem worse. Okay, but tech that can also be an asset. So you can get your product into users hands more quickly. You can explore product fit without making big investments. So you can test new capabilities and respond, respond quickly like think of like the agile, the agile framework. Like you do something in a sprint, you see how the customer interacts with it, you can go back and make changes. Okay, so let's think of it in a way that manages these concerns. So there's this concept of behavior value and structure value from, you know, famous or infamous depending on how you think about it Robert C. Martin. Yeah, he's definitely an asshole, but like, I think he's right about this. And so what he's saying is that software has two types of value. One is behavior value and one is structure value. So behavior value is the value that the stakeholder slash customer makes or saves from the product. So this is like what you get out of the, like what you actually get out of the product. Right. So like, the app, like you people talked about a lot today about shiny apps. So like what the value the customer gets by being able to use a shiny app to download that data versus, you know, emailing you for it. So that's behavior value. So structure value on the other hand is the ease and promptness of creating and changing features in an existing system. So if, if we're providing someone a shiny app and I'm like, you know what, I really need a, I want to be able to download it as an Excel CSV or text. How hard it is to add that button is the structure value of of it. And the value of the whole system is those together. It's both the ability to change and what it does right now. So this is the frame. So that when we came to it like that before again to like the framework of taking on taking out good tech debt versus bad tech debt is, is that it's that value plus structure. And this was the way that I was able to communicate to my non technical like coworkers, the importance of taking this into account. So it's like, Hey, so and so asked for this, this button or this data set. This data set this analysis. I want to get it to them, you know, today tomorrow. It's like, okay, well, I have, you know, all these other things going on. I can do that. But it's going to make it harder to to answer the next request. So let's think about it. Is it worth it to do that. It could be, you know, who's asking for it. What does it contribute to like the long term vision of this product of this company or is it just like, I need, I need to make the client happy I gotta do it, which isn't really like a good way to, you know, like that type of development isn't like a good way to develop. Okay, so framework. You know, the debt metaphor, I think is a really good one in finance. They talk about, talk about like present value versus future value. And I'm not sure how many of you are familiar with this concept, but that's in in finance. I take out a loan today for let's say $100 in 10 years is I'm going to get $110 back. So that's the present value versus the future value. So this is like the thing with responding to a customer's need or want like, does the present value I get from delivering this equal more value later on. Or do I get, am I going to have to do more work than it's worth later on. I think someone earlier today was talking about like is the juice worth the squeeze. So good way to think about in the same, the same context. So smaller or variable loan versus large fixed loan. So, this is, you know, to carry on that finance example. So is there a way to make yourself more flexible than than really commit right now. Sometimes it's better to make a decision that is suboptimal but it leaves your options open. This could be like, look for, you know, for us, it was our cloud environment. We originally went to AWS commercial, and we had to move over to GovCloud because of client requirements. But in the beginning, that gave us a lot more options. There's a lot more services in commercial. It's less, it's less expensive versus going over to GovCloud, even though in the long run, that's what we needed to do. At the time, we need to keep our options open. So it was better to do that. All right, this is probably the most. I don't know if this is the most important, but I think this is a lot where you get to make a lot of different decisions about you what you want to avoid and what you try to not want to avoid. I think people incorrectly sometimes think about tech debt is write bad code now fix it later. I don't. Maybe that's the type of tech debt, but I think that's just bad. Like you should just right try to write the best code you can. Especially because you will have to fix it later. Like you're not going to you will have to change that code later. And it's so much harder to change poorly written code than well written code. Like, I mean, it's a joke. You know, it's it's a it's an overuse expression, but it's true. Like you need to write your code so that you remember it next week. I mean, you really for me, like tomorrow. So if I wrote bad code, it's going to be hard for me to tell what it is. All the more so when you're working with a team, when you're building a product and not just doing analysis. So that's just like bad code. Okay, that's the type of tech debt. Try to write. Don't don't write bad code. Okay. But sometimes you take you take out tech debt. You take out tech debt deliberately. So for example, our company right now has a, like a professional front end with JavaScript developers that's like pretty cool. But originally we couldn't afford. Yeah, like an engineering staff like a front end engineering staff. So we built our markdown websites and deployed them using our studio connect. And it was like a good enough solution. We knew that someday that wasn't going to work anymore, that we were going to have to rebuild everything from scratch because we were going to outgrow that system. But it was good for then. And that was acknowledged versus like reckless and accidental tech debt. For us. The biggest example for us was our approach to cyber security and cyber security compliance. We just didn't think about it. And I've spent the past year just basically working on cyber security compliance, let out not even really touching actual cyber security. And now we finally, now that we finally understand the ramifications of that, we've hired someone to really like fix those problems that we've created over this year, much harder to do that for a system that's already built, then baking it in from the beginning. All right, design for flexibility. I think this we've kind of covered this in a few ways, but you really have to leave yourself open because you don't know how things are going to change. And I think a good metaphor for this is that software data science, it's not like building a house or car. Like you, you often when you start your project or start working with a client, you don't really know where you're going. You have an idea and you're building towards that. But that's going to change as you go on, whereas like a house, you know, I hire a general contractor to fix my kitchen when he's done with a punch list, it's over for a software project for a data science project when you're working with someone, they're going to be changes to that system through lifetime, and that's expected. So you have to build in flexibility where you can. All right, and then the last. Was I a slide that's what I think I must have a double there. And so the last thing is to invest in professional development. We talked about good code bad code decisions, but so many of these inadvertent tech debt that people take out is because they just don't know you don't know what you don't know. And if you, as you learn more, and you invest in your people to learn more, they will. And then you have to know what type of decisions they're making. And I think like that's why, you know, we have things like this getting together here like I've learned so many things that I didn't know about talking to you people to help us with that. All right, so, so recap. And tech aren't separate parts of the organization. They are the organization. You have to find a what you have to think of yourselves as partners and find a way to move forward so that you're all operating together. So tech debt can be a mission problem or asset, depending on how it's management, how it's managed you can take out bad tech that just is a, that is just with you and like a something that you're chained to as you move as you grow, or it can be something that helps you grow. And then, you know, these five points for take for managing tech debt, try to understand whether the present value is greater than the future value, whether the smaller or variable loan is is better that's better than taking a large fixed loan, making sure that you're doing things deliberately versus accidentally if you can, building a site that's important, then you're going to have a little bit of a technical disability and investing in professional development. So lastly, I'm actually, I've talkedumps a few of you about this. I'm actually writing a book about tech debt and about how it applies to not just code, but to all aspects of a technical organization. So I think that you've decisions you made that you wish you didn't, or times that you made quite what made of been a questionable decision or seemed like a sub op and mold decision, but that was the right decision. I'd love to talk to you. I'd love for you to reach out to me. And thank you."}, {"Year": 2022, "Speaker": "Drake Gibson", "Title": "O*NET and ORS Tasks Classification Project - Topic Modeling", "Abstract": "The Occupational Requirements Survey (ORS) is used to collect information on requirements related to the critical tasks of a well-defined and specified job. Field economists use the ORS to collect information regarding the critical tasks of a job as open text. While these critical tasks are essential to the primary purpose of the job and are collected in the ORS survey process, this text is not fit to publish. Our goal is to classify tasks to further the extensive research into task data and publish task data for public consumption. We can use another data source to support the task of classification. That source, Occupational Information Network or \u201cO*NET\u201d, contains only occupation and not job level data but could be leveraged as a taxonomy for classifying tasks. O*NET could be a way to classify ORS task and act as a taxonomy. Topic modeling is a type of statistical model for discovering the abstract 'topics' that occur in a collection of documents. In our case, the documents would be O*NET generalized work activities. We use topic modeling to classify the data and then we evaluate the results. We create LDA models to describe and fit the data. We hope to present to you all our findings of measuring models of O*NET data to compare to ORS and hopefully lead to publishing task data for public consumption.", "VideoURL": "https://www.youtube.com/watch?v=-qXcmJoVz1o", "id0": "2022_23", "transcript": "beyond having great last speaking skills. He used to be a tap dancer. Now can we ask him to do some tap dancing? Is that too much? He doesn't have the right shoes. All right well please welcome Drake down here. Thank you for allowing me to come and speak. Hopefully everybody is still awake so I was a little bit of energy. My name is Drake. Thank you. Thank you. My name is Drake Gibson. I'm a data scientist at the Bureau of Labor Statistics in the Office of Compensation and Working Conditions. Today I'll be talking about our Ours TASA classification via topic model. So quick little roadmap wouldn't be a presentation without this. We're going to talk about the two data sources that we're going to be comparing and using to classify our task lists. That would be ONET and Ours. I'll give you their acronyms now. You'll know more about them in a couple of seconds. The purpose of this project, I'll also talk about leveraging TASA data, the methods, and topic modeling itself. And then finally I'll show a quick demo of the visualizations. Okay so ONET stands for the Occupational Information Network. It is sponsored by the Department of Labor's Employee and Training Administration. There we go. And also it was done on behalf of the North Carolina Chamber of Commerce, I believe. Don't quote me on that. It is on the DOL website so they will give more information. It is marketed as the nation's primary source of occupational information. As an employee of VLS, I would kind of not say that, but again, you know, these views are mine, not VLS's, not DOL's or anybody in the federal government. I just am really proud of the work that VLS does. It highlights changes in the workforce. It helps people find training and needed for jobs. And also it classifies TASA. Well right now ORS does not have ability to classify TASA to be published and actually used for public consumption. We want to be able to use ONET to help us do that. ONET classifies our TASA using generalized work activities or GWAs. And this actually helps them build out a taxonomy for their TASA data. It's also publicly available. So ORS or the Occupational Apparmment Survey, it is administered on behalf of SSA or the Social Security Administration by the Bureau of Labor Statistics, specifically the Office of Compensation and Working Conditions. Our program office is a really hard work to get this out to take in the information, send out a general work file to SSA and hopefully publish our TASA data in the future. So this helps support a adjudication of SSA's disability program and shows like the lowest amount of requirements to actually complete an occupation. So it captures certain requirements like physical demands, environmental conditions, cognitive and mental demands, and also education training and experience requirements. So things like literacy, credentials, and on-the-job training. We get the TASA data. It's very, very, very unstructured, comes in in different ways. Sometimes it's a run-on sentence, sometimes different words, different letters of capitalized, where you have misspellings or an incorrect character. I think when I was running this model originally, we had a degree symbol in there. So I was fun to deal with. So what's the purpose of this project? So the purpose of this project, it's a research project, which is personally my favorite because we get to set the beginning and end of it. And this can go on forever if I really wanted to, but unfortunately, I have other things I kind of have to get to as well. So when we want to be able to publish or TASA data to the public, we want to be able to give that to people to say these are the minimum requirements actually complete an occupation. And these are the tasks that go along with it. But as a part of that, we need to build a taxonomy. So on that has a taxonomy with the generalized working, the generalized work activities or GWAs, to actually be able to publish information and categorize it. Within statistics, within the federal system, we have classification systems. So you have NAICS, or the North American Industry Classification System. You have SOC codes or the standard occupational classification. I mean, classification, I see the codes, the classification, I can't remember right now and I apologize. But we, as the basically what I'm trying to say is we classify everything. Everything has to be labeled, and then we can then publish out to the public and actually say this belongs here. So we don't have a way to do that with OERS yet for our TASA data and we want to be able to do that. So how are we going to do that? We're going to use topic modeling. So topic modeling, as you can see here, we have two definitions. It's a type of statistical model for discovering the abstract topics that occur in a collection of documents by Susan Lee and a publication about topic modeling and LDA allocation. We'll be talking about that a little bit more in the next couple of slides. Julia Silge and David Robinson for Text Mining with our title approach. It's the method of unsupervised classification such documents similar to clustering of numeric data. You can read the rest of that. My personal favorite one I didn't include on the slide is topic modeling is an unsupervised machine learning technique that's capable of scanning a set of documents, detecting word and phrase patterns within them, and automatically clustering word groups and similar expressions that best characterize a set of documents. Basically, we just want to classify things and let the machine tell us where it's supposed to go using statistical methods. Tommy Jones had a great speech yesterday about LDA and all of his advances and actually I use his package, sexminer to even get to this point. So really excited to kind of talk about those results later on. As I said, we used latent Dirichlet allocations. That's one of the more common approaches to topic modeling states that each so for us, our documents are our tasks. Each task falls into a topic and then each word within each task falls into a topic as well. So we'll have something that says like maybe a statement or a test that they decides to type a lot. They are scientists type a lot. Of course, A will be removed as a stop word. The rest of those words will be organized in the different topics. If we had a two topic model, you would say that maybe one word would be in topic two. The others would be in topic one. We then said this task falls into topic one, obviously using a lot of statistical things behind the scenes. We can talk about that further later, but right now for our purposes, we want to give a quick simple example. So for this task classification, originally, I just created a four topic model as a group of concepts to myself that I actually understand was going on. So I was following through the tidy, the text mining with our tidy approach, use their package to create a four topic model for for ores. Then from there, I used the mallet model because it kind of ran things a little bit easier and it has some built in functions I liked. And then from there, because I wanted to build more models and larger size, I use the text minor package, had more built in functions I would like to use and it was able to help me visualize and explain it to people in a better way. Like it just gave me a better understanding of what was going on because initially, I had no idea what I was doing. I did not know what topic modeling was. And this was maybe the third time I'd actually done machine learning. So I'm still was developing my skill set and developing my capacity. So the models that we're going to be talking about and what I've been kind of displaying to my team and also to management, we built a topic 16 32 48 and 64 topic models. I chose those numbers arbitrarily. It wasn't any rhyme or reason behind I was trying to figure out what would be the best way to be able to classify the data. I looked at the common words and verbs between both data sources. And we're still working on how we actually have a final suggestion. This is a part of a two prong project, one using supervised machine learning and one using unsupervised. I can, if you ask me later, I'll give you a little bit information about the supervised, but I didn't really touch it that much. So I can't give you all the ins and outs. Just a little bit kind of talking about what I covered a little bit before, before I own that topic model. This is these are the results we're going to talk about today. I can't talk about Ours because it's not publicly available yet. It's still confidential and I don't want to get in trouble. I do like my job. So again, we we looked at a lot of things here. I looked at word clouds. I did I looked at the difference in long likelihoods. We went over coherence of even this imperbalness things as well. And then finally, I used the R squared built into the text minor model to also look at a way to see how well the model fit. So these are results when looking at the onet topic modeling. As you can see here, you have a coherence measure and then you have the R squared. So I'm going to go quickly over coherence coherence is how well words associated within the topic. You have a coherence measure for each topic within the model. And then from there, that final coherence measure is the average of all those coherence measures for that entire model. So you can see here that the eight topic model has the best coherence out of all the models that are listed here. And the 64 topic model has the second highest for R squared. You can see that the 64 topic model has the highest R squared. And we could talk about that a little bit more further from what my understanding is is that as you get more topics within the model, the R squared kind of the model fits a little bit better. But if you can counteract that with how big your document or your corpus is, that it will then counter counter counter act. And I can talk to Tommy later if I miss if I messed that up. So, oh, sorry. So to go back to that also, within this model for the eight topic model, topic five had the highest coherence. And then topic 33 had the highest coherence within the 64 topic model. So we did this thing, we did this project, did this research project that finally kind of came to an end more recently. We were able to classify each of the tasks. But what is the optimal what is the optimal number of topics for this exercise? We still haven't really found that yet. I would like to do more testing, take more approaches, keep going until they tell me that I have to stop saving, I'm not going to let me do this anymore, take my laptop away. But for right now, we've come to a stopping point. I believe personally, and again, this is not a reflection of anybody else's views, I would want to have as many topics as possible to get the best, I guess, way to classify all the diets. But I'm still trying to figure out which is the best approach. One of the members of my group actually did a test and actually kind of went through the data and said that it kind of lined up with our standard occupation class of class of standard occupation classification system, which is kind of disheartening, which means that we could just use SOC codes to just say this is how our task was supposed to be organized, and then I just wasted six months of my time. But we're still trying to figure out how to really classify it. I want to thank my team, which is a Nicole and the story I can David O. David O is another data scientist within BLS within my office. And Nicole and the story arc runs our research department within that office as well. She they're both brilliant people. I would not be able to do anything that I was able to do without them providing data, moving things out the way and explaining certain things and actually them being guinea pigs of does this make sense? Does this work? Does this endogram actually look like it's conveying what I wanted to convey? So I'm going to end the slide show real quick and do this demo. Okay, so this is using the LDA Viz package. So the LDA Viz package, it allows you to put your topic model into the into their functions and into their package, and then it creates a web page for you. So technically, when you run this package, and when you run the functions in the package, and you like turn it off, the web page goes away. So I had to manipulate some of the back end to keep it and keep it running. And I could talk a little bit about that as well, if you have any time after this. So on the left hand side, you'll see that it has the well, yes, you all is left. It's so is the intertopic distance map. So it's a bubble map on principal components. We don't I'm not going to go too deep into that because that's still something I'm trying to wrap my head fully around. But it shows that the share of of the actual corporates within that topic. So topic one has a largest of 16.2 percent and you can kind of when you highlight them, it goes through. So it the map is supposed to show how closely related the topics are or lack thereof. So you see that topic one is topic three are not closely related, but topics who are basically on top of each other and may contain some of the similar some similar things. On the right hand side, you'll see here that it has the most same-line terms, which are the terms that occur them that have a pretty large frequency within the model itself. The equation for it is at the bottom here. They have a working paper explaining that. For me, I'm more focused on the relevant terms. So you can click the the bubble there. It shows the most relevant terms that also how how many times they appear within the model itself. So you'll see that it's system, system appears pretty frequently within this model, especially in topic six, develop within two and four and one as well. But I'm changing this lambda here. They say that the optimal level for the lambda is like 0.25, but I just want to go all the way to zero to show. These are the terms that are most unique to topic one and don't really appear anywhere else within the model. So you see something like a certain compliance, green impact, things like that. And then we want to look at two and four because they look pretty closely related. So you'll see here the most salient term well before hand, most salient terms, but these are the terms that are kind of unique to topic two here. And then if we go back up here, these are the ones regardless of their frequency and the rest of the model are pretty relevant for this topic. And then with topic four, you see here is more of a medical focus. So I'm still trying to figure out why those two are so closely related, but that's what the model is telling me at this point. We can quickly go through some of the other models. This one is a this one was an eight topic model. This was a 16 topic model. So as you go through here, you'll see that more topics are closely related. Still topic one is kind of further away from everything, but you still see a little bit more overlap than I would really be comfortable with, to be totally honest. She the same within our 32 topic model, a little bit even more within our 48. And then finally, within that 64, you see there's a lot of clustering over here on the right hand side. Still have topic one and topic three being pretty pretty far apart from each other. And we see that the shares also is gone down a little bit as well. But here's my contact information for my email. And yeah, that was my presentation."}, {"Year": 2021, "Speaker": "Aaron Mannes", "Title": "Big Data @DHS: Vast and Varied", "Abstract": "If big data is characterized by volume, velocity, and variety, the Department of Homeland Security (DHS) is the ultimate big data organization. In its mission to protect the American people, DHS undertakes an array of diverse functions and often has to make decisions in real time. Using data analytics to enable these missions requires a blend of creativity and pragmatism.", "VideoURL": "https://www.youtube.com/watch?v=7hodI5LTdkM", "id0": "2021_01", "transcript": "Our first speaker is actually one of the leading experts on the vice presidency. So you have any questions about vice, is the person you want to ask? Everyone, please welcome Erin. So I'm here to talk about big data at DHS, vast and varied. I'm Aaron Manus. I am a support contractor at the Data Analytics Technology Center, at the Department of Homeland Security Director, Director of Science and Technology. The Director of the Data Analytics Technology Center is Alex Fusvath. Our contact information is right here, and I'll show you my contact information again. Before I talk about the data analytics work, I always like to ground things with the discussion of what DHS is, because the scope and missions of DHS, quite simply are huge. 22 components, 240,000 people. We all know about TSA, whenever I talk about working with Homeland Security, people mention their experience with TSA. But that's only a tiny piece of it. Even the TSA, you see, TSA is not the air plane, air traffic security agency. It is the transportation security agency. So what you see is only a part of TSA. TSA is also thinking about how to protect surface transportation all over the country, city buses, trains, mass transit. And that is only one of the 22 components. I just want to give a quick sense. DHS is responsible for securing US borders while for facilitating commerce. I'm going to talk about that a little bit more later. We protect cyberspace and critical infrastructure. That's another way of saying we protect everything. And what we've learned with the pandemic is critical infrastructure isn't just the electrical grid, chemical plants, it's education. It's supply chains. It really is everything. It could be in its own cabinet department. A lot of people don't realize the second largest federal law enforcement agency, law enforcement investigative agency, Homeland Security Investigations is part of DHS. Their remit is to investigate any crime that crosses US borders. That includes national security export violations. That's a fancy way of saying sending sensitive dual use technology, say high tech chips, to somebody that maybe shouldn't have it, like a state that supports terrorism. But HSI also investigates human trafficking, child exploitation, money laundering, international gang activity. Just a huge range of issues. We oversee lawful immigration. We are the lead agency for responding to natural disasters. I'll talk about that a bit more in a few moments. I really like to talk about the Coast Guard because they're sort of in miniature epitomized this huge array of missions we have. They are a military agency, a law enforcement agency, a regulatory agency, a safety agency. So they are supposed to protect our coasts, but they also have an environmental protection function. They also rescue voters at sea. And guess what? They're also the lead agency for the Arctic. Secret Service is part of DHS. We protect the president. And finally, there's the Department of Homeland Security and then there's the Homeland Security Enterprise. The Homeland Security Enterprise means we partner with state, local, tribal and territorial agencies. First responders, public health, emergency support. Another point I have to make absolutely is that we are founded on the bedrock of treating individuals with dignity and respect, safeguarding civil rights, civil liberties, and privacy protections. This is fundamental to our mission. Every data science project I've been involved with, these issues have been front and center. Now, you're a bunch of data scientists, so I'm sure a lot of you are already starting to chunk some of these problems and see them as big data science problems. For example, protecting critical infrastructure, you can see that as a big network analysis problem, right? Critical infrastructure isn't just one facility, it's a web of interconnected facilities. And we have to identify the critical nodes and how best to see them. We have to identify the critical nodes and how best to secure them. Whether from virtual or real world threats, you can imagine using big data analytics to look at cybersecurity data, analyzing web traffic to critical websites. The next speaker I know is gonna talk about a natural language processing project, and that is huge for us because we generate lots and lots of written reports, which have all kinds of insights and the ability, people can't read all of these reports, the ability to extract meaning from them. Consider the range of criminal investigations we have, all of the evidence collected, tools that can help us find valuable intelligence and then better protect the American people. Here's the thing. So you can see a lot of our missions as big data science problems. But we are constrained. The first thing I have to say is our core function is anomaly detection. And I like to say anomaly detection is not finding a needle in a haystack. You can brute force that if you have to. No, anomaly detection is finding a needle in a field of haystacks. Sometimes we don't know what we're looking for. Sometimes detecting the signal from the noise can be extremely difficult. Finding anomalous activity, we don't have a baseline for what's normal activity. These are the challenges we face when we're thinking about illicit cargo entering the United States, when we're thinking about countering violent extremism, when we're thinking about threat assessment. The other thing is a lot of our goals are not subject to easy metrics. Some goals, you know, some goals are, but define, when I talk about disaster or sump bonds, what is an effective disaster recovery? How do you measure that? The other thing is our stakeholders want perfection. We have a very high cost for false positives. We don't want false negatives. We don't want to miss a threat, but we also don't want to spend time looking at something that isn't a threat. I mentioned DHS is this amalgam of 22 components, each with different cultures, different operating standards, operating procedures, different missions, different world views, different concerns. Imagine linking multiple legacy systems. Well, that's us, except that the systems are made up of people. It's a real challenge, and the multiple stakeholders requires complex coordination. Now, I work at DHS Science and Technology, one of the 22 components. We are the research and development arm of the Homeland Security Enterprise. We do everything from, we might make critical gadgets, sensors that firefighters or police might carry on their belt. We might be developing sensors to detect WMD material and cargo entering the United States. We identify best practices. And my particular corner, the Data Analytics Technology Center, we're doing big data research to support Homeland Security missions. And that can scope from real nuts and bolts problem solving to big picture cutting edge research. And I'll talk about both of those. One of our big functions, our core function is to help DHS components generate actionable insights from the data they collect. And one of our core functions is, we don't, I should say, we are within DHS Science and Technology, we're part of something called HSRPA. We're like DARPA for Homeland Security. We're DARPA without the D. The D stands for dollars. We don't have them. We have to be extremely strategic and thoughtful in the investments we make. One of the best things we can do is look at the data analysis tools that are already out there, analyze them, determine if they're a match for DHS components, meets their workflow. And a big part of that though, is a lot of rarely do commercial products simply meet our unique needs. So it's a matter of really the back and forth between the DHS component, the vendor, making it work for us. I should also say that, in a lot of cases, we need solutions that scale. We're talking about serving giant government organizations. Sometimes we can hack a small scale solution and solve the pain point. But sometimes we're looking at big picture items that can really serve all the agents. Now I want to talk about a couple of specific areas where we operate. Customs and Border Protection. Their mission is to facilitate trade and travel, collect duties on imports and detect illegal entry of both criminals and also smuggling. I've got that picture up there of the San Ysidro border crossing, but if I could also show a picture of the Port of Los Angeles, all of the cars, all of the people, all of those cargo containers that we're reading about entering the United States. The vast majority of it is legitimate traffic and we want to facilitate that. But buried is also significant illicit cargo. This is an enormous complicated anomaly detection problem. And we've worked on this where we have to sit with the subject matter experts, the agents in the fields, go back and forth, identify the signatures of what might be suspicious and what we should investigate further. I want to mention this, there are two particular lessons from our work with CBP and other agencies. One is our subject matter experts know so much but they're in the weeds. They are so engaged with the problem set that getting them to abstract because when you're doing data analytics, you can't get wrapped up in the details. You need to be able to abstract the problem so that we can then do the analytics. And I find that's a huge barrier. It takes real patience. The other thing is, besides the fact that our adversaries are adaptive, things are changing all the time is the data is messy. When you think about US import export data, we're talking about hundreds of millions of entries and there's all sorts of noise and mess. The same business registering an import can be spelled 20, 50, 100 different ways. I'm sure you all know this but the big lesson is your data hates you and wants to break your system. And it sure does with us. Disaster assistance, this is a critical mission. FEMA is part of DHS. This is only going to get worse. We've, with climate change, we're seeing forest fires, we're seeing hurricanes and floods and now the pandemic is a whole new type of disaster that we have to respond. Disaster assistance involves three bits. There's preparation. That's what we call left of the boom. Before the disaster, what can be done? This is a great big data problem. Can we look at information about a community and identify the most vulnerable points and render assistance beforehand, strengthen them? During response, this is action packed stuff, triaging information in real time, developing decision support systems. However, it tends to suck all the oxygen out of the room. We focus on how do we deploy helicopters? How do we identify the most critical needs in a crisis? That discussion sucks the oxygen out of the room and prevents us from focusing on the preparation. And then afterwards, recovery. How do you measure effective recovery? There's obvious metrics. Did people move back? Did we rebuild houses? But how do you measure restoring a community? These, this is at the cutting edge of where data science meets social science. This is the world where I live. My PhD is in public policy, not nothing mathy. We've got to get this right because the problem's only going to get worse. Finally, we have a very active program called CILAB. We're doing this with the Cybersecurity and Infrastructure Security Agency. It is a multi-cloud collaborative environment for cybersecurity and infrastructure security research. Long and short, we've built a play space, a big ecosystem where across multiple clouds, we can bring in datasets, bring in tools and really run them through the mill and determine if they're going to work for our huge cybersecurity needs. We are directly responsible for the cybersecurity of the US government, but we are working with private industry to effect, to try and secure the entire US internet. This is a critical realm. Ultimately, we want to, we're exploring ways to extend this beyond just DHS, S&T, and SISA and incorporate the private sector because that's really where a lot of the action is. These are a lot of our nuts and bolts projects. I wanted to talk quickly about looking ahead because we do have one foot in the practical immediate. We also have a foot in research looking farther ahead. We are heavily involved with these National Science Foundation AI Institutes. There's the Future Edge Networks and Distributed Intelligence Institute at Ohio State. There's the Edge Computing Institute at Duke. There's the Dynamic Systems at the University of Washington. And of course, intelligent agents for next generation cybersecurity, we're really interested in edge computing. There's going to be a lot of sensors, but leveraging these complex, diverse streams of data in real time to support disaster response. Is a real challenge, and that's one of our big picture items, Ramnets, and this is, which is real-time analytics for multi-latency, multi-party metro-scale networks. As we move to smart cities, we're going to have all these varied sensors, and they're going to be breaking all the time. And we can't just shut down parts of the city because some of the sensors are down. We need in real time being able to assess information, make decisions, and that's why we're supporting these AI institutes so that we can start building that capacity. Let me end by saying you are most welcome to engage with DHS S&T. We have a website, we're on all your social media, you can contact DHS S&T, and you can also contact me. Thanks for your time, look forward to any questions in the chat."}, {"Year": 2021, "Speaker": "Jordan Jasuta Fischer", "Title": "Ensemble NLP to classify medical conditions", "Abstract": "Medical terms are linguistically very specific: a letter or two can completely change the word, and prefixes and suffixes can link two words that otherwise look wildly different. As such, many typical methods of natural language processing (NLP) are ill-adapted to work with medical records and their specific vocabulary and syntax. When a government client needed to classify medical conditions for record processing, IBM built a hybrid ensemble model that incorporates both rules-based and machine learning classification, to accommodate the client's system structure while flexibly handling the nuances of medical terminology.", "VideoURL": "https://www.youtube.com/watch?v=YHAeGBM9z3I", "id0": "2021_02", "transcript": "Our next speaker likes to in her free time she trains acrobatics. So I want to know is she being trained in acrobatics or is she training other acrobats? Everyone please welcome Jordan. Thank you so much Jared. To answer your question, prior to a knee injury that I am currently dealing with it was both. Now it's nothing. But back to the subject. So I will be sharing with you guys today a model that we built to classify medical conditions using natural language processing. Natural language processing abbreviated NLP is basically when we use AI to understand human language instead of ones and zeros right so English, Spanish, Mandarin it doesn't matter. This type of AI is really good on unstructured data like text or audio and the techniques that can be used to accomplish this type of analysis range from you know rules based entity extraction and entity recognition to statistical modeling to machine learning and deep learning. So in particular when it comes to medical data NLP has a really important role for a number of reasons. So first of all if you're more than a few years old probably some of your medical records were originated on paper and we need to be able to pull the text off that paper and understand it if we're going to be able to analyze it. Even in terms of recent records sometimes some hospitals or healthcare providers are working with legacy systems. Maybe they print out a form and you have to fill it out by hand because they don't have enough computers. And then even if we have the most up to date perfect technology when we talk about you know the clinical progression of an illness or sort of abstract symptoms that we're feeling that we don't have a diagnosis yet we're going to need to use descriptive language and kind of descriptive summaries. So NLP has a really important role in analyzing medical data. But when we talk about medical language so of course especially if I'm a patient describing my own symptoms probably I'll use really simple words like my ankle hurts or my ankle is swollen. But a doctor prescribing a diagnosis or you know a patient repeating a diagnosis that they have received is going to involve you know words like osteoarthritis or these more technical specific medical terminology which come from Greek and Latin roots they have prefixes and suffixes and honestly the syntax around the way we put those roots together is a little different than the way it would be in English. So any NLP modeling that we use to handle medical language is going to need to be able to accommodate both regular simple English and these more technical Greek and Latin based medical terms. So if we look at the client case that we built this model for there actually were many steps but we're going to focus on these three so we needed to work with records as far back as the 1950s so some of them were definitely paper. We needed to be able to scan records off of paper extract the data and save it as text. We used to do that we used OCR or optical character recognition. You've probably seen these eights before. It's just machine learning that specializes in recognizing characters like numbers letters punctuation in two-dimensional images and saving them as the corresponding ones and zeros for the computer to understand what character they are. Once we have this text saved our classification task which is what I'll focus on was to sort the conditions into medically relevant categories for processing downstream in their system. So therein lies the first challenge which is that we had over 100 medical categories that we needed to be able to sort cases into. So you know we know that your joints are orthopedic injuries or conditions but each joint has its own specialist so you have a wrist specialist an elbow specialist a shoulder specialist etc. And then you can get into the internal medicine like the body systems. You probably remember those from eighth grade biology. You have your digestive system your respiratory system your endocrine system and extra points if you can if you know what the endocrine system does. So we had a lot of different categories that we needed to be able to sort these strings of text into. A couple other challenges we were facing is that we actually did receive training data from the client which was great that doesn't always happen but some of the data that we received had contradictory labeling. So for example a single condition would be labeled as two different categories throughout the training set. And there were also some categories that we needed to be able to classify conditions into that we didn't have any examples of in the training set. So the training data was obviously better than nothing but it wasn't ideal. We still had some problems there. And then because of the nature of some of the scans that we were working with the OCR was producing some errors with you know poor scans or perhaps great scans but poor handwriting. You can see some examples here. So actually to address that the first thing we did is make sure that we had a spell check layer that we could feed the text through before we even got to any classification to correct for those small OCR errors. So for some examples we can see so diabetes affects the endocrine system. Now you know carpal tunnel is a risk condition so that goes to orthopedics risk. Pancreatic cancer is cancer so it's going to be oncology but the pancreas is part of the digestive system so it's oncology digestive system. On the other side we have pancreatitis. Your pancreas is still your digestive system but it has nothing to do with cancer. So you can see how we have some really thin lines between the categories. And then so bullet wound in left hip there's a category for like wounds and trauma super fun category. So the original model that we tried out we had the input text we put it through the spell check layer like I mentioned. Then we extracted some features. We extracted keywords specific you know names of illnesses, organs, body parts. And then we also extracted trigrams. So in this case trigrams is referring to groups of three characters. So for the word trigrams it would be t-r-i-r-i-g-i-g-r and that helped us a lot with the like prefixes, suffixes, Greek and Latin roots and the fact that a lot of the medical like technical medical terms were very commonly misspelled which I really can't blame anyone for because I can't spell any of those words either. Once we had these features extracted we threw them into a multilayer perceptron which is a simple neural network and it's about a classification for us. So this worked okay. We had a really high a really wide range in accuracy from retrain to retrain. Even though we had over 20,000 turning cases rows of training data we also had over 100 categories. So if you think about it's really not that many examples per category and the distribution of cases across category was really varied based on just what had come in in the medical system that they were pulling the data from. So it was a pretty unstable model and it really struggled with cases that crossed categories. So bullet wound in shoulder. If you think about it statistically both in the training set and it was reflective of so is a US-based client. So in the US there are many more orthopedic shoulder injuries than bullet wounds and trauma wounds and shoulders. So you know statistically speaking the model has learned that if the word shoulder is mentioned it's more likely that it'll be an orthopedic shoulder classification than a bullet wound classification and playing around with that in terms of weighting the data set was causing a lot of issues with over-correction and it really didn't seem to be the solution. So you know we were looking at this and trying to figure out how we could improve this under-performing model and I think a lot of times in our field the instinct is to kind of plop the big guns you know. What deep learning can I throw at this? What super fancy model can solve my problems with magic you know but the main advantage that deep learning brings in NLP is that it allows words to be understood in their greater context right if you have like a paragraph of text or even a sentence but in this case we are looking at very short strings of text maybe three or four words excluding stop words and even you know people will write just a list of words or kind of shorthand so there wasn't a lot to be gained from contextualizing the words that we were looking at so deep learning really wasn't the solution. And I mentioned earlier you know we had we basically were swinging the model from side to side like we would get some success in some parts with one model some success in another in another in other parts of the classifications that we needed with another model. So we really couldn't you know neither of the things that we were trying were really solving our problem and so what we ended up doing was using both and so we had the best of both worlds. So we moved to an ensemble model which was hybrid use both rules based classification and machine learning classification. So I'll take you through it we had the input text we ran it through the spell check just like before and then we had we we ran the text through a set of rules and these rules encompassed covered a lot of the cases where for example with trauma we needed the model to focus on particular terms weight particular terms more than others and just waiting the simple training set was not was causing errors in the opposite direction it was over correcting. So for example for the bullet wound we had a dictionary of terms that describe violent traumatic injuries it was a really pleasant dictionary to you compile and if any of those gruesome words is found in the text that we're trying to classify then the rule about wounds and trauma catches that text and sends it straight to classification as wound slash trauma it doesn't it just bypasses the machine learning models completely. So the last rule that we send it through if none of the other rules caught that text is to determine if there's any indication that the text we're looking at involves cancer in any way because the client had emphasized that even if multiple conditions were present if cancer was one of them you know the cancer will have a lot of like secondary conditions that come from it but they cancer is the root so they want a cancer to be prioritized and so we made a cancer model if that the text would be sent to if it had any indication that cancer was present and a non-cancer model for all the others and those are mutually exclusive universes of classification I guess. So these machine learning models work basically the same way as the original model does they're just streamlined in their classification tasks so each of these sub models has a smaller set we have three sub models and each sub model has a smaller set of classification tasks which allows them to really focus on I know I'm speaking about the models like they're people but you know we kind of feel like they are so it allows them to focus on their more narrow classification task and the results are less diluted by the sheer number of categories that they're trying to sort things into. So back to these examples if we see it in practice for the first two it didn't really change much we had diabetes and carpal tunnel they aren't picked up by any of the rules and neither of them have anything to do with cancer so they're just classified by the non-cancer model. Pancreatic cancer of course cancer is an indication that cancer is involved so it gets sent to the oncology model which now has a much easier task of just it only has oncology classifications to output so I think there were 13 different oncology based classifications and they you know once we know that it's cancer it's really easy for the model to understand that the pancreatic means digestive system and so it doesn't it doesn't dilute the probability of that specific classification by having the other digestive system category there watering down that probability so on the other side of the same coin we have pancreatitis is in the non-cancer model and that non-cancer model can classify it as digestive system in a really confident way you know with a really high confidence without having that confidence watered down by the presence of another digestive system classification and finally our favorite bullet wound classification bullet wound is obviously in that gruesome dictionary of wounds and trauma terms and so that was caught by the wolf-based classification and sent directly to that classification without needing to involve the machine learning models so this solution resulted in a much more stable model it was much more accurate much more consistently and it really fell in line with exactly what the client wanted you know and they wanted certain things to be weighted we were able to accommodate that without sacrificing sort of accuracy in other places so it also allowed us to overcome the gaps that we were facing in the training data instead of having to follow up with the client that we really need this data we can't we can't finish the model until we have it we were able to say we were able to just devise a set of rules by having a simple conversation with a subject matter expert so that was a lot less burden on the client it worked for us everybody went home happy and then this solution also allowed us to have that clear distinction between these kind of tricky cases to know exactly where we needed to weight the which those particular terms that were more important than the others and finally it doesn't it wasn't strictly part of the ensemble model but we did while we were adding rules into the mix we did add some new tricks like we added a rules-based flag to identify particularly severe disease for prioritization in the system so basically the moral of the story is you can have it all you can't if two models are working you know one on one half of what you need and the other on the other half of what you need um hybrid models and ensemble models are going to be a really great way to combine them and kind of you know get the best of both worlds for optimal uh optimal results and that is the end of my presentation thank you guys so much you you"}, {"Year": 2021, "Speaker": "Cezary Podkul", "Title": "\u201cData or it didn\u2019t happen\": How to make data-driven news stories happen", "Abstract": "\u201cData or it didn\u2019t happen\u201d is a credo we all live by. It\u2019s especially important for data journalism, but sourcing data for an investigation is rarely easy. In this talk I will walk you through how a data-driven story comes together and share some ideas for how the public sector and journalists can work together more effectively.", "VideoURL": "https://www.youtube.com/watch?v=mEwFCQI7Of4", "id0": "2021_03", "transcript": "He just started learning Cantonese, which is probably because when you see where he is, he just, not just. It's been a while now, I've been following on Twitter, moved to Hong Kong. So he's learning Cantonese, I guess he could go around and communicate better. So please welcome coming live from Hong Kong, Caesary. Great, well, thanks so much for having me, and thanks for that warm introduction. I am indeed here in Hong Kong, where my wife and I moved out here last year. I'm a data investigative journalist. It's what I've been doing now most of my career. And last year we moved out here, and I started teaching data and financial journalism at Hong Kong University, while pursuing projects for ProPublica. And it's been really fun. You'll be glad to know that your copy, your autographed copy of R for everyone, made the cut for what we put on the container shift to come out here to Hong Kong, because I now teach R for my journalism students at Hong Kong University. So it's been a real treat. And actually, you mentioned Cantonese. Cantonese makes it a little bit easier to explain some of the core concepts of R, like R being case sensitive. From learning Cantonese, there's a big difference in tone. So we have to say, Cinque, yap versus Cinque, yap. Are you saying Monday? Are you saying Sunday? There's a big difference. One tone can make a difference. And of course, it's the same with R, whether you capitalize or not. So I use lots of teaching analogies when I teach R, and I really enjoy it because it's great to support the next generation of journalists to help them learn, to do the kinds of things that I do. And then I hope to share with you in the process of my talk today. So with that, I'll get started. I titled my talk, Data or It Didn't Happen. And I did that very purposefully because it's one of the main creatos that I use when I work with Data for News Stories. So the reason I say Data or It Didn't Happen is because without a data story, we often really have nothing to go on to our findings. So whether I've been working as a journalist for doing stories for USA Today or for ProPublica or the Wall Street Journal or Reuters, really throughout my career, everything that I've done, all the big investigative projects that I've pursued have really had what I call an empirical spine at their core. And what I mean by that, it's really some central finding that is held up by data that shows you why the problem that I'm writing about and spending 2000 or sometimes 5,000 words explaining to you, why it's worth your time, why you should care and why it's a big issue. So the reason for that is because, a lot of time as journalists, the way news tips find their way to us is, someone comes to you and says, usually some version of, hey, something is happening, you should look into it. And it's really important, it's a big deal. And you should really look into this because it's worth a story, right? That's generally the approach that you see people take when they come to you with a news tip. And if it's interesting enough and you think it is worth your time, then obviously you're going to have to go about finding how it manifests itself in the real world, right? Where can you find that sign that what they're talking about actually is true, that it is happening, and it is as big of a deal as they say it is. So, in my teaching, I've come to use a phrase that I really like that I use for this, which is, I call it imprint of reality, because it's really easy to understand when you think about the fact that everything from an astronaut stepping on the moon, leaving a footprint to physicists trying to find a new subatomic particle, signs of a new subatomic particle, they're all looking for that imprint of reality, that evidence that the thing they're looking for actually exists. And I borrowed that approach from the scientific method in my work to basically see if I can find the footprint left behind by the phenomenon that I'm investigating. And I assume that for many of you who are in the data science field, you take that approach as well. It's why in your profession, we use the word data science, right? The word science there is very intentional, right? And same with data journalism, right? In data journalism, we're looking for that imprint of reality, the footprint left behind by that phenomenon that we're investigating. And so that's what I go looking for when I find a use tip that I'm interested in that I think is worth spending the time on. And really the first thing, really the first thing that I go looking for is data, because data is crucial. Without it, you have no story. So hence the title of this talk, data or it didn't happen. I do mean that quite literally. So with that overview, let me give you just a quick example of what this actually looks like in real life. So in 2016, I worked on an international investigation that ProPublica did in coordination with news outlets in Germany, Denmark and the Washington Post. And basically we had looked and we decided to look into this practice of dividend tax arbitrage, where basically it's a bit complicated. But there's a situation that we were made aware of, where US funds that share US funds that hold shares of German stocks would lend those shares out around the time of the dividends to funds that were either exempt from having taxes withheld, or which could claim those taxes back after the dividend was paid and basically save money on taxes. Shortly after the dividends were paid, the shares that had been lent out to those tax exempt holders would make their way back onto the books of entities that would have been subject to those withholdings had they owned the shares around the dividend time. And basically, once the deal was done, people would split this proceeds and instead of having your taxes withheld, you would get some portion of that back by splitting the proceeds with the tax exempt investor, the investors who could claim it back. So everyone who participates in the deal wins, the only losers were German taxpayers and taxpayers in more than 20 other countries. Where this was happening, but it was very pronounced in Germany have been happening there for a long time. So we decided to look at Germany as a test case. And obviously, it takes kind of a complicated chart that you see here to really kind of explain how this works. But when you break it down, really, if this is all true and it's happening, then it should leave that imprint in the data, right? Naturally, if this is happening, you should see around dividend time, the surge in the demand for shares to be lent out. So that's what I started looking for. I started looking for, does it leave that imprint in the data? And I was sparing the boring details of how hard it was to actually get this data, but it did finally manage to get stock loan data for Germany's largest publicly traded companies, known as the DAX30, which is the main index there. And so I got the data for all of them and I started looking to see, basically, if you were just to map out the number of shares on loan and compare it against the time when dividends are paid, do you see a pattern? And indeed, there was a pattern, it was so pronounced that we call this chart, tax avoidance has a heartbeat because if no matter which company you chose, you saw this very pronounced pattern where basically around the dividend time, everyone just wanted to lend their shares up, right? Usually, there's very little demands to borrow these shares, but all of a sudden, when these companies are about to pay dividends, and German companies typically pay one big dividend a year, then everyone wanted to lend their shares out. And so this was a clear sign that basically something was indeed going on here and this phenomenon that the source had described to us was indeed happening. And this was really a good example of how that dividend tax avoidance strategy was so pronounced that it was actually leaving an imprint in the data, in the stock loan data. And this was a really, really pronounced pattern that was just unmistakable to anyone who would look at this data. So we put that together and the story ended up having a tremendous impact. Germany actually passed the law just shortly after the story ran to ban this practice and make it hard to do these sorts of tax avoidance deals, which is really the main reason why we do these types of stories, right? We do them because we want them to have an impact. And so in this case, that happened and that kind of taught me to bring a data-driven mindset to all of the reporting that I do. And what I mean by that is that pretty much anything around us can be turned into data, right? Whether it's forms and websites, tweets, photos, you've seen all these things, you know, get put together and turn into structured data, emails, you know, meeting logs, I've gone through bond sale documents before, even something like applause, like when a politician gives a speech like the State of Union, how many times did the audience break into applause? If you count that, you'll have a better, more, you know, more detailed, rich story. So I'm of the mindset that pretty much anything can be turned into structured data. And that's what I try to do with my reporting. It's not always easy. And frankly, I'll tell you it's often very hard because a lot of times, you know, the data that you're trying to get out of documents or whatever other sources you're looking at, you know, isn't very easy to extract. I've worked on many stories where it leads to a situation like this, where I'm literally going through, you know, dozens and dozens of documents and individually typing in manually inputting data because, you know, either the data can't be scanned or digitized or can't be parsed and really manual entry ends up being sort of the only way to do it. So if you watch the movie, Spotlight, it's very close to reality. You know, if you haven't watched it yet, please do, because it really gives such a good portrait of what this work can sometimes be like. It can be very boring and kind of mind-numbing at time, spending so much time manually inputting this data, but it's very important because without it, you know, you'll have no story. So they did that very famously for the sexual abuse in the Catholic Church investigation that they did and really, you know, it's a powerful example of, you know, how important it can be to really do everything you can to try to get your hands on that data. Luckily, there are a few time savers, which I'll mention here, because they can be such an important resource when we get started on these investigations. The three that I want to mention today is since we're at the Our Government Conference government reported data, which can be a terrific resource, especially when you're looking for something that, you know, isn't easy to get, but there's a reporting requirement somewhere that someone must report the data to the government. So government reported data is the first thing I always try to look for. Secondly, I always try to look for organizations that have already collected it and put it together into a user-friendly format, because, you know, user-centered design is something we hear a lot about, but it isn't always implemented, especially, you know, in the public sector. So it's something else to look for is, has anyone else actually collected this data and put it together in a user-friendly way that will help you do your work faster? And then finally, you know, I would be remiss if I didn't mention our favorite programming language are, of course, which makes life a lot easier, especially when you're working with large data sets. So those are the sort of the three time savers, and I want to spend the rest of the presentation just to give you an example of how it worked on a recent project that I worked on earlier this year. Back in the spring, I was just finishing up another project, and I got talking with my editor, you know, the usual conversation you have about, hey, what should I work on next? And he suggested looking at unemployment insurance fraud during the pandemic. During the pandemic, obviously, unemployment insurance, which you can claim if you lose your job through no fault of your own, had become a crucial lifeline to millions of Americans. But months into the pandemic, people started noticing that the weekly new claims report that the government puts out, showing how many people were claiming unemployment insurance benefits. People were noticing that they just stopped making sense. And so I had seen this story that I have here on the slide from the economist, which they titled with one of their usual puns, wild claims, talking about how the claims data just stopped making sense. And if you were to look at what it was implying, it was basically so many unemployment insurance claims had been filed during the pandemic that they basically implied that 50% of the American workforce was out of a job. And if that's true, then obviously we were facing, you know, really, you know, one much more catastrophic economic downturn than the one we were in. And of course, COVID was bad, but the economists asked the smart question, you know, was it that bad? Really? These numbers make sense? And I circled in here the one thing that really caught my attention in this article, which was they mentioned in there that another source of the distortion in the data could be widespread fraud. So these are questions, things that people were beginning to think about. And that got me thinking, you know, is there a room for a story to look at, you know, whether fraud was a factor? And if so, how widespread was it? And to do that, of course, you would obviously, you know, the first step, at least in my reporting process is to find that imprint, right? To go out there and look for that imprint that this phenomenon, if it exists, the imprint that I'll be leaving in the data. So the first thing I did is actually started doing a lot of contextual research just to see what was being said about fraud, where people noticing it. And indeed, there were lots of reports already from the Office of Inspector General, the Department of Labor, in which they were pointing out things like this. You know, just one of these reports in this excerpt, they mentioned that they found from just one small sample of the data they analyzed, they found an instance where someone used one Social Security number to file a claim in 40 states and received $222,000 on the same Social Security number from 29 states, which obviously shouldn't happen because you're not allowed to file claims in more than one state, yet people were clearly doing this. And so that was one indication that obviously fraud was a factor, and that leaves open the question, how big of a factor was it, and how many similar instances were there, where other instances of fraud that you'd be able to document. So one way you might think about doing this analysis, if you get assigned the story as I did, right, said, I'm interested, count me, it's okay, let's get started. And one way you might think about doing it is kind of taking a bottoms-up approach, right, and filing Freedom of Information Act requests to states to get their data and do really what you would consider like a bottoms-up analysis, of looking at the individual claims, seeing who's filing them, what are the Social Security numbers being claimed, right? But does it show up in other states, is the same email or variance of the same email, with dots in between, you know, if it's a Gmail account, is that showing up in different states? And you could look for those patterns, but to do that, you need the unemployment insurance, you know, claim-level data. And that is actually not going to be possible to get via the Freedom of Information Act, because in this case, you know, wouldn't be able to do much for you. Unemployment insurance claims are exempt from disclosure under public records law. Here's an example of just, you know, one state, Pennsylvania, that makes that abundantly clear on its website and the carbon of labor and industry, before you even file a Freedom of Right to No Act request in their state. They tell you, you know, don't try trying to get unemployment insurance claim-level data, because it's just not, you're not going to get it. It's exempt from disclosure. And even the Department of Labor's Office of Inspector General, they had trouble getting this data. To get the actual claims from individual states, they had to file subpoenas with the states to get this data. So it was really hard to get, and it was basically going to be an impossible avenue to use for this investigation. So you have to come up with something else. And something else for me to turn out to be after doing some more reporting, I realized that I could just look at aggregate state reported claims data instead, and try to use that to work out a metric that would indicate whether there was something funny or unusual going on in the numbers. And the metric that I worked out that would seem to make the most sense is to look at, well, is the number of claims that's being filed? Because it's so large that it actually exceeds the workforce, the broadest number of people who would possibly even be eligible to file these claims. And if that's happening, then it's clearly a sign that something's going wrong, right? So I decided to look at that just out of sheer curiosity. And luckily for me, in this instance, the Department of Labor has copious, just lots and lots of reporting requirements for states. They file reports weekly, monthly, quarterly, by monthly, and annually, giving all kinds of data to the federal government about the state of their unemployment insurance programs, how many claims are being filed, initial claims, continuing claims, first payments, just all kinds of data, which they post here and they update very frequently. And you can get those reports along with the reporting instructions, examples of what the actual forms look like, right? And you can get into that and really start doing whatever analysis you want to do. And even more, this is helpful, but it's not really user-centric design, right? It's good that it's there, but it's not necessarily the easiest to work with as a starting point, but at least it's there. So I looked around and turned out that the Century Foundation had actually created an entire unemployment insurance data dashboard where they downloaded this data and they made it available on GitHub. They created an unemployment insurance data portal using Shiny and R, which is great, and they made it available for anyone to download and start using and do their own analyses. So I thought this is great. This is a terrific starting point. So I used that to then create a script that would help me basically make sense of this data. And so what I ended up doing was writing this script, which I just ran today. It still works. So, yay, if we're reproducible workflows, it still works. And I commented it very copiously so I can still understand, you know, six months on what's going on, which is always good. But basically what it did was it downloaded the data and put it in a created all sorts of summary reports that would help me basically understand, you know, state by state, how many initial claims are being filed. And then I compared that against some relevant metrics of the labor market. And then the Department of Labor has a report that they started putting together a while back that estimates the number of layoffs month by month. So you can actually see an estimate for how many people are laid off in each state each month. So use that as a denominator to look at claims over a period of March to December of 2020, and to total up to claims versus the total layoffs over that period. Secondly, you know, compare that the number of claims over that period against the pre pandemic labor force to see how many people were actually in the labor force before the pandemic hit. And then comparing that and calculating these in a bunch of other ratios to basically do, you know, what I call contextual analysis to look at the context surrounding this data and to basically, you know, scratch your head and say, does it make sense, right. And so I put that together and mapped it out using your GG plot and the, and the US maps package which I love. And it would allow me to really get a good sense of what was going on and true to what others have been saying, you know, the data really wasn't making much sense. If you look at it, this is one of the one of the maps that put together. So this just mapped out initial jobless claims from March to December 2020 and just looked at it as a share of each state's pre COVID labor force and there were several states. Where the number of claims filed just in that short period actually eclipse the entire labor force of the state, right, which doesn't make sense, right. You shouldn't have more people filing for an employment insurance than you have people who are actually eligible who are workers workers in the state. And so in Arizona was really bad and in Arizona and Rhode Island, the states that were putting out press releases and talking about talking about fraud were actually the ones who were showing up in this darker colors in here. And then when I calculated as a percentage of layoffs, I got an even more alarming, you know, Matt, because, you know, in state after state, you saw that the claims that were being reported were actually, you know, many cases, many multiples of the layoffs that happened during that time period. So that led to this data graph that we put together for the story in which, you know, we kind of summarized the findings of the analysis. It's something you always do in a data driven story. And we said that nobody has yet come to close to putting a definitive number on it, but the data hints that there has been fraud in the system and it's, and it's potentially has a massive scope. The number of initial claims, you know, nationwide equated to about 68% of the country's pre pandemic labor force. And in five states, Arizona, Georgia, Hawaii, and about an Rhode Island that actually outnumbered the entire pool of civilian workers. And if you compare that against the number of workers who had lost their jobs, you know, just numbers really weren't making sense. So it was clear. There was some other exponential explanations, but it was definitely the case that fraud, you know, was it was a factor here and it could explain some of not a significant portion of these elevated claims figures that we were seeing. So we had, we married that with some additional, you know, on the ground reporting, just around the ground doesn't have contact. Not quite on the ground, but doing some additional research to look at what was going on. We found bots filing fake claims, you know, one company that was working with states to try to prevent these fake claims. They had detected 23,000 claims coming from just one MacBook Air computer. There was, you know, there were telegram chat rooms, which were aiding fraud in which people were just openly talking about methods for filing fraud. Fake claims poured in from around the world and they often got paid. You know, we're talking about, you know, situations like one state that had IP addresses from 70 different countries where people were trying to get violent file claims. And states were understaffed and under resource to deal with the fraud. You know, in Kansas, they had 33 workers for their call center to take calls in April of 2020. At the time, they got 12.5 million calls. So you can do the math for how many people actually got through to get help with their claim when they ran into trouble or, you know, couldn't have their claim stopped because someone was filing a fake claim in their name. So that's how the story came together. It was a really important reporting project because it shed light on how big of a problem this really was. I encourage you to check it out. It's still, it's a stop on ProPublica. It's a really, you know, exhaustive analysis of everything that's sort of gone wrong in this area and why we saw, you know, so much fraud during the pandemic. And I want to close just by very briefly. Giving some lessons since we're at an R government conference for how I think the public sector and journalists can work together when we pursue these sort of large data driven investigative projects. But the first thing is to follow reporting obligations, right, because two plus two should equal four and whether it's Kansas, New York, Illinois, or any other state. And if it equals five in one of those states, then obviously we have a problem. One of the issues that happened was some states were, you know, reporting the data in different ways or they were reporting with a huge lag or they weren't reporting data at all. And so that led to obviously issues that made it more difficult, not just for journalists, but also for academics and others to analyze this data. So it's important to follow reporting obligations when you have them and to report the data in a consistent fashion, the way the federal government requires it to do it as quickly and as consistently as you can, because it makes your life easier and everyone's life easier. Secondly, make the data easy to access. It's great that the Department of Labor had it all sitting out there. But again, we hear a lot about user center design these days. And I think thinking about that and taking it into account when you're designing those data portals is really important because the easier you can make the data more accessible, the API can make it much easier to work with. And then finally, you know, please answer questions, you know, most importantly, you know, when journalists like me when they come to you. A lot of times we're not necessarily looking for a quote, we're just looking for insights to help us understand and make sense of things, you know, as I mentioned, in the case of unemployment insurance data, this is what the form looks like, right. So, you know, I'm just looking for regular claims just for, for this for the, for one of the claim programs you can see that it's not, you know, it's very clear what's going where but it's not necessarily easy to figure out how to make sense of it. So I had to find people who could help me do that I'm really, really grateful for them because this is an example of the answer I got from an answer that I got to a question I sent to someone about what, how to make sense of the data and how to aggregate and I'm really glad this is the only time in my career that someone sent me an actual table to answer a question. Because it was a bit complicated and then helping make sense of it. And so getting this sort of leveled this level of detail to a question is really helpful and just really impressed on me and I hope to impress on you that, you know, it's really important to answer questions if you're going to put data out there, whether it's through an API, a government data portal or something else. It's important to have a point person designated who will work with people and try to answer those questions. So that's let's all go out there and find those imprints in the data for whatever it is you're investigating or looking at whether you're a data scientist or a public sector in the public sector whatever it is you're doing I wish you all the best of good luck. Go find those imprints and I'd love to stay in touch. Here's my contact info. Don't be afraid to reach out and thank you so much for having me really appreciate the opportunity to share some of my knowledge with you all today. Thank you very much for that talk. It was a pleasure seeing you all these years. Hopefully one day I'll see you in person again. Absolutely same here."}, {"Year": 2021, "Speaker": "Coline Zeballos", "Title": "Ensuring the quality of your R packages for regulatory submissions", "Abstract": "Ensuring the reliability and quality of R packages used in regulatory interactions for drug approvals: a view on how Roche participates in enabling submissions in R.", "VideoURL": "https://www.youtube.com/watch?v=AyGmyZWh10A", "id0": "2021_04", "transcript": "Our next speaker is, well, has two atypical passions. They are classical singing and kite surfing. Everyone, please welcome Colleen. So my name is Colleen Sebios. I am our strategy lead in Roche Pharmaceuticals since two years and we'll be talking about ensuring the quality of your R packages for regulatory submissions. A view on how Roche enables submissions in R. This title is a bit ambitious, I have to say before we start. We don't fully know yet how to do clinical submissions health authorities in R, but I'll share where we are in the industry, in the pharma industry and specifically at Roche. A couple of words before I start, because it's probably the first time you meet me. So my name is Colleen. Swiss and French and based in Switzerland. And I think I have a pretty international background. I've lived in several countries, as you can see here. And I'm also married to a Bolivian and US citizen men. So for all of the reasons you can either speak to me in French, English or Spanish, but don't try anything else. My educational background is in business and I went into the world of data science in Python a bit later after I started my career. And I like to say that I use my business and data science and sorry technical experience to lead experts in delivering value, business value. So I'm really in between the technical world and the business world. I think it goes without saying that the technical experts are needed in our industry, but we also need people who can translate our experts work for non-technical people and stakeholders who usually are also the decision makers. So that's what I do in my current position. Actually right now at R as R strategy lead in the pharmaceutical division of Roche, I drive the IT strategy to support the use of open source programming languages like R and Python. And one of my major contributions recently is around enabling R package validation in our company. Outside of work, as Jared said, I'm into a extreme sports, it's considered extreme sports called kite surfing. So it's a really cool sport that makes you travel around the world to find the perfect wind and wave conditions. It's highly recommended. So that's it about myself. So don't hesitate to connect with me. I'll have my LinkedIn and Roche email at the end of the presentation. So we'll be covering today the following topics. I'll give a little pharmaceutical context of, because we are talking about building package, our packages of quality, but I come from a context. So keep in mind that this is a, from coming from one lens. We're looking at this from one lens. We'll talk about this transition from CES to R and what health authorities expect. Then I'll give you some tips about how to ensure that you build good quality packages. And finally, we will, I'll share some, most of the initiatives actually that Roche is a part of and also not just Roche with the pharma industry, what we do basically to enable submissions in R, which is what we are all interested in today. So the pharma context, as I said, I work at Roche Pharmaceutical, but within pharma, I work in IT to support the clinical product development. So we see here the typical, let's say, drug development process that a pharma company needs to follow to put its drug on the markets. In at Roche pharma, we are really interested that collaboration and open sourcing of analytical tools is the right way forward. And there's a trend right now to use more R in all the big pharma companies. This is really a unique opportunity to change the way we work together and the way also we bring new drugs to the markets. I will share later with you the different initiatives, as I said before, that are ongoing to enable submissions in R. But we are really at the beginning of the journey. And that's what I was mentioning to you at the beginning in my first slide, is that we are, we just started this journey. Actually in 2019, so two years ago, Roche T-centric is one of our drugs, one of the working groups of a drug, they completed their first R based FDA submissions. And to our knowledge at the time, it was the first submission within Roche, for which the majority of the analyses were done in R. Also, it was the first time an R package was submitted to the FDA. So you see we come a long way. And a couple of weeks ago, actually the news just went out yesterday, the R consortium, which I will talk about later, and Roche is an active member of the R consortium, they successfully submitted an R based test submission package through the FDA submission portal. And the package was well received by the staff, the reviewers, and they were able to reproduce the numerical results that we obtained. So it's really a great step forward, but just wanted to remind ourselves that we are moving, we are at the beginning of our journey, basically. I wanted to say we're moving slowly, but we are moving at the pace we are moving at, which is already really good. If we take a bit of distance from what we're trying to do in this industry, the general public and pharmaceutical regulatory agencies expect medications and medical services to be the result of a good and sound science and accurate analyses, right? So bugs and defects in the program used to analyze this clinical data can result in correct analyses and therefore can have a damaging consequence to public health and trust also, right? So it's important for us to follow quality control processes to identify errors in our analysis programs. Because we need to remember that everything we do is for the patient at the end, right? There would be no concept of validation if we didn't have to deliver drugs to patients. So we have a responsibility and, yeah, that's for us. Then from SAS to R. So we come really from a SAS world. As I said, we were just starting to submit work with R, but we come from everybody using SAS to now at ROSH, as you can see here, we have 45% of our users who use R for more than half of their programming work. So there is, and this was really lower, of course, a year or two ago. So we really see in our company and in the industry a move from SAS to R. And I want to remind ourselves that the use of SAS is really not mandatory. So when people ask me, can I submit software programs other than SAS? Yes, so actually the use of SAS is not mandatory in the analysis of clinical study data. So instead other software is like R could be used. The thing with SAS is that we do not need to validate the software, we only need to validate the outputs, and then we archive the programs for future reference. For R and Python, we need to also validate the program functionality and the package system. And then we archive the programs and we need to keep the package snapshots so that in the future, if the program is run on the same data sets, health authorities and reviewers can obtain the same results. So we have a new, let's say set up a complexity with open source programming languages like R compared to SAS, which was a licensed and not open source programming language. Reviewers normally in health authorities, they can generally use the software that suits their needs of their choice. So we really need to give an understanding and give them an understanding of how our methods work and our analysis so that they can reproduce our work. But this also means that the health authorities need to train themselves to gain their capabilities for receiving, running the programs and the software that we provide to them. It's not just us who need, in the industry who need to adapt, it's also the health authorities who need to adapt their internal capabilities and knowledge. And also another question I receive is, do I need to follow a particular style or a convention when I write software programs? Basically, a general rule is that your software program needs to be, let's say, clear and of good quality, regardless of whether you're going to submit it to health authorities, this is something that we see as big value that we develop as our developers, that we develop code of good quality. And there are a lot of best practice guidelines and guides available on the internet and in several, let's say groups know who provide their guidance. Yeah, that's really where we come from, right, with SAS. But I just wanted to break down a myth that SAS is the only programming language that DFDA allows. This is not true. So how do we ensure quality of our packages? So I'm going to give you a couple of tips in general. So these are tips to ensure the reliability and the quality of our packages used in regulatory interactions. Not just for drug approval, not just for the pharma industry, but this is general good, good practice. So obviously, some of these might seem obvious for you, but they're not really, I think. So unit tests, so this is a primary concern for the reliability of the package. It's really the accuracy of the code. It needs to be, let's say, covered. So unit tests are basically granular pieces of logic present in the package that is tested to ensure that the outputs match reference values. So it's really important to specify the unit test, to document to also document statistical methods implemented and so on. Consider assessing also inputs as part of a function. This means to consider checks that can be, for example, performed on inputs and where logical outputs and intermediate values to minimize the risk of the package function being used with inputs that were unexpected by the developers. Peer review seems like a very basic one, but this is really, I advise you to look out for people who can extend and build on your code. Because when a package has significant use by other people and other also packages in a company's workflow, CI-CD testing, so continuous integration, continuous development, deployment testing should be extended to also rerun the unit tests against packages that depend on the developer's package. So really use user network to build on the go-to package of good quality. Automation is obviously a very key point. I just mentioned it with the CI-CD code maturity as well. It's also, it's very useful to clearly state the current maturity of the code, particularly to let people know if the code is still in the pre-release stage, for example. And there's one method that you can use called the lifecycle, you can use lifecycle badges. So, you know, to use this lifecycle badge and write in print and send it to your unit. That's a small tip. Also, refer to the intended use of the package. This is very important. It's really a best practice to ensure users are aware of the context the code was written to be used in. So you can use vignettes, for example, that's a method to explain the intended use of the package. And when appropriate, the vignettes can be annotated with notes on the use and the reference to any relevant literature that you leverage and the design of your package. Another one is, so in the R language, what is, as you may know me, I'm sure we have a lot of good programmers here on the call. What is called R-Core is composed of base R and recommended packages. And the base R- package or an in between each version. So a set of validation tests are maintained by the R-Core team to enable the testing of code against known data and known results. So any error noted during the test, they are resolved prior to the release. So this is really handled by the R-Core team for R-base package. But more work is expected on your side when you're using between quotes, maybe riskier packages and less widely used package. So this is to keep in mind. Double programming as well is one, I don't know if you've heard, we use this term a lot in the pharma industry. Basically, this is the idea that, you know, we do a double program analysis when we're planning a regulatory submission in multiple languages. So the quality control step of analysis programming is a good first use of open source technologies. And by using multiple languages, basically any errors or difference in the default option setting or algorithm that you use in one programming language can be taught and can be identified by double programming by doing it by the other language, basically. So you compare the same code, but into the same website, yeah, the same code, intended code in different two different programming languages. So what we're doing in this space, this is important to say that to influence health authorities and for them to accept, to change or to receive something new, or, you know, if we have a, if for example, Rosh comes to the FDA, it says we would like to submit our outputs in this format. It's very unlikely that if Rosh comes alone to the FDA, that the FDA will say, okay, fine, we will adapt our processes and meet your need. No, usually it works more at a sponsor level and an industry level. And the next three slides will be about those initiatives at a sponsor level and also that Rosh is a part of, obviously. So the R Consortium, I imagine most of you have heard about it, the R Consortium members are probably most of the big pharmaceutical companies and biopharma companies. The idea is really to provide, let's say, support in developing, maintaining, distributing and using R software. And they are organized in working groups. I listed here a couple of working groups that are of interest in our topic, especially the first one, which is the R Submission Working Group. They focus on IT and platform challenges that must be addressed in order to make an all-R regulatory submission. They provide examples of submission material. They make it available to the public to identify potential gaps in R based admission. So they really work hands in hands with the FDA. That's really important to say because that's the only way we will move forward. It's really to work with the FDA. And recently, this is a key milestone. So actually, I already mentioned it before, but this is really, the goal is to allow easier R based clinical trial regulatory submissions. And I already mentioned it, which is massive, but I wanted to repeat it. Then we have another working group, which is the regulatory reporting table. Here they create standards to guide the development of R tools for creating tables that meet requirements of the FDA. So that's also a important one. Then we have R certification, who works on making sure we have material and curriculum for statistical programmers who have skills to move in set in R. So that's important. Then I'll go to the front reverse because time is quickly. This is in progress, but it's very promising. So I think it's worth mentioning. I cannot share yet a link, but the idea is to provide open source R packages from the whole end to end clinical reporting packages needed in the end to end workflow that we saw before. So really from the creation of TTM, which is a format that we need to create, then from ZTM to add on, which is another format. From then the creation of TLGs, the table listings and graph. And then for to submit, and this is for the E submission process. So here we have this really across pharma collaboration. We have Rosh, Merck, at least, and the other ones I cannot divulge, but this is really, really promising because we will have really a set of packages that we can use for your enter and analysis. Fuse also accumulos, I don't have time, but these are like key initiatives happening cross pharma. And here at Rosh, we have two pieces that you can use and reuse for your validation work that I shared here. Yeah, time is passing by. I have much more to say, but thank you to the Arc of Organizing Committee, and of course to all the industry collaborators. Thank you very much."}, {"Year": 2021, "Speaker": "Jared Lander", "Title": "Using {targets}, {arrow}, Docker, Postgres and the Command Line for Medium Data", "Abstract": "Most companies don't have big data, but rather medium data, that awkward in between where the data are too big to fit in memory but not big enough for Google-scale systems. Fortunately R has many options for working with data of this size. We will look at using the command line, {data.table} and {dplyr} to clean the data and load it into a Postgres database inside a Docker container. Then we will use {targets} to orchestrate the whole process.", "VideoURL": "https://www.youtube.com/watch?v=8X_Zjn8JGV0", "id0": "2021_05", "transcript": "It gives me great pleasure to announce the next speaker for the conference. Many of you know him by many names, which I probably don't know. But one thing I will do want to share with you is A, he's a good friend. And B, you know, it's he's been putting on a great conference over the last couple of years. Everyone, please welcome Jared up on stage. When you're working on a project, there are many moving parts. And each part of this data pipeline has its own considerations. And those considerations become more difficult when you're working with medium data. Medium data is harder to handle than people think it is. It's big enough that it doesn't fit into memory, but small enough that you shouldn't be using Google scale tools. So you need to figure out how you're going to work with it. It takes a little bit of thought. So we're going to discuss some of those tools. And as our motivating example for today, we are going to be talking about taxis. Yes, that's cash cab. And yes, I was on it. Very, very early as one of the first contestants. The New York City taxi data is publicly available on AWS for anyone to get and examine. In fact, it's been used a lot for medium data case studies. Our goal for today has a few steps in it. First, we are going to pull the data from S3. But we only want to pull the data if it has changed because we don't want to pay for that egress fee. Then we are going to keep data only from certain geographic areas of our data set. And then we want to only keep rides, which are represented by a row, where all the rides for that given day were over some threshold, say $6 million. So if all the rides across all taxis for a given day was over $6 million, keep the rides for those days and not other days. Then we want to load our cleaned up, ready to go data set into a database. So first up, we have to go get the data. And we can do that using the AWS.S3 package using the save object function. We are going to get this object, which is in our case a file from that bucket, and save it to this file locally on disk. And this brings it down. We can read in the data set just a few columns using F-read, take a sample using slice sample, then convert it into an SF object so that we can plot it with leaflet. And we very quickly see there are a bunch of taxi pickups in the middle of the ocean near Africa. We probably want to remove those from our data set. Notice for a moment, though, instead of using the add points function from leaflet, I'm using add gl points from leaf gl, because it uses Web gl so it can plot much quicker and it can plot more points than the standard function can. It's a nice add-on that makes life really, really good. So now we have our data, we want to geo-filter it. And there are a few strategies we can take. We can load it up into postGIS right from the get-go into a spatial query, but that's our end goal, and the data is not ready for that yet. So we could read all the data into memory, convert it into an SF object, and filter out the ropes. But we can't be so certain that the data will fit into memory. So we can split this one big file into multiple files, then read and filter each file individually. So that's the strategy I want to go with. I want to split the files. I wrote a function that first deletes any files in the destination folder, saves the header row to be used later, and then uses process X, which we are going to use a lot, to call the bash command split. We call split, we give it a bunch of extra information how we want the data split up, and then we keep track of all these files we generated. When we run this command and we take a look at the output directory, we have 319 files plus a header file. So our data is all split up and we can operate over that. In order to do the geo-filtering, we first need a map of our desired area. Luckily, that is freely available online, and we can read it straight from the URL, which is hosted by ArcGIS, into read SF. And we are going to use this map to keep just some records. We check out the map to see we have each borough of New York City, and we are just going to focus on Manhattan just to make the problem a little bit quicker for today. Also, please note that this map used exactly four colors, and according to the four color theorem, you can plot any map with just four colors. And I think we just proved it with this one simple example. So now it's time to do our spatial filter. And we're going to do this for each of our hundreds of files. So I wrote a function to do that. First, I read in the header because not all of our split files have headers. Then we use FRead for its speed to read in this one file. Convert it to a table because it's easier for me to work with. Convert that into an SF object and set the lat long. Then we do use ST filter. ST filter is like a spatial join, but it's only going to keep the columns from the first table, and it's going to reduce the rows based on which points in the first table fall into the polygons of the second table. After we have filtered down our data, notice this is all just one long pipeline, we then convert our geometry object away from well-known text, the way it's stored, into an EWKB that is a special type of well-known binary, because we can save that in the CSV and load that into post.js very nicely. And we said hex equals true, so it comes across as text instead of raw. And then we use FWrite to write this out to a file. And we do that for each file individually, and we mix very nicely data table, Dplyr, and SF. We check, before we can apply the function, actually, we have to create a mapping of all of our input files, this raw files, and the corresponding output file. That way we can iterate over this data frame and do this one file at a time. We then delete any files that already exist in our output directory, and use walk to iterate over this data frame of file names, calling remove points on each file individually. Once we accomplish this, we read in one dataset to see what it looks like, we have all the columns we expect, and our geometry column, the last one, is stored as a character because it's a well-known binary, not a raw standard geometry column. With our data filter geographically, we can now filter based on an aggregation. We want to only keep the rides from days where the daily total was above a threshold. So you can no longer do this one file at a time because you don't know which rows are split into a bunch file. I need to be able to look at the entire dataset, and a great tool for that is Arrow. This will allow me to operate on the entire dataset without pulling it all into memory, while still using a dplyr syntax. This is going to be very, very helpful for us. Now, installing Arrow, even though it's on CRAN, can be a little difficult, especially on Linux, especially in an air-gapped environment. So luckily, there is this GitHub content, which has a function called install Arrow, which will install the Arrow system libraries for you to make it a little bit easier to have the package going on. To do this step, I wrote another function so it's easy to apply because there are multiple steps in this process. First, we use Open Data Set to open the entire data. We are not reading it all into memory, but we are making it available to us. That's the nice thing. It is not all in memory. It is just being made available to us. And that's very important. We then compute a group-by summarization. We group-by-date. We summarize by summing up the total fare. And then we filter out to make sure the fares were over a certain threshold. Now, this has not been computed. We've just constructed the query. We will then use this query in a semi-join, which is a filtering join, that will only keep records of our main dataset, which have a date corresponding to this filtered outdates. And then we write this result out to a file. Only parts of the data ever make it into memory, which is really, really awesome. So then we call this function and we check it out and it produced one file. We could have written multiple files, but I want just one for our next step, which is loading to the database. This is our ultimate goal of getting it in here. And for my purposes, one file will be easier. Dilling of databases can be a bit tricky sometimes. So instead of installing the database directly, I am going to use Docker. I'm going to use this whole database this way. I can't screw up other databases. Rather than build my own Docker container, I'm going to use an existing one, Cartoza, PostGIS. And I will use Docker compose for spinning it up. I'm using a volume to store the database data that way survives reboots. This is also a nice feature that any SQL files I put into a certain volume will be ran the first time the container is spun up. I also set certain environment variables like setting a really secure password such as Docker. And I make sure there are extensions such as PostGIS and HStore and some other ones. And I declare the port to be 5436 because I have multiple of these containers going on for all my projects and it can't all use the same port. That SQL file that creates a table has a regular create statement with a bunch of columns, each of different types, including our geometry column with constraints to make sure it is a point, not a polygon, not a line string, but a point. I don't set any indexes because I'll do that later after I load the data. To spin up the container, I use process X from inside of R and I call the bash command Docker compose. I tell it which Docker compose file to use because I have multiples. I tell it up and dash G means detach it this way it doesn't call it my terminal. With our database running, we can finally copy from the CSV into that table. We are not going to use R to do this because that whole CSV might not fit into memory. Instead, we will use P SQL, a command line tool, which reads straight from a CSV into the database. And I need to provide it a set of SQL instructions. This file starts with begin and ends with commit. And that means that all these steps are part of a transaction. If any one step fails, it will undo all the steps. And that's really important. It's a really great way of making sure you don't screw up massively. Always use a transaction. First, we drop the indexes because it'll be quicker to delete and both load into the database without indexes. We delete all the rows on the current table that we care about because we're just doing a delete and replace. If I click on the rightesh, that flower chicken is gonna be a certain thing. This will change the existing size and then exact match to each other. Now once you haven't already had success sharp tempo, type inatial. Big word and change your velocity what all for each other layer of cases. of writing to the database slash copy is really incredibly fast. When I call psql, I will not be able to pass in a password. So I need to provide that some other way. And I do that by storing all the database information, including our super secure password in a config file called .pg underscore service.com. We put all of our connection information inside of this file. And then when we call psql, it gets the password automatically. Super easy, really works really well. We then use process X to call psql. And when we do that, we say service equals medium data. So it knows to use this database information that we got from our service file. And we provide the file of instructions using dash F. So all those steps were really great. They, a lot of them depend on each other. There could be a lot of complication. You don't want to make sure you don't duplicate. So it's important to use some sort of orchestration tool. You want to run those pieces smartly. You want to run them in the right order. You don't want to repeat steps. You don't want to have to worry about memory usage. Thankfully, we have targets. This puts everything together. It only runs steps that need to be reran. It doesn't waste any computation. It builds the DAG for us that directed acyclic graph. It moves objects in and out of memory. It's very reproducible. And it's really changed my entire workflow. When you're using targets, your directory structure typically looks like this. You keep all your custom code, particularly your functions, inside the R folder, inside the file called functions.r, the naming is optional. Then you put all of your instructions inside of underscore targets.r. Everything goes inside of there. We'll look at that extensively. You set config and underscore targets.yaml. I put my database instructions inside copy2db.txt. You may or may not have that. I put my database files inside the database folder. And I have a docker compose file for spinning in my container. Today, we are going to focus on targets.r. Inside there, you call library targets and optionally target types, which gives you more advanced versions of targets. You source any code you need by sourcing, in this case, R functions, then all of your commands, all of your steps, go inside of a list, a plain old regular R list. Nice, super simple. So let's go back to the beginning where we started, and let's get the data. We use tarforce to first check has the file changed, because we don't want to do this if the file hasn't changed. Tarforce calls getbucket.df, which tells us if the file has changed. And if it has changed, our next step calls save object to download it. This target is a tar file, because the outcome of this function is the name of a file on disk. And later steps will check if that file has changed, and tar file keeps track of that for us. We can visualize our deck by calling tarvisnetwork. Even though we only had two steps, it shows us three, because tarforce is actually two and one. Typically, this visualization only will show the difference between outdated targets and up-to-date targets. But since I use tarforce, that runs every time. So even though subsequent targets will be up-to-date, they will still be shown as outdated. And there's nothing we can do about that right now. Next, we need to split the data. Luckily, we wrote a function for that, and we can call that function. First, we split the data. And this time, it's a tar target. It's a regular target, because then our object is returned to data frame of file names. And that feeds into our next target, which creates a mapping of input files and output files. When we visualize our deck now, we see we have these extra steps. And the raw file step will only be called if the raw data step is changed. And that is only changed if we downloaded the data, which only happens if the file on S3 changes. To do our geo-filtering, we need our map of our desired areas. We need to go download it. Again, we only want to download it if it has changed. So tar URL, given a URL on the internet, will keep track of that file on the internet. And we'll let you know if that file has changed remotely. If it hasn't changed, you're good. If it has changed, the next step, tar target, which reads in, this URL and to SF, will be fired. So it's a great way of keeping track of files that are remote to you. So you don't have to worry about if they have changed. This part of the deck has just two steps, checking if the file has changed on the internet, and reading it into an SF object. You can see that these two steps are completely disconnected from the rest of the deck. They're independent of each other, because they don't have anything to do with each other. For now, because next we are going to do our spatial filtering, and that's going to combine our data and that map file. To do this, first we make a list of any existing filtered files in our output directory, and we delete them all. We don't want them around, because we don't want overlap. We then take our data frame of input and output files, and we map over that. See the pattern equals map? That allows us to iterate over one row at a time of this data frame to call our function remove points on each one individually. You can see here that this function depends upon our NYC map, our file name mapping, and in curly braces, we included remove filtered files, because even though our function doesn't make use of it, I want this step to only occur after that removing data step has been successful. Looking at the DAG now, we see that removing old files, making the input output list, and the NYC map, all feed into filtered files. Now filtered files is a square, not a circle, because rather than one step, it is a pattern of steps, because it's going to be called for each of those rows in the data frame. So it's iterating over that pattern. With our data all processed, we can now do our aggregation based filtering. This step uses tar change, because our function which calls arrow doesn't actually refer to any of the previous steps. So I'm saying change equals the filtered file step. So if those files change, go ahead and run this step. The outcome of this step will be a file, so I say format equals file. And I tell it to load up the package arrow, because this way I don't need to specify the arrow namespace in that function. Targets and arrow don't really play nicely together, so you need to create and destroy the arrow object within a single target. And this gets even harder if you want to share schemas. Our tag now shows the process files, which will only run if the filtered files have changed. Next up, we need to get the Docker container running. So I create a step called DockerUp, and I actually have a list. Our step is a list and has two parts to it, because your steps can be any valid our object. The first step calls Docker Compose, like we saw earlier, this spins up the Docker container. But since calling Docker Compose on container that's already running gives a different message than on a new container, I also check when was this container created, and I will use that later to let us know if we should run other steps. This DockerUp step is this one step by itself disconnected from the rest of the DAG. With our database running, we can copy from the CSV to the table. So we want to call our process X to call P SQL. We need to remember to feed it our database information, which is medium data, this way it gets the password. And this step runs if either the Docker up time changed or the process file itself changes. So that's why we use Tarte Change and specify the change. Our DAG is now complete. We have all of our steps we need to take all laid out for us right here, and everything will happen nicely and automatically. And if we say target's only equals false, it also shows us our custom functions to make sure that we know if our function has changed, it changes the graph. So now we can run the whole pipeline, or at least whatever needs to be updated by running one line of code. We run TarteMake, and everything just runs all nicely taken care of for us. If things don't need to be run, they don't need to be run. If they do, they do. But better yet, we can do this in parallel. That's really great. Let's take advantage of our split files. Remember, we had hundreds of them. And let's do each one individually in parallel. The first step to this is setting pattern equals map equals file name of mapping. This iterates over all those files, and since they are done independently, they can be done in parallel. Next up in our arrow step, we need to be careful here. Arrow runs implicitly in parallel already, and we want to keep it in parallel. But we don't want to race condition with our other parallel steps. We want both. So we set deployment equals main for the arrow step, and that will run by itself in parallel, and then everything else will run in parallel also. We then make one change to our targets file. We say future plan of future multi-session, and I set four workers. I could have done more workers, but I set four workers. And now anything that can be done in parallel will. That means if you have two steps that have nothing to do with each other, they could run in parallel. If you have a step like our geo-processing, which maps over a bunch of files, that will be done in parallel. If you have a step which uses a function like future map, that will be done in parallel. It's all nicely done for you. And the one more change we need to make is instead of calling tar make, we call tar make future. That's it. If that runs everything in parallel, even across multiple machines, if you want to do, you'd actually run tar make cluster MQ, but same idea. And this runs everything in parallel for you. You don't need to worry about it. You just go along from there and go on your smooth road. So we put together a lot of different tools. Let's see what we did. We use AWS for cloud storage. We use the command line for splitting and pre-processing files. We use data table for reading and writing data, and then de-plyr and per for data manipulation. We use SF for geospatial filtering. We use arrow for operating on out-of-memory data, Postgres and PostGIS for holding the finished data, Docker for isolating the database, and targets for orchestrating everything. And this is great. This package put everything together, built the DAG. It keeps track of what needs to be computed. It's easier than other orchestration tools, and it is all R-based. And that one package pretty much sums up the whole project, literally, it gives us the whole project right here, and this one visualization tells us everything that's happening, and does it all for you, allowing you to combine all these tools to help you work on data that would otherwise give you a bit of trouble. Thank you very much."}, {"Year": 2021, "Speaker": "Asmae Toumi", "Title": "Using data and R to improve substance use disorder care: the startup experience", "Abstract": "The U.S. opioid epidemic, or opioid crisis, refers to the substantial medical, social, psychological and economic consequences due to the misuse and overdose deaths of a class of drugs called opioids. The number of drug overdose deaths increased by nearly 5% from 2018 to 2019 and has quadrupled since 1999, and over 70% of the 70,630 deaths in 2019 involved an opioid (CDC, 2021). PursueCare is a telehealth startup offering comprehensive care for opioid use disorder and other substance use disorders by combining telehealth technology, medication treatment and counseling. Asmae Toumi, the director of analytics and research at PursueCare, will talk about how data and R/RStudio\u2019s public and professional tools are being used to uncover trends, deliver care and improve outcomes.", "VideoURL": "https://www.youtube.com/watch?v=bqESMOn7EVY", "id0": "2021_06", "transcript": "Our next speaker, you may have seen her from such conferences as the New York Art Conference a few months ago. And very importantly, she is a Jeep Wrangler owner and a Goodwill and Thrift Store power shopper. Everyone, please welcome Asama. It's really my pleasure and my honor to talk about what we do at Pursuit Care. We are a startup company which specializes in treating substance use disorder. And specifically today I'm gonna talk about how we use data and R to improve that care. Okay. So I wanted to start with just impressing upon you all the enormous human toll that opioid overdoses have on this country. Every day 136 people die from what is essentially something that could be preventable. And so in understanding this epidemic or really this crisis, what we mean by it is the substantial medical, social, psychological and economic consequences due to the misuse and the overdose deaths of a class of drugs called opioids. From 1999 to 2019, nearly 500,000 people have died from an overdose involving any opioid, which includes both prescription and illicit opioids. And that number of drug overdose deaths has quadrupled since 1999. The reason why we focus or so many of our efforts are focused on opioids specifically is because over 70% of the deaths in 2019 have occurred because of prescription or illicit opioids. So since 2019 and since the pandemic started from the CDC's 12 month ending provisional numbers, we've seen a sharp increase in the number of deaths, unfortunately, predictably so. Opioid use and misuse can be thought of as a disease of despair and I think all the challenges many of us have encountered during this pandemic or even more amplified for the marginalized populations who suffer from opioid use and misuse. We've seen racial disparities get magnified across all economic stratus. And so we've seen the number of overdose deaths jump from 56,000 to 75,000 in just a 12 month period. So many efforts have been deployed to curb this crisis. Payers, government agencies, private companies such as ourselves. And what we do to do our part is we wanna be able to give patients the ability to access care in a convenient way and that fits around their life. And so many of our own efforts are deployed to reach at risk populations and give them the care that they need from their own homes. And so through a combination of telehealth and in-person clinics, we deliver a continuum of services from screening and assessment to medication treatment to case management and cognitive behavioral therapies. So that includes medication assisted treatment, individual group therapy, psychiatric care, digital therapeutics and medications from our own pharmacy. And so my role in all of this was to build a data science driven company given that we are a digital first company. So everything is truly data. So my personal goals when I joined the company was to understand the business and the scope of the crisis that we're experiencing. In fact, for my first three weeks, I didn't write a single line of code. Next, I needed to determine and anticipate the needs of the key stakeholders in the company. So that involved a lot of sit down meetings with different departments and understanding how they currently utilize data if they do so and then anticipate what kind of data needs they would need to help them better improve their own internal processes and help them become more efficient. And thirdly, I needed to just assess ways in which I can use data to help us deliver on our mission and core values, which is to improve patient intake, care and retention. Given that retention is really the biggest predictor of long-term successful recovery. And so I was trying to find a gift that kind of summarizes the experience of being a company's first data scientist. I think it starts with really elation at joining a company with no legacy code, no legacy tech stack. You can kind of get to choose and pick what tools make you the happiest. But then I think it can be stressful given that you are wearing multiple hats. Data scientist, you're also kind of a data engineer and a research scientist and a project manager and a software engineer and IT and DevOps. And I'm not off. So it can get to be a lot, but it's incredibly, incredibly exciting. And I think the elation continued for me because we truly have incredible tools at our disposal. And I'm going to talk about those for a little bit. And so before we really talk about the tools, right, I think it's, it's, you run into trouble when you look at tools and you're like, okay, how can I use the tools as opposed to having an approach first of like, what do I need to accomplish? And then thinking about the tools that help you accomplish those goals. So some of the key considerations when I first started is, okay, where is the data? How is it stored? Can we maybe think about better ways in which it can be stored more efficiently? And is the data even ready? So do we need to process it or anything like that? What tech stock is needed for that data? And what tools are best for efficiency, automation, reproducibility, and reporting? And finally, what is the personnel needed to make all this happen? So starting with personnel, actually, that may be the very first consideration in all of this. And I remember tweeting out not long ago, jokingly, like, what do data scientists want? And turns out they only want one thing. And it's disgusting. And it's data engineers and found that out pretty early on that I really could not do everything I could do. And I really could not do everything. I couldn't be both a data scientist and a data engineer and a research scientist and all of those. And so investing in that personnel was really key in getting us up and running. So I think really minimally, this is the team that we first started with with me, a data engineer and IT support. And the next thing that I think was important for us was to use internal packages. We have such, such a small team, really as me being the only and principal data scientist. And so I remember reading Emily Ryderer's blog post about internal packages. And it really just opened my eyes to how useful they could be. And I'm just going to take a quote from her. So internal tools have great potential to improve an organization's code quality, promote reproducible analysis, frameworks, and enhance knowledge management. And this graph she used was perfect because you can see that she used was perfect because I thought I could pretty much do it all and relatively fast and relatively well with the open source packages that our studio have. But as we can see from this graph, they do address a lot of problems and goals that you want. But their solution breath may not be as high as internal packages because you can sort of configure these packages to be the most efficient for the use cases that you have. And so given once again how small our team was for the sake of efficiency and automation, investing in making our own internal packages was key. So some of those packages involve a data package. So one that pulls data from different sources that we use on a regular basis such as data from the CDC, data from the New York Times for COVID-19 data, data from government agencies and cities. Next we wanted a package to help us just with database connections and querying, API requests and ETL processes, and then a package for the data manipulations and the data visualizations that we were doing over and over and over again. So zooming into the data manipulation and visualizations, including for our internal packages, we make very heavy use of the tidyverse. Not going to go through each package or anything, but just to highlight the ones that have made just a tremendous difference in our efficiency. Deep Liar, of course, for general wrangling, GG plot two for visualizations, string R for text stuff per tidy R for functional programming, lubridate to wrangle dates, skim R to continuously preview and assess the data, janitor for cleaning and quick frequency tables. And many others that we utilize also on a regular basis such as tidy text for text mining and some packages from the open source community like Layla's package called do for drug parsing. For modeling, we actually only make use of tidy models really because it's just a tremendous ecosystem of packages that help you with the modeling process really from start to finish. And we use the tidy models ecosystem to perform statistical inference, survival analysis and prediction. And so these allow us to detect and assess trends at the population level, at the provider level, at the individual patient level. They allow us to evaluate practices internally, which then help us inform what those practices ought to be. So this is hugely helpful in personalizing care for our patients. And then of course the continuous monitoring of outcomes and assessing who may be most at risk for treatment lapses and so on. And so one thing that Jared has talked about is this common pitfall that many of us may encounter in which you open a project, you run the code, depending on how computationally heavy that could be minutes or hours. You wait for that code to run and then you run into some errors and then you have to go and fix them and then have to restart this from scratch. And so we don't have to live this way. People have created packages for us to do that. And the targets package is extremely powerful in that sense. So it is a pipeline toolkit for reproducibility. The amazing thing about it is although there are many tools, this one is specifically designed for R. So you can stay in R the entire time. It saves on computation with parallel computing, it skips steps that are already up to date and don't need to be run. And then that diagram that Jared shows gives you confidence that what you're doing is truly reproducible. And so given that we are a shop that uses data science, we're also very invested in research. And so this is a huge win in that regard. And so one of the most important things, of course, for us is reporting, right? So the key considerations here is that I did want to stay all in R or R studio. I wanted that reporting to be customizable in which I could throw up my company's logo. I wanted it to be lightweight and portable, so easily shareable. And I wanted, of course, version control. And so there are amazing packages to help and do that. So starting with blastula, which allows for you to send emails from R. So that's really helpful for daily KPI distribution, and for the performance indicators, our markdown and flex dashboard together to make reports, HTML reports, or to make those into dashboards and sharing in for presentations. And so these four we use every day for KPI distributions, monthly, quarterly, yearly reports, dashboards, and presentations that we can make iteratively without having to recode them every single time. And so that's all great, you know, making this, but how do we actually deliver it to key stakeholders? And the tool we use is R studio connect, which for the consideration, we just spoke about about wanting this day and calling R and R studio that goes obviously a huge way. And so R studio connect is that publishing platform for those dashboards and those on demand and scheduled reports and those presentations and even APIs and many more. So the advantage is there once again is that you never have to leave R and R studio. It also can publish Python code for you. You never made use of Python, but maybe one day. And then the third and possibly one of the biggest considerations is security, given that we are dealing with confidential patient data. And so you can very easily control who sees what on that platform. And so I just wanted to end here with, you know, sort of the motivation for why we do this work and why we get up every day is for patients like this who continue to inspire and astonish all of us. This patient says, I just wanted to thank you and everyone else that has gotten me this far. I didn't think it was going to happen without me driving the Pennsylvania. But thanks to you guys. I finally have a doctor appointment and I really appreciate you all. So that's what we're all about. We're about lowering that barrier to treatment access. And we're very happy to have the tools that allow us to do that every day. So huge shout out to the open source community and our studio and everyone that I got to know through Twitter couldn't make this happen without you guys. Thank you. And if you do want to reach out by email or Twitter, please feel free to do so. And I hope that if you want to get involved, there's many opportunities to do so both code wise, but also community wise. I encourage everyone to look into Narcan training, which is oftentimes free and offered multiple times a year in which they teach you how to recognize the signs of an overdose and then administer a life-saving drug called Narcan, which reverses overdoses. So with that, thanks once again and looking forward to connect with you all."}, {"Year": 2021, "Speaker": "Brook Frye & Ben Witte", "Title": "Data and Decision Making in the Legislative Process", "Abstract": "In New York City, the City Council has many functions, including oversight of the Mayor's operations and generating legislation that compliments the oversight function. We will provide a general overview of how data is used to underscore the rationale behind legislation and how the City Council works to ensure that these data feed into an overall ethos of transparency and evidence-based decision making.", "VideoURL": "https://www.youtube.com/watch?v=F_tXZOn9pE0", "id0": "2021_07", "transcript": "We have two speakers actually. There's gonna be a joint talk one speaker has gone the whole pandemic wearing a Professional shirt up top wearing crazier and crazier bike shorts or no one can see Apparently the bike shorts just got an out of hand craziness so you know, yeah Crazy bike shirts and the other speaker loves the reading and his favorite book or book series or book Musher is the savage detectives Please everyone welcome Brooke and then we will let you guess which one of us is wearing crazy bike shorts, but My name is Ben. I'm a data scientist my colleague and fellow presenter is for a price She's a senior data scientist. We are both with New York City Council And we're here today to talk about data as part of the legislative process So the data team's mission is to promote a data driven approach to the council's decision-making process Prior to this, it's largely been a very analytically informed process and we wanted to Bring a new edge to it, right? So we split our time between laws analysis and visualization In regards to laws mainly our responsibility is legislative inputs to acquire better data Brooke's gonna talk a little bit more about that later, but once we get good access to strong data We can perform our analyses and we've got a lot of folks with very strong backgrounds Who can do a very good job of this and of course stronger analyses means stronger legislation and stronger policy? Lastly in regards to visualization our goal is always to make things clear and comprehensible both for the council and the public So we want to make sure we spend a lot of time conveying our results in ways that are useful So I'm gonna hand it over to Brooke. She's gonna talk a little more now about the kinds of data we have access to Yeah, I'm just gonna go over our most common data sources and other creative ways that we employ to get information that we need in order to investigate common problems relevant to the city of New York So getting data happens to be a large part of our job or sourcing data We rely heavily on the open data portal which came to be because of local law 11 of 2012 More commonly known as the open data law It mandates that any information collected by mayoral agencies that are relevant information that's relevant to the function of that agency must be made public and made public on a sort of searchable portal so When we when the data that we need is not on that portal will sort of look to other public data sources We like to rely on the census data. I mean American Community Survey for Demographic information we look to the 500 cities Data portal put out by the CDC to get at Modeled health metric data and then we'll use the Landsat data or satellite data to Get information about the environment When our data is not on the information superhighway Proper we'll get a little clever and we will scrape data or we'll We'll arrange data sharing agreements with proprietary companies such as safe craft Be safe for after the pandemic to understand movement around the city during the pandemic And finally when we can't find it anywhere We can't find the information that we need anywhere We will rely on a really big tool that we have the reporting bill tool and reporting bills are what the city council sort of works on and Uses in order to get information from the merit side of things relevant to their operations and so mandates that the agencies report metrics and those metrics get turned into data sets and Back to the beginning those data sets end up on the open data portal So a little bit more about the reporting bill process It's a pretty Collaborative process we sit down with city council legislative staff to understand what the issue is What the legal parameters are and then what the best way to? Sort of get this information is so we turn this sort of like issue or these sort of investigative problem into a quantitative question One thing that we are constantly thinking about is spatiotemporal granularity So how far down into the data do we need to drill in order to get the information necessary to answer the question? We always want you know the most sort of granular information But we do have to balance that with the privacy concerns of the average individual we So we have to sort of do this negotiation around like what is needed and what is unnecessary and finally we Because it's a law and because it is going to exist in perpetuity. We want to make sure that this law is future proof So we can't Can't specify certain technologies that will eventually become obsolete. We know how fast that happens So we can't you know ask for data to be on like a CD ROM And then you know we'd still be getting data on CD ROMs and we just have Clasets full of CD ROMs which are useless at this point. I think Someone can argue with me So an example of a reporting bill is the benchmarking energy reporting bill local law 84 2012 Which started with the question. What is the the landscape of energy used by buildings in New York City? So we know that or we knew anecdotally sort of or through some like disparate research projects that buildings are responsible for about 70% That is a hard sort of estimate to really get precise on but we know that buildings are the lion's share When it comes to producing greenhouse gases and so we wanted to know what kinds of energy are being used and how can we ultimately Use this information to reduce in the civite in a meaningful way in New York City So yes, we needed more data. We didn't have this data at hand So we wrote a law that mandated buildings of a certain size self-report their energy use each year and The privacy concerns weren't that Interesting with this law that we did sort of have to think about the scope of the bill We weren't really concerned with individuals use so like single family homes We're more concerned with these like sort of larger buildings that have more complicated energy issues So we focused on buildings that are 25,000 square feet or more They seem to be like the right that was like the right mark to get at sort of the issue and then also to be able to affect the issue And so and then as far as making this future proof Well, we just want to be able to monitor this issue Over the years we want to make sure that we can track if buildings are doing better or worse And when and where and why? So upon getting this data we were able to do a little bit of investigation and We wanted to sort of understand like right like globally What is the distribution of greenhouse gas miss cities wide? this chart shows you sort of chunked up into groups and the Unit is on tons of carbon action carbon dioxide squared for square foot and so we wanted to see you know like who are the what where are the poorest performers and Where are the best performers and is there any information we can get based on? Based on the characteristics of those buildings that would lead us to sort of more achievable and far-reaching greenhouse gas bulbs so we Looked at that data and and that data sort of led to this larger and More comprehensive plan called the Climate Mobilization Act of 2019 Where we used benchmarking data to set greenhouse gas thresholds greenhouse gas limits on certain buildings so we began with the overall distribution We looked at the worst performers the best performers And we thought about this goal that the city has had for a while now that it wants to reduce greenhouse gas emissions They 80% by 2050 So how do we make it so that these buildings have achievable targets and can reach goals in a meaningful way so we looked at We use actually boost an R to predict emissivity we Pulled out the importance features and we looked at those important features as a way to sort of group buildings with like buildings so the first the most important thing that we saw was that The work that's being done in a building is most predictive of how much How the amount of greenhouse gases they're emitting so we then grouped buildings with like buildings and we Defined thresholds so if a hospital there's one hospital in Queen that's doing terribly But there's another hospital in Manhattan that's doing grateful It seems fair to say that this Queen's Hospital can maybe do a little bit better given that they're doing the same kind of work The operation is very similar. So ultimately we decided that for the first goal We would look at the worst performers the 20% At the tail end of the distribution and we we want them to reduce to the 80th percentile So we need to move them over to the left and then for the first goal So get those really really sort of bad actors essentially doing better and then ultimately for the next goal We need to get the whole entire distribution except for maybe a couple to get to the 20th percentile mark so That's that's our sort of initiative or our whole entire strategy in getting the city to get To be calm a little bit more energy efficient unless reliant on Energy that produces greenhouse gases so electrification of the grid essentially I'm gonna pass it to Ben to talk about pay equity a little bit another project that we've worked on Yes, so I guess in contrast to the climate work that Brooke was just discussing Which ultimately led to strong legislation pay equity is still in an ongoing oversight phase so This is the result of local law 18 2019 which is another reporting bill that we passed it took Eight years to get it through and it provides the council with unprecedented access to Information on pay disparities allowing us to analyze them across protected categories this effort was led within the council by Councilmember combo who we can see right here. Thank you to her It really provides us with the opportunity to hold the city accountable for correcting pay disparities where they exist Within the city workforce part of the reason why it took so long to get this data though To relate back to what Brooke was mentioning regarding reporting bills It's very important to take new account for privacy concerns and future proofing So we have ultimately managed to get this through an API and that's finally how we've gotten access to the PII for pretty much the entire city workforce I Just jump in and talk a little bit about the model we chose and why we chose it The data in its the data the structure of the data has some patterning There's nest thing that occurs And so we wanted to account for that as well as factors outside of protected classes Which were first and foremost interested in that meant might affect Or might influence pay so we had pay the log of the salary as the outcome And gender race ethnicity and age but then we had to control for many other covariates that might impact pay We We just to make a note there because this is sort of like a bunch of disparate Data sets that are sort of like put together a lot of it Had some or a lot of the columns had some issues So we wanted to include education level, but there was way too much missingness. So we just couldn't but regardless We ended up with a pretty strong model We accounted for the nesting that happens within agencies. So an agency might have There might be an agency that has a title that is across many different agencies But that title might mean something totally different across the different agencies And so and then that in and of itself might impact right so you wanted to account for that sort of Internists that happens So we had random effect on agency random effects on agency and random effects on agency and the title and the title And I'll give it back to them Yeah, so as a result of the mixed model that Brooke is discussing there We were able to compare essentially the unadjusted and adjusted it pays According to the protected classes. So at the top of this page You'll see the unadjusted pay is the raw comparison of race ethnicity and gender and at the bottom We have controlling for length of service civil service title code suffix and level the agency and the managerial status of the individual now in the adjusted pay You can kind of see within a couple of cents Most of those pays are pretty close to one dollar But in the unadjusted pay it's significantly different to that and that we believe is more or less Due to opportunity access So if you have the same job and the same experience you're going to get more or less the same pay given that we Like Brooke mentioned don't have all access to factors like education It may or may not explain some of those cents on the dollar differences And so that is something to look into in the future But we know for sure that the higher paying jobs are overwhelmingly held by men over women And by white people over people of color Just as an example at the bottom of this screen. This is a basically fdny job titles grouped by their median salaries and women are in red very well represented in many of the lower paying job positions within the fdny And increasingly less so as the pay scales go up and ultimately at the very top of that pay scale completely unrepresented So while we don't know for sure at least the the model that we have can't necessarily speak to all these factors Undoubtedly historical inequalities maternity Education access poverty factors are are all very much related to this now the good news At least from our end is that this will be an annual report and so some of the questions that we couldn't answer now or can't necessarily See you know back into the past now we can Continue to examine moving forward to continue to provide oversight in an attempt to correct these trends And I just want to mention that these this pay equity analysis was specific to the new york city workforce We know that things sort of play out differently elsewhere But because we have these civil service titles that sort of track people pretty tightly um, there is like a little bit more equity um when it comes to you know job title and promotions and whatnot With the caveat that You know women and people of color often have less access to higher paying jobs Thank you No problem So um, yeah Thank you, no words Uh just to talk about challenges, right? We've talked about two examples where we've had quite a lot of it success in the council But we do also run into ongoing issues One of them and really the the main one that that leads to the others is the proactive Nature of the work that we try and complete versus the reactive political cycle So inevitably the council being a legislative and political body Uh, we're sort of on on a sort of news cycle within the city And we do have to as a result sometimes respond very quickly to new things that come Uh, or we have to work with data either that doesn't As of as of when it's important have the level of granularity that we're looking for Or that might have high missingness in both cases. It's sort of like a garbage in garbage out situation So that can limit our effectiveness when we're on a tight cycle Totally But we're often successful and we should focus on that I guess And the reason why we're successful is because we have had support from leadership, especially the speaker of the city council of kori johnson has believed in since the beginning data-driven legislative decision-making process and Reviewing data to inform us in all ways Um, we also find it. I think I mean I find it really Meaningful to engage with the larger council staff and the public in general And visualizations is the best way To do that. Um just getting people to understand and be sort of like kind of involved with the process of Data analysis the you see a more sort of informed and engaged Um citizen and it's really fun to see that process Um, and we're also just you know our backgrounds make us expert in reframing legislative ideas or big ideas into quantitative questions and sort of more scientific Science-based sort of approaches Um, yeah, so in summary, we're just here trying to make laws a little bit smarter to improve the lives of New Yorkers and um, thanks Thanks for listening Thank you very much That's one of the things about this conference is that it's government public sector and we have all levels of government We have federal we have state we have local we have NGOs It's nice to see a local government talk. I know that New York has been at the forefront of data analysis for quite some time Um, that's been a nice thing to see the city embrace data but as a community in the city but also from the government on down So thank you very much for the talk and for bringing data to government Thank you. Thank you, Jared"}, {"Year": 2021, "Speaker": "David Shor", "Title": "Bayesian statistics, government, and the 2020 election", "Abstract": "A walk through how statistics and data science are commonly applied in politics and government.", "VideoURL": "https://www.youtube.com/watch?v=9HZDQiDqd7c", "id0": "2021_08", "transcript": "Our next speaker. Actually, I first met him years ago at a paella party in Washington, D.C. That's first time we met each other. And despite his well-knownness, well-knownness, well, consider that a word, despite his fame, they're actually 60 people in the United States named David Short. And three of them are exactly 30 years old and work in progressive politics. So despite him being one of a kind, he is one of three and one of 60. Everyone, please welcome David. Real honor to be here and present to you all. You know, even though most of my shop at this point is this Python, I am the lone, I mean, not the lone. All of the Bayesians are the lone. Our holdouts are hold a very special place in my heart. And, you know, I think it's the basis of most of the things that we discovered. So this talk is going to be about politics. Hopefully people still like hearing about politics. And in particular, I'm going to talk a little bit about the work that we did in 2020. And, you know, obviously we're still doing the same kind of stuff. So the first is who are we? You know, I'm one of the co-founders at Blue Rose Research. And so the first thing is it was founded by alumni of the Obama 2012 campaign and their analytics team specifically, which at the time was known as the cave. In 2020, we worked with basically every 2020 Democratic presidential campaign during the primaries, most of the major party committees in Super PACs and most outside groups, which is a fancy way of saying groups like labor unions, Planned Parenthood, etc. The kinds of problems that we try to solve are election forecasting, resource allocation, predictive modeling, content testing, and attribution, which is a lot and keeps us very busy. In doing that, we conduct in 2020, I think with the exact number, as we conducted 6,000 RCTs on over 6 million people, we helped allocate hundreds of millions of dollars worth of media spend, and we produced forecasts for every single House Senate, Governor, Presidential, ballot measure, and state legislative race in the country. And in doing that, had to score billions of rows per day, and train on millions of rows per day. The actual team, as of right now, is about 7 machine learning engineers, 3 data scientists, and it's unclear what the division between those two are, but those are the titles. Two Bayesian statisticians, 7 software engineers, and 4 political operatives. So, just to what is it that analytics and data science and politics actually does, I'd say there's kind of, we have a boring, unromantic way to look at it, is there's kind of three core questions. What races are close, and where should I spend my money? What messaging should I base my campaign about on? What should I talk about? What creative works? What creative doesn't work? And then once you decide what races you're going to focus on and what you're going to talk about, how do you actually reach voters? And how do you target? And all of those things, coming out of 2016, were broken in some way. Traditional survey research doesn't work super well anymore, as I think any observers of polling can tell. The way that campaigns historically figured out what worked and what didn't was by asking small groups, and I mean like six or seven people, to self-report how much they like different things, which has problems, and traditionally, and the way that campaigns talk to people hasn't changed in 30 years. So, first to talk through each of these, one by one, or at least two of these. This is a graph going back to 2016, kind of all of our happy places, I suppose. It has a lot going on. Every single bar is net spending by electoral vote, segmented out by state in the battleground states. And what you can see is that even though Hillary Clinton had about three times more money overall than Donald Trump, Donald Trump outspent Hillary Clinton in the tipping point state that actually decided the election substantially, which is quite bad. And the reason why this happened was basically that all of the polls showed that Wisconsin wasn't going to be close. This is, in general, public polling has stopped working over the past six years or so, and the root cause of this is that survey response rates are a lot lower than they used to be. Polling working is fundamentally dependent on the exchangeability assumption that the people who answer your surveys are the same as the people who don't once you control the things that you control for. And survey takers have now become so strange that the traditional set of things that people control for are no longer enough. The answer to this is either to get people to take more surveys, which is very hard, or to control for more things. And unfortunately, traditional waiting techniques fail when you need to control for 20 things instead of four things, which leads you to using bias estimators and all this other stuff. So, I think this slide originally was immediately before the 2020 election, but basically what you can, a lot of people coming out of 2018 thought that polling was fixed. But what you can see is that here this is showing polling bias by state in 2018 and in 2016, and you see a pretty consistent error pattern in 2020. Also, if you made a similar graph would look identical, just shift it down. So, talking about messaging, I'd say the highest impact thing we've done is that we've built out the infrastructure to do RCTs. Of TV advertisements in 2020, we tested every single Democratic and Republican ad that went on the air. You know, what we found was that roughly 20% of ads that Democrats made made people more likely to vote for Republicans. And generally speaking, the more people in the office liked the ads, the worse that they did. So underrated theory of change is find those ads and not show them to people. So, big picture just to describe what we actually do. I'd say that we have, we do a lot of surveys. We survey millions of people. I think we surveyed about 6 million people in 2020 and something like 3 million in 2021 so far. Then we have a voter file of every single person in the country and a variety of first party data from campaigns. The Democratic Party has over the course of the last 30 years contacted roughly 186 million people at least once. Obviously some people much more than once and some of those people quite far away. The database administration archaeology is fun. There are the Minnesota Democratic Party has roughly half a million Michael Dukakis IDs from 1988 that somehow were stored. Though obviously Minnesota Democratic Party is unsurprisingly better at data management than other states. And big picture what we do is that we fit models on our survey data and then we score basically every person in the country. And that's kind of the big picture of what we actually do is fit those models. So, why is this hard? The first is that politics has a lot of structure. There's a voting behavior is driven by a bunch of highly correlated yet distinct factors like ethnicity, socioeconomic class, religion, etc. Many of which are themselves blatant and that we don't get to observe directly. The relative importance of all of these correlated factors can vary a lot across space and time. And because we have to do all of this pooling and because surveys are expensive. You know, we have to do a lot of regularization across time, space and context. The other issue is that our data is very biased. You know, mathematically our training sets are very different than our scoring sets. People who take surveys are incredibly weird. And we have to adjust for things like attitudes toward the Bible or social trust or attitudes toward corporal punishment in order to make survey takers even kind of resemble the overall public. And then, you know, the other point is that even though, you know, we're fitting models in order to do all of our inference, our actual objective functions are not related to AUC. You know, there are lots of things that do not affect the overall performance of the model. You know, one example is that in, you know, southern West Virginia, there are an enormous number of people. I think in West Virginia's third congressional district, something like 55% of people are registered Democrats, even though, I think Joe Biden got about 18% of the vote there. And it's very easy for a variety of models to get that wrong and getting it wrong because not that many people live there ends up not impacting your AOC. But if you end up allocating millions of dollars into a place that isn't actually close, then that's actually quite bad. And the other point is that we are trying to estimate a lot of effects that are quite small. But unfortunately, sometimes can become quite large. And then there's another point, you know, just to talk about this bullet is that Barack Obama got 52% of the vote of the two party vote in 2012. And Hillary Clinton got 51.1%. And that 0.9% drop is not very large, but clearly ended up mattering a great deal to the world and to all of us. Just to talk about some of these non response issues and how they can end up impacting accuracy. Here, we're looking at social trust using the GSS's social trust question. You know, they basically ask people, do you think that people can be trusted or do you think that people cannot be trusted? And, you know, what you can see is that among folks who trust the people around them, that group swung toward Democrats from 2012 to 2016. Well, regardless of education group, but if you look at folks who do not trust the people around them, then that group as a whole swung heavily against us. And unsurprisingly, people who trust the people around them and institutions are much, much more likely to answer phone surveys, which is, you know, one of the big reasons, you know, why we saw the public, why we've seen the repeated public polling areas that we've seen. And this kind of shows how there are a lot of things like this that you have to account for in order to actually make public opinion estimates reasonably accurate. Another big issue is that, you know, there are a lot of deviations from linearity across context. So this is a fun table that shows basically two-way support for Clinton, as well as three-way ideology by race and religiosity and education. And what you can see is that there are a lot of deep interactions here where, you know, just to call out some of them, Clinton percent is highly non-sensitive among African Americans to all of these things. But when you look at identifying as a liberal, actually African Americans and white folks are relatively exchangeable with respect to religiosity and education when predicting whether or not you identify as liberal, even if that's not true for Clinton percent. Other fun examples are that if you are white, then generally speaking, going from the highest level of religiosity to the lowest one, generally speaking makes you more supportive of Democrats by very large amounts. But among African Americans, the opposite is true. Generally speaking, highly religious African Americans are more democratic, even if they are more conservative than secular ones. And, you know, that has a lot to do with the historical context of black churches and other things. Other fun examples of this are that having a college degree, generally speaking, among white people makes you more liberal, but if you are highly religious, then having a college degree makes you kind of understand that you're supposed to be more liberal. And actually has the opposite sign. And just to say all of these numbers are based on surveys of roughly two or three million people. So all of these differences that you see are statistically significant, which is fun. Another fun example of this is that there's still a lot of regional variation in politics across both space and context. So here I'm showing in Florida and West Virginia, two way democratic vote share for the president of the United States. And then the Senate race in 2018. And what you can see here is that in Florida, party registration closely tracks both presidential vote and Senate vote. But in West Virginia, where a very large fraction of people are still registered Democrats, even though it's now a highly Republican state. Hillary Clinton lost registered Democrats, because a lot of those registered Democrats have since changed party. But Joe Manchin, who was the governor, because they still remember the Republican Party. And if they still remember who he is and like him, those registered Democrats still vote for him, even though they, even though they're still, even though they're Republicans federally. And there are a lot of these kinds of things and we have to detect them across, you know, the literally thousands of bases that exist in US politics. And the big thing is, you know, this is this is a graph from 2012, where we compare basically the actual, you know, the doubt weekly gout polling for the presidential race, versus, you know, our internal tracker, which was based on modeling and a bunch of other things. And you know what you can see is that the vast majority, if you actually do a decomposition of variance, about 95% of the variance in public polls is either sampling air or poll survey us. And only 5% of that ends up being public opinion. And that's something that is very hard and annoying, because measuring small effects in a highly noisy context is annoying and difficult and hard. But even though, like if you go over here, throughout the entire campaign, the every public opinion stayed within a one point range, sometimes things can change very quickly. So this is showing support for Joe Biden in the Democratic primary between Nevada and Super Tuesday. And you know what you can see is that after the South Carolina primary, Joe Biden had its highest support increased by something like 30% over the last year. And so that's a real problem. If you're fitting Daisy and models, highly parametric Daisy and models, sometimes stuff like this happens and you need your systems to be robust to that, even if they very rarely happen. And then, you know, it turns out doing all of this different work really does. Lead to materially better forecasts. You know, here we show comparing to 538, you know, we've had roughly half the mean squared error of 538 in our last two election cycles, generally speaking, our September forecasts are more accurate and then public polling's election day forecasts. And you can see that, you know, in the Democratic primary, we also did quite well compared to our competitors here. So just to talk through some challenges doing data science in politics, you know, the first is that, and I think this is probably common, is that business questions from clients, you know, things like, what do we do about Georgia? Usually lack a straightforward mapping to a well defined statistical question. But if you focus on problems that are well defined, then that heavily limits your relevance and your impact. And even when well defined, there are a lot of important questions that are impossible to answer with certainty. You know, there are one big, just for example, one big problem that is important to what we do is allocating TV ad spend. And, you know, we do are the best that we can, economically, to estimate things like, you know, the decay of ad spend or the overall treatment effects or all these other things. But there's a lot of things that are impossible to know. And we have to be willing to step back on parametric models of the world and do the best that we can in order to avoid decision paralysis. Because at the end of the day, there's $500 million, it has to be set and arguing about instruments, you know, ultimately, it doesn't do anything. And then the other point, and this is something that I've evolved a lot on, is I remember in 2012, when I was 20 years old, I really resented, I think a lot of the pointy headed bosses, I guess, are the old school consultants who didn't have a background in social science or statistics or math. But looking back, you know, I think that 80% of my disagreements with them, with 10 years of experience and more data and everything, I think that 80% of the time they were right. And they were right and I was wrong. And so I think big lesson there is that, you know, in the real world, in real organizations, it is very hard to rise to the top. And even if folks don't know about math or social science or statistics, they actually do have quite a bit of important wisdom and ignoring them as a bad idea. All right. So just to talk about Stan and R, you know, all of our production models are Bayesian, which is very exciting. All of our time series tools are fully in Stan. You know, we, in terms of the actual nuts and bolts of what we do, generally, we do everything with variational inference, intensive flow probability. You know, this, this generally speaking means, I mean, we have to do this just because, you know, you have millions of rows and hundreds of columns. But as I think if anyone here has ever worked with variational inference, it does not work very well. And we've really had to do a lot of oracle testing and validation in order to trust the results. And even then it really does break a lot, unfortunately, doing large scale hierarchical Bayesian modeling is very difficult. All right. Well, that's the talk. Thanks again for letting me present here. And I'll hang out in the chat. If anyone ever wants to ask any questions or reach out. Thank you."}, {"Year": 2021, "Speaker": "Alex Gold", "Title": "Reliably Reproducible R-Tifacts", "Abstract": "Some people get to write YORO (You Only Run Once) code, not really worrying about whether it\u2019ll run again. You probably aren\u2019t one of them.\n\nMore likely, you have to be ready to re-run analyses months or years later. That\u2019s a tall order given the constant changes to the R language and package ecosystem.\n\nIn this talk, you\u2019ll learn a taxonomy of reproducibility for your code, and be introduced to the foremost tools \u2014 `docker` and `renv` \u2014 for making your work environments more reproducible.", "VideoURL": "https://www.youtube.com/watch?v=-6TlzhRA9vA", "id0": "2021_09", "transcript": "Our next speaker is a PhD dropout. And right after he dropped out of his PhD, he moved to DC and was a Segway tour guide. And he felt the need to constantly apologize to all the DC natives for blocking their way with Segways. So please welcome Alex to the stage. Thanks so much for being here today and for coming to talk about reliably reproducible art effect. I was so proud when I thought of this. Like, I probably should have thought of it a long time ago, but I was really proud of this fun. So again, my name is Alex. I'm a manager on the Solutions Engineering team at RStudio. You can tweet at me at AlexKGold. And the slides and other stuff are not currently, but will be available on my website at AlexKGold.space slash speaking. So we're going to start off just talking a little bit about reproducibility and what's up with reproducibility. And then we'll get into some of like real nitty-gritty, how and why. We're going to try and get through R&R studio package manager and Docker in the next 19 minutes and 30 seconds. So like buckle up. It's going to be a wild ride. All right. So why should we care about reproducibility in general? And the obvious answer here is that you want to share with others or with future you. And that's a really common reason. The other answer, and this is often true of particularly if you're in a heavily regulated or a government industry is like you don't have an option. Like you just have to make things reproducible. You have archival and reproducibility requirements that you have to meet. And so like what is reproducibility? I think of these three as synonyms, like reproducible, self-contained, portable. Those are all sort of intermingled to me in terms of how I think about reproducibility. And so then the question becomes like how do I make things reproducible? And first, it's important to think about how reproducible does something need to be. Things can be sort of not at all reproducible. You can never run it again. It's never going to work again. That's obviously probably bad for the most part, unless it's really a one off kind of thing. It could be somewhat reproducible. It runs, but you've got to spend some time getting it up and running. And there are conditions where it would probably break in the future. Or you could get to like fully reproducible, which I think is usually an illusion, but like way, really reproducible. And you can move up this spectrum of reproducibility. But also the thing is, as things get more reproducible, it's more work. It's harder to be more reproducible. And so it's worth thinking about for the projects you're working on what's the right level of reproducibility for the thing you're working on. Again, oftentimes for things with the government or with the public sector or with regulated industries, you don't have an option. But sometimes you do, and you have to choose the appropriate spot to be on the spectrum. I think about reproducibility for data science is following along. There are three things. And I have to say that this is the only pie chart that has ever appeared in a presentation of mine. And it is purely to have three colors on the screen. So don't hate me too bad. But the three components of reproducibility for data science, I think it was being the code. Can your actual code be reproducible? Does it run? Things like setting random numbers. Do you have a stable way to get random numbers into your code where you need them? Data. Can you get the same data through your code over and over again? Or does your data change every time you run your code? And the environment. Can you actually set up a place where your code can run that's reliable? And today, for the purpose of this talk, we're really just talking about the environmental piece, the code and the data. There's lots and lots of good tools and writing and resources out there. I highly recommend you check them out. That's not what we're talking about. So we're going to start talking at the level of R packages and reproducing your R package environment. So let's take a hypothetical set of three interconnected packages you might use in your analysis. So we'll say we have the packages B-plyr and TidyS. They have a dependency that they share on S-lang. So when I go install that package B-plyr, TidyS, I get all of them in my project. Let's say for the sake of argument, all version 1.0. Well, now some time passes and I need to do this again or somebody else needs to do this on their machine, some time later. Like, what versions am I going to get? I install that package as the specified versions. And so I don't know. And this is bad. This is a bad place to be. It's very irreducible. And I would say this is sort of like the bottom level of reproducibility. So the first level is to just get those package versions recorded so that they can be sort of stable or stored. And the way to do that is with the R-env package. And this does two things. It creates a lock file for archiving and sharing and a project with the library to protect projects one from the other. And we'll talk a little more about this in a minute. At the next level, right, what happens if I archive a project? I need to resurrect it years later. So let's say let's go back to this project I had. I used R-env. So I've got my project. I still have S-lang B-plyr and TIDS 1.0 in them. And I come back two years later. And I want to install, say, EE-plot-floor for the popular plotting library. And at this point, EE-plot-floor has moved along. So it's at version 1.2. And but that pulls in S-lang at 1.2. And I don't know. R-v-plyr and TIDS going to run. Like, uh-oh, that's bad. And so we're going to talk about R-Studio package manager or repository snapshots as a way to fix this problem. And so so far, like, you know, we've got your code. We talked about how to reproduce the layer of R and Python packages. And I'll demonstrate it a little bit. We're not done with that. But we, you know, R-env, package manager snapshots. But like, there are more things down the stack you're going to have to reproduce. For example, your versions of R and Python, your system libraries, your operating system itself. In some really highly-regulated industries, you need to go all the way to the layer of the hardware. If you need to do that, like, I don't have any solutions for you, you need to keep a server for 20 years. And like, that's on you and your ITT. But if you don't need that, you might be able to get some help from Docker, right? Docker is a really popular solution here. It's a tool for creating, saving, and quickly restoring computational environments. The biggest benefit of Docker is that you can sort of, like, take a whole computational environment, save it, and then bring it back months, days, weeks, years later. All you need to do is make sure that your host has Docker. You don't need to worry about any of the other things. They'll all be in the container. And I'll show you this in a little bit. But there are other ways to create virtual environments. Docker's biggest advantage is it's very lightweight. It's very sort of fast to start, and we'll see that in a little bit. Before we head into, like, demoing things, I just want to talk a little about Docker, because people here are probably less familiar with Docker than some of the other components that are, you know, R-specific. So just to give, like, a quick, simple mental model on Docker. You have a Docker file that's code that tells your computer how to build a Docker thing. You build that Docker file into a Docker image. So a Docker image, you can think of as, like, a bunch of compiled code. It's like a .exe, right? It's like it's there. It's ready to run. It's good to go, but it's not sort of anything on it. It's just the image. And then you run a Docker image as a container. So a running Docker thing is the running container. The image is the sort of frozen snapshot, and the file is the code that it comes from. There is a little terminology here, right? Like, sometimes people refer to all of this as the container, you know, sort of generally referring to it as the container. But it's good to differentiate from a running container versus an image that is not running, but is ready to be a running container. OK, with that, let's go to some demo. I'm actually going to stop sharing and reshare my other screen. And let's pull up our studio. OK, so here's what we're going to do. We're going to start a new project. We're going to go all the way from new project to Docker container in how much time do I have left. 12 minutes, we got it. We're going to hit it. All right, new directory, new project. I'm starting a new project here in our studio. I'm going to use Rn. For those of you who haven't used Rn before, Rn works really nicely to isolate project libraries and help you archive them and save them later. So to give you a very concrete image of what Rn does, if I type library R Markdown, well, it's not going to work if I miss spell it. If I spell it correctly, even, there's no R Markdown packet in here. Even though I've installed R Markdown dozens of times on my computer. And that's because Rn creates an isolated project library. Now, this isn't a big deal because now if I go install packages and I'm going to do a few packages that I know I'm going to need here, just get them all. We are Markdown, Mym, and Markdown. What you'll see is that this install is going to be almost instantaneous. And that's because Rn keeps a cache of different libraries across my projects. And so that's why that install was almost instantaneous there, which is pretty nice. I like that feature. So now that I've got my Rn project, I've got the libraries I need here. I could, for example, let's bring in an R Markdown notebook. My Markdown. I'm just using the Hello World one here because it's good enough for what I'm trying to demo and we'll call it My Markdown.RmD and we'll save here. So now let's take a look at the Rn.Lockfowl. The Rn.Lockfowl is the key to this whole operation. So what you'll see is that it's recorded the version of R that I use. That's 402 here. So if I need to get back to it later, I know what version I need. It records the repository that I got the packages from. So that can help me restoring packages later. And it records the actual packages I use. Now, what you'll notice is that there's almost nothing here, just Rn itself. And that's because I haven't actually used anything. I have my Markdown document. But if I want Rm to recognize that, I have to do the Rn. And let me make this window a little bigger so that you can read it more easily even if it gets a little screnky. I do an Rn snapshot. And so what Rm snapshot is going to do is it's going to add all of those packages that I need. It's going to sort of look through all these files, identify the dependencies, trace out all the dependency trees, and add them all to my little lock file here. So now they're all documented. And you can see you've got the version, the repository. Now, if I take this and I give this to somebody else, right, they can restore all these packages in their environment as well with a simple Rn restore command. And they get back the same package environment. Boom. Level one reproducibility achieved. So this is like minimal to me, minimal package reproducibility. Just use Rn. It's good. It works really nicely. Use Rn. Okay, let's say we want to sort of take things to the next level. And remember that issue that we had where, okay, now, if I come back in a year, I'm going to go back to the next level. And I want to install Ggplot2, right? By default, what I'm going to get is the current version of Ggplot2 that is current a year in the future. And I don't know if that's compatible with the packages I have now. That's not good, right? I want to be able to install something that if I want another package in the future, I know it's going to work okay with this set. There are also places where you may need to audit, right? Like where did that set come from? And, you know, just saying that I took what was latest on Kran at the time, right? And that's what that's what this means. Not good. So in order to get around that, what I'm going to do is I'm going to use a snapshot of repository. I can move my, sorry, give me one sec, my window out of the way. So I'm going to use RStudio package manager. And so what RStudio package manager does here, and this is our public RStudio package manager, package manager. RStudio.com. I can choose a dated repository. So I'm going to choose today. And you can see this repository, it's pack2man.RStudio.com. It's the repository, everything. And it's today's date plus like this hash thing that has to do with the time of day and that sort of thing. And so now if I copy this, I'm just going to copy this from package manager. And I'm going to bring it into R&B and just put it here on that repository line. Boom. Now, at any point in the future, if I'm inside this project and I install into this project, I will always be locked to today's date, which is a really good thing. Right? Because then I know that my packages will work together nicely in the future, which is really great. So now, okay, so level two achieved. Right? So now I have a set of packages that I know go with this project. I can share it across projects. And just one thing to note is that when you share R&B does some smart things with getting yours. So for example, if I were just to add this whole folder to my Git repository, the R&B dot lock file would get added and almost nothing else, excuse me, which is really handy because the lock files, right? I want to share the lock files so other people can reproduce the same library. I don't want to share the actual library because, for example, what if we're going across operating systems or something like that? I don't want to just share it in the library. I want to share it in the lock file. Okay. So then what we did right is we went to a snapshot of a crayon repository from our public RStudio package manager. And what that does is that ensures that in the future, I will always be able to get packages that are sort of concordant with the current state of my library. Okay. Now here's where things are going to get crazy. We're going to head into Docker territory. So we talked a little bit about a Docker file and what that would do. So let's pull up a Docker file. So this is a Docker file. And like every Docker file, it starts with a from statement. So this is like, what is the base Docker image? In this case, we're starting with a very, very, like this is the base of the base, right? It's just a Ubuntu operating system image has almost nothing in it. It's like got Ubuntu and like nothing else. Like you can see, for example, here I'm installing Vim and curl. It has nothing in it. So that's what I'm doing. I'm taking this, this very basic Docker image. I'm installing some system libraries and I'm installing R. Now for the purposes of this, like if I were just doing this, actually, in real life, I would probably just use like the rocker images. You've never used them before. There's an organization called rocker or ROCK AR. You can Google rocker Docker. And they make our Docker images for the community. They're awesome. And you should probably use those if you're doing something as simple as this, just some system libraries and a version of our. But what I wanted to point out here is that if you have specific system libraries, you have version constraints you can put in, right? You can do this yourself. It's not very hard to build this Docker file. So or to make this Docker file. So this is my Docker file. This is my base image. So now what I want to show, so now let's, let's before this, I compile this into an actual image, right? So remember the image is the actual sort of compiled version of this Docker file. So if I do Docker images, I'm going to look at just the ones that are relevant to today because I have, oh, I got to go to the terminal. Console hard doesn't know what to do with command like Docker images. I'm going to go Docker images, DCR star. And so what you'll see is I have this DCR base image, which is that one. We'll get into these others in just a sec, although hopefully we won't need backup. So you know, I've got this image now. And so I can do now Docker run DCR 2021 base. Okay. So now what's happening is this image is coming up. It's running and then it's going back down and, but nothing happened. That's fine, right? Because there's, there's nothing to do here, right? It's just, it's an environment. It's Ubuntu. It has some system libraries. It has an R. It has R in it, but like nothing happens. So that's totally fine. That's great. I can also, the other thing that you can do is you can run Docker containers in interactive sessions. So if I want to do Docker run it DCR 2021 base, you don't need to memorize this. If you Google it how to run a Docker interactive, like you'll, you'll find it pretty quick. And so now I've got a terminal right into the running Docker container. And so like this is a case where even if the container is doing anything by itself, right? You can see this is R for two that I've got in the container. This might be useful in case you're kind of tripped out. Yes, this is trippy to be running or in the terminal in a Docker container from inside our studio. That is weird. Now we're out of the Docker container. Okay. So now let's talk about how we're going to archive this particular project. So we've got our markdown document here. And let's open up. I have another Docker file. It's sort of a project level, not a project like an app, a doc level Docker file. And we're going to pull that into this project. We're going to pull it into the project. I told you before the talk that I was going to live on the edge and do all this live while I also had hop in running. And hopefully that won't bite me in the butt. Okay. Let's rename this for some reason that keeps running name at Docker file that ocker file, which is not a thing. Cool. So now I've got this Docker file that is specific to this piece of content. And actually while I'm talking, I'm going to have this build so that you can sort of see how this works. DCR is doc. Okay. So that's going to build a dot. That's going to build in the background. And I forgot that this is called my markdown in the file name. Gotta make sure that's up to date. I'm going to build for real this time. So you can see that all happening in the background. So here's what this Docker file does. It starts from that DCR 2021 base. It copies in that R and a lock file and my, you know, markdown file as a doctor and D. Right now it's installing the RN package itself to sort of get itself ready to go. And then it's going to run an R and restore, which is going to pull all of those outside packages in and get them running. This is already running about twice as long as it has any time in practice on this step. So I have a feeling we may have to resort to the backup, but I can let it run for a few more seconds. You can see in this, in this Docker file, I haven't do something with a CMD command means do a thing. And in this case, going to run an R markdown render and render my R markdown document. All right. So I'm going to run my Docker container. And what this part is going to do is going to pipe the results into this pipe, the results from inside the container out into my results folder. The main thing I'll write just to remind, right, the reason this is going to do anything is this command line. This says Docker when you run actually do a thing. So let's let's hit it. It's running. And what you're seeing is it's running this R markdown render and the main thing to notice here and the people, reason people love Docker, the results, right, that was a new folder is created. It's got some stuff in it now. The main thing I just want to note is that like that ran almost as fast as running an R markdown document locally. And this is why people love Docker because you can run stuff almost as fast in a Docker container as just like knitting in the IDE. And that's pretty awesome that I just stood up a standalone development environment and still could do that in the Docker container and then it took almost no extra time. So I know I'm right up here at time. That's what I wanted to show. And so a couple of little last things. Thank you. Thanks for tuning in. So a reproducibility happiness, R and repository snapshots Docker. So good. If you're interested in this kind of thing, you think this kind of thing is cool. I am working on a book called DevOps for data science. You can read a draft. There are a few draft chapters there. It's supposed to be out in June. So I've got a little time, but working on it. And then lastly, I'm hiring on my team. If you think this stuff is cool, you're really interested in the intersection between sort of administration and data science, please reach out. Twitter is probably the best way to get me. Happy to talk. The other kind of folks we're looking for is we're looking for managers on my team. So people who particularly if you've led a data science team, maybe done a little bit of admin stuff and are really excited about sort of growing people and growing teams, I would love to talk to you. Please reach out on Twitter is the best way to get me. So thanks."}, {"Year": 2021, "Speaker": "Wendy Martinez", "Title": "The Journey Continues: Using R at a U.S. Government Agency", "Abstract": "This presentation will continue the story that I started at last year\u2019s R Conference | Government & Public Sector. At the previous conference, I described some of my experiences \u2013 both successes and failures \u2013 using the open-source statistical computing software R at several U.S. government agencies. I described the goal of my journey, which was to get agreement from my agency to use R in the production of our official statistics. I am happy to announce that I have reached an important waypoint in this journey. R has been approved for production at the Bureau of Labor Statistics! Notice that I did not say I reached the end of my journey. This is because there is still a lot of important work ahead of us. In this talk, I will briefly recap the start of my journey, how I got to this point, and our way forward.", "VideoURL": "https://www.youtube.com/watch?v=WDGb-JvIORk", "id0": "2021_10", "transcript": "Our next speaker is a repeat speaker. This might actually be like a bit of a sequel to last year's talk. And in her previous life, she wanted to be an historian, which is a pretty cool job in itself. But something I discovered about her last year, and I introduced her this way last year is that she went from being like a closed source expert, like writing books about closed source software to being this huge open source advocate. And I think that's a really nice journey, going close to open source in a brazenly open source community. And the talk today is partly about getting open source into big government. So I am super excited about the speaker, about the journey, about everything. Everyone, please welcome for a second time at these conferences, Wendy. Thank you for inviting me back. I guess I must have done at least an okay job last year since Jared asked me back for this conference. So I'm very happy to be here to talk to you about my continuing journey as an advocate for using R at a US government agency. And in particular, my goal has been to get approval for us to use R in what we call production. So here's the game plan for my talk today. I'm going to sort of talk about the beginning of the journey. So some of this will be a repeat of what I talked about last year. But then I'm going to let you know where we are today. And I have to say, at first I was going to say, this was my end game, I guess, that I reached the end, but I really haven't reached the end yet as you'll see at the end of the talk. So a little bit about who I am, education wise, I have degrees in various different disciplines. So I have a bachelor's degree, I had a double major in math and physics, went on to become an engineer. And then I got my PhD in computational sciences in informatics with an emphasis on computational statistics, which if you look back at the curriculum I had, this was back in the early, the mid 90s, was really what we would call today data science. And during a lot of that time, I was also working in the department of defense. So I had probably over 25 years in the defense department. And then I moved to the Bureau of Labor Statistics, which is sort of the world of surveys and what they call official statistics. So I kind of have experienced in both sides of the government. And I really did start using R when I was working on my PhD. So I have a lot of experience with R across the federal government. Right now, as I said, I'm working at the Bureau of Labor Statistics. This is our building. If we weren't working remotely, this is where I would be. This is the postal square building, it's right across from Union Station. And that door you see next to the tree is for the Smithsonian Museum, the postal museum. So once things open up a bit more, I'd encourage you to go there. It's a beautiful place to visit. So I want to give you a little bit of background about the principal federal statistical agencies, because this has a bearing on the hurdles and things that we've had to overcome in order to use open source software such as R. So there's 13 principal federal statistical agencies. And these encompass a lot of different areas, as you see here. So crime statistics, economics, labor, transportation, demographics, energy, and more. So in the United States, we have a decentralized statistical system like some other countries where it's one national statistical office. So what's key here is that these agencies, we are responsible for producing certain official statistics. In particular, there's what we call principal federal economic indicators. These are what BLS produces. And the three down here at the bottom are some of the ones that we produce. And I'm sure you know, are familiar with most of or at least two of these. One is the employment numbers and employment situation that's published every month is sometimes known as the jobs report. And of course, the consumer price index. So as you could maybe imagine, getting these data published, the statistics out there is vital. And we have to do it by law. And a lot of people around the world look for these numbers. So former commissioner of the Bureau of Labor Statistics, Dr. Erica Groszian, kind of developed this acronym. And I really love it because it describes the heart of what federal statistical agencies are all about. And I want to talk about this a bit here because at least three of these, I think are relevant. Or I guess I should say what we use for software or our IT is really critical to make sure we achieve these priorities. And first of all, we have to get it right. What we produce has to be accurate. Also, kindly, you know, we have to what we use in terms of IT has to be stable because we can't afford for things to fall apart. And then we have to tell the public, sorry, we can't publish the jobs report because, you know, something happened with our software. And it has to be accessible. So this has to do with how do we disseminate, visualize, what have you the data and the statistics that we publish. They're all a little bit about the Rocky Road. And this is the Rocky Road to getting our approved for production at the Bureau of Labor Statistics. As I said, I started at the BLS about 10 years ago. And when I got to BLS, our was being used by a small group of people, mostly for research purposes. But when I got to BLS, I soon became an advocate for R and in particular, using R for to be sort of innovative, to develop innovative applications and approaches and then use those in producing our statistics and our data. Now BLS, we do have a system that allows us to request that we be able to install software for the purposes of either research or production. And so at one point, when my first got to BLS, I started going through the kind of the wickets to or the process to get it approved for production. And you can see this graphic right here, my co-author Brandon Kopp went back through our approval system and looked at the approval history for R starting in 2015, which is kind of when I started this whole process. Now the blue bars are approvals for research purposes. The green bars were approvals for production. So you can see right from the beginning here in 2015 or so, it was approved for production. And then all of a sudden somebody said in our IT, oh wait, that's not good. We have to take that back and you can only use it for research. Then again, production, no, and then just research purposes. So what's just kind of interesting to me here is it's sort of an indication of this kind of circular road for getting our approved for production that I've gone through over the years. And this was a graphic that I showed in my talk last year, so it hasn't been updated yet. So why should we use R at the Bureau of Labor Statistics or the federal government? Well, first of all, the cost. Then there's the fact that when somebody, say a researcher, develops a new methodology, oftentimes they write an R package or something or functions to implement that methodology. And so that is available right away for researchers, say at BLS to use in producing what we do at BLS, which is, and then often that type of functionality is sometimes not ever available in commercial software. But in any case, it takes a long time for it to be implemented and say something like SAS. We've also faced with hiring people right out of school and usually in their universities, they don't know, they don't learn SAS, but they might use oftentimes a learn R or say Python or something else. So they come to BLS and they're told, oh, you can't use what you used in school. You have to go and learn something like SAS, which doesn't have maybe the functionality that you need. So this leads to issues with employee retention. So we've hired data scientists, they come in, they're all excited, and then they're told, oh, you can't use R or Python, you have to use, again, something like SAS or some other software. So they get kind of frustrated and they leave. And of course, we asked them to be, we want them there so that they could be innovative and kind of help us modernize. And in order to do that, we really need to have the use of these tools. So what were some of the issues that our IT and upper management brought up when we were trying to get R approved for production use? Always, there's the issue of security and protecting data. There was a confusion about what does it actually mean to use R or anything really for production. And as I said, we have researchers who are using R to develop new and innovative methods. And so there are researchers need to have the ability to kind of install packages as they want to and as they need to. And so that kind of made our IT folks uncomfortable because sometimes you write something for that uses a previous version of R or a package and then there's updates and then things get broken. So there has to be some stability. This is the government. They do like to have commercial support. They want to be able to call a help desk and get some help fixing something. Remember, I said one of those priorities was we have to be timely. We can't be late producing our statistics. So we have to be able to get help when we need it. And then one thing that was, so remember, it's the researchers, not the IT people who are developing these new methods. They're the ones on the being innovative. And so what happens is the IT folks, they say, well, what happens when the developer, the innovator leaves the bureau, who is going to take care of that, who's going to maintain that app or that package. So what I always push back there say, well, you have programmers in the IT department, who they don't stay there forever, maintaining the same program. So there has to be ways to mitigate that risk. So where are we now in this journey? I really like this little road sign up here because I think it kind of exemplifies what I've gone through over these past many years and kind of twists and turns and detours and circles and kind of three steps forward and two steps back. But we have made some progress that I'm happy to report. So we're actually ready to, in 2018, late 2018, we're ready to get approval from our executives to use our production. We had a way to govern the use of our and they pushed back and said, well, we want to have a process to approve any software for production. So not just our. So things like MATLAB or Python or sort of commercial or open source, any kind of software. So a team worked on an application process. In 2019, December 2019, we presented it to our executives and that was an epic failure. They kind of hated it. There's a lot of misunderstandings about where we were coming from. And so early 2020, I was ready to just throw in the towel. I just kind of lost, I guess, motivation or something. But fortunately, Cooler heads prevailed and our deputy commissioner came to me and said, well, I think we should have a smaller group to take your process and fill it out as if you were going to get our approved for production. So we did that and we had some good support and it was a smaller group, which I think really helped. So we, instead of going back to the executives and saying, okay, here's our application for our. They said, let's form a little smaller group where the members have sort of the authority, the decision making authority to approve our for production. And so that's what we did. And as of last year in December, when I spoke at this conference, we didn't have a decision yet. So it was still, you know, up in the air. But I'm happy to report since then it's now approved. So this is a picture kind of of, I guess, my inner child. I'm at the top of the rocky stairs, you know, and, and I've got my, my arms up and victory saying, yes, it is approved. So I'm happy to say that R is officially approved for the use in production at the Bureau of Labor Statistics. And I want to say that I think maybe another agency could disagree, but I think BLS is the first agency, principal federal statistical agency, to do this in an official way. So here is sort of the continuing journey. We're not, we're not totally there yet because there's a lot of things we need to do to make sure we're using our properly for production use. So we did set up a governing board. And this board was chartered to develop an infrastructure to implement processes and procedures to use our production. And so some of the things that we've been working on so far this year is to find out across the Bureau what, how are the program offices using our production, because even though we didn't have sort of a measure of production, we didn't have a measure of production. And so some of the things that we've been working on so far this year is to find out across the Bureau what, how are the program offices using our production, because even though we didn't have sort of an official approval to use it for production people were developing programs and applications. That really did fit this definition of here as production software. We, the board also listed various risks and issues we needed to consider and developed a mitigation strategy for those. And we're also trying to determine how can we sort of find a version of our that would be used for, if somebody's going to develop something for use in production, this would be the version of our production. And of our that they would have to use including packages. So ways that we could manage that process with respect to validating upgrading and so on. And we are researching various tools to help us with that. My colleague Brandon wrote this. For the board, on behalf of the board wrote this paper on the 10 principles for using our production at the LS and it's really very interesting and a nice paper. And I just want to point out some of these that we've been working on focusing on lately, which is to kind of centralize if you would these these applications so get them off of individual computers version control I've already talked about. We discussed various style guides that we might use. And of course, the importance of documenting your work. So I'd like to acknowledge the members of the governing board. We have members from our program offices, as well as our IT department. So these are my only friends that I've had along the way on this journey. There's another one that I mentioned last year, the interagency our users group. This was a grassroots group that we formed in 2015. We have no official standing or sponsorship, which I personally think is great. We just come together because we want to share. We have members from many use agencies and also the department of defense. And we've just been recently joined by our colleagues in stack Canada, which is really exciting. And it was through some of my strong supporters on this interagency our users group that we got connected with Joe Riker at the our consortium. And he has just been a fantastic friend and colleague. And he's been so patient with me as we kind of forge away ahead. And in this through his support and others that we've established a group, a new users group under the sponsorship of the our consortium called our GOVES. And this is our GitHub site. We also have a meetup account. And we're just getting established. Okay. So. We don't have, you know, we're just kind of trying to put it all together here. And just to note that our GOVES is open to everybody. We really look for international members. And just anybody who's interested in. You know, are used in either the government or with official statistics or government data or what have you. The interagency our users group would still going to continue. And we think that group will be more government folks because the this group will maybe wrestle with issues that would only be of interest say to somebody who's, you know, a good government employee. So, you know, how did we get through the the IT regulations and that type of. So we're really trying to solicit ideas for our goodies. We haven't had a meeting yet. Our first one we're going to try to have in January. And hopefully that one will have a solicit ideas for what we might do in the future. We're hoping that local groups of our GOVES can form local meaning geographically or maybe by areas of interest. And we're thinking about maybe having the our GOVES sort of sponsor if you would an Arkansas she and working group. And some of the ideas for that are listed here. So I hope you get involved feel free to contact me for information on any of these things are GOVES the interagency our users group or even just our BLS efforts using our."}, {"Year": 2021, "Speaker": "Marck Vaisman", "Title": "Creating and Managing Your University Course with R", "Abstract": "Did you know you can use R to create and maintain teaching materials, including slides, assignments, exams and even a website? This talk will illustrate how several R packages -including but not limited to {xaringan}, {rmarkdown}, {distill}, {xaringanThemer}, {ghclass}, and {Rexams}- are used in preparing and maintaining  materials for courses I teach at Georgetown University and the George Washington University.", "VideoURL": "https://www.youtube.com/watch?v=uYa798eNB7g", "id0": "2021_11", "transcript": "Our next speaker, I've known for many, many, many, many years. And it's hard to decide what I would like to introduce him as. There's so many fascinating things. Aside the fact that we're twinning today, when he comes on, you'll see we have the same shirt and practically the same glasses. But also, perhaps this is in his attempt to up his hipster street cred. He started to build his own bikes. He even builds the bikes like the wheel from scratch. You told me how you do it like you. The spokes are just giant screws and you could screw it up. He gets a frame, he builds the wheels. He does the whole thing. It's quite impressive how you can build a bike. I ride bikes. I can't even fix them up. He's building it from scratch, which is really, really awesome. So please welcome giant R nerd and bike hipster, Mark. So I am going to talk to you about today about basically how I use R to manage all of my teaching. Well, R teaching because this is really joint work with Abhijit and I'll get that in a second. But how I use R for creating content and managing my university course and doing a whole bunch of different things. And obviously the impetus for this talk is as many as those that know me know, I've been teaching at Georgetown and GW for several years in the data science program at Georgetown and the master of physics analytics at GW. I've been teaching a big data class for several years and now I'm teaching more. So more recently, I've been teaching a data this class. So I'm going to share with you what I've done. Again, I'm Mark Weisman. I am a cloud solutions architect at Microsoft. Our nerd is Jared said meet up founder data community DC founder and just all around kind of data geek. So what I'm going to show you today is I'm going to show you a little bit about using sharing and I know other other speakers. I've seen some of the other slides today and many people are using sharing in. I'm going to go into a little bit more detail just because I'll just share with you the tips and tricks that I've learned along the way to make kind of nice really slides. Same thing here is I'll be showing you about the sharing in or sharing and I'm not really sure how to pronounce it. Sharing in the mirror and sharing an extra packages. I'm going to show you how we build the course website with the still. And then I'm going to touch on these two other packages a little bit because these are packages that we've either looked at or used or are looking into to incorporating into a workflow. So package called exams and a package called GH class, which is really a wrapper around the GitHub API for doing a lot of rubricle stuff. And then I'll talk about some of the challenges that I've found along the way. What's the inspiration? Well, like I said, as a as a as someone who's been teaching now for a while, I'm always striving to become a better teacher and I always I'm always striving to have good materials. You know, I want I want slides that are clean. I want slides that are easy to understand. I want slides that are easy to maintain. Long gone are the days of doing massive PowerPoint slides or at least I haven't built. I have to build them for work because I work at Microsoft and I wish I could do more stuff with like something like this when I'm working on it. But I think I've been tinkering with some of the many packages for doing slides over the year. So beamer like our I started with our sleeve a long time ago and try to build beamer presentations and that, you know, just writing late tech is just a real pain in the butt. You know, played around with Mark down with to build some of the I think not reveal there's some of the other web frameworks. But anyway, I've been using sharing for about two, three years now. I really like it and also also like, you know, some of the things that we see out there from the R studio community. Excellent educator. So folks like Gary, Adam, I think I'm pronouncing his name right. Alison Hill, Monet, and Jenny Brian. You know, these these are folks that really publish a lot of great material and I've drawn a lot of inspiration from from these materials. And so, you know, again, just trying to incorporate best practices or what I think are best practices into my teaching. So I'm going to start with sharing it. So obviously right so why sharing it. So we know sharing in is a package that was written by you and basically it takes our mark down and it converts it into really beautiful. I think it's revealed. I'm not remember the underlying JavaScript framework but, you know, it allows you to incorporate code and stuff like that. Why sharing it right because it really I think it renders beautiful slides, of course, using modern web technologies using CSS using kind of the modern web stack. You can combine code and output. I mean, we know that right from our mark down. It's code based version controllable. So again, like I said, I really don't want to get into maintaining slides that much. So I want to try to have something that I can just manage and maintain and not necessarily rebuild from scratch. Why sharing it because it's web based. You can host it within a website. It's easy to display again using a browser. You don't, you know, you can just display using a browser. You need a server. But that's a whole other thing. You might need somewhere to host it or you can actually even run locally. The thing is why the way I use it is I really want to use it in building modular content. So I don't get to that in a second. But one of the things that that personally I like about using sharing in is that it really forces me to think about the content and the layout of the things that I want to show. You know, rather than just like doing drag and drop and just throwing everything to a slide and have a slide that really is not. It's just not pretty or it has a lot of material. Like it really forces you to sort of stop and think. And then we don't need to leave R and I always like to reference Jared's old talk of like R for everything where you can do everything from within R and as we go along, it seems that we can do a lot more with R these days. So obviously we're all our lovers here. So I don't need to tell you that. Some of the challenges though so as long as you know as much as it has it's good things it also has it's not a drawbacks but there are definitely challenges. So the first one is there is a learning curve. So if you've just written plain markdown. That in and of itself is not a big deal. It's just kind of how do you incorporate the there's syntax with the three dashes and like the difference in sort of things and just kind of how to get everything controlled. But the other thing is even though you don't really need to know CSS for display. It really helps if you do just because you could obviously tweak it. Going back to like forcing you to to think about what you want to show the one of the challenges is any sort of graphic that you want to put into your slides. You need to have an advance right so it's not like a PowerPoint where you can draw a circle or draw a square so you can't really you can't I mean I there might be some JavaScript tools that allow you to like draw on the man. But you know besides the point I have to go and think through what I want to show so if I'm doing screenshots I have to go get the screenshot save it as a PNG or gift or JPEG or whatever. And then they're like just have the graphics in advance and as I'm thinking of building the presentation. You know, of course, adding more material to my images folder. No wizzy wig right so obviously your writing code and you kind of go into this right render right render right render until you get it right. Sizing the draft graphics can be a little bit tricky. You know with experience you can figure out like you kind of know the size of the graphics that you want to show and embed in the slides. And then like it can be really slow for me writing a nice set of slides. It takes a long time and I really struggle with getting content from my head onto paper not nothing new particularly. But you know as I'm doing things I pull in images and the resources thing so those are the things you know these are these are my thoughts. And you know please I'm happy to hear comments or thoughts if you've been using sharing and kind of what you find good and bad. So obviously you know like everything in R the defaults are actually pretty good. So when you build a slide with the defaults this is what you get. Right this is the this is when you start a you you go to our studio and you open up a new R Markdown slide and you do this. And you choose the sharing and template this is what you get you know you get a slide looks good you know it's pretty. It's kind of black and white this is just the default template that our studio that's for you. So I don't love it. You know I want to make this prettier I'm pretty is the right word I just want to change it so what I do right is. I use sharing in theme and sharing an extra and I'm going to show you how how I actually do that. So basically this is what my lecture template looks like right now. So I'm going to go through these are just sample slides. So this is the title slide. You see the blue background I chose the colors I like looking at the color wheel so I try to use complimentary colors and that sort of thing and different font and sort of things like that but. This is kind of the headings you know level one little to level three four. That's a italic text and then the highlighted text is in a kind of a agenda. And then that's the same slide but with inverse colors right so when you use the inverse class you get the inverse background you can define what that is again how did I get this I use both sharing and. To create the theme. I'm sharing an extra actually I'll show you what sharing an extra does but here just a little bit more about the slide so other things that we use in these slides are incorporating tables so using cable K a. It's part of the network package so just rendering tables into HTML so they look nice. But then one of the things that we use which is really really cool especially for our data visualization class and like I said before I don't know if I would just on but I would just go up that. And then we also started the our made up with me many many a long time many many a decade ago. If not more. He he and I are co teaching this class so we do a lot of the development together and we do what we do is we actually teach the class in both our Python so we. We focus on sort of the theoretical aspects and and the how I'm sorry the what and then the how we do it in both an our Python so we do a lot of slides that look like this we. We show slide and we show something here, you know how to do something in our and then using the panels right what you get from sharing an extra. So you can show you can show similar code that does the same type of thing with within a panel and it makes it really, really easy to show the different that you're not showing them side by side but at least you can switch and talk about the intricacies of either language. So the first thing that we use a lot is just do something like this so we have something where this is this is a unit square data frame that I built this just some random numbers between zero and one. And then you know here's a bunch of GG plot code and then what happens is actually the output on the right shown over here. Alright, so we so this is just part of the template that we use. So finally you see here I'm going to highlight a couple of things you see how when I hover over the code. There's a little triangle on the left hand side on the actual code changes color that's provided by sharing an extra it's a don't remember the exact function, but this is really useful because rather than like selecting or something you can just kind of hover over and highlight a piece of code in a dynamic sense. So I also provided by sharing an extra so the ability to be able to do this. Okay. Alright, let me move on to the next slide. So this is what sharing an extra does for you so it gives you a lot of functionality if you look at the repo. It's not a crayon I think you still have to install it with on github but it basically it's a lot of rappers around advanced CSS functionality. You don't have to know CSS but if you do it makes a lot easier but Garak has written these packages both sharing and theme or sharing an extra. And what we use right is the panels and attack you on so we also use tacky ons tacky ons is like advanced CSS, which is coded with like a dot command. I don't have examples to show you but that's the other things that we use for for this. So how do I jump so as you see over here right there's actually three themes in the slide deck. The first one is the theme of the slide that you're showing so the black background with the purple. The second is the theme that's inside this default and then the third is the theme of my of our of our template so each of these was created with a different theme. How do we create theme or how do you create theme so it's really easy. So I'm going to use library sharing and theme or and then you can use either pre built themes and then override some of the faults so the one I'm showing you here today is this one right here what I did is I ran the code. This actually outputs I created out file called art of 2021 CSS and thing is once you generate a CSS file you can use it over and over again you don't have to have the share in theme or code in your presentations every single time you can all you need to do is create the CSS once and over again and I'll talk about that in a second. This is the theme right so on the right hand side you see the yaml header for the file so this is the yaml header for this slide deck I'm showing right now. As you see here here's my CSS which I generated before and then I'll get into other things I use other non defaults and I'll explain what those are in just a second. Okay. So strategy for building slides, like I said I really want to focus on building modular content. Because I don't want to have a heat you know when you start writing these really expensive when you have 30 40 50 you know even like 60 70 slides. It can be a real pain to manage so the way the strategy that we've adopted and I've been using this now for quite some time I found a blog post I have a link to it at the end that showed you how to do this and I just found this phenomenal. So what I do is I actually write the slides in a modular approach so I write kind of blocks of slides by topic. And then so if one lecture I'm going to have multiple topics I have a top level slide top level RMD which calls child RMD files over here. So basically this is what the code would look like right so here this will be the yaml header for the top level. And then what happens is all I would need to do semester to semester right is just change the yaml header or something along those lines. I don't have to go back and edit you know or find anything here so the way this works is you have a top let's call it a top level RMD. And then you run this art chunk what you say our child equals you know topic one dot RMD topic to RMD. I found this to be really really useful and helps you actually organize your your content in a much better way. So this is kind of around sharing and I have some additional thoughts about that later but those are kind of the things so create your own theme. Use modular content and you know as you do this you have to kind of understand the the tricks of incorporating images whether you're going to use an HTML image source or you're going to use the the knitter command which I always forget what it is a really easy. Alright, for distill so one thing is making the content for for the slides for the lectures the second thing is hosting is now we at Georgetown we use canvas. And LMS at GW we use blackboard another LMS. What I've done and I kind of see this being done elsewhere is is many structures just create a website. So this poses a couple of different challenges one is you need to have the content in the LMS but you also want to have the content separately so managing content on both sides can be kind of a pattern. But anyway, how do we generate the website so let me show you this. So we use distill so distill is a I guess distill is a is a framework for scientific publishing and it's a format somebody created I don't know who created it but again using modern web stack. It's clean it's it renders easily and then you can use a lot of helping function so you can use that you know our distill package for our leverages the distill framework if that's what it's called to build a website and the defaults are actually pretty nice so what I'm going to show you here is again what is this still I just got this from the distill website over here but then this is what our website for the course looks like so this was actually generated this is hosted on Netlify. You could either do this hosting this on GitHub pages or Netlify we chose Netlify because you can't really host the distill GitHub pages is static rendering and this is static rendering to but it's it's a lot more flexible so we're hosting the website on Netlify with distill. But you know as you can see you get kind of a I don't want to be a web developer like I just I just don't I just want to do something that looks good that's accessible that's easy to navigate and that sort of thing so we have the calendar we have the syllabus right and then here like when we go to the slides you click on under given wake we probably want to do a better job of maybe not calling it week by week but something along those lines so you know here you see the slides for the for a class and this is all hosted on the website. So how do you start with a building a distill shell library distill create website my website and so on and so forth so it's pretty easy that gives you the scaffolding for building the website. So how do you create the navigation so you have to have the underscore site yaml file for the website where so this is what our navigation looks like we have the the left math bar over here we've got the menu stuff over here the only drawback to this is we actually have to edit it every time we have new calls. So if we have a new content we actually have to go in and get it edit the site yaml which is can be a little confusing sometimes we add a slide and we forget to add the link so you know we are sick for 13 and we didn't do that so we actually have to go back and do that the content is there it's just it's not linked. How do I bring it all together so we created a structure a repo is all this goes into repos in a github repo. We designed it is at the repo itself is private. But because we're using not Lefy right you can actually host if you need a premium account if you're going to use github pages to do github to host a file with github pages and that will fire your your replicas will be private. The website obviously is going to be hosted and shown. But the way we designed it is we have you know this is a repo for the class so we've got admin here we have our roster a bunch of other things that we need to have. quiz bank this is where we use a package called our exams to generate some questions that we uploaded into canvas for the online quizzes slides is where we actually. This is sort of our current architecture, this is had many iterations. But and then obviously the read me which is the read me repo the site yaml and then the index, so this is the site yaml for the website, this is the index rmd for the website, so the index rmd is at this top level directory. When you render the website everything we you know everything gets actually rendered in the public website directory, but the index rmd has to be over here obviously any rmd files related to the website itself. Within the slides directory right, this is what we do so we create a slice directory like lecture one lecture two and so on, and then we have a directory called format. Again not sure if this is the best way to approach this because within each slides directory. I'm sorry within this within the this upper level directory we have a format directory in that format directory we you see it's at the same level hierarchy as the lecture lecture directories we have out the output yaml. We have a bunch of setup options rmd and a bunch and CSS of anything. I'll get into that in a second in the lecture directory, we have a sub directory for data if there are any data files that we're reading in a sub directory for image if we have any images that we're reading in so obviously like with the lecture sort of self contained, but you can have, you know still top level r markdown and a child level r markdown. So for the setup options we so what we do here is in in this directory. So as you see here in the lecture directory, what I do is I create a sim link to the output yaml in the format directory that way all of my slides have the same format and I don't have to copy over that output yaml file because if I change it. If I copy it over and I change it I have to change it everywhere this way. I don't know if there's a better way to doing this. I'm sure there is, but the way we've done it is we just have the formatting in the format directory. So everything is similar to that format directory. So if something changes in the formatting all and we have to rebuild the slides, obviously, but every, you know, it picks it up. These are some of the setup options in the rmd again the way I treat this is I treat this as a child. So in every sharing and top level I read the setup options here. I have some some things I've learned to use along the way like the nitter options and some of the different options. We render a lot of HTML and JavaScript content for things like d3 and that sort of thing. So a widget found out what we need these HTML tools dot preserve dot raw. I'm not sure what it does, but it needs to be there to work. And the other thing I do here is I read in all my different packages. This is the output yaml. So what I normally do is we do We do this we do self contain equals true. The reason for that is because that generates a single HTML file for the output. Now those can get really big if you have a lot of that's kind of the drawback is those can get really big and they could cause some issues with your GitHub. I do the incremental. We don't use the incremental slides to change the slide number like the 16.9 ratio and that sort of thing. So that's kind of how we create the content for for the website and for the slides. GitHub we use GitHub for the assignments. So for GitHub, right we use so what we've been using is we use a GitHub classroom and GitHub classroom is basically an automation platform on topic. GitHub that allows you instructors to create an assignment and then students accept the assignment. And what that does is it creates a copy of the assignment using a source repo. It sets all the permissions. So you can do that via GitHub classroom through the website like over here. But you can also do that with GH class and we're actually starting to investigate the use of GH class because it's within our and we call the GitHub API over there. But you know why do we use GitHub classroom because you know that's you need to know version control like you just have to and it shows a real world workflow. It's easy to use. You know we can use pull pull requests. We don't really use those because the grades actually go in the LMS. So as you see we're trying to manage a bunch of different platforms separately. Exam so this is a package that allows you to create using markdown files or. Or late tech. You can actually write questions that you can then import in this specific format is called QTI, but then you can that's kind of a standard format and then you can import those questions into the LMS so we write the questions just using our and and late tech and. Or our markdown and then load the questions up into canvas. It's a little bit time consuming because you have to understand the syntax. But again we want to version control these things like we went just in the leverage this because at the end of the day these are all text files. Alright future work and challenges so there's a lot here to manage a lot of the workflow is is manual so we actually have to generate the website it's all in the same directory same repo we have to generate this website in one set of files we have to generate the slides in another set of files. So we're thinking through ways to like fully automate this where once you like say you create an RMD file or you need it or every time you every time you knit the website it picks up new content like there's all these sorts of things that you need to do we have to add the links to the site Yammer. You know that's the kind of thing that we want to do I would love to integrate this with the LMS. That's really not up to me that's up to the school so the school needs to give us API access to the LMS which. Always not going to happen. And then you know figure out the best kind of structure for the repo especially because we have all this modular content what happens is when you're trying to render files with sharing in and you've got content in one place and images in another place and then you have a website hosted it just you have all these reference these the when the HTML is rendered I think it uses a relative links so if you have things all over the place it your stuff doesn't render right so we need to find a way to do this in an easier way. Here I kind of have a bunch of useful resources these are all of the blog posts that I use to learn a lot of what I showed to you today. It's great content so there's Gary's website there's mannez website which is linked over before there's Allison Hills introduction to sharing again which is amazing. And then again I will post the slides on github and I will send out the link with Jared. Folks I hope to see you next year in New York and in DC. I think all of you know that I'm Jewish so in Judaism right at the end of the major holidays there is a blessing that says next year in Jerusalem well I'm kind of saying the same thing here like next year either in New York or in DC so. Thank you very much."}, {"Year": 2021, "Speaker": "Lauren Lombardo", "Title": "Building Government Platforms", "Abstract": "Public sector organizations are increasingly turning to platforms as ways to improve service delivery while reducing costs. However, the term \u201cplatform\u201d has been used to describe several different architectural designs and operational approaches. Decision-makers need to understand how their selected architectural design and operational approach, which are constrained by government structures, will impact their implementation of government platforms. Without a clear definition and an understanding of the technical decisions that must be made it is impossible to responsibly build and implement public sector platforms.", "VideoURL": "https://www.youtube.com/watch?v=TfGegZqaibA", "id0": "2021_12", "transcript": "Our next speaker has moved, I want to know the timeframe, has moved 28 times across nine states. I have questions like are these small states or big states, how many times in each state, how many time, over what time period this was, where you being chased out by pitchfork. So we have lots of questions that we may or may not get answered from Lauren. Hi everyone. I'm so excited to be here today sharing some of the research I've recently done with Microsoft's worldwide public sector team on how to implement effective government platforms. Before I start, I'll answer one of those questions, which is the 28 moves is across 27 years. So lots of moving and we'll read the rest of the details of mystery for now. I'm Lauren Mumbardo. I'm a graduate research fellow at the Harvard Kennedy School where I've been studying technology policy and digital government and a variety of ways to improve the way that government delivers services using improved technical systems and modern technology. As part of that work, one of the things I've spent a lot of time on is thinking about government platforms. Specifically what should government platforms be thought of? How should governments think about using developing and implementing platforms? Is that different than private sector platforms? And once a government decides that they want to use platforms or platform ecosystems to better provide service delivery, how should they go about doing that? What are some of the big questions that they're going to have to have answers to and who are they going to have to convince? So with that, I'll go straight into the presentation and I am always happy and willing to talk about this so if folks have questions or want to sort of chat more about this after the conference, it would be very interesting and happy to do so. To start, this research really centered on the idea that governments need to have a shared language for what they think platforms are and how they want to build platforms in a government ecosystem. Government platforms I found in this research are not really a new concept. In 1086, William the Conqueror documented the people and land and buildings in England and sort of created this national registrar that was used for tax purposes. That's pretty broadly considered to be one of the first government platforms. Obviously not what we would think of as platforms today, but as we start to think about how governments can make more digital type platforms, some of the same principles apply. We have some core components that we built on top of, having some reusable systems and having real policy input implications for how services are delivered and how people interact with the government. Also governments can receive a lot of benefits from using platforms. We've seen this to be true in the private sector, but these benefits are really contingent upon how the platform is built and it's really important for developers and policy makers to both have a similar and clear vision for what government platforms should look like. That would allow them to have really specific conversations about what the government needs and what should be built and how. So that being said, the first thing that we did in this research was really think about how we wanted to define what a platform is and isn't. As part of that process, I found that a lot of times platforms are defined as levels of abstraction. However, it became really clear that while thinking about platforms as a level of abstraction is interesting and frequently partially correct, it's not necessarily nuanced enough to help government decision makers and policy makers fully understand what they are going to build and implement. This doesn't tell us enough about the governance choices that are made during this development phase, which is really important when you're thinking about the scale and the types of change management projects that a government might be looking at. Just further, not every abstraction tool should necessarily be considered a platform. Platforms, products, and services all abstract away some part of the technical infrastructure in some way. And platforms tend to have a really unique sort of feature where you also have compliments that are built on top of that technical infrastructure and operate within this ecosystem and beat off each other. And so as we're thinking about building government platforms at scale, it's also really important to understand how platforms can go beyond just abstraction tools. This led to a working definition of platforms as ecosystems, low and high-rise components, with specified and stable interfaces through which one accesses those components. This is a working definition of platforms that is used in this research that came from about three dozen practitioner interviews with folks who are building a platform in government or otherwise dealing digital government work and a pretty comprehensive literature review of relevant topics. We've come to this definition as sort of this three-part ecosystem in which all three of these parts are equally as important for something to be considered a platform. And if something doesn't have one of these parts, frequently something might not have a system of high-variety components that operate within this ecosystem. It might be an abstraction tool, it might be a really interesting product that helps governments still have their services in really interesting ways or otherwise is quite powerful. But for the purposes of this research, we're thinking through how governments should be building an operationalizing platform. It's just one platform. This definition isn't necessarily just about what it is for the government platform. It's designed to give us information in language that we need to have policy makers and developers work together to discuss how to build the desired features within these parameters. Just to expand on the definition a little bit to make sure it's really clear before going into the next part of the presentation. This definition sort of at a high level can be matched to a component of the iOS operating platform. Their low variety components is highly reusable elements from the ecosystem like iOS that sort of don't change as frequently in a core of VApps and compliments both on top of it. There are high variety components, sort of the number of components that are curricular like applications, and there's a stable interface like the App Store. Their language applications can be marketed on this platform ecosystem. That's what makes the iPhone sort of a formic system in a way that is not true in products and services. So using this definition of platforms, it's interesting to think about how governments might decide to build a platform ecosystem. And once types of choices they would need to make to ensure that they're building something that's going to meet their own goal. There are ways to build something that has high and low variety components in stable interfaces in a variety of ways that meet not only the goals of the government but also the real and perceived constraints that come with doing technical work in a government space. So the things that influence how this platform might be built in this way are first and primarily technical and policy choices. Some of these technical and policy choices should be interface, be open or closed. There are a lot of decisions that go into why a government building a platform ecosystem might want to have an open for a closed interface. And that decision is going to ultimately dictate how powerful and useful that platform ecosystem is within the government. For example, a platform that is maybe built by the US digital service or by AT&F might have a closed interface not because they don't want to have an increasingly large number of applications or components that sit on top of their infrastructure but because there are government structures that prevent them from allowing anyone to use their platform ecosystem to provide applications and compliments to the government. You also need to decide if you want this platform ecosystem to be mandatory or voluntary. Is the White House or is a centralized government agency going to build a sort of shared platform service like voggin.gov or the Federalist or something like this and are they going to mandate that that platform ecosystem is used across the government? If it's mandated then the way that you might build a government platform is going to come across differently and going to maybe follow different structures or technical purposes then if it's voluntary and you're trying to entice folks to join and transition their existing government infrastructure to your platform. There's also a greater constraint on the ways that you might charge or fund your platform ecosystem. If you're just going to use it within a single department that can be built differently and thought of differently than if you're going to try to use platforms across the entire government. So this is just an overview of sort of the variety of technical and policy choices that go into the decision to specifically build platform to then a government ecosystem. There's also this concept that you need to understand how you would operationalize the platform which can be quite different in the government space than in the private sector space. The government, the research showed that there were really three ways that folks operationalized platforms to be used in government. So you either see people using a platform ecosystem meaning that they use it within a single department maybe this really just translates to someone moving over to AWS or Azure or some other type of platform that they either build or buy but it's pretty concentrated within a single agency or department or program. You could share a platform ecosystem which really speaks to these shared services and reference to a web blog and a cloud doc or something that is built either within a department or in a centralized area but is really designed and meant to be shared across more than just one department or you could attempt to be a platform ecosystem which is frequently referred to in the government space as government as a platform. The misnomer of government as a platform is that it should be the end goal for governments who are looking to use platforms as a means to service delivery. However, I feel that this is frequently not the best or the most correct way for a government to operationalize a platform approach. It frequently requires a much greater leap in technical capabilities and political power and influence and doesn't necessarily can prevent all the benefits that could be achieved from having shared services or even just from getting departments to migrate to AWS which in some cases would be a great leap and a great increase in government service delivery. So all of that said, there really are three ways that you might operationalize the platform ecosystem that you choose and whatever this operational strategy is, it's really going to dictate how the platform might be built. So government as a platform really needs to have an ease of integration for broader interpretation and adoption across the entire government. You would want all departments and agencies to be able to access that infrastructure and that data and to build and talk of it in a way that wouldn't be true if you were just going to migrate a program to the cloud or just going to try to get all of the departments to adopt the same login.system login.system like login.gov is attempting to. So what influences the technical choices you might make and the operational choices you might make when building a government platform. This is really, I think, what makes this technical work so interesting and the government space is because there are government structures that really create a different set of opportunities and strengths and weaknesses and this all changes what's possible Governments have to work within these structures when thinking about building platforms in the government and it's important to figure out how these change, what's possible and how that constrains what you're going to be able to build. For example, budgeting constraints, you might find it more difficult to identify funding for a centralized platform because there won't be as direct impact towards a specific service and it won't be as easy to identify cost savings or value of that project or in some instances in the interviews I conducted in both the US and the UK government systems specifically we found that attempting to build a centralized government platform resulted in a centralized budgeting authority saying that's great and that's really interesting who is going to use it so that we can take money in their budget fund the work that you're going to do. This obviously is a really challenging way to convince people to use a tool that you have not built yet if they are going to lose money up front and be committed to whatever you are going to provide to them and so there are real structural challenges to the ways that you would fund a budget or budget for a government platform in this way that could indicate that you can't do government as a platform or a shared platform service right away and you might need to within a single department look to just migrate some small programs of services onto a platform ecosystem or on inclusive of cloud infrastructure in a way that can sort of show that there's progress and that there's a need and then scale up. That's just one example of how that government structure plays out in this way and constrains the type of work that you can do to build an operationalized platforms in the government space. That same structure could really come to fruition in a different way in a different government space or even in 15, 20 years from now could be viewed differently in the US or the UK context. Another structure that was really interesting that came up in this research was the idea of dependencies and accountability and ownership. Incentive structures in the government also play out differently than the private sector and people have a need to stay connected to the infrastructure that they're using because they have final responsibility and accountability to the executive or to their boss when something that they're overseeing breaks. This can make it really hard to want to rely on a shared platform or a shared service and certainly really difficult to want to rely on or create government as a platform in the way that it's frequently defined. And so that might be impossible to work around or it might require sort of a shift on the way that we think about government accountability and incentive structures. There's also this need for political support, of course, and that's going to change what is able to be done and implemented. Really the point here is platforms aren't new to government. We've been using platform-like concepts for a really long time in the government space and government should use platforms because there is an increasingly obvious economic benefits and savings that can occur when you are able to reduce the redundancies of cost and access the economies of scale and scope that come with leveraging platform ecosystems. But the ways that governments can build platforms can be different from the private sector and has to be done in a really, really intentional way. This is because all of the technical and architectural decisions made while platforms are being designed and developed are really political choices. Developers and policymakers need to be on the same page about what they want a platform to look like, what types of technical decisions need to be made to allow that platform to do or to not do what the government needs, and how they want it to be operationalized such that it's being used in the most correct and efficient way. This work can be really hard and that's why this research primarily focused on creating some shared language and a shared definition around what a government platform might be and what it's not. With that, I'd just like to end with that definition being that a platform really is a three-part system of high and low volatility components that are accessed through stable interfaces. And as governments start to coalesce around sort of a shared definition like this or or something else that allows developers and policymakers to have a shared language around what exactly should be implemented, we'll be able to really move forward in thinking through what a government platform that you should be and how government platforms can improve both service delivery to constituents and internal government operations. Thanks so much. Like I said at the beginning, I'm really interested in this research and I've been working on this for quite a long time both at the Harvard Kennedy School and with Microsoft and would really be interested to continue the conversation with folks that are interested or that have questions and to expand upon what has really been a condensed presentation to fit this conference framework and would be happy to share the full report or to talk in greater length and detail about how we came to these definitions and who we talked to to come to these definitions and really how we have thought about the technical and operational choices influenced by those government structures that define and constrain what's possible than thinking about building government platforms in the public sector. Thanks so much."}, {"Year": 2021, "Speaker": "Madhava Jay", "Title": "PETs: Remote Data Science Unleashed", "Abstract": "Ever wished you could get access to more data for your data science problems without the painful and slow process of existing data access agreements?\n\nData Scientists are limited to the data their organization has painstakingly acquired a copy of. Data which often requires phone calls, contract negotiations, lawyers and special onsite security policies just to access. Getting to analyze personal data can take anywhere from weeks to months even if you understand the whole process.\n\nPrivacy Enhancing Technologies (PETs) are bringing that time down to seconds while giving data subjects even stronger privacy guarantees.\n\nIn this talk we will:\n- Examine the privacy problem and the field of Privacy Enhancing Technologies (PETs)\n- See how Syft\u2019s Automatic Differential Privacy and Secure Multi-Party Compute feels like magic\n- Hear about OpenMined\u2019s free online Privacy Focused Data Science Courses\n- Learn how to participate in Federated Networks and fuel tomorrow\u2019s life changing discoveries\n- Discover why being a nonprofit foundation is key to OpenMined\u2019s Mission", "VideoURL": "https://www.youtube.com/watch?v=qVf0tPBzr2k", "id0": "2021_13", "transcript": "And our next speaker has a Golden Doodle puppy. But a puppy in itself is interesting, but he is teaching his puppy named Pickles to speak using a soundboard and augmentative alternative communication. So I'm sure we have lots to learn about that or his talk we probably won't learn about both. So please, everyone give a nice warm welcome to Mahatma. Hello everyone, welcome to my talk, Remote Data Science Pets Unleashed. My name's Madhava, I'm an engineering team leader open mind, I work predominantly on Sifting Grid and I'm coming to you from sunny Brisbane, Australia. In this talk, I'd like to discuss motivations for remote data science, meet two pets, DP and SMPC. You'll get to see Sifting Grid in action. Later, we'll discover our private AI series and finally, you'll get to hear a bit about what makes open mind unique. But I'd like to start with a key premise. We believe that our ability to answer important questions is limited because we can't access existing data. Existing data in another country, existing data in another organization or even in another department. And the solutions to many of mankind's most challenging problems exist, but simply that the access to train the models, the access to the data necessary to train those models does not. And there are many problems like this, but I'd like to drill into two specifically today. The first one is breast cancer. Sadly, one in eight women will contract breast cancer in their lifetime and tragically over 700,000 women a year die of breast cancer. One of the reasons why this number is so high is that if you today went into a hospital to get a mammography, the radiologist might look at it and they may give you a negative result. But there's a one in four chance in some places that you would get a false negative, which would mean that you would actually have cancer. You would go on to live your life for several years, only to discover later and your treatment options would be greatly reduced. Now, this seems like the perfect problem that we could just throw a deep learning model out, right? Classifier and solar. Unfortunately, in 2021, there's been a paper that's shown that 38 different AI breast cancer models out of those 36 performed worse than a radiologist. The remaining two were worse than two radiologists. So, it's our belief that the reason why these models are so poor is because they probably won't train on enough data. Now, if we've learned anything from the latest state of the art results over the last few years, that particularly for challenging problems, three million images, which is what the estimate of the current largest data set available is, is simply not enough images. Despite this, we can estimate that over the last 10 years, we've probably had about eight billion images collected. So, the data exists to train this model, but we're only using a small fraction of it, and hence we're getting sub-human results. And so, I want to hand it back this point, that solutions like deep learning models, they exist, but the access to train on the private data necessary to solve these problems, it does not. And there's another example I'd like to show, and that is global trade. So, currently, the United Nations has started a new pilot program where they're experimenting with some of the pet technologies that we'll be discussing in this talk, called the UN Pet Lab. And what they're looking at is a sort of important export trade data that crosses borders between their member nations. Now, in this case, you can see that things normally match. Every now and then though, things just disappear. On the left here, we had $3.6 million worth of straw, but on the American side, we're receiving $200,000. That's a huge difference. And this is a global problem. Up to five different member nations are getting together to try to solve this problem, all without having to share each other's highly valuable in secret trade data. And so, you can imagine in this case, the solution, which is simple arithmetic, already exists, but the access to this private data does not. And so, you might be thinking, well, why don't we just put it all in a big database? Why can't we take eight billion images and train, put it all in one database and train the world's best breast cancer model? Why can't we just put all of the world's private trade information into one database to answer this question? There are a lot of reasons why we wouldn't want to do this, but I'd like to focus on one in particular today. And that is that it's simply not convenient. So, imagine I wanted to solve this breast cancer problem. What I would do is I would need to go out and collect more data than anyone ever has before, right? So, let's see what that would look like. Well, I'm gonna have to contact the first hospital or get on the phone with their legal department, you know? And we'll have a lot of conversations backwards and forwards. And, you know, our lawyers are now gonna have to negotiate and this takes a really long time. Eventually, they're gonna be asking us, like, where are you gonna put that data? How are you gonna manage the risk? And ultimately, we may need to do some kind of background checks for our staff. And, you know, this really just doesn't scale. Imagine we completed this exercise, now we need to rinse and repeat this with slight variations, hundreds or even a thousand times. This problem is intractable. It hasn't been done despite the money and the willpower. And so, if you think about it, actually, before the internet existed, we had this same problem with public data. If I wanted to book a flight, you know, I wouldn't just be able to go online and do it. I'd have to pick up the phone and talk to someone. You know, if I wanted to find out whether a shop was open, I couldn't just Google it, I'd need to pick up the phone and call them. And if they didn't answer, I just wouldn't know the answer. And so, what we kind of need here is the same self-service model, but the internet has provided for public data, but for private data. So that data owners don't need to be on the phone, you know, sitting, standing by, waiting for your call 24 hours a day, you know, or answering lots and lots of emails to do with legal negotiations and contracts, just so that data scientists who want to study the world's, you know, private data to solve the world's most challenging problems, so that they don't also need to have legal agreements and contracts with hundreds or thousands of different organizations. So, you know, the thing we're talking about here is kind of called remote data science. And I'd like to drill in a little bit and show you sort of how it would look. So, imagine you have, you know, your sort of organization with your private data, you upload that into a server, we'll call this a domain, right? And then you have your external data scientist, and they are, you know, sort of interacting externally over the network with this server. Now, they're able to ask questions and answer, you know, the questions they have for problems by your API without having to copy any of the data. They wouldn't just be interacting with a single organization, they would be on a sort of like federated network of global private data. And you might be saying, well, we have an internet and we have servers, why can't I just take my data, put it into a server, create an API, give a username and password and be done with it? Well, unfortunately, this is a problem called adversarial attacks. So, I want to give you an example here of how this looks. Imagine we have a table of data containing the COVID positive results for a bunch of people, right? This is private health information we need to protect it. But we also want to allow data scientists to be able to study it, right, in aggregate, and get some population statistics. So, imagine we have this API and they can say like, okay, how many people have COVID, right? In this case, we've got 11, that's 11 out of 20, gives a 55% answer, right? But what if we have a data scientist who's a little bit naughty, a little bit nefarious, and they want to learn something about one about subjects? If they were able to perturbate the query in such a way that they could exclude a particular subject from the data set in the query, then they might be able to learn something about that subject. So, imagine in this hypothetical query language we have, they could do something sort of like, you know, names greater than or equal to B star. If they're able to do something like this, they could exclude someone like Andrew from this data set. So, in this case, we're gonna have, you know, 10 out of 20s, the result has gone down. Now, if we step back and think about this, if we're able to perform a query that's not modified, we potentially don't know anything about this subject that we're trying to uncover. However, if we were able to exclude them from the query and compare the results, if that person had a positive result, and they were removed, there would be less of them, and so we could infer that the number, if it went down, that they did in fact have COVID. And conversely, if the number went up, we could infer that possibly they did it, right? So, you know, what's the solution to this problem? Anyone? Pets. Let's meet our first pet. We've got Laplace, the differentially private puppy, and I wanna talk to you about differential privacy. So, imagine, you know, I ask you to flip a coin. If it comes up heads, I want you to just tell the truth. So, in this case, if you have COVID and the result is true, then you say yes, and if not, you say no. And, you know, flip the coin, and if you get tails, what I want you to do is you're gonna have to flip it a second time. So, on this second flip, if it comes up head this time, you're gonna answer true positive yes, I have COVID, irrespective of whether you do, and if it comes up tails, you're gonna have to answer no, right? And this could be a lie, but it sort of depends on what your original answer is. Now, what this results is, you know, if you look at the ground truths and the answers that are given at any given output, if you were to uncover what an individual's answer was, there'd be just as likely a chance that they were lying. So, what this does is it gives us some interesting properties, right? So, one of our goals here is to ensure that statistical analysis doesn't compromise privacy. You know, output should appear identical with or without a given row. So, if we remove Andrew, it should look essentially the same. And you wanna give our data subjects a concept of plausible deniability. So, they can always say, well, you know, I was lying, even if you figured out what their answer was, right? And to make this sort of work across large amounts of different types of data, we need to be able to tune the amount of noise that we're adding so we can adjust for a sort of privacy budget. And most importantly, you know, this noise that we're adding, we're in control of it and we know how much it is, so we can just remove it, leaving the signal, allowing us to train models and do any kind of regular machine learning. So, you know, you can think of this, it sort of works on, you know, mathematical operations and functions or database queries. And finally, there's this concept we have of a so-called accountant. And what that does is it allows us to internally track the privacy of individual data subjects between queries and even between data sets. So, there's a problem here though, you might have realized, earlier I was saying, like, don't centralize all your data. And there it is, that's a central data set, right? So, what can we do about this? I mean, we could split it up and decentralize it, but then like, how do we query it? Like, that doesn't make any sense, right? That's ridiculous. So, does anyone know what the solution is? Petz. All right, let's meet our second pet. We've got Beaver, the SMPC cat. SMPC stands for secure multi-party compute. So, imagine that Andrew has the number five and he wants to have somebody else multiply it by two and give them back the correct answer without ever revealing the original number. That sounds like a riddle, doesn't sound possible, right? But it is. So, what we're gonna do is we're gonna take that original number and split it into two secret shares that when you combine back together, equal the original number. We're then gonna get two friends. We've got Critica and George. I'm gonna say to them, okay, we're gonna give you a secret share each, hold onto it and don't share it with anyone. We're then going to say at this point, essentially, we've achieved encryption, right? Neither party knows the original value and they can't just guess it by checking different numbers, right? And additionally, we have this concept of shared governance to resolve the original number or the final answer. We're going to need to get all parties to work together to decrypt it. So, let's forward on the request we have to basically multiply by two. So, everyone's gonna go and do that operation and they'll keep the result locally until such time as we're ready to bring it back together. So, now we'll say, okay, let's bring all our shares together. We agree, let's add everything back. And you can see that the result 10 is the same as if we had multiplied the original five. So, if you think about it, models and data sets are just large collections of numbers which we can encrypt and which we can compute. If we go back to this problem from earlier, right, how would it be possible if I could try to solve this, right? Without getting on the phone, without doing legal negotiations and contracts, without being able to copy any data? Well, why don't we try some remote data science? All right, we've got our little friends and we'll go to our repository. Now, I know that this is an our conference and we are working on different language platform clients but at the moment it's predominantly Python. But if you pip install SIFT, right? And then you load up a Jupyter Notebook which of course is where all good data science begins. We're gonna jump in here to the first step. So we import SIFT. The next step is we're going to go looking for federated networks of data and we can use dot networks. And this is sort of like a Google search for private data. What we're gonna see here is a collection of different networks that we can join. So let's go look and drill into the first one here. We'll get a little connecting to that. And the next thing we wanna do is go looking at what domains or what websites of private data are available. So you can see here, Canada and United States, perfect. That's exactly what I was looking for. So I'm gonna jump in, I'm gonna get a sort of variable, a kind of handle to connect to these domains. And once I have that, the next thing I'm gonna wonder is like what is my current privacy budget, right? Now what is a privacy budget? Well, if you remember earlier, we discussed the idea that with differential privacy, we wanna be able to sort of like tune this noise. And we also wanna self-service model that means that the data owner doesn't need to be there just to answer our queries and requests and approvals. And so if we're able to somehow measure the amount of privacy that's revealed during a query, we could estimate the amount that protects the data subjects while also allowing you to automatically download your results without anyone being there. So let's have a look, we've got 200 privacy budget. That's awesome, okay, let's keep going. So now we're gonna go try to find the data set we wanna work on. So in this case, we have on the left, we've got the Canadian domain, right? And we're gonna go find the data set that responds to the American data set. And we can see we have some tenses in there with shapes, that's great. And on the other side, we're gonna go get the United States domain, we're gonna go looking for the data relating to Canada. And what I'm gonna do is I'm gonna try to get two variables that represent each side. Now, if this was normally in my local Jip and Notebook, right, these would be variables that would contain the actual tenses in memory. But we don't want that, right? We can't copy the data. So instead, what we have is a concept of a pointer. So think of it as like a proxy object. It looks and feels exactly like the original object in my Python environment, except there's no data inside. I can't see it, I can't copy anything. So the simple example here would be like, hey, why don't we just subtract one from the other? Then we can figure out if there's a huge difference or not, right? That should be easy in theory, but I would need to co-locate the data, I would need to copy it. So this shouldn't be possible. I'm here in Australia, I haven't made any phone calls or emails, right? And the data on the left pointer is all the way over in America in an encrypted government institution somewhere. And the Canadian pointer points to data that's over in Canada and a completely different computer system. So this should not be possible, but it is possible. And the way it works is, as soon as I execute this command, we create a shared multi-party compute tensor. And under the hood, it points to the original data. And just like you saw before, we're going to pass on this operation of minus. And both sides, both domains, are going to communicate to each other and agree on an encrypted protocol. They will encrypt their data, they will execute the operation that I've asked, and they'll store the results locally. At this point, we've achieved input privacy. No data has left either of these domains, right? So we haven't copied any data, but we've still managed to do encrypted computation. Now, the next thing is, there's no point running a query if you don't look at the results, right? But to make sure that our data subjects that are inside these data sets are protected, we need to apply some kind of differential privacy or output privacy. And so in this case, what I'm going to do is I'm going to tell the system, hey, I would like to look at this result in a secure way. And so I'm nominating some privacy noise that I'm willing to add. And what's going to happen is the accountant is going to look at all the data that I've used. And it's going to think, hmm, OK, this is how much you're using. This is how much privacy it would reveal. This is how much budget you have. Oh, you have enough. That's great. OK, so we're just going to subtract it from your budget, and I'm going to publish the results into the domain available for download. So now, at this point, I can take that with a simple dot get, pull back the shares, decrypt the result, and now we have it. There's a significant difference between these two things. So that's super awesome. And finally, I still have a little bit of privacy budget method. So you might be wondering, wow, this is pretty interesting. How do I find out more about this? How do I learn about the technology, what it means, all this terminology about pets? Well, we've created three courses that you can take to learn all about this. The first one's called our privacy opportunity. And it's tons of awesome video and quizzes that help you understand the terminology about pets and private AI. It takes you through the implications for society and governance. And I highly recommend everyone takes this. It's non-technical. And then our second course, which came out the start of this year, is called Foundations of Private Computation. This has got a whopping 60 hours of content on federated learning, cryptography, homomorphic encryptions, cure, multiparty, compute. You name it. It's in there. If you take this course, you will even know more about this specialized field than almost anyone else on the planet. And finally, we have a new course that just got released last week. It's called Introduction to Remote Data Science. And in this course, you're going to learn how to create your own domain, start it on your laptop or your own infrastructure. You'll learn how to take your data set and upload it in a secure way. And you'll learn how to do remote data science between you and your own domain or another domain. And finally, we'll show you how to join a network so that you can become part of the amazing discoveries of tomorrow. So there's three courses, all free and available now, on courses.openmind.org. Go join it today. And you'll be one of 8,000 people who have already started. And finally, I want to talk a little bit about what makes OpenMind unique. So OpenMind is globally distributed. I'm in Australia. As far as I know, we have people on every continent, except Antarctica. So we've got people in Brazil, in the United States, and Canada. We have multiple African nations, people all across Europe, Russia, China, and lots of people in India. And this is really important because we believe that we want to build tomorrow's federated global private data network, and it needs to be accessible to everyone in the world. It needs to be equal. This isn't something we're just building for Silicon Valley. And so it's crucial that we take into account all of the needs and requirements of everyone in the world. We're also a charity under Open Collective, which is a registered 501c3 in America. And essentially what this means is we can really focus on that mission. And I encourage you to check it out and possibly contribute some money. Another big, really important thing about what we're doing here is it's all open source. We do all of our development on GitHub. We have over 90 repositories full of all sorts of interesting experimental encryption and cryptography projects, as well as obviously our big ones like Pysift. And recently, I did a check. And there's been about 500 unique contributors to all of our code over the years. So come join us. Open a pull request. Everyone's welcome. And finally, we have a very active Slack community. It has over 13,000 members. You can go join it at slack.oormine.org. If you have any questions about this talk, it's a perfect place to go. If you're taking any of the courses, you can get support and help on there. I'm there. So come join the conversation. And finally, I just wanted to say the point of this conversation has been to give you a bit of an introduction to pets, to help you to understand that pets aren't just cute and furry. I think they're actually the future of data science. So tell everyone you know about pets and consider adopting pets in your organization today. My name's Madhava. You can find me on this Twitter and Slack contacts. And I just wanted to say thank you for your time. And I hope you enjoy the rest of the conference."}, {"Year": 2021, "Speaker": "Tommy Jones", "Title": "tidylda: Latent Dirichlet Allocation Using 'tidyverse' Conventions", "Abstract": "tidylda implements the Latent Dirichlet Allocation (LDA) topic model in a way that is fast, flexible, and most importantly tidy. Wait. Who needs another LDA implementation though? Tommy will talk us through what makes tidylda so unique and provide examples to stir your imagination on new ways you can use topic modeling in your own work.", "VideoURL": "https://www.youtube.com/watch?v=Pt6FYgFejvc", "id0": "2021_14", "transcript": "Our first speaker is one of your fearless leaders of this data community DC. When I say fearless, he is fearless in many ways. He got us about his past profession when he had to actually go and actually literally risk his life. Or he could talk to him about his love of ironing boards and his crazy expensive ironing boards. Today is a very special day. It is our speakers 40th birthday. So he is here celebrating with us at the R conference. Everyone, please welcome to the stage, Tommy. So hi everyone. My name is Tommy Jones as Jared said. And I actually believe I've had the honor to speak at the DCR slash our gov conference every year that the Jared's held it. In real life, I wear quite a few hats. I spend my days running back and forth between the statistics, machine learning and business communities. I work on the technical staff and an investment firm called ink util, where I help source investments in machine learning startups. As Jared said, I'm the vice president of data community DC, which is a nonprofit dedicated to building a community of practice for data scientists in the DC area. And I'm also a PhD candidate at George Mason University's Department of Computational and Data Sciences. And so that's sort of relevant today, that last one. So I'm studying latent air clay allocation to help develop more statistically principled ways to analyze language. And because language is an abundant and information rich data source. In fact, it's so information rich. It's literally the protocol that we use to convey information to each other. I dream of a world where we can measure ideas and culture through language with the same level of scientific rigor that we use to measure the economy, like unemployment or prices. Or the effect of a medical intervention, like the effect size of giving a medication to a treated group. And to do that, we need two things. We need a lot more science. And my dissertation is a drop in that bucket trying to develop some statistical theory for working with LDA models. And we also need tools that are intuitive and user friendly. And this is why I love the tidy verse so much. So drilling a little deeper into that. The current generation of language models work so well. And LDA is now about almost 20 years old. So why am I still studying it? There's a couple of limitations, the current batch of language models. First, they're inherently task based. In fact, they operate on something called the common task framework. So they're built for doing discrete things like building chatbots, document summarizers, part of speech taggers. But they aren't really fit for making inferences on populations from samples, this sample, this inherently statistical principle. Second, they're almost all deep neural networks. For all the predictive power they bring, they're basically black boxes when it comes to interpretability. And I would like to see text analyses enter the statistical mainstream and focus on what statisticians do best. This getting inferences on populations from samples, not just building better chatbots, although building better chatbots is important. I've taken to calling this corpus statistics to differentiate it from this common task framework based natural language processing. And so with the goals of corpus statistics in mind, LDA has some nice properties. It aids in interpretability and uncertainty quantification by embedding text into a probability space. In fact, when you look under the hood, this is sort of how we do all of statistics. We can lean on statistical best practices when we're using it since it's a Bayesian parametric model. And we can study it as a data generating process to help provide sanity checks when we're building models in the real world. And on the tool side of things, in recent years, R has come a long way in developing intuitive frameworks for working with data. And tidy text in a handful of other packages highlighted in red on the left side here do this for text data. And now back in 2015, when I released TextMiner, the text analysis ecosystem in R was not at all easy to work with. But since then, I've fallen in love with the tidyverse and the growing ecosystem of tidy text mining tools. And so in that vein, I'd like to introduce tidy LDA. It's a package for building late and dear clay allocation models using tidy principles. And as we'll see later on, it has some unique capabilities for transfer learning that are based off of my dissertation research. So for a quick review, here's an intuitive take on LDA. It takes a collection of documents there on the left that are full of words and splits them into two groups. Topics, which are now collections of related words and simplified documents, which are collections of topics. And these are represented by our variables of interest, beta and theta respectively. Here is a more technical take. I'm not going to get too much into this. But you can't have an LDA talk without the plate diagrams. So in addition to beta and theta, the parameters of interest, because this is a Bayesian model, you need these prior parameters for tuning, alpha and eta. And then they all work together in the model that's drawn by this diagram. But as I said, I'm not going to spend much time on that. But I will quickly turn to a demo. And so Jared, please pipe up if you don't see our studio instance. So I'm going to demo some basic functionality here of tidy LDA. First, I'll just load some libraries. We're going to pull data out of the project Gutenberg. So let's download the War of the Worlds here. Take a moment. And if we look here down at the bottom, we've got a table with three columns. And the middle column is really the most important one, which is one line, one row per line in the book. And so I'm going to throw some regular expressions at this to try and get two columns that calculate the, which paragraph words belong in and delete extra blank lines, as well as which chapter. And you can use paragraph and chapter to group some of these together for modeling. Then we're going to create this tidy table using the UNS token functions from tidy text. And so now it's one row in this table is one word for token. And we still have the paragraph and chapter indices there to work with. And of course, you know, I'm not going to do much with it, but before you do any modeling, you should start summarizing your data. And so this is counting the word frequency by chapter. So, late and dear clarification is a statistical model. So the first thing we need to do with it is to get a data matrix. So we're going to, this function will create what's called a document term matrix. Just to clear in the function there, a document term matrix has one row per document, one column per word or token. And we're going to build a document term matrix on the first half of the chapters in this book. And so we'll do that. And I printed the dimensionality here. So even though this document term matrix is, let me get my environment up here, is this weird DGC matrix up there. It still has dimensionality so 388 paragraphs 3285 words there. And so now we can get to modeling. We use the tidy LDA function here. And you just pass it your data, which is this document term matrix. It doesn't have to be this one. It works with several different commonly used formats for document term matrices. I chose 27 topics, that's just one per chapter here, and a few other arguments that we'll hand to the GIB sampler. I run that, you can see we have this nice progress bar on the bottom. If I print this after the console, it gives us some useful information. So we have an LDA model with 27 topics, 388 documents, 3885 tokens. So that's the same dimensionality as the data matrix we handed to it. You get the call here. So for some reproducibility, if you can hand it a model, you can at least see what arguments were passed to this. R squared, which is a metric, it is R squared. I derived it. This is some other research, but it's the proportion of variability in the data explained by the model. So you can interpret it very similarly to the R squared and linear regression. You get your top five most prevalent topics. So these are ones where they have the most words associated with them across the corpus. So the topic index here, the prevalence score, this is the percentage of tokens that are associated with that. Coherence, I'll talk about that in a minute, and your top five words in that topic. You also get the five most coherent topics. So coherence is a metric that's designed to mimic human interpretability. The coherence metric used in tidy LDA basically looks for words in a topic that are correlated with each other in a statistically dependent way. And so these are those most coherent topics. And if you want to get that information for all your topics, you can look at this model summary object in your model. And that gives the same information for all 27 topics there. So as I said before, one of your variables of interest is beta, and we have a tidier to pull out a tidy table for beta. So now this is, you've got this three column table topic index, your token, and the probability of that token in that topic. We can use that to create nice little plots. This will take a moment to render. I pulled this from Julia Silge's vignette on tidy topic model. So no original work here. It takes a moment to render. We get these nice bar plots. And you can see that all the topics are decreasing in terms of their top 10 most probable words. If I want to look at say topic 21, which is the most prevalent topic here, or topic 10, which is my most coherent topic there. You get that plot. We also have a tidier to pull out theta, which is our other variable of interest. That's the probability of the topic within each document or each paragraph. So you've got the paragraph number, topic number, and the probability of that topic within that paragraph. We could make similar plots to this one, but I'm going to leverage the fact that we know that this is these paragraphs came from a book. So we're going to have a little bit of a call in sequential order. So you can get this nice little time series plot. And what was it? Topic 10 was our most coherent one. So let's look at that. And you can see that there's sort of a burst here in the first half or right around the halfway mark. Or I guess that would be about a quarter way mark through the book where that topic's really prevalent. So we're going to have a couple of different types of books. But we can build a document term matrix on the second half of the book and predict topic distributions under the model. And so here looking at the dimensionality of this new matrix, we have 508 paragraphs. And now 5,017 unique tokens. So that tells me that the second half of chapters actually has more words and is longer than the first half. So we have all this predict function. It works just like predict functions in the rest of our hand at your model object, your new data, and some additional arguments you need to hand to the Gibbs sampler. I get the little progress bar again. And it creates here, if you can see my mouse on the right, just a matrix, 508 rows, 27 columns. And we can tidy that up using the tidier for theta here. And so this looks just like what we pulled out of the model, but with new documents, the probability of each topic within that document. And under the model, I can see how frequent that topic 10 is in the second half of the book and we see it has a bit more of an even distribution. And so right here, I'm going to just run this really quickly for time, but we can do a bar plot that compares the topic prevalences in the first half of the book versus the second. So in orange, we've got the distribution of topics in the first half of the book in blue is a distribution in the second half. They're pretty equal, but there are some topics that are more prevalent in the first half in the second, but because they came from the same model and it's the same book, maybe it's not too surprising that you'd get a similar sort of overall distribution for the percentage of words. I think turn back to the slides here really quick. And again, Jared, Chairman, if you don't see the slides. Over the last few years, there's been a paradigm shift in the field of natural language processing. Previously, researchers built models end to end on one data set and treat that's what we're doing today. But now researchers are starting from pre trained language models so they were trained on a large corpus of data. And then they take their data to this pre trained model and fine tune those parameters to update the parameter values with their own data set. And these pre trained models can be fit on hundreds of gigabytes of text data. They're these massive neural networks. And just the proxy for the popularity is we can see the number of stars in the hugging face. Transformers library, which is a repository for a lot of these pre trained language models. And something I'm doing as part of my PhD is I've implemented a model that enables fine tuning for pre trained LDA models. This is part of my PhD research. I won't get deep into the math here, but the gist is that you would use the weighted posterior of a pre trained model with the prior for fine tuning on your own data set. And this is implemented in tidy LDA and I'll give you a quick demo of that here. So moving back to our. So we have this refit function that looks a lot like the arguments right a lot like the predict function. You hand at your model object that you want to start with and then your new data and. Some arguments to your good sampler as well. Very similarly, you're going to get a progress bar and we're going to print this out to the screen once it's done. Okay, so same information here. You can see it called refit now so we know that this was fine tuned R squared most prevalent most coherent topics. So I can create a bar chart that's just like this one that you see here, but now we'll compare the prevalence of topics. From the model train just on the first half of the book and the model that was fine tuned folding in information from the second half. We run that here. And so what we can see is that there's some significant changes in the overall prevalence of the topic from. So this is just predicting under the model where you can see this topic here was very prevalent in the second half of the book, but once we folded in the topics that went down and some of these other ones got a bit more mass. So basically these models drifted apart a little bit by fine tuning and folding in that information from the second half of the book. So the sake of time I'm not going to get into all of this, but you can see which topics change the most linguistically that's what this code does and I'm going to make it as part of a vignette. I'm still working on for tidy LDA. So I'm going to see that it looks like topic 17 had the highest linguistic changes. If I just look at the model summary from model one. Taping that in here. So what was it topic 17 here? We can see those are the top five words in that topic. Remember those words and let's let's compare. To model two. And you can see that that topic 17 those words have shifted appreciably from what they were in model one. Shepherding boats lawn in way to it's that there's bit where that should be a. A pretty big change there. All right, so I will go back to the slides here. So I don't have code to show you this today, but that same fine tuning mechanism can be used to construct time series of topics where you're updating the model based on new data as it's coming in each period. So this graphic is from a model I built on grant abstracts in the federal small business innovation research grants database. The bottom shows how the prevalence of this topic has changed over time from these funded grants and the top shows how the distribution of words has changed within that topic over time. So data is the most prevalent word and it continues. It's one of my favorite words. I hope it's one of your favorite words. You can see the the other top five terms have changed over time where some words have dropped out and some new words have come in. And so one thing you can do with the refit function that I didn't show you is that you can in addition to fine tuning the pre trained topics. You can add additional randomly initialized topics and top of that pre trained model. So if I have a model with 10 topics, I can fine tune those 10 and add two randomly initialized topics to try and discover new topics. This isn't a very pretty chart, but it does show some promise that adding these new random topics can be used for topic discovery. So when I built the SBIR model every year, I added two new topics. And in 2020, the model picked up an emerging COVID-19 research grant topic. So not rigorously researched yet. That's all ongoing, but there is some promise there. So tidy LDA is still under active development. I imagine the API will stabilize shortly after I finished my PhD. But in the meantime, I would love users and feedback, especially as trying as I'm trying to build some helper functions. So what common topic modeling tasks should be a single function call versus chaining together many operations like I did in that demo? That's all I've got. Thank you so much. Here are several different ways to get in contact with me, and I appreciate your time today."}, {"Year": 2021, "Speaker": "Surabhi Hodigere", "Title": "Governance Playbook for Digital Public Goods", "Abstract": "Inspired by the open-source movement, Digital Public Goods are not only non-rivalrous, but sharing them across jurisdictions could lower costs, speed adoption, and create standards to facilitate cooperation and trade. However, the joint management of any resource between sovereign entities\u2014particularly of key infrastructure for the maintenance of public goods and services offered by the state\u2014carries with it significant questions of governance. A team of researchers based at the Ash Center within the Harvard Kennedy School are publishing a report that proposes five governance best practices for DPGs\u2014Codifying a Mission, Vision and Value Statement, Drafting a Code of Conduct, Designing Governance Bodies, Ensuring Stakeholder Voice and Representation, and Engaging External Contributors. These five recommendations seek to nurture institutions that will create public value, possess legitimacy, and maintain the necessary support and operational capacity.", "VideoURL": "https://www.youtube.com/watch?v=Kz6UOsRXLR0", "id0": "2021_15", "transcript": "So our next speaker has only been in America for about 12 months. And she spent three of those months, and I want to make sure you get the direction correct, going from the west coast to the east coast on Amtrak in 2021. So I want everyone to think about what that was like. I know this is sort of this romantic ideal, and I want to hear if she was in sleeper cars or regular seats, like how much is just like, oh, going from one city to next and waiting a week or spending overnight in sleeper cars. So I'm really interested in seeing how that all went, because there is this sort of like romanticism about riding the train across country. So everyone, please welcome Saradni. Hello everyone. Thank you so much for having me here today. My name is Saradni Vyhodeh and I'm going to be talking about best practices for the governance of digital public goods. These are primarily research findings from a research report that I worked on housed at the Ash Center in the Harvard Kennedy School and supported by the Rockefeller Foundation. So very excited to dive in and take you through this, through the preliminary findings of our report. To start with, I'd like to acknowledge that I'm here on behalf of a fantastic team, led by a lecturer at the Kennedy School, David Ease, some of you might know him, and also my colleagues, Leonie and Amira, who could not be joining us today, but have given me this responsibility to introduce our report to you all. So what are we talking about today? We're talking about digital public goods. I don't know if you've heard of the term or not, but digital public goods are essentially categorized as free and categorized as such when they are freely and openly available with minimal restrictions on how they can be adapted, distributed, and reused. So they're essentially public goods, but for the digital space, that's how we're thinking about them. Now, there's one thing that makes digital public goods incredibly different from public goods that we know of in economics and otherwise, which is that term that I've highlighted, the minimal restrictions. So when DPGs, that's how we'll call them as we go about in this talk, they voluntarily agree to restrictions, restrictions on who makes decisions on the development roadmap, what the requirements are on the membership fee, membership criteria, all of this to mean ensure that there is a commonality and between the sort of views that can be brought as well as to ensure that costs are coming up. Just to give you a quick example, I wanted to introduce my favorite DPG, which is the X-road in Estonia, an integrated infrastructure that allows for data sharing between Estonia, Finland, Iceland, and is governed by the Nordic Institute for Interopability Solutions. This is a DPG that we have referenced throughout our report and really is in some ways a guiding DPG for a lot of research. Now going forward, why did we choose to spend 12 months of our time thinking about digital public goods governance? Like I mentioned, when digital public goods come together or digital public goods exist, they do so through minimal restrictions. These restrictions are important for the very existence of a digital public good because they ensure that it's both restricted as well as coordinated in such a way that the digital public good doesn't splinter into different bespoke solutions. Instead, it is the governance in itself that allows for this coordination and then thus the restriction. I know it's a lot maybe to follow for people who are not in this part of the, you know, not thinking about this world, but stay with me as we go forward. We decide this is why we focused on governance and we felt that a digital public good without governance is indeed not a digital public good at all. Just picking up code, public sector code, throwing it on GitHub does not necessarily make it open. It does not necessarily make it a digital public good. That's where we feel that governance comes in to set certain norms, standards to ensure that additional public good really lives up to its public good part. Governance we think is stand consuming. Governance we think is hard. Of course, there are these constraints. The risks, however, especially on emergent and small digital public goods is high and therefore we think that governance is indeed essential. You might be wondering why am I talking about something in a digital public good lingo when all of the scenes so much similar to open source. After all, digital public goods are open source products. That is a requirement as per the definition that has been set by the digital public good alliance based on the UN report that we have referenced. But what differentiates digital public goods from open source solutions? Four things that we have recognized. One is that intentionality. OS projects are often born in a very organic manner. Think Linux, think all your big sort of open source communities. Whereas DPGs are more definitely more mature, relatively more mature and intentional and defined. Equity, a big concern for anybody working in the government space. OS projects don't necessarily have this as a focus but an often open source projects can lead to underrepresentation of certain groups because it's not intentional. Whereas the government needs to always keep in mind equity concerns. Sustainability, again, because of the nature of open source projects, they have different sustainability or often unclear sustainability models. Whereas with digital public goods, it is a big criteria. Sustainability is a big criteria. In terms of capacity, again, not everybody is, not every government is equipped to attract as well as retain developers as large corporations or organic open source projects can do. Which is why we differentiate digital public goods from open source and have spent this much time focusing on this very niche category of public goods that is emerging in the world. So coming back to governance, where does governance fit? There's all of these other facets to a digital public good that could have been studied. I told you why we focused in on the government's module and however, I want to recognize that policies, thinking about funding, especially funding, assessment capabilities, all of these are all incredible parts that we do hope that others conduct research on. And I could also reference a few others who have conducted research on the same issues. What do we mean by governance? The key decisions around governance, who makes the decisions and who decides who makes the decisions? This, I think, is the crux of what we've been trying to figure out. As well as the development of the roadmap. How is it done? This is what we will refer to as strategy as we go forward. Of course, there are other peripheral decisions as well. Who can participate in developing, maintaining the DPG and given that it's an open source inherently open source sort of product. How do you enforce community norms and rules? This is how we're defining what governance is in this entire spectrum. What did we do? Essentially, this is the fun part of my grad school life. The last 12 months spent a good amount of time reading literature on governance and open source for somebody who has no technical background and has to go through all that jargon. It was a lot of fun to do. But learned a lot through this literature review and we focused it on governance. And as I said, conducted more incredible interviews with leading experts, guided our recommendation and with existing leaders in the additional public good space. This was conducted across different time zones in the world. Thanks to Zoom. We did have two limiting factors though. And that is that there was only a small number of DPGs which have robust government structures. Nice, as I mentioned, is one of those, the Nordic Institute of Interop and PDA solutions. And of course, as anybody in the public sector would attest to the visibility and access to governance failure. So we really didn't know whether when something goes wrong, nobody wants to talk about it, especially in government. So visibility and access to governance failures was definitely poor. Which is those were our limiting factors. I want to very quickly run through the two frameworks that we used that one is Mark Moore's strategic triangle, born here at Harvard Kennedy School. Essentially that focuses on balancing three different angles or three different important considerations, legitimacy and support, public value and operational capability to create any sort of net public value. This is a framework that we have used in our analysis. And some of you might be aware of this. This is Simon Wardley's maturity mapping where we have taken different DPGs which have different goals and levels of maturity and sort of map them in this spectrum of whether they're from experimental to standardized infrastructure. So there's folks and solutions and product in between. Yeah, so those are the two frameworks that we used. It's academic report, so we made sure to reference these frameworks throughout our work. Now I will very quickly sort of jump into our recommendations and sort of take you through this for the next 10 minutes. So the first and my slides are, yeah, okay. So the first recommendation, we like to call it a playbook because after all, you don't necessarily want to give out a bunch of tools that DPGs that have very varying goals and contexts as well as different sort of intentions to say that this is exactly what you want to follow. So we've been flexible in what we have said is observed across different DPGs, emerging DPGs as well as majority DPGs and here's something that you could be doing here, something you could try. So the first two, however, are finding a vision and mission value statement as well as drafting a core of conduct. We have recommended to everyone. It seems very, you know, it seems very basic at least. That's how I thought about when you were thinking through that any sort of organization need product coming together would be, would sit down and think through finding a mission vision value statement. But it was important to sort of say that this is a requirement for a DPG to think through. And I think expecting this in the public sector is really holding the organization to a higher standard already. So let me just dive into the first of our recommendation. What it allows, as you all know, it's a vision vision, MBB statement allows for the purpose and the objectives to be clearly articulated and therefore through the articulation, it allows for it to shape strategic decision making. And our guidance here is that really invest in a well-defined MBB as a starting point because this will allow for DPGs to think about what the government's design should be centered on and how it maximizes public value. On the right hand side, you'll see that this is applicable to all of the, all sort of levels of maturity of the TPGs and it maximizes public value in the more strategic triangle. Going forward, drafting a core of conduct. Again, seven open source communities have we've heard case studies, you know, certain stories with essentially about how managing contributor behavior, who leads, who makes decisions, what sort of norms should be followed, has been a cause of concern and which is why we recommend as a best practice to come up with a core of conduct for contributors and really sort of put this out in the open so that everybody's on the same page with respect to this code of conduct. The third recommendation that we have had is the crux of, you know, what we've been trying to work on, which is how do you design governance bodies? I know there's a lot of technical folks in this conference and of course it's called the our conference and often what happens is that, you know, this might be an afterthought but through our report and through this particular recommendation, in fact, we've tried to bring the focus on how governance, designing governance bodies is essentially not, should not be an afterthought and institutionalizing, you know, authority and accountability for decisions, especially for digital public goods, which are intentional, which start with a lot of majority sometimes. Ensuring that they have governance design, you know, they design their governance from the start is incredibly important and the one thing that we felt really works or rather, we've observed in our research that works is, which is an emerging best practice, is that separating the strategy and the technology implementation and allocating different sort of decision rights and responsibility to each has been something that we've seen in show better sort of governance design. The strategy board would be focused on roadmap and community right decisions, whereas technology board would be focused on, you know, code and managing that and day to day operations. So this is a, this is a third recommendation, very quickly going forward. An important concern, especially with respect to our equity value, is that we've observed a lot of dilemma in what happens when decision making culture becomes very unanimous, and also when, what happens when a DPG scales. So for when additional public goods scales, we've noticed that stakeholders that control the strategy board essentially control the direction of the DPG, while we're not making a recommendation or a guidance in this specific case, we are saying that this is something that DPG should be cognizant about. And this, in the second and third observation that we saw was that unanimous decision making and a lack of sort of taking all stakeholders together, the guidance that we're putting forth is that, of course, design and plurality and uplifting voices very intentionally from these those constrained stakeholders. And the last and final recommendation that our playbook part that we've had is this fantastic, you know, part about open source is that it allows for contributions and therefore, we've noticed amongst existing DPGs that there are three types of contributors, voluntary contributors, contributors who are either paid by an external entity or paid for by members of the community. And DPGs can engage these external contributors through three types, essentially, formalized without any sort of formalized structure or with a formalized structure that has a contribution mechanism, but also a third type, which is, which my slides are eating up for some reason, but a formalized structure, which has a governance consideration as well. Of course, the third one is something that we would recommend, but at the least, we would say that it's essential to have formalized contribution mechanisms and not stick with the lack of any such thing. I have no idea how I'm doing on time, but I would like to thank you all for listening to our talk. I really hope that this might talk. I really hope that this was informative. And like I mentioned, these are preliminary data findings, research findings, and we will be publishing this report in the next few months. I hope to share it with the art conference community. In the meanwhile, please feel free to reach out to me. My email ID is just my first name and last name at hks.hardward.edu, and I'm on Twitter as well. It's very open to hearing your thoughts. This is a very niche subject that we've just picked up on, and we're very excited to publish it and take it forward and also hear all your feedback. So thank you so much for this opportunity, and thank you for listening."}, {"Year": 2021, "Speaker": "Jasmine Ye Han", "Title": "A Data Journalist\u2019s R Toolbox", "Abstract": "Some reporters chose journalism because they hate numbers. Data journalists are a group of story-tellers who like numbers and can code. And R is one of the most popular languages among them. This talk will introduce how a data journalist uses R, from web scraping, analysis, creating graphics to just automating the boring stuff.", "VideoURL": "https://www.youtube.com/watch?v=b3d8kxIh0vI", "id0": "2021_16", "transcript": "So I first met this next presenter when she attended one of the R workshops I was teaching. And recently I was very excited to be like, hey, let's have you speak at this conference because you do really cool work in a very public facing field. So I'm very excited to have her here. And she wants everyone to know that she spent a summer in North Dakota on an internship. And she was there to write a story about a small town that had a population of 30 people. And she says, while visiting the town for interviews, she increased the Asian representation in that town from 0 to 3%. So please, everyone, welcome to the stage. Jasmine. Thank you, Darrin. Thank you for inviting me. Thank you, everyone, for being here. I'm excited. My name is Jasmine Han. I'm a data reporter with Bloomberg Industry Group. We are part of Bloomberg. Okay, additional fun fact. While I was in North Dakota, I did a liking of American food because I'm originally from China. I have Asian taste, but anyway, I moved to the US in 2014 for journalism school. And now I'm based in Maryland. In my free time, I like cooking, plants, clamander and doodling. So on to today's topic, data journalists are toolbox. So a lot of people I meet in real life, you know, new people I meet ask me what I would do. I tell them I'm a data journalist. They give me a very confused look because there's a notion, at least in the past, that journalists are good at words, but not at mass or anything in adjacent. But there's actually a bigger and bigger community of people who like doing both, telling stories and doing mass or coding, programming. So yeah, today I'm here to show you that, yes, data journalism is a thing. It's cool. And then how we data journalists use are. So all right, so what is data journalism and why should I care? I don't know how you all keep track of the COVID pandemic, but I keep coming back to this New York Times landing page is pretty much all information I need. There are a lot of independent efforts tracking COVID out there, but I find there's to be the most intuitive and most comprehensive. And they also, the data quality is very trustworthy because they have a team dedicated to the scraping, the cleaning and the reporting, etc. So based on data they keep track of, they did a series of stories on the impact of the COVID pandemic, including this one that shows racial or ethical inequality of COVID contraction rates, and including this one, which is a interactive map that allows readers to just look at how full the hospital ICU's are in your own neighborhood. So this series won the 2021 Pulitzer Prize in public service. And the jury said that they filled a data vacuum that helped general public to be better prepared and protected. So yeah, data journalism helps fill a gap of information. They turn data into information that can be used by the general public. It's a public service. And in addition, data journalism allows us to tell stories that go beyond words. And readers are put directly into the story and they can tell stories of their own. And I'm using a quote by Aaron Williams. He's a former Washington Post data reporter who did this beautiful piece that used maps to illustrate the trends in America's diversity, but also segregation. And at the end of the story, he allows readers to zoom in on your own neighborhood, an area of your own interest, and tell your own story by gaining insights that's relevant to your own experience. So yeah, and last but not the least, data journalism exposes systematic problems in ways that journalists just can't otherwise. For example, this investigation by the Atlanta Journal Constitution exposed the broken system that allows doctors discipline for sexually abusing their patients to continue to practice. So the reporter scrapes doctor disciplining action documents from state bar websites around the country, all of them. They collected, it must have been over a hundred thousand documents. I need to have a check the reverse, but it was a huge body documents and they developed a regression model to filter down these documents based on keywords down to about 10% that were potentially sexual abuse offense. And then the reporter is that it, these 10% documents and drill the rest of the reporting from there. So if it weren't for the data, it wouldn't have been possible to prove that this is a systematic problem happening nationally on a large scale. And it weren't for the model. It would have been impossible for humans to go through all of these arguments. And sometimes when the data is really big, it gets journalists around the globe to collaborate and reveal the rich and powerful, sturdy secrets. So this investigation by international consortium of investigative journalists along with about 100 media partners. They used a leak, leaked documents about 11.5 million documents that exposed the secret financial dealings by some of the most powerful people around the world. So the ICIJ built a Neo4j graph database that allowed reporters to uncover basically relationships between key stakeholders, key entities, etc. So, yeah, as a result of the story, it revealed companies that helped Syria's deadly ear war or a network of people close to Vladimir Putin that secretly moved billions of dollars through banks and offshore companies. Then a secret company of former prime minister of Iceland. And he had to resign over the citizens' ravage and protests. So basically as a result of this investigation, governments filed authorities launched tax probes, criminal investigations. So it is very impactful. So hopefully I have so far convinced you that data journalism is very cool and very impactful. And so what does it have to do with ARN? We're in our conference, so we're going to talk about ARN. It was a life cycle of data analysis projects and data journalism projects shares very much parallel between these steps. So the point of showing this chart is to say that a lot of these steps we can do in ARN. So we start from an idea, we understand the data, and then prepare the data, sometimes script the data, clean the data, and then we do exploratory analysis. Maybe in this analysis, do some visualization, and then we validate the data, what we're finding, sometimes using ARN or by talking to sources. Or subject matter experts, and then moving on to the production phase, we produce visualizations, and then a story of words. So, for example, I want to highlight a project I did about a few years ago on tariffs. So I learned so much about what ARN can do from this project. In 2018, former President Donald Trump imposed tariffs on imported goods of steel, aluminum, or Chinese goods. So that meant that US companies trying to import these goods will be faced with increased costs, so they're being challenged. But the administration does allow companies to submit a request saying, can we be exempted? What we want to import is not manufactured in the US for reasons like that. So they can submit this request form. In addition, they allow third parties to object to submit requests. So some companies might say, you're a liar, we actually manufacture those in the US, you can buy from us, so you shouldn't be exempted. Just for an example. And then the administration or the Commerce Department then make decisions for these requests, and they issue this decision in a decision memo PDF documents. So they actually post these documents on individual documents for these tariffs on regulations.gov. So we were able to scrape these documents, process them, analyze the data in it, in them, and then did a series of stories. For example, this one, that looks at how corporate America is sort of on the losing end in a bid to ease the tariffs and how US manufacturers that are seeking relief are just left in limbo in the backlogs and red tape. And we found some of the top automakers are imploring to get relief from Chinese imports, including so Tesla's requested relief from a product they call that serves as a car's brain. So that's very essential to their product. And then also how pet supply industry also took a big blow. So here's how we did the scraping. We use these libraries. And here is our repo screenshot. We have several scraping scripts, but to just boil down to one line of code. What's most important is this line. So we sort of piece together a URL for the API call. Then we use the get function to retrieve that document. And then we use XML parse and then XML to list function to parse it. And then we are able to sort of pick what we need. And then you might notice that we use sys.sleep and try catch a lot. Because these prevent us from being seen as a bot and then get blocked from the website. And then try catch allows the code to keep running when we're hit by an error. And in processing, we relied heavily on string R and PDF tools. Here's an example of a request form. You can see it's very heavily styled. A lot of merge cells, blah, blah, blah. So it's not the clean data frames that we're used to seeing. But deep down, it's basically a great system. You have row number, cell number, etc. So to extract information, we sort of need to use these section, you know, the section numbers. And then some of the keywords to sort of serve as the road sign almost to tell us, you know, where the information we need is at in this grid system. So we use which and grab to find these row signs and then, you know, get back that index that row and cell and then retrieve from there. And this is an example of a decision memo. And there's some pattern here. They always reveal the decision at the end of the at the end of a letter is big X. So I proved denying this exclusion request. So what we are looking for here is that big X. So we did with these PDFs. We first of all, parse them with the PDF text function and turn it into like a vector of sentences, like each row is one. One like each row serves as one row. And then we find this big X, right space, X space using regular expressions. So we get to that line we want. So that's very thus vastly simplifying the process. But I think I've included the key information here. And then after the scraping and processing, right, we sort of dump the data tables into a postgres SQL database, which we then are able to access via the RODBC library. And for the analysis, we build a shiny or dashboard that allows us to do preliminary or exploratory analysis, keep updating it based on the most updated data. And we relied heavily on high chartered library to produce interactive visualizations. So, for example, we looked at still exemptions, how many requests got a decision, how many didn't, how long they had to wait for a decision. So these trends informed the story on the US manufacturers facing backlog in RFP. And here is a map that shows which congressional districts these requesting companies are from. So we can infer, okay, which legislators might be interested in this issue. And then we also included some charts on top companies, which are the top companies submitting requests. And we actually also included a searchable table that allows us to search company names. So that's a quickly, that's a quick tool for us to find, you know, automakers and pet industry companies. And it was very useful for our, for those stories. And down to the graphics. So we are, we have a very small graphics team in our newsroom. So a lot of times my colleague, my colleague, Aaron and I, we produce sort of a barebound version and then head it off to graphics for for styling and we use these libraries a lot. So for the maps, I believe I use team map for this one and T-verse is also often used so we can graphics artists. It's really hard for them to sort of hand draw all these vectors as maps, but we can easily do it in R and then pin it off to them. And for charts, what I really like about Ggplot to, it allows you a lot of customization. So I, we were actually able to style it exactly as how our style guide would require, like, including, you know, the font, sizes, colors for each section. So it's really neat and I can pretty much get the charts 90% of the way before I can hand it off to our graphics colleagues. So it's a huge boost in productivity. There's so much more I wanted to talk about, but because of the time limit, but feel free to reach out, keep in touch and yeah, thank you."}, {"Year": 2021, "Speaker": "Jorge Luna", "Title": "Hospital analytics approaches to help inform strategies to reduce 30-day readmissions", "Abstract": "Reducing preventable hospital readmissions is a national priority for government payers, private payers, providers, and policymakers. Machine learning has emerged as a critical tool in seeking to improve health care, lower costs and generate more value for patients. This short talk will discuss the following key questions that hospitals analytics teams will consider when developing ML and predictive modeling to reduce avoidable readmissions:  \n\n\u00b7         What are the hospital business problems that motivate the need for machine learning and predictive modeling?\n\n\u00b7         What specific events, outcomes, or quantities need to be predicted?\n\n\u00b7         What is the specific population for which a prediction of the event/outcome/quantity is needed?\n\n\u00b7         At what point or stage of the phenomenon should the readmission prediction be made? On pre-admission, admission, 24 hours post-discharge, within 7-days of discharge, etc.?\n\n\u00b7         If predictions were made available, how will the predictions be used?  What are common interventions linked to predictions?\n\n\u00b7         What are recent trends in hospital analytics & analytic partnership?", "VideoURL": "https://www.youtube.com/watch?v=8BCrhXl73jk", "id0": "2021_17", "transcript": "So this next speaker is one of my students, so I'm very happy to have him on stage. And when I say student, but I've tried like a decade ago, I think at this point, right? I think I'm going to age both of us here. I think it's been like a decade. But I'm very excited. It took me 10 years, but he's going to be speaking in front of this crew here. He's actually been a very good friend of the meetup community. His organization has hosted the meetup for the meetup groups, for the workshops we've done. So before we even got the stage, a big round of applause for all the help he's done for the community too. And he loves to scuba dive. And his favorite place to dive is Costa Rica. And he has been face to face with a tiger shark. So you need to let us know in reference, how big is a tiger shark from this to Jaws? What are we talking about? So please welcome Jorge. Today's talk will be about healthcare. Jared brought me in to give a little bit of background on some of the work I've done. I've been 10 years in healthcare, probably more than that from graduate student, epidemiology PhD student working on the hospital side at Sloan Kettering, New York Presbyterian Hospital, the Columbia Cornell Medical Centers. And now my recent hat is as a payer at nine CVS. What I wanted to talk about was to give some intuition on how we should be thinking about readmission predictions in the context of quality for hospitals. So now the goal of this talk will be to give you not just so much like, hey, this is a state of the art, here's the AOC curve on good models, but really like what are the useful and how we should be thinking about pairing a prediction with an intervention? Because I think I start here with this slide, which is to say, you know, us in the bioinformatics, the statistics or data science phase, we like to create models and create complexity to try to solve and get an interesting segmentation or stratification or identification, but at the end of the day, we're not going to be doing the clinical work. It's in this space in this picture, which is the provider and the patient that you kind of want to be invisible. You don't want to be in there, but you want to help inform and give enough tools or all the tools or all the information necessary for the provider to respond and to prevent. So that's sort of the space that I want to discuss, but before we discuss on how to intervene, I think it behooves us to understand what are the contexts. Why do we care about hospital reduction readmissions? Where does this come from? So I start with a little bit of context, and then we'll jump into state of the art and house some organizations and groups are thinking about it. Okay, so I think everyone saw this mainly from the abstract for the talk. So context, outcomes, population, timing of modeling and then interventions, right? So that's sort of the flow. So let's hit it. Okay, so this first one, if you go to this website, www.Medicare.gov Care Compare at the bottom, you'll find that CMS Centers for Medicaid Medicare, right? This is government and student. So CMS has done an amazing job over the past decade putting together what is a tool to give patients insight, to give whether you're looking for a doctor, a hospital, a nursing home for a elderly parent, hospice care, anything, basically most of the facilities that you search for care at, CMS has done this tremendous job of putting together a view for someone who's not clinical to very quickly surface, hey, which of these providers, which of these hospitals gives me superior care for an efficient cost? So this notion of value is something that CMS started more than a decade ago, passed through the quote unquote Obamacare, but even before that, it was a lot of wrangling and data collection and getting to a framework where you can actually, I would say force hospitals to give this data such that we can put together a national vision and a tool like this. So it's no small feat, very political, very technical, but we're here. So you go to this website and then I can search for a hospital, say I'm looking for an acute care hospital for, let's do me and hip. And I'm here in New York and lo and behold, in the same grid, you know, less than two miles, three miles away from each other, there's two hospitals. I mean, this is public, so I'm not calling out names, Elmhurst Hospital Center and then hospitals for special surgery. And what we see here is an incredible summarization and quick jump into, hey, this is a good or bad quality hospital. And hey, the patient satisfaction scores, surveys that are on our patients can represent the level care you're expected to receive quality-wise. And then the hospital, the care services that you'll receive as a patient, your experience of being in there. And this is pretty, pretty drawing that you can have two hospitals so different, so close to each other in New York City, but that's the true nature of the healthcare system. A lot of differences in care, a lot of imbalances and resources. And as a patient or as a care, you know, as someone who takes care of someone who's sick or is fighting disease, you know, these tools are here for us. And then in this, part of this whole system is one of these measures, or any subcomponents I'm going to talk about, which is the hospital readmission measure. That goes into the start, the notion that says, hey, if you're treated here, you'll likely be taken care of while inpatient and you'll likely be connected to a system that'll carry you through to get you safely and back to your daily life activities. Right, so this is sort of the motivation. And again, how do we get there? And what am I really talking about, right? So on the left-hand side is this whole milieu of measures. Each one of these measures has incredible validation and two retrospective studies to demonstrate that the measure is stable, that it can be reproduced. And it's all through the National Quality Foundation. I showed maybe the 23 measures here. They have a catalog of over a thousand. And they're constantly adding, it's a big area of research. And what I'm calling out here is really these three types readmission rate, acute myocardial infarction, readmission rate heart failure, readmission rate, pneumonia. But you see that there's many, many measures that get publicly reported. And specifically for the subset of readmission measures, what goes into this, the secret sauce for this measure is first the data collection. So you have more than 3000 hospitals nationally pushing data in on a quarterly basis, being assessed, being collected. That's where taxpayer dollars go when they go to Medicare. I mean, this is really difficult work to manage this whole process. But then once you get into this phase, where, okay, I've collected the readmissions using standardized ingestion protocols, then we move to the next phase. And this is where CMS, I believe the contract would be able to do this. They take this and they start making inference over this data, right? And I'll go into this in a second. But the idea is we create a hierarchical model with all the hospitals and patients necessary in the hospital. And then we collect features that are risk adjustment, they call case mix adjustment for more complex patients, less complex patients. And then we were able to rank the hospitals or you were able to compare the performance from the very honorable or lofty Mayo clinics or Hopkins down to the community hospitals in Idaho, right? They're all put on the same standard. And we try to, or CMS tries to drive and it constantly increased performance. So you have the modeling, you have a standardization such that they're comparable, and you get these distributions for risk ratios to say, at 30 days, medicine, surgical overall, how is your hospital performing? And what CMS does is they draw a line. The line's on here, they draw a line, they say, if you're on this side of the curve, well, you need to improve. And what they do is they apply a penalty, a penalty which is 1%. And that motivates, right? So when you're hitting the bottom line, that definitely causes motivation for hospitals to enact processes, to enact procedures, to deploy an analytics team's to roll out predictions that they can do something with in order to improve the performance, not be on the outlier side, right? But the way this is structured, everyone's trying to improve all the hospitals, because there's always gonna be someone on the right hand of that curve. But the implications are much more profound than the so-called penalization or financial push to improve quality. In fact, sorry, in fact, what happens is this now becomes public. So that same website that I showed you, I'm making decisions not to go to this hospital if I can go to this other hospital. Some cases you don't have that. They are hospitals, are monopolies in some regions, like they have one system. But if you can choose, you choose, you can definitely choose at the provider level. So there's public reporting, New York Times wrote an article about you if something's really awry. Then this link's a reimbursement, but also commercial payers, private payers have followed suit. The big expression is CMS is the grill in the room, whatever they do, sort of they suck the air out and all the other payers are like, well, let's do that. And in this case, CMS paved the road in order to drive this big quality value-driven care initiative because this gets pushed back. But they can demand this because they cover so many lives. So because of this private payers in Zoom, and then the home notion here is that there's a lot of incentives if you're a good performing system, good performing hospital. And then if you're a good hospital system with good measures, well, then you also attract a lot of the talent. So this is sort of the thinking and how it goes. And does it have an impact? Yeah, definitely does have an impact. So this is in 2020. This is the Kaiser Permanente Health Network, basically half the hospitals received some level of penalty. And just to give you context, a hospital like New York Presbyterian would probably have 600 million. Might be more now, might be like 900 million in dollars from Medicare. So you're getting a 1% hit annually, you're looking at somewhere between six to nine million dollar penalty and that could wipe out the margin for a month. So you don't wanna be there, it does hurt financially. And then there's this push. However, or let's just say that the fairness aspect of this is important. I'll just say that sometimes small hospitals have a hard time doing this right because they don't have the funding and then the question becomes, well, do we penalize them on top of, they're already underfunded, that's why they're not performing well. So the push has been to sort of understand how to correctly adjust in one of the right sort of course matching levels of hospitals that should be compared to do this benchmark distribution and assessor penalty. So that's sort of the conversation. Okay, but, and then I believe this is my last context slide. And then what we think about is that there's this whole flow of care, right? So when you're home and you're at the gym all the way on the left-hand side, you have no acuity, maybe you start developing some comorbidities, hypertension, diabetes, see your doctor. Maybe you have a condition that keeps escalating and eventually you're in the ED in the hospital, right? The correct path is end to go to rehabilitation. If it was a surgical procedure, go out patient, get out of the center, get at home, or maybe you're permanently gonna need some help long-term. This is this flow, but the idea is to return you back to healthier activities. And in the general situation, this is the way it goes, right? For the vast majority, but as you age as you get older or you have a chronic disease, sometimes that's not the case. Sometimes you start seeing this recidivism back into the hospital. So instead of going all the way back to home, you jump back in. This is the notion of the readmission, but not all of these are preventable or avoidable. Sometimes you reach the end of medicine, so you're not gonna have the ability to jump back. So again, the idea is to create an avoidable 30-day readmission. This is a little schematics, but the idea is that if you're admitted, let's create an inclusion criteria where we can split you up into surgery, cardiovascular, these different cohorts, and then we assess an outcome. And then the outcome, there are certain things that might be planned for that index event, your hospitalized index event. And then these are the things that you don't want to penalize for, but you follow post discharge 30 days, and then you assess if a hospital has a thousand patients, 30 of them came back, what's the rate? How does this compare to other hospitals that had a thousand patients, right? So this is just defining the outcome. There's an exclusion, there's a set of ICD-10 codes, I'll talk about that one second, that will say if you're re-admit because of one of these, it was planned. They're scoping you, you're getting followed care. It's a good thing you're back. Now you're gonna have a surgery procedure that's gonna prevent the next one. So it's not always that, it's only if it's avoidable and there's good structure around that. And then the intuition here is that, what you wanna do is you wanna identify the patients that have this risk, that because of the milieu of conditions of comorbidities or context at home, that you can do something. Hospitals have the largest budget in the community, there's many clinical organizations in the community. Hospitals is the biggest budget, they have the most money to do something and organize and partner. So that's sort of that makes sense that you use hospital as a center to base these programs. The idea here is, if you have no strategy, then basically you have some at risk people you don't know and you're getting penalized. It's costing you money, private and government payers will not pay for some of it or penalize you. So, and it's costing you money, right? You're not putting in maybe much more lucrative procedures and you have a readmission for a sick patient, that's not reimbursed well. So there's motivational throughout tip fix this to act on this. So the idea here is then, well, okay, well, let's not differentiate the population and let's just swab and prevent infections in the hospital, generalized, and then that should reduce by some level, so you drop off to patients, some level of benefit is observed. The idea here is that maybe some people didn't need this intervention, right? Or you're spending money or you're not spending enough money in some groups to prevent it. So this is lo and behold, where risk profile patients come in and this is the notion of readmission that this is now denoted by different colors. If you're really sick, you kind of want a differentiated intervention. And this is what predictive modeling in the inpatient setting lets you do. It says, these people will be fine. These second people, if you intervene a certain way, you'll reduce, if you intervene another way for the most at risk, you'll reduce, but you'll never reduce it to zero. So it's just the nature of medicine, some people, it is quote unquote third time. But this is the underlying intuition. And as you benchmark, and then now as you create predictions to segment and treat differently, you wanna take account into the differences of the individuals, right? So how do we do this? Right? How do we cut out the populations that matter? First, what are the populations? This is a quick snapshot of a report that a hospital would get from CMS. Just a lot of information, I just wanted to be in the deck so you can see it later, but a hospital will get this report for my card, that's heart attack, COPD, heart failure, pneumonia, cabbage and total hip knee replacement. So these two are surgical procedures that should not have a readmission. And then you should be able to manage these four conditions very well. So they're not readmitted. So this is fair game, what's been allowed in the rule books right now. And then you get this model in, what is your predicted, what is your expected and what's this access amount? This is the foundation of how you're gonna be judging on the benchmark. And how CMS is doing it now, how do they get this predicted readmission? You're in this modeling framework, right? So you create a generalized linear model. So I know there's statistical teams here, but the gist here is that you have a matrix of information here, the BCI J, which is hospital J and patients I, and you're going to adjust for comorbidities. So conditions that would also drive the individual to individual patients to be readmitted, and then you fit a random effect for a hospital. And this data and these risk-diracial factors are very important to differentiate this hospital, treats sicker patients at that hospital, so when we compare, we need to give hospitals credit for the care that they give, and that's the idea why we do this model. And then we fit the random intercept for a hospital effect. We don't have, they don't include hospital characteristics per se in this model, but we get the patient characteristics, but we do allow for random intercepts. And then we do an indirect standardization such that we get apples, apples, comparison, right? So this is sort of behind the scene what CMS is doing, and the features, an example of the features, they're based on ICD-10 codes, right? So here's an endocrine metabolic disorder, this is a mapping from ICD-10 codes, diagnosis codes into this grouper, this aggregate map or that maps codes to an ontology that we put in a binary feature, and then it fits into this hierarchical model. So this thing increases risk of readmission, right? Basically they all increase risk, we don't, we're not modeling preventative effects. This is all from methodology from CMS papers. So, and then I'll say that more or less the C-statistic on these models is around 75. So a lot of the times when we're talking about modeling here, and I'll show you now, even as we get more advanced, you're not beating this readmission prediction at 30 days, more than 70, 75, 80 max that I've seen for area under the curve of C-statistics, because a lot of what happens half the battle or literally 25% of the battle, doesn't happen in the inpatient setting, it happens with the connection posts, right? Because you have 30 days where people are on their own, but a lot of it is controlled by the inpatient setting, that's 75%, but a lot is not, right? So this is the world that we live in, hospitals have the mandate to intervene and have budgets to intervene. So then that becomes a question of how do we pull this all together? So now just to give you a quick notion of, I have timing on top, but the idea is now that this is a standard type of readmission prevention or readmission prediction paper that a hospital group would pull out, this is Will Cornell, a lot of different groups that put this together. Now, instead of just claims, you're using electronic health records, so this is labs and medical notes and an rich assessment of other data, and what they do is at discharge, or at admission you know something, at discharge you know a bit about the individual, and you create different types of ensemble models to try to predict what is the risk of this patient in terms of coming back in. I believe this is what we're looking at, 3000 or so patients. Okay, and so what you get is this, right? So this is a 30 day model, so readmission at 30 days. This is heart failure cohort around 3000 change patients. This is outcome at readmission at six months, and the idea here is you have slices of data at the index, so when you're admitted you know some information, some red blood cell measures, some hemoglobin measure, then at discharge you've treated the patient, now you have a delta, you have a final state, you can aggregate the features, take me in the sake, exponential smoothers, you can do many, many, many different things, and then you started saying, okay, well this measure here, measure here, now I can predict. And then again, you're still looking at different information at different time points, but it's not great. It's not really good classification by any means, it could be improved, but this is the state of affairs in terms of how we can predict, let me see where we are. So this is how we can predict and get at the risk. If I tell you that cutting edge is, this was the Google deep learning paper, 2008, so they use billions, I believe a billion discharges from different medical systems, and they're also looking at this, you know, more or less 70, 77, 76% readmission prediction capability, but so then where's the secret sauce, right? I think it's this, and this is what I wanna leave you with, right? This is a mediation framework, and then the idea here is that as we start looking at what is your risk of an event, you also wanna look at the context. So this is in this framework of the F geography, this is the risk adjustment factors, demographics, the per member per month spend, utilization prior to an event, but then we wanna look at what are the context, right? What are the standard of social determinants of health? Are there barriers that you cannot, that the patient themselves can't overcome? Well, if so, then let's intervene, let's give them money for an Uber, right? Let's get them some discounted medications so they can improve adherence, right? There are many things that let's get a hospital to their home and pay for it, because it will likely be cheaper. So just to give you context, I think this is where most of the hospitals, most of the providers are now considering is to say, let's take this environment, and let's turn it into causal framework, and let's start experiencing, given a risk score, how much can we mediate through one of their, one of their elements that would reduce impactability? And then this is area where research is ongoing, what are the right interventions, piloting different interventions, and you'll find that a lot of hospitals who have assumed more of their risk, start thinking and saying, let's spend some more than what we normally do, because if we intervene on these mediating pathways of risk, then we can have an impact to reduce readmission to reduce mortality, to connect, and a lot of these are a further connection with partners in the community, right? So I think this is where we go, that's the message on a land-on. And then, of course, doctors like anything that can help them save money. And then finally, I'll say, a lot of tools have been implemented, right? You have to think about the system to intervene on in order to drive change and do impact. I challenge you to read this article, a lot of innovation in AI for COVID, but didn't really help. So how can we be more helpful, and how can we facilitate that relationship between the provider and the patient? I'll stop there, I hope I sparked some curiosity to read more, thank you everyone."}, {"Year": 2021, "Speaker": "Vivian Peng", "Title": "Building Blocks of Design", "Abstract": "When we think about design, it's common to jump immediately to thinking about what colors to choose or what graphs to make. The design process starts further back, by getting to know your audience at a foundational level \u2013 what motivates, challenges, and inspires them. At a time when we are overloaded by information, and desensitized to numbers, how do we develop data tools and visualizations that create an impact?", "VideoURL": "https://www.youtube.com/watch?v=_LUDtoC0bxs", "id0": "2021_18", "transcript": "She recently got a dog named after Japanese fermented soybeans, Natto. Can we see, well, you're not on screen yet. Maybe I'll ask you to see the dog once you get on screen. So everyone, please welcome Vivian. Hi everyone, my name is Vivian. I'm currently the senior data scientist at the city of LA, the Mayor's Office. So today I want to start by playing a game with you all. And I know it's a little cheesy, but I want to tell you a story. And as I go along the story, I want you to do, I will prompt and ask you to share how you feel by adding any emoji to the chat on hop in and we'll go along this journey. So I will prompt you as we go. It's a little cheesy, feel free to play along or just sit and listen to this story. So the main character of our story is the first born in the art family. Their name is one. Okay. And so one day one is walking along and they encounter the first born of the Python family, name zero. And so they say, hey, we're both first borns, let's be friends. So then walking along, they want to go get ice cream. Now they're standing inside of an ice cream shop looking at the menu. They have one and zero standing next to each other. And so what if I told you that this number represents how much money you have to spend in the ice cream shop. How does that make you feel? Go ahead and add an emoji into the chat of how if you had $10 to spend on the ice cream shop, how would that make you feel? Am I seeing these emojis? Okay. No, I want to play my game. Jared is happy. Jared loves ice cream. I know that I drew ice cream cone on his wedding invitation. Okay. So, oh, there's a delay. Okay. That's okay. It's coming through. So we'll keep going on this journey. So next, you know, they're walking along. They come across this bank and zero runs into their extended family. And they are so excited because they haven't seen each other in a while and they line up to see how tall they are. Okay. So now this number represents how much money you just won from one of those like scratch or lottery games. So this is 10,000 free money that you just have in your bank account now. How does that make you feel? Share some emojis. I see smiley faces. I see celebration. Free money. Sounds fun. Right. Okay. And so over the years, some of those family members have children and now they're walking and they come in the neighborhood and they see your dream home. So they're standing outside, lined up, looking at your dream home. And this represents how much money it costs to buy your dream home. $1 million. How does that make you feel? Cheers dollar signs. Okay. And so let's say one day one wants to go on a vacation. And so they say, I'll have my simply come in and help out and watch the kids. Right. And so one leaves and two comes over. So now this home is $2 million. How does that make you feel? Shock. Okay. I think you're getting in the hang of this. So let's keep going. So what if instead of $2 million, this home was now $3 million. Continue to share emojis. Okay. So $3 million. Sorry. You need to see some sweat. $4 million. Okay. $5 million. Oh, I see some really intense. I emojis. $5 million. Now this home costs $5 million. How does that make you feel? Mind blown. Yeah. So I see a lot of mix of emojis. And so similarly, I had a journey as well, throwing up emojis at these. Yeah. I went through a similar journey. So this is how I felt going through the home analogy. When it was $1 million, I felt sad because I thought, you know, I don't think any time within my near future. I can have a $1 million home. I felt really, really despondent. Right. Once I got to the 3 million point, you know, it was completely beyond my realm of possibility. So instead of trying to think of how can I own this home, I can just stand and awe and admire how beautiful the home is. Then I was a 4 million. I started thinking, what are people doing in their lives that they can afford a $1 million home. I think I was a 5 million and I just entered into this existential dread. Right. Like, what am I doing with my life? How come I am not making that much money to have a 5 million home? So that was my journey. And I see kind of similarly, like the evolution of emojis that you all shared in the chat. Um, now speaking of existential dread, you know, what if that 5 million instead of represented how much your home costs are represented the number of global COVID deaths that we've experienced. How does that make you feel? Um, sad, right? I'm imagining sad. I feel sad when I hear that number. It feels like a crushing number of people. Um, but how sad are we, right? It's the kind of sadness that stays with us when it's top of mind and we see it, but it's also the kind of sadness that kind of goes away when I move to the next slide. And I noticed this, you know, in the US when we hit reached our 100,000 deaths due to COVID in the US and New York Times dedicated their front page to all those who passed, you know, all the names and stories about each individual. And this was widely shared, right? A lot of people gave, you know, shared on social media. There are special dedications and stuff like that when we reach this number. You know, four months later, we reached 200,000 and it felt like people barely bad and I, you know, there are no longer any special kind of dedications or are a moment that was around reaching this number. And I think that has something to do with how numbers scale in our mind. You know, how do we really distinguish what 100,000 feels like compared to 2 million or 5 million. It's really difficult to tangibly feel what those numbers mean. And we kind of reached this threshold where, you know, in our brains, we can't hold the truth of the number, the difference between like 1, 2, 3, 4, 5 million. As data people, I think we understand the scale of it and we really feel the difference we're working with like numbers or data that is in that skill. But when we're just seeing the number on the screen, it just looks like one digit is changing. And then, you know, the impact of that number is kind of lost. So most of the work that I do is trying to get a sense of that impact and really get a feel of what these numbers look like. Previously, when I was at Doctors Without Borders, my work was a lot around storytelling, you know, creating illustrations and animations around the issues that we work on. And since I transitioned into a data scientist, my work looks more like this. You know, I spent the past year and a half tracking COVID trends in LA city and building reports and dashboards that help inform the mayor, policymakers and the general public around our current trends. And as I'm creating these tools, I think a lot about the game that we just played, how numbers for humans can both be, we can both be impacted and desensitized to numbers at the same time. And going into the third year of our pandemic, I think about how do we keep these numbers relevant, truthful, and maintain a level of urgency where it can still command someone's attention, given the context of everything else people are dealing with. So today I want to share, pull back a little bit and share what I believe are the building blocks of design. This is not a talk about color theory or what charts to use. It's really going, you know, what I think back to basics and sharing how to like think like a designer. So in the design world, we spend the majority of the time, not just understanding the problem, but understanding who is impacted by this issue. And I know you, you've heard it before, right? Everyone says, you know, define your audience. And usually what I hear people, how people answer this, I hear, you know, for example, the mayor or the CEO or, you know, millennials who are about to be first time parents. You know, it kind of stops at that one line. And I want to share today, you know, how a few questions that I use to help define my audience and how that framework can be helpful and informing the visuals that you create. So the questions that I explore are one, like, who is your primary audience? How do you want them to feel and what action do you want them to take? So what I mean by primary audience is really beyond like their job title, right? What do they care about? What problem are they trying to solve? And something that I think is really important is what limitations are they working with. So everyone has some kind of time limitation, especially when you're creating things online. You know, you're always going to be competing with people's attention span. Are there tech barriers that are limiting data literacy, right? Everyone, regardless of your data scientists or not has like some level of discomfort with with numbers. And so, you know, but it's as a designer, it's up to you to think about how do you adjust for that level of data literacy. Is there bureaucracy, you know, so within an organization, maybe they really care about the issue, but they may not themselves have influence to be able to make decisions to change around that. And so if you identify that that is something that is impacting your audience, then that helps you go beyond and think, okay, who do they have to influence to help kind of change policy or to be able to make decisions. Next, I think a lot about feelings, obviously, you know, and I think about one, like what emotions are they walking into this with. And just understanding that generally people have a discomfort with numbers, you know, they may be feeling timid or some kind of like imposter syndrome walking into something that is looking at really dense numbers, right? But when I think about if I have a chance to interact with something that I work on, how do I want them to feel walking away from this after having engaged with the report or the dashboard. And when I was doing work as an advocacy person, you know, that emotion changed depending on what I was working on. Sometimes I really wanted people to feel angry and take action with us. Sometimes I wanted them to feel sad so then they can be empathetic and have a conversation around this. And oftentimes just maybe relief too that someone is working on this issue. If you are kind of stuck and you don't know how you want your audience member to feel, you know, this is a philosophy that I've been kind of working on lately is you don't know up for happy, like choose happy, right? The world is really, really dark these days and heavy. And if someone has a chance to interact with something that you create, you know, why not give a moment of love and lightness. And I don't mean to that, you know, you should do like rose tinted glasses and if a report is really dense kind of like change the numbers or, you know, change the frame so it's all light and happy and positive. What I mean by this is really giving a sense of kind of grass over the numbers. So I think a lot about how people are uncomfortable with numbers. And so if I build a dashboard or report for them, you know, I want them to feel like confidence like, Oh, I understand what this means. I can then take away these numbers and trends and then be able to share that with someone else. Right. I want them to feel confident about the numbers and not be so intimidated by it. So that's what I mean by happy. The next is, you know, from there, like what action do you want them to take? Is it a one time action is ongoing? You know, that helps me think through. Okay, do I need to do like a one off visual or graph or, you know, I'm a shiny. I'm always saying, do you want to do a dashboard for this issue? What can they control versus who do they need to influence and what's the key takeaway message. So these three questions are pretty, pretty basic and easy to answer. And I do this often with my team as I set time, you know, just even 10 to 15 minutes to kind of brainstorm and sit together and say if we're thinking about launching this product, you know, who is our audience and brainstorming with like all levels of my team. I opened this up to to everyone, not just the data scientists working on it. And because then you can go through each kind of audience member and understand like, what are the key key motivating points for for this project and then be able to kind of get different perspectives from everyone who is working on it. And so this is great to kind of start and launch a project, but then also it's a good reference to come back to as you are developing the product, because, you know, in the creative process, there is going to be ups and downs and, you know, things will can iterate and it makes start to, you know, change and evolve as you go. And using this as a reference to continue to come back and think through is still aligning with the things that we identified as important from the beginning of this project, right. And sometimes, you know, it's going to be hard to navigate that conversation to think through is still aligning. How do we change and evolve it without shutting down ideas. Right. So I want to end on just sharing some language that I have helped found useful that helps me navigate those kind of conversations where you can still be creative and open and not have it shut down the process. So here's just language that I have found useful in thinking of sometimes when I see, I see something in my mind, I'm thinking, why, why is this here, right. But if you're on the receiving end of the why I have often been on the receiving end and it doesn't feel good. It feels like, you know, there's no confidence in the work or the decisions that we're making. And so I have found it helpful to instead shift and say, can you help me understand how this is being used. And if you look at these lines here, you know, the questions that I am suggesting is kind of more, you know, how about if we tried this wouldn't make a difference if we shifted, you know, here to this to here or change the colors. Tell me what you think about this what's working. And the theme along all of this language suggestion is all around getting at helping people define like what is working. Right. And if it is working, then let's keep that. But if that is not coming across like at a first glance, not intuitively, how can we evolve it so that it does feel intuitive for everyone who comes across it. And that way you can continue to keep the core of what works. And then have it iterate and evolve without it shutting down and kind of keeping the conversation going. So I hope this was helpful. Feel free to take what you think is useful and evolve for how this might be helpful and the work that you do. Thank you."}, {"Year": 2021, "Speaker": "Boriana P. Pratt", "Title": "Running simulations in Parallel in R with doParallel package", "Abstract": "Simulations are often run to benchmark a method using data where the results are known or to compare a few methods on a nicely structured (simulated) data. Simulating data in R is not hard. If you have to simulate many different datasets, tweaking some parameters, how to automate such a process to run multiple times and maximize the use of computer or server resources.  In this talk I will show how I was able to run multiple simulations at the same time using the doParallel package to run a few R threads simultaneously (from within R) to simulated multiple datasets with genetics data under different scenarios.", "VideoURL": "https://www.youtube.com/watch?v=dxJek4-hpBo", "id0": "2021_19", "transcript": "The next speaker, I actually found she was teaching a winter course at Princeton. I got the email saying, oh, there's a class on the tidyverse. I'm like, hey, what's your research in? It's a cool stuff where she works. And if I had heard it come speak at this conference, a very short notice. And she said, yes, we're all thankful of her turning around such a quick turnaround. And English is technically her third language in order of learning. Everyone, please welcome to the stage, Voriana. Hello, everyone. My name is Voriana and I work at the Office of Population Research at Princeton University. I've been using R for over 15 years now. And it's been incredible to see how the language has evolved. All right. Let me tell you a little bit about DNA, which genetics data. A DNA looks like a twisted ladder with steps that the ends of which are these four nitrogen. A basis that have the Latin names I can't relate for us properly. But if you uncoil, I'm calling it, you can imagine two lines running simultaneously or in parallel. And with the number with the letters, A, T, C or G. With the requirement that A and T. Pair and C and G. Or in other words, if that a particular spot in one line, you have an A. Then on the corresponding spot on the other line, you can have only T or A. And the same with the C and G. And people refer to a spot on the genome as a locus or a SNP and refer to these basis as alleles. And if the two lines or sometimes the refer to them as strands or a number, the alleles could be numbered. But the more interesting or. The issue between alleles that is made is one of them is called the major allele. And the other one, the minor and that depends on which one is more prevalent in the population. So at this particular spot or locus that I have sectioned. If more people in the population have an A on either spot. On either line, then A would be called the major allele and T would be called the minor allele. And so genetics data sometimes could come in the form of two lines per person with these letters A, T, C or G. But more often the way it comes is in one line and at a particular spot or locus. There will be a number of how many copies a person has of the minor allele. So if there are T and T on both lines, there will be two copies and T is the minor allele. There will be two copies. In this case, there will be one copy or in the case of where it will be A and A. There will be zero copies and then there will be another file called a map file that has all these spots or locus enumerated. And then the second column that tells which is the minor allele, the particular spot. So, so I was given a program that would generate this type of genetics data and a trade. And it would take about like 20 parameters. And then it would produce four different files, one at the family level and three at the siblings level. And the basic idea was that some of these parameters didn't vary from simulation to simulation, like the total number of families. How many children each family would have the number of snips or loci. And there are these other parameters that I was supposed to give to the model. It was a very complicated model. The program was written by a biologist who took into account all kinds of theories that I don't know anything about. But I was given a list of parameters. Then it was decided that I should make 10 simulations. And of course with all these random numbers being generated, I need to keep track of random seats so I could reproduce things. So we had total of 50,000 of families and 1200 snips. That's like the gist of this slide. And then there was this other parameters that they're like broken into sets of two. I'm not going to go into too much detail about them, but they had values that had to vary. And one of them had to do with heritability, which people define as the proportion of variation of the trade that is explained by genes. So there were two else parameters and we would try low, medium and high values for them. And then there's like two parameters for correlation between siblings of some part of the environment. And then there were two parameters for these dominance part of the model. And early on, it was decided that one of them would just keep it zero and not very. But you can calculate quickly. There's like 72 combinations. So it's not a huge number. But something that I had to work with. Okay, the analyses that I had to run had a very similar flow to the simulations. So I get the simulated files prep them make like an input file to the analysis program, and then I would get some results. And so the goal was to simulate 10 cents for each parameter combination. And to begin with, I said, okay, let me take one of them and just run it on a Linux server. And it took eight hours to simulate the data and produce about 10 gigabytes of data, which is not a huge amount, but you know it adds up. And I'm like, okay, well, let's then run the analysis program, some one of which I wrote, and that one took about two hours to run. So I thought, okay, I need to do something here. But before I tell you what I decided to do, which you already know what it was, let me kind of tell you what kind of analysis people run on this types of data. So the, the very first, and it's still being run. The analysis that people run is to discover SNPs, that means to discover SNPs that are associated with the given trade. And to do that, they run thing as something called genome wide association studies, which basically is like a regression or association models that would. Use all available genetics data and see if any SNP or locus has a significant association with the outcome. And usually the results are plotted in something called a Manhattan plot. And people use a very, very strange and p value, because there's like multiple comparisons being made because human genome has like. In the millions of these SNPs, they're short on the first slide. And so you can see the very few of them actually make it through the curve for the p values. And that's why they call this Manhattan plot. This actually use very successfully to find diseases, for example, that are caused primarily by one gene, something that they call Mendelian disease. But people after running many, many of these, and of course you need with so many SNPs or like variables columns, if you will, you need a good number of people over rows to be able to get it. So yes, they, people came to realize there actually a lot of traits, a lot of disease and a lot of traits if you're interested in social traits, or even height, biometric traits are actually many SNPs are associated with what they call it, they're polygenic. They're kind of defined, not defined, but they're caused by many of the SNPs. So notice people sometimes would do other types of analysis, but for me, I just had to run this to see if it would pick up the SNPs that we by design, simulate it to be associated with the trade. Then another with common type of analysis that there are lots of papers written about it is estimating heritability, which I, as I said, it's the proportion of the variation in the trade that is explained by the genes. So people usually employ something called grandma, which is basically genome restricted maximum likelihood from like mixed models. And so some of these parameters that we gave to the simulation program, we were specifying heritability and we were trying to see if we can recover, because we kind of knew what it should be. So to sum things up, it's the same program for all the simulations and the last is programs was the same and each simulation was independent of each other. So it was like a classic case for trying to do things in parallel. And I realized that a lot of people probably at this conference use big data and are very used to parallel processing. So I apologize. This is nothing new. It's my one sentence about parallel processing, which is basically executing repeated tasks simultaneously on different course or on different notes on a cluster. So as I mentioned, I started by running things on our Linux server, which someone after I realized had some limitations. So then I had to move the simulations to a high power computing settings, which this is my very basic picture of it, which stacks a lot of course, processing power in one place. But we're on a lot of computers nowadays. They come with multiple cores, I like this picture because it shows you the course that they're kind of separate from each other and they don't really talk to each other. So with the hyper computing people program ways that they could talk to each other. So you could have run. Not quite independent tasks on them. So how I did this in R was. I use the for each package in the blue parallel package and the for each package. If you're not familiar with that. It runs pretty much like a for loop in R. If you use it with the do operator. Except your output would probably get produced. We'll get back to you as a list. But the for each loop has the capability to use the do power operator, which allows each like iteration of the loop to use a different core. But you have to tell it how many clusters or how many cores or workers you want to use, and then you have to register them. So from then on my job is very easy. I would just put all my parameters into separate vectors and then call them right before I call my main simulation program. And then if you use a do parallel package, it's a good idea to stop your cluster so so that your memory gets clear. So that's what I wanted to share with you today. And thank you for listening."}, {"Year": 2021, "Speaker": "Mayari Montes de Oca", "Title": "What works to support refugee children? Using BART for impact evaluation", "Abstract": "Rigorous evidence of what works to support refugee children is scarce and challenging to attain. During this talk, Mayar\u00ed will share with us the strategy that she used to study the impact on children\u2019s reading skills, of attending a remedial support program, brought to Syrian refugees by the IRC and NYU. She will share her experience working with machine learning and statistical frameworks that can be helpful to 1) leverage the information available in understudied contexts and to 2) better account for the problem of self-selection into different dosage levels, under a causal framework.\n\nIn this talk you will also learn about the data challenges of conducting research with vulnerable populations and the R tools that were helpful in the process.", "VideoURL": "https://www.youtube.com/watch?v=s3ZKvxD9A7Q", "id0": "2021_20", "transcript": "She loves playing of loops, both in R and in music. And she has a looper to layer her voice, and the looper's name is Lupita. So everyone, please welcome Mayari. Today I will tell you about my experience doing analysis to study the impact of a program for refugee children using tools in R. Before I get started, I'd like to mention that this analysis has been done in collaboration with some really wonderful colleagues, mentors, and overall compassionate and ethical humans, Jennifer Hill, Larry Aber, Carly Tops, and Kelly and Ajay Cali. I'll start by giving you some background about the project that we were involved in, so that you can have a little bit of context about our research question and about the methods that we use. Since 2011, about 6.6 million Syrians have led to nearby countries because of violent conflict. At least 855,000 of them are living as refugees in Lebanon. 90% of the Syrian refugees who live in Lebanon live on less than $3 a day. And around half a million of them are estimated to be children and youth. So under this circumstances, it's obviously difficult for refugee children to have access to school and to a normal academic development. And those who do get access to public school face different challenges than the local children, like language differences, different grade placement because of previous school interruptions, and also cultural tensions with the host communities. So some of the efforts of the global community involve after-school remedial programs that take into account the realities of Syrian refugees. The International Rescue Committee created a partnership with NYU to implement and to study a remedial support program for Syrian refugees in Lebanon. The remedial program is called Healing Classrooms. And on top of their academic development, it targets the children's psychosocial recovery. And it does it with awareness of the impact that violence and displacement have in their lives. The original study design involved randomizing refugee sites to receive access to the healing classroom. And the data was collected at the start, in the middle, and at the end of the program. But being randomized to receive access is not the same thing as receiving the actual substance of the program. Among the children that were offered access to the healing classroom, attendance, French from zero to 80 days. And it was all over the place. So hopefully you can already imagine how bucketing everyone who was given access as treated can be a little bit misleading when you're assessing the potential benefits of this type of support. They all got different dosages of the intervention, and some of them are closer in dosage to the control group than they are to their fellow treated. Our goal was to learn what can work for these children's development. So one of our research questions is about the causal relationship between attending the program and the children's reading skills. The sample that we use for this particular question involves around 1800 Syrian children from 33 refugee sites that were offered access to the healing classroom for about 26 weeks. But when we talk about causal relationships, a crucial concept is that of counterfactuals, which basically refers to what would have been if we were able to travel back in time and switch only the amount of exposure that a child got to the program. A perfect randomization would provide us with a good alternative to time travel, but attendance was voluntary. It cannot be randomized and enforced, and those who were able to attend may not face the same circumstances of those who were mostly absent. And when exposure cannot be perfectly randomized, we need stronger assumptions and we need more resourceful methods. So the first assumption that we needed to make in cases like this is that we have measured all the pretreatment covariates that help predict both attendance and the reading outcomes. A second assumption is that we have enough children that are alike across different exposure levels so that we can estimate reasonable counterfactuals. And that is known as having common support. Another assumption is that the level of attendance of any one child should not affect the reading scores of other children. This assumption, by the way, would be necessary even under a perfect randomization. But the thing is we cannot ensure that children didn't help each other outside the classroom. So we have to recognize the possibility that our estimates may be smaller than the true impact. Lastly, there are parametric assumptions that are implicit in the modeling approach that each researcher decides to select. There is always a tacit belief that the final model we choose is well suited to capture the true relationships between the pretreatment characteristics, the treatment, and the outcome. So Bart or Bayesian additive regression trees is one modeling approach that is well suited to make counterfactual estimations. Which means it's helpful to estimate causal impacts of treatments that cannot be perfectly randomized, like attendance. Like the name suggests, it is a tree-based method. So it naturally accounts for nonlinearities. It limits us from making generalizations from Western-based research to Syria refugee populations. And it allows us to adjust for a very large number of pretreatment characteristics. Bart is a backfitting algorithm that involves a sum of weak learners. It incorporates a Bayesian framework and a regularization prior. And this ensures that each tree contributes only a small part to the overall fit. At the same time, it provides us with good tools to quantify uncertainty. Bart can estimate the uncertainty of each counterfactual prediction, and it gives us a framework to assess the presence of common support for causal inference. And also, when compared against all their causal inference methods, it has been shown to be among the top performers. There are a few packages in R that can be used to get a Bart fit. The ones that I have had experienced with are Bayes tree. It has been written in C++ and it is faster than Bayes tree. And also Bart Cos, which is a wrapper of debarts that is already tailored for causal inference. Bart Cos made our lives easier because it automatically computes the posterior distribution of the specific treatment effect that we want to get. And it also automatically flax the observations that lack common support. But Bart Cos is currently focused on binary treatment variables only. So for our attendance question, we began by splitting children into a low and a high attendance group based on the median attendance. We fit the model with this binary version of attendance, and we let Bart Cos flip the treatment assignment. It estimated counterfactuals for each child, and then it computed the posterior distribution of the average treatment effect of moving from low to high levels of attendance. We looked at four different reading outcomes, and we found a significant effect on two of them of moving from the low attendance group into the high attendance group. But not so much for the other two. This estimates, by the way, we're obtained after addressing the missing data, and they already incorporate the uncertainty brought in by that missing data. With a basic approach, we get a distribution of the treatment effect, and not just an estimate point. And on top of that, here I'm showing you how those distributions can vary across different possible scenarios because of the missing data. And the two outcomes that we could not see an effect for are those that were most affected by the missing information, which you can see in the first and last figures of this slide. But what if the missing data wasn't the only reason that we saw different impacts of attendance for those two outcomes? I mean, attendance is not binary in nature. We wondered if there could be more to the story, like, could it have increasing or decreasing returns on children's learning? So we dove into the world of continuous treatments. The idea behind it is that we can estimate a counterfactual scenario for different levels of dosage for each person. If we could travel back in time as needed to adjust the dosage level and test how the outcome changes, then we would know how each person responds to different dosage levels. But while we're not time travelers, we can still use part to estimate those counterfactuals. Once we have estimated the different counterfactual states for each person, then averaging everyone's predictions at each dosage level allowed us to get what is known as the average dosage response function. Notice that this function can be different to a regression-like approach, which would use only the observed states to feed that curve. The causal DRF package in R did a lot of the heavy lifting for us, and it can estimate the dosage response function using BART by calling the base three package. Our results suggest that three of the outcomes have a positive causal relationship with attendance. But the dosage level at which the marginal impact seems to fade is around 40 days of exposure for two of the outcomes, but around 20 days for the last outcome. This gave us additional insight of why an impact was not showing up before for this particular outcome, since we were comparing low versus high attendance using the median split, which was 37 days. This results made sense to us considering that this was one of the most advanced reading tasks, and most of the kids started with very low reading scores. And also the intervention only lasted 26 weeks. We also made an effort to assess the proportion of good counterfactuals that we have at each dosage level. But unfortunately, we don't have time to go into the details of common support right now. But if you're interested in knowing more, please just feel free to shoot me an email. But there's still one reading outcome for which we saw no impact at all. And the first thing to point out is that that particular outcome had a significant larger amount of missing data. We decided to do a little bit more digging about that measure, and we realized a few things. In contrast to the other measures, that one was less detailed. And it provides information of full proficiency on different types of reading skills. And it creates students on a very, very blunt five point scale. Another thing we realized was that that measure was implemented independently by the IRC. And its data collection did not follow the stricter protocols that were used for the collection of the other measures. And finally, we also considered that there could be some unmeasured causes for not responding that particular assessment, which could induce bias in our impact estimate. A good chunk of our manuscript is actually dedicated to discussing that just in case you're interested. So some takeaways don't get too confident just because there was some randomization in your study. Randomization can, it can go wrong sometimes, especially with vulnerable populations. And the answers that you can get when looking at a randomized treatment are not always the most meaningful ones, especially if you want to learn about the substance of a program. So think about whether you're making valid comparisons for causal inference. Because in social studies, if you ask the tough questions, you will often need additional tools to answer your questions causally. Well, some of the tools in R that I found useful are debarts, bard costs, and causal DRF. Understanding the assumptions that you are making and being transparent about them is very important. And also try to recognize the pitfalls of your data. I know it's uncomfortable. Missing data is pretty much unavoidable, especially with vulnerable and migrant populations. So make an effort, both prevented and to address it. You won't really know just how much it matters unless you actually assess how different your results can be under different scenarios. And missing data can have different implications depending on each outcome and on each research question. How refined or how rough a measure is has important repercussions in its ability to capture moderate impacts. So it's always a good idea to get the input from a psychometrician and from content experts at some point of your project. And lastly, without data quality, your models have nothing to stand on, no matter how fancy they are. So make sure to invest in data collection and data verification protocols and make sure that they are well suited for the population that you're studying. And that's it. Thank you for listening and please feel free to shoot me an email if you have any questions. Thank you. ."}, {"Year": 2021, "Speaker": "Benjy Braun", "Title": "We can\u2019t help if we can\u2019t communicate: Conveying data science findings to non-experts", "Abstract": "\"The end-user understanding how something works is just as important as the result.  Concepts and techniques that come naturally to us as Data Scientists can be totally perplexing to non-expert consumers . . . and that\u2019s when critical analysis gets lost in translation. \n\nWe can\u2019t help if we can\u2019t communicate, explores how we bridge the gap between Data Science practitioners and the government executives who use our findings to develop policy.  The talk will explore two use-cases\u2014one failure and one success\u2014from the speaker\u2019s decade-plus of US Federal Government experience to establish a set of best practices in conveying data science findings to non-experts.   \n\nWhy it\u2019s important:\n\nGovernment needs our help, but we can\u2019t help if we can\u2019t communicate.  If we can\u2019t convey what our findings mean and why they are important, essential decisions will be made without the data or analysis needed to back them up. \n\nParticipants who attend this session will leave with:\n\n\u00b7 A set of best practices for ensuring data science findings and products remain accessible, relevant, and actionable\n\n\u00b7 A deeper understanding of how government executives make decisions\n\n\u00b7 Where data science fits in to the decision-making process\"", "VideoURL": "https://www.youtube.com/watch?v=UUIda4-CSXQ", "id0": "2021_21", "transcript": "The next speaker was a member of a step dance team in college. Please everyone welcome Benji. I'm really excited to be here today to talk about this topic. Over the course of the past two or three days, if you've attended the workshops, we've learned some really amazing techniques from some of the most talented people in the art community. And that's really awesome. But here what I want to do is do though, is take a step back from the technical aspects of our job and focus on the human interpersonal aspect. And that's because no matter how awesome our products and insights are, they won't be a thing if our non-technical colleagues and clients can't understand them. So to many of our non-technical colleagues, what we do is a complete mystery. And I've found that it fosters a strange mindset. Data science seems to be able to do both everything and nothing at the same time. So for certain problems, people think they can do everything. They think there were magicians or super talented athletes that can overcome any obstacle. They think there's some off the shelf deep learning and it's always deep learning way to predict any fluctuation to classify any document and reduce human work time by 90%. And while it might be nice to be thought of this way, having these kind of expectations is a really big problem. That's because these expectations are impossible to me. And if we can't meet and achieve them, our work is deemed a failure. On the other hand, there are many who think data science can do nothing and think that their area of expertise their workflow is so innately human and requires so much complicated thinking that no amount of technology can help. Repeatable script instead of hand jam spreadsheet, an app, some basic filtering. All of these are too simple for this bespoke, indescribable work. So we all know that the truth lies somewhere in the middle. There's hardly any human centric effort that can't be aided with technology. Can it solve all our problems? Of course not. Can it help things along in ways both large and small? Certainly. So the problem we find ourselves in is a problem of communication. How can we tell our clients and colleagues what we can do to help while recognizing and respecting their capabilities and limitations? Beyond telling, how do we actually integrate our findings and products into their solutions? In the government, this can be a particular challenge. A lot of policymakers tend to be old school. Much of the work is people driven. There are countless committees and hierarchies, new ideas need to go through before they can be implemented. At every step, there's someone who can stymie our efforts. That's why it's so important for all data scientists to be good communicators, but especially those of us who work in government and who are trying to enact positive change. All right, so why should you listen to me about this? I've worked in the federal space, specifically the DOD and intelligence community for over a decade as both a civilian and a contractor. I've had the opportunity to support operational and be in operational and policy roles and supportive variety of missions, including counter terrorism and counterproliferation, as well as supporting supply chain risk management, like I am now, IT integration, training and even diversity equity inclusion initiatives. And all my stops along the way, I've had a lot of communication failures and some successes. So let's look at one of each one failure and one success and what we can learn from them. So let's start with an example from early in my career when I was supporting the diversity inclusion section of the human resources directorate of a federal law enforcement agency. At this time, and I'm sure still, leadership was very concerned with the feeling that minority employees were receiving disproportionately low performance assessments. This led to widespread resentment and distrust of the process, but it was just a feeling. They didn't have any data to back it up. So when I heard about this in a meeting with the chief, I immediately said, I can see if that's true. Let me get to work. I knew that we could test her hypothesis that minority employees receiving disproportionately low scores through a chi-square test. And so when I give this talk to students, I go into the details about how I got the data and so on, but I'll cut to the chase here. These scores were not due to random chance, like p-value of 0.000001. And when I got this result, I was really stoked. I had the data that the chief wanted and it confirmed for thinking. I wrote up a report and I put a presentation together and said of a meeting to talk to her. And it was a total dud. I tried to explain it, but it just didn't land. And she went back just to asking me for the other regular reports I was giving her. So why did it fail? It took me a long time and some good reflection to come to this, but what I've settled on was that I just told her I was going to solve it. I didn't actually talk to her about the results and what they would actually look like. So when I showed up to her with a p-value, there was nothing for it. She didn't know what to do with it. I never asked her how she was actually gonna use this information, like where, what she was gonna do with the results of this analysis. And I made it about me. It was just a stats report, not part of an integrated solution. My customer could actually use. So the bottom line for this is that if you're final deliverables of p-value, it probably isn't gonna land. Okay, so let's turn to another example at a different agency where I did the opposite, where I was successful in delivering a data science insight using better communication. And interestingly, this problem was actually more complicated, and had a more complex solution. And this time, the key of success was working with the clients to understand their process, how it would be used, and how data science could help. So every year, this agency would devote a large percentage of its budget to grants for R&D. But previous year's grants proposals were kept in a folder as Word documents with just alphanumeric codes as the names of the files. So basically, it was impossible beyond remembering from past years if a new proposal was like any formal proposals. They had no way of comparing past proposals to new ones, no way of knowing if they had awarded something similar in the past and if it was a success or failure. Or if they hadn't awarded something similar in the past, how was the new proposal different? How may it actually change things? So the solution we came up with was to read all the reports in and use Dr. Vakt to take a current grant and compare it to all previous year's grants and get the top and most similar grants. So to be fair, we didn't use R for this, but we made a shiny app to use the output. So that's how I'm justifying using this example here. And what was really successful about this is that the clients actually use the results of this in the grant awarding process. So why was this successful? I explained from the very beginning what the outcome was going to be and how it could be used. I explained using the minimum necessary info, so I wasn't talking to them about skipgram or bag of words or things like that, but I didn't dumb it down either. I explained to them that how what we were doing was in magic, it was math, cleverly applied to and that the technique had been in use now for a while. And they actually started using the results of this system in the grant awarding process. Now, to be fair, the client didn't actually use the app themselves, we did. And I'm sure a lot of you who have worked in government space are used to setting up with BI tools like Tableau or other things like that, where your end customer still ends up having you print out the reports for them to read. All right, so what are the basic, the do's and don'ts then of communicating data science findings to non-experts? Number one is understanding your end user. So what are their strengths and weaknesses? How will they use what you're giving them? I think if you compare the two case studies that I've talked about so far today, you see that in one, there wasn't really much of an effort to understand the process or the end user. And the other, there was a lot of care and effort. And that goes a long way in actually being able to affect change and get what you're producing into action. Really important, especially in government, is understanding your end user's boss. This is so essential because almost every decision will require approval or at least consensus from multiple people and multiple levels of authority. So it's not just enough to understand your end user. You need to understand their ecosystem if you want your product to be successful or your insight to be heard. So number three is don't make it about you. So don't do what you want to do. Do what your customer needs. Does that mean sometimes using a less sexy approach so you can explain it? For sure, even in the first circumstance I talked about today, something pretty basic in our minds wasn't able to land. And that's really because I made that process about me and not about how my customer could actually use something. So for be humble, you can only do so much. No matter how smart and talented you are, you're a part of a large organization and you have to understand that. So we're not going to change the world in a day and we need to understand how we can contribute while staying grounded. So five, determine and advance how your product will be used. So this is more tactical but totally vital. Before you get to work, work with your end user to see and deliver a product. There will be usable ideally in the existing process. So in this case, whereas in the first example, I did some analysis with no thought of how the end user would actually apply it. In the second case, I spent a lot of time working with the end users of understanding the process and how the analysis could actually be used. And I think that's why it was actually successful. Six, bring them along your journey. And I think what I mean by saying this is keep people informed as you move along. This lets your initiative, your insight, your product become something they feel ownership in. And that will help lead to its adoption and lead to their greater understanding. And last seven is be prepared to defend your work. So take your work seriously and your customers will too. Even though this should be a user-centric process that doesn't mean they're calling all the shots. So you're the expert here. You need to work to understand their process. But keep in mind who you are and your talents and skill sets. So that's my talk. Thank you very much. And please feel free to get in touch with me to talk about this type of work in the government space."}, {"Year": 2021, "Speaker": "Abhijit Dasgupta", "Title": "Multilingual pipelines for data analyses: R as glue", "Abstract": "We live in a multilingual computational environment, where each language provides certain advantages in terms of developed packages and capabilities. Often, we are faced with utilizing multiple languages to create efficient data analytic workflows. In this talk, I'll describe some experiences in integrating languages utilizing R as the backbone and glue.", "VideoURL": "https://www.youtube.com/watch?v=XJhRvb1y6zI", "id0": "2021_22", "transcript": "And our next speaker, beyond being this repeat speaker four times now, he's past the hat trick, he's onto the four feet, has been tossed around in the air so much, and it's Ikedo, he's being tossed around as part of a Japanese martial art, you know, unless he's a stunt double in a movie and we don't know about that, which would be kind of cool if you were to be very cool. He's been tossed around so much that he should get freaking fire models for it. So everyone, please give a warm welcome to Abajit. So today, I'm going to talk about multilingual pipelines, language in terms of computer language, not in terms of, you know, the entire conversation about English and Spanish. So one of the things that I'm going to start off saying, I'm both an affiliate at Georgetown and I work at AstraZeneca, both of which are in the DC area. Standard disclaimer, this stuff is personal, nothing to do with work. So if anyone's calling it work, don't fire me. So the reason for this talk, the reason for this talk is that I think pretty firmly that we are now in a multilingual world as far as data science is concerned. I don't think you can get away with just using one language. You might not use other languages well, but you can't get away with just using one language. And so I think that we are actually in a place where we are polyglots by necessity. And so there's a few ways that I have used multiple languages. Back in the day when, you know, this is 10 years ago, when I got frustrated with R and how long it took to load stuff into R, I used Python to ingest data, which was faster and then used R for analysis. This was by conductor stuff. And then R Mark done for reporting. So that started me on this. And this is like way before the days of reticulate or anything. This is just literally Python ingested, makes into a form that I can use R to take it over. Some another path that I've used is I've got data on databases and I use SQL for the munging and extraction and then R for the analysis. And that can entirely be from R could be SQL R separately. I know there's several of you who have sort of come from this sort of background coming from a database background and learning R for other stuff, typically visualization. And this is the last one is something that I'm actually doing right now. It's I'll talk about a little bit later in my talk, but it's using R for the data ingestion and munging. So how far we have come that we went from not using R for data ingestion and munging to using R for data ingestion and then using both R and Python synergistically in a machine learning pipeline and then kicking it back out to our next, you say, our end Python for visualization because certain things now, especially with the JavaScript stuff is better in Python than in R in terms of interfaces. So lots of different ways of doing this. And this is not the limiting thing. You can I know people who have SAS and are using R on the side from SAS IML. And entire SAS via Python thing is happening. But a lot of stuff from R. So there's a huge R JavaScript thing. There's a huge obviously R Python thing going on. So lots of multilingual efforts going on. So part of this is based on use cases, right? And based on where things are. So the data is where it is. I'm not going to pull it out of my SQL database just because I need to just because I can or need to. A lot of the bioinformatics data set that I work with now are actually distributed as RDS files. Or it could be on the web and I have to download it. This is becoming more and more of a use case that the algorithms are available in one language or the other. This is especially true for deep learning kinds of things which are almost entirely Python based. Whereas a lot of the mundane visualization reporting is R based. And so I'd rather not have to port everything to R to do. I'd rather have a good product where it is. We can now decide which parts we want to make speedy. So certain things are faster in certain languages than others. And even without going to the C++ or Julia or shell routes. There are pieces you can do that will be speedier than others depending on the right tool. And as I'm not starting a fight with the SAS folks, even though it happens. And I'm avoiding R Java with a 20 foot pull for this talk and really in everything. R Java has done better but I'm still not taking it. So about this talk, I'm not going to talk about just data ingestion. Data ingestion is an old problem. We've sort of figured it out. Data is in different formats. You can either bring it to a common format or just read it natively. Haven is a great one for the SAS folk. We've got all the DBI based stuff for databases, HGTR for APIs. There's stuff for JSON. There's stuff for Excel. So we've sort of gotten a pretty good handle on this. But really actually running stuff outside of R but from R. So R being the controller. That's sort of what I'm interested in here. And so there's a few connections that I use. This isn't by no means exhaustive. This is just the tip of what I tend to use. You guys will probably do a lot more. Actually the one connection that is not here that I'm increasingly using is with JavaScript. There's databases. There's stand. For those of you who don't know that stand logo that's that red big dot with the sort of contrasted S in it. Python of course. The shell, SAS. And there's different directionality set. R and Python tend to be bidirectional. Stand tends to be I'm using stand to just bring stuff into R and so on. But this is sort of my ecosystem that I keep playing with. Like I said, this is each of you can have your own. And there's all sorts of different players that you can put into this. So I know that people want to go to particularly this. Particularly this cool and fun and so on. But let's hold our horses. Just talk about a little bit about some of the other things that we can use and I use. I'm old. That's going to be set by the gray hair on my beard. So I still use the shell. And I know there's young people use the shell too but I'm going to just say that because I'm older, you keep using the shell. So Mark Weisman who talked yesterday and I teach a big data class at Georgetown. And one of the things we've actually talked about a lot is that these kids have to be taught the shell. Otherwise we can't teach them big data on Amazon. So shell scripting is still important. And it's available for everyone on every platform now. And I think it's just as important as it has been. But one of the things that shell and all of this stuff is that people forget, even though it's old, it's really efficient because it was developed at a time when there was no compute power and you couldn't just throw another two GPUs at it. So these are really fast and really quick. Not quick UIs but they're quick. And so everyone knows the system stuff in our that's people have used that I think more often than not. But the stuff that I actually got excited about last year when I discovered it is that you can actually pipe a shell command into an F read for data table. And so you can do some pre processing at the shell level before you even put it into R. And it turns out that someone actually benchmarked this and showed that you can actually something have something a 15 fold reduction in memory and threefold increase in speed by doing this for a particular thing. They were just doing a grep, just a search pattern. But this is really, really useful. So for some things, we can certainly do set an arc and things like that, which are fast and just do things well. And you can just do the pre processing there and keep things lighter in our. And there's a suggestion I haven't played around with as much as I should, but there's also G new parallel, which allows some of this shell stuff to be paralyzed, which makes things even lighter and faster. So something just remind ourselves that, you know, shells still around and still good. And it does talk with our quite beautifully, actually. So something to something to sort of think about. Database, of course, are a very common thing, but not just for the data side, but we're actually using it to munch using SQL, but still living in R. So the ODBC world that we have. The cool thing that happened, I think maybe five years ago now with the with the DB ply and DB player package and some other stuff that our studio did, is that you can do munging in a database using the player syntax, which makes it much easier. So I don't have to remember SQL as much as I used to have. So probably can you do. DB player syntax to run stuff in in in an SQL database, but you can also using the SQL SQL DF library use SQL to run stuff in R. So actually, it's really we can really be bilingual here. And I think this is a this is a boon for everyone who's, you know, come from SQL and learning R or have coming from R and having to learn SQL. And I know several people who are making that transition right now in a corporate environment. And the fact that you can be bilingual in this and take it either way is I think it's fantastic. And this is sort of old, not necessarily brand new cutting as so, but I think I use it enough that it's that it's cool. So given time, I decided to choose my battles here. And so I'm going off to reticulate and it took me about to understand why reticulate was named so until I found the picture. So reticulate is another form of Python. I think many of you probably know that, but if you didn't, now you know. So reticulate is is a cousin to anaconda is cousin to Python. So the big thing that reticulate did is it allowed for a persistent Python instance to run alongside R. There's always been like little bits where you can just run something into Python, have it put an output into the onto your disk, pull it back in. And then next time we do it is a new Python session. It keeps a persistent session on, which means you can ping on me, if you will, can do something in R, take it to Python, do something in Python, bring it back to R and so on and so forth. And that is useful in a lot of different things. And I have actually a use case I'll talk about in a little bit that goes to that. Just a side note for those of you who are in Python, I'm getting less and less fond of Conda and more and more fond of the traditional pip through poetry just works better and faster for me. And you can use any of these virtual environments in reticulate using the use Conda or the use virtual end functions. So really quick use case, I know this is R.Gov and I used to be at NIH, which was part of big part of Gov. So I'm going to talk a little bit about bioinformatics, which is sort of 60% of what I do now. So we're looking at things called single cell RNA sequence analysis, fancy name basically says we have a way of taking a sample, separating out the cells, finding what the genes are in each cell and trying to figure this out. Because it's bioinformatics, bio conductors the way people think. So R again. And so things like C-RAPT, which is more pure R based than bio conductor, have already been out there, they're beautifully documented, they're very popular, they're widespread. But there's an increase interest in using deep neural networks and variational auto encoders to look at this kind of complex data. And I'm kid to not, there's literally a deep learning method of the week for single cell RNA data in nature, or nature genetics or data or nature methods, or one of those. There's literally one every week. So it's like it's sprouting and mushrooming and doing all sorts of, you know, reproductive things in the ether. And so these methods are leveraging TensorFlow and torch and pyro and technologies that are sort of based in Python. And the thing is that both classes of tools, both the R group pipelines and the Python pipelines are actually quite useful to gain biological insight in different ways. And you sort of want to use the Python bits to inform the R bits and vice versa. So one of the things I've been working on for the last year or so is creating a pipeline. The details aren't necessarily important as to what the pieces are. But what is important is that there's pipelines that are running in R and in Python in parallel, they interact and then they sort of merge at the end to do sort of language agnostic stuff. And without particularly this really can't happen. So this is something that I've been working on for different purposes over the past year. And it's really fascinating to see how well sort of these, these co-lingual pipelines can be worked out now. So this is sort of what motivated me for a lot of this. The other piece that motivates me in this sort of multilingual world with R is R Markdown. And so I've become a big believer in polyglot R Markdown. Well, for those of you who didn't know, R Markdown actually always been polyglot. When NITRA was written, there were chunks available or processors available for over 45 languages, including some shell stuff, some of our competitors in SAS and Stata and the But it's always been polyglot. But with Reticulate, you can actually build because Reticulate keeps the Python on as you're doing things. And so I sort of took that to heart early last year, just before we were going to lock down, I was asked to teach a 3D workshop in data science for Python. And I wrote the manual and the manual is written in R Markdown. But there's no R code in it. It's all Python code. This is open in a family's interest sector, this is a little round. But the fascinating part of being able to run do Python, an entire Python book, essentially, using R Markdown instead of doing Jupiter and anything else was really nice since I'm an R first kind of person. And this was really, really nice. And with R Markdown in R Studio, you can do a lot of this sort of multilingual things in R Studio. There's now native support for six different kinds of chunks, other than R. And so you can do a lot of stuff internally. So there's some stuff that we were just teaching in our visualization class, where we're putting D3 into these Markdowns to create D3 graphics and the like. So there's a lot of stuff that I think really interesting and the ecosystem sort of ripe for exploiting this now. Between what R Studio has, what Visual Studio code has, and what the kibbuzar are in terms of packages. So just leaving with some final thoughts. I think we've got the critical mass of connectors, which don't have to be jerry rigged this much. There's actually packages. So reticulate, we've talked about RCPP, the old nugget. Julia call gets to interact with Julia, R extender, works with Rust, R2D3 with D3. There's the entire HTML widgets group packages that connect with JavaScript, torch and Keras are R packages that are using reticulate to connect to the Python versions. Alt-Air, I always laugh at because R package Alt-Air is a reticulate wrapper around the Python package Alt-Air, which is a wrapper around JavaScript package Vega-lite. So it's like we're using a three-step removed package through two translators. So that was sort of fun. And then the reticulate polyglot piece is, I think, something I use a lot. I think it's something worth thinking about for publishing, because I think the R Markdown, Bookdown, Blogdown, and Ecosystem is a very mature sort of data science and even non-data science environment for publication. And using sort of these nature engines, you can do stuff that is not just R. You can do a lot of other stuff. So in the interest of time, of course, I'll stop here. So thank you. And I think I end up bringing up the rear every year, but I love it. And thanks for your time. And hopefully next we'll see each other in person."}, {"Year": 2021, "Speaker": "Sydney Coston", "Title": "Using R Shiny to visualize stem cell treatment data over time", "Abstract": "This presentation will introduce an R Shiny app that my partner and I have created to examine the trend of negative outcomes after stem cell treatments over time. The dataset used is from Sloan Kettering Hospital and includes 5 years (20 quarters) of de-identified data from adults and children. The app allows the user to see the proportion of patients who experienced each negative outcome (toxicity) per quarter for the data set of their choice. This app reveals concerning trends in certain toxicities over time.", "VideoURL": "https://www.youtube.com/watch?v=gBZ9r_7wUjs", "id0": "2021_23", "transcript": "So today we have a cadet from the academy speaking to us. And she's the oldest of five children and one of the other ones also attend West Point. So that's a family tradition there at that point. Everyone please welcome Sydney to the stage. As he mentioned, I'm Sydney. I'm a cadet my third year at the United States Military Academy. And I'm very excited to speak to you all today about a project that my partner and I have been working on this semester. So unfortunately my partner is not able to speak with me today. So I'll be presenting for both of us. The two of us have created an app in our shining that allows the user to examine trends in the side effects of stem cell transplant treatments over time. So we started this project over the summer where the two of us shadowed Dr. Sean Devlin who's a biostatistician and memorial Sloan Ketter and Cancer Center. Dr. Devlin gave us access to de-identified data from patients who had undergone hematopoietic stem cell transplants to treat certain blood cancers. So during these treatments, the patient undergoes extreme chemotherapy and radiation treatments. This results in the destruction of the cancer but it also destroys many of the patient's blood cells. So after the cancer has been eliminated, the patient is given transplants of stem cells and these either come from a sample of blood taken from the patient before they're treated for the cancer or from a donor. So autologous treatments are treatments that use stem cells from the actual patient and allogeneic transplants use stem cells from donors. And each of these transplant types has its own set of possible complications. Allogeneic transplants especially have a host of undesirable side effects. And one of the most serious of these side effects is called graphs versus host disease. This is where the immune system and the donor cells attack the host cells because they don't recognize them as their own. So a common method used to mitigate the threat of severe graphs versus host disease is to deplete the donor cells of T cells before they're transplanted to the patient. And this minimizes the risk that these cells are gonna react negatively to the host body. Another variable in graphs versus host disease is donor selection. So donors that are closely related to the patient or are genetically similar pose a lower risk because the T cells in this blood is more likely to identify the host cells as belonging to the donor. Changes in the effectiveness of T cell depletion and identification of donors are partially responsible for some of the variation in the proportion of patients suffering from graphs versus host disease after allogeneic treatments. So in addition to all of these different variables, we were also given a mix of pediatric and adult data and this made it difficult to know how to handle the data. So my partner and I decided that we would break down the larger data set into smaller data sets that were more specific to the treatment type and the patient being treated. So the table on the slide shows the original data set. Each row represents a patient with a multitude of attributes. The first few columns of this table indicate treatment specifications. So that would be type, TCD, HTC type and service. And the remaining columns up until the very last column are composed of binary results represented by zeros and ones. So this is pretty simple, whether or not the patient experienced x negative side effect or not. And from now on, I'm gonna refer to these side effects interchangeably as toxicities. And then finally, the last column in the data set indicates the quarter in which this patient was treated. So these quarters range from negative 19 to zero, where zero is a present quarter. So we were given five years worth of data. On this slide, you can see how we modified the original data into data that was easier to work with. So first, we broke the original data set down into pediatric and adult data. We further divided those data sets into autologous and allogeneic data. And then lastly, we took the allogeneic data and we split it up into the different treatment types. So these treatment types just refer to the way that the donor blood was collected and delivered to the patient, as well as whether or not the blood was depleted of T cells before it was transplanted. So for each of these data sets, we found a proportion of patients that experienced each toxicity per quarter. So this gave us a total of 14 data sets with 20 rows each. And this table shows one of our finalized data sets that went into our app. So as you can see, each column is a toxicity. And then on the right, the last column is the ordered quarter and we have one entry per ordered quarter. And then the individual cells represent the proportion of patients that quarter who experienced that negative side effect. So once we add our data sets, we set about creating our app using Shiny. The basic features of this app include data visualization, linear regression equations with associated R squared values and a table of outliers. So pretty quickly, I'm going to give you a demonstration of how the app works. So this is our app right here. And as you can see, the app allows the user to view the proportion of patients who experienced each toxicity per quarter for the data set of their choice. So first the user can choose their data set. And these are the finalized 14 data sets that I was talking about originally. And then next they can choose the toxicity of their choice from within that data set. And this toxicity list will update once you choose a different data set. So this data is displayed in scatter plots with trend lines that show the change and the proportion of patients experiencing each toxicity over time for the data set of the user's choice. We use plot link to make the scatter plots so the user can hover over points and see the coordinate where x is the quarter and y is the proportion. And since some of these linear regressions predicted proportions less than zero, all negative predictions were changed to zero. So as you can see here, this trend line would have gone negative but we changed it to zero. And then finally with this slider bar, you can change the standard deviation of the residuals that you would like to consider normal. So when you change that, it updates the gray area around the line of us fit. So this is the first step. So this is the first step. So this second tab will show you the regression equation and the R squared value from your data set and toxicity of choice. And then finally in this third tab, a table is displayed of all order quarters with data points that are outside the given tolerance that you set right here. So when you change the standard deviation, again, this will update. The data points. And the intent of this tab is just to help the user identify quarters where the proportions are significantly different than the rest so they can investigate possible causes. And so although proportions that fall below the given tolerance indicate success rather than failure because it means that a lower proportion of patients experience the negative side effects, we still included them in the table because this is for medical professionals. So we just thought it's best to leave all the data in there. So moving on to interpreting results, there are three basic ways so we can analyze the results from this app. So first, we can look at the slope of the trend lines. Second, we can look at outliers in the output table. And third, we can compare results across treatment types. So the results of this app show that for most transplant methods and toxicities, the proportion of patients experiencing the toxicity decreased over time. And this is obviously a good thing. And then some exceptions are shown on the side. So on the left, we have one exception that doesn't necessarily cause concern. And this happens when the proportions of most of the quarters hover around zero. And these trend lines are often horizontal like this, but that doesn't mean that it's bad just because it's not negative because obviously proportions are never gonna go below zero. The trend lines, however, that do cause concern are like this picture on the right where it's positive. So this is an example of the proportion of adult patients who were readmitted to the hospital within 30 days of an autologous treatment. So this uptick in readmittance may not necessarily be a bad thing. It could simply be the result of a change in protocol, increase in precaution, et cetera. Either way, it's worth investigating. So as I mentioned before, the second way we can interpret the results of this data is comparison across treatments. So just some general trends, autologous treatments in general had lower rates of serious toxicities and lower rates of fatality. And this is to be expected just because there are fewer risks associated with using blood from the patient as opposed to using blood from a donor. However, there were more risky and less risky allergenic treatments within that subset. So the allergenic treatment that appears to be the riskiest as a treatment that uses unmodified bone marrow. This treatment had very high standard deviation and very high average mortality rate. In general, T-cell depleted transplants also showed lower proportion of patients experiencing toxicities, especially when you're considering grass versus host disease and suboptimal cell dose. And what's really interesting is actually that the toxicity patients with suboptimal cell dose was zero for every single T-cell depleted treatment. So while the after mortality rate across quarters was about the same for T-cell depleted transplants as it was for transplants where they did not deplete the T cells, the trend lines for T-cell depleted transplants have more negative slopes. So this kind of indicates that the hospital is getting better faster at treatments where they deplete the T-cells than where they don't. So we can't say that the treatments alone are the cause of all these variations because treatments are assigned to patients based on conditions that affect their proclivity for each toxicity already. And our app is merely a display of the data that medical professionals will be able to interpret. And then finally, the third way to analyze our results is by looking at outliers and our data. So this example shows the results from adults who received allogeneic treatments and got third or fourth grade grass versus host disease within 100 days. So the point at quarter negative 17 marks by this orange star right here is disproportionately very high compared to the rest of the data. So we had Dr. Devlin look into this outlier and he found that Sloan Kettering had tried out a new protocol during this quarter, which was not successful. So this just kind of shows a real life example of where you can look at an outlier and find a tangible reason for why it was out of whack. And so while my partner and I lack the medical knowledge and history of hospital protocols to fully interpret results like this, the target audience of this app is the doctors, biostatisticians like Dr. Devlin at Sloan Kettering who will actually know the medicine behind the numbers. So the intent for them is to interpret these results and use their findings to improve stem cell treatments and also to better know when to use which stem cell treatment. So in the future, we would like to improve our app. We would like to allow the user to first set a range of quarters that they would like to see the trend line for. So this addition would allow the user to view regression lines and outliers over specific time periods. This could be helpful to analyze the results after protocols have been added, after protocols have been removed, things like that, or maybe new technology was introduced to the hospital. We would also like to include a summary table of all the toxicities with positive slopes from the specified data set. So this way instead of clicking through each data set to see where we have positive trend lines, that'll just be an output table as soon as you pick your data set. And then finally, the biggest challenge in improving our app will be accounting for certain data sets with very small sample sizes. And these data sets almost exclusively fall under treatments given to pediatric patients. So honestly, this is good because it just means that children need fewer stem cell treatments than adults, but it does make linear regression maybe not the best choice for these data sets. So this is an example right here. And it's the proportion of pediatric patients who experienced primary graph failure after an allogeneic treatment. So as you can see, almost every data point, the proportion is zero, except for the data points at negative 17 and negative two. And these data points show that 100% of patients had primary graph failure for a quarter negative 17 and 75% had primary graph failure for a quarter negative two. These numbers seem astronomically high when you're thinking of them in terms of proportions. But when my partner and I looked back into this data set and investigated, we found that there was only one patient in quarter negative 17 and only four patients in quarter negative two. So obviously this is not a large enough sample size to derive really meaningful results from a linear regression. So an alternative method that we thought we could implement for these data sets is Fisher's exact test. This would help out a lot with these data sets because that test has no qualifications for sample size or anything like that. So we are looking back into that. And then other applications of our data set just kind of include different ways that this method can be used to visualize data. So while our application is specifically for stem cell treatments, it could be used to display trend lines from any data set in a similar format as the modified data that we had. And even if the new data comes in a format similar to the original data set given to us, the script that we use to create the smaller data sets can be modified with very minimal adjustments to apply to other data sets. Additionally, these linear regressions can be fine tuned using hospital data that my partner and I don't have access to. So because of patient confidentiality, Dr. Devlin had to clean all data of any identifying features before we could use it. So this includes potentially very impactful variables, patient age, gender, pre-existing conditions, things like that, which was really unfortunate, but patient attributes like these could be easily inserted into data sets in the regression equations, especially by biostatisticians because our script is pretty intuitive. And these variables could provide even more information on the success of stem cell treatments within smaller subsets of the groups that we already have. So our hope is just that this app can help slow and catering, advance their stem cell treatments, maybe that they could even use this app for other data sets as well. And before I close out, I would just like to thank you all for listening to me today. It was a great opportunity and I'm very flattered to be chosen for this. Thank you to our hosts, you guys put on a great conference. And then lastly, thank you to my project partner, Tobias who couldn't be here, but it's super smart and helped me create this app. And then to my research advisor, Major Lasseter, for guiding us along the way."}, {"Year": 2020, "Speaker": "Gwynn Sturdevant", "Title": "FasteR Code: Vectorizing Computations in R", "Abstract": "Current innovations in coding have focused on ease of learning and reading. Unfortunately, a byproduct of these features is an increase in computation time for some coding. This talk will focus on vectorizing R code, or writing code that reduces computation times in some cases.", "VideoURL": "https://www.youtube.com/watch?v=d7Cudw0EJNg", "id0": "2020_01", "transcript": "She lived in New Zealand, the birthplace of R for 15 years. So please everyone welcome Gwen to the stage. Hi. Can you see me, Jared? Now I can. Oh, perfect. Okay. So hi, I'm Gwen Steard Event. I'm a postdoc at Laboratory for Innovation Science at Harvard Business School. I'm so excited to have you here at my talk and to listen to all the other talks of this conference. What a great thing and thank you, Jared and everyone else for putting this together. I know how much work it involves. So let's get started and we're going to talk about faster coding or vectorizing computations in R. I did a little bit of a talk about this at use R 2020 and I'm not going to go through all the details, but basically I had a sample. I was going to use a bootstrap to estimate the population mean. So I wrote some tidy verse coding and it was taking forever to run. So what I decided to do was I thought, well, I'll just write some vectorized coding. I'll see if it runs a little bit quicker. And in fact, it did. It ran much quicker. So here you can see how much quicker in fact it ran. So this is the tidy coding. It took a lot longer, sometimes about nine, 10 times longer. And so I thought, oh my goodness, I didn't realize that. I was really surprised that the tidy coding took so much longer and I was thinking, oh, you know, vectorizing is a really good thing. And of course, we'll talk a little bit more about what vectorizing is, but I want to get back to the slide just a little bit because I think that there's an important point about some of the really basic differences in tidy verse coding and in vectorized coding or, you know, some people, it's basically written in base R. So if you look at this vectorized coding, or let me just start with the tidy verse because I think probably more people are familiar with this. So tidy verse coding, you read from top to bottom, right? So we have this, we take the sample, or, you know, I change it to a daddy frame. I take the sample, I pull the splits, and then I take a column mean. So it's kind of, you know, you read it like you read English. Whereas vectorized coding, or which, you know, is a base R, first you take the sample, so it's kind of, you know, it grows from out to in. You take the sample, and then I'm replicating the sample n times, and then I take the column means. So that is, this is much more familiar in, you know, in algebra, in algebra when we come, you know, when we do a composition of functions, we go from out to in, and it gets bigger, right? You know, it gets, it goes like that. So I just wanted to point that out briefly here at this slide and because we're going to see it again. Okay, so when to vectorize, now I really like vectorizing functions. That's kind of, you know, one of the things that I think about first is how we're going to get the code to run fast, but, you know, don't be like me, don't spend forever, don't spend forever vectorizing your coding, and this is just a simple thing. It says, if you do it five times a day, and it takes one second, over five years, you're going to save two hours. So, you know, keep that in mind. Keep that in mind when you are thinking about vectorizing functions and how you want to work on it. So, right, and what I'm going to talk about today, this is a tweet from Elon Musk, where he was a little grumpy. He took four COVID rapid antigen tests, and on one day, two came out negative, and two came out positive. And so what happened in, with that test is that, that COVID test trades speed for accuracy, right? So it's quick, but it's not as accurate. And there's other things that aren't accurate as well. For example, blood pressure, your blood pressure measurement, it varies all the time. Your cholesterol levels, and also, what's the third one? Cholesterol, oh, it's fasting blood glucose, which is used to diagnose diabetes. So these things vary a lot, and so there's a lot of noisy measurements, just inherent biological data. And so I'm gonna put some complicated equation on the side. Say, for example, that we only have a really noisy measurement. What we want, what we will do, and we're want to study time to event, will, which is survival analysis, we'll have this really horrible discrete survival analysis formula here, and then we'll have this, we could have this gamma i and delta i t i, which are to take into account sensitivity and specificity, right? So this is the specificity measurement, this is the sensitivity measurement, and why we would be closing the derivative, coding the derivative. I mean, I guess the parameter that we're probably interested in is beta, is the treatment better than the control, which arm does better? Right, what was I saying? Oh, yes, and you might want to vectorize, you might want to code the derivative because it can help some things to run a little bit faster. So, right, that's really icky, it's really a horrible equation. And so the first thing that I'm gonna tell you to do when you code something like that in R is to simplify as much as you possibly can. And so if we look at this coding, we can see that there is a number of, you know, repetitions, we have this one plus e to the gamma naught j to the negative e to the e x, sorry, x transpose, the xi transpose beta, okay? So we see that repeated a number of times. And so if I wanted to simplify this function, I could make this substitution. And if you look carefully, you'll see that, oh, look, this is one minus, this we call this the has a risk function right here. So it's one minus that. So, right, so if I wanted to simplify this function even further, what I can do is make it like this. So it's much easier to deal with, okay? So now we've kind of simplified it. Now the next step that I recommend doing, and you must do, if you're ever gonna code something horrible in R, is to get a lot of clarity on what's happening in that function, right? So exactly what's happening. So let's look at this again. And let's take an example, you know, T. So in this case, we have TI, but we also have TI observed. There's two different values and the reason for that and because there's a lot of noise, there's mismeasurement. So let's just look at this first part because that's the simple part. And I have this j goes from, it's a product, this capital pi means product. And so this j goes from one to five. So I have the hazard risk function evaluated at xi for a person i at gamma not j beta, right? And so the only thing that changes is the j. So I'm gonna have this function evaluated at gamma not one and then a two, I'm gonna multiply them all together, right? So this is what I have in that first part. Not very pretty, you know, not very pretty. You can see why they did this, but it's there. So let's look at this one here, this next part. So this next part's a bit a little bit icky. And so let's just take one value of k. So if k is one, then this whole thing, j will go from one to zero, this goes away. And I'm just left with the complement of the hazard risk function evaluated at gamma not one times delta i one, right? So that's all I have. Now let's go back and if I do this at two, what happens? Well, I end up with j goes from one to one. So I'll have the hazard risk function evaluated at gamma not one times the complement of the hazard risk function that I evaluated at because k is two now at gamma not two times delta. So you can see the pattern, right? So I just add this and then if it's three, I'm going to have this evaluated at gamma not two. And I just want to apologize for going through a lot of ugly math, but I think that the coding is really important for you to understand this little bit of coding, even though it's not the derivative, so that you can understand the rest of how I'm vectorizing it. So here I have three gamma not one and gamma not two. And then I multiply by the complement and multiply by capital delta i three and so on. So that's what happens. Okay, so before I'm going to, I go into vectorizing this whole thing, I want to talk a little bit about these much simpler functions, this capital gamma i and these delta, delta i, k or whatever. So, oh, excuse me, before I do that, let's talk about the derivative of this function. I forgot about this. So let's look here. So this is not a derivative taking class. I'm not going to go through the whole derivation of this, but I do want to talk a little bit about, if I'm going to take the derivative of this thing with respect to one of these gamma nots, whatever, L, that everything else is kept constant, right? So that's the way that we do this. This is the product of all of these ones that don't include L and then I have the derivative with respect to gamma not L of this hazard risk function. I just multiply it at the end. Okay, so now I'm going to talk about, what I talked about earlier is doing the coding, this and these things, because I think that's a good, so a good place to start. So in terms of vectorizing, so indexing is something that we talk about in R and for loops is one way to do it, there's other ways to do it. I'm going to introduce you to a new way to index, or perhaps you've seen it before, I don't know, but I'm going to introduce you to another way to index in R. So we have this equation here. So this is the same equation that we saw in the slide, an earlier slide. And what I'm going to talk about is how to find this. So what I want to do is I want to multiply, let's just see, I have an example here. I want to multiply these Phi's. So if I have these TI observed, it will, is two, then I'm going to be multiplying just this number here. So if I'm looking at that, I just want to multiply these, which means if I'm going to multiply perhaps I want to multiply across, and that's a good way to do it, the rest of these need to be one. So let's see, I have this one as five, and I'm just going to talk about this, so I go from one to four, so that means I'm going to multiply all of these, and then this one, which is one, will be one. And then if this is one, then this goes from one to zero, so it goes away, it becomes one, and so then all of these will need to be one, right? So how can we think about doing that in a way that's really quick and really fast? So let's look here. So here's my TI observed, my TI, oh, excuse me. Oh, it's good, I think it went right back to where I was. Sometimes I touch my mouse and it goes through the slides. So anyway, if we're going through TI observed as two, then I want to everything, all of these in lots of fee that are greater than, so let me see. So I just want to multiply this one, I said that before all of these need to be one, right? And so I can use this lots of I to index and say any of these lots of fees that are bigger, you know, that have where this value is bigger than or equal to where the index is bigger than or equal to the TI observed, I want them to be one, right? So I just replace. So for example, this one, you know, in fact, this case it is one, right? But for this one here, all of these that are bigger, I use this lots of I to index off of it and I want to change all of these to one. So in other words, what I'm doing is I'm going to make it look like this, right? So I have my point eight here, all of these are one, this one is also two, so I have a point eight. And so how can I do this? Well, it's really simple, it's one line of coding and here it is here, right? So I have lots of fee, I have lots of I, I say whatever of these lots of I's are greater than or equal to T zero, change them to one and then I just do this apply and I find this product and I've found this part, which is the most difficult part. And then this one I can just index off of use the TI observed, right? So, this is survival analysis, so DI observed as either zero or one, either an event occurs or it does not occur and so one of these will go away and I'll be left with the last value of fee. And then I've done the whole thing, I've done that whole thing. So when you look at this coding, you might wanna do like 15 for loops, but you don't need to, please don't, just keep it simple, okay? And this is part of what I really enjoy about ours thinking about ways to code where things can be vectorized and they can run quicker and they can be optimized, okay? So let's talk a little bit about this other one, which is delta I TI observed. So here I have the TI observed, I think this is pretty clear how to do this one because I just went over it in the last slide, it's the exact same case. And then let's talk about this one here. So here I have this one minus theta J and I have TI and I have TI observed, okay? So let's just look at this. So for this first one, the TI and the TI, oh, excuse me, I just did something, there we go. The TI and the TI observed are the same. So that means that I, let's see, what does it mean exactly? Yes, so I'll just be multiplying these and this whole thing was gonna go away, right? Because it's gonna be from three to two, doesn't make any sense. And then an event isn't observed. So let's look at this one. So here I'm going to have, this is a little bit more interesting. I have TI is three and then I have TI observed as five and an event occurs at the fifth time point. So here I have, I'll multiply this times, I'll multiply this twice, right? I'll multiply this two times, I'll multiply, excuse me, phi phi one times phi two, and then I'll, I want one minus theta, right? So I have phi one times phi two and then I want one minus for these two. So this is from three, three, two, four. So I'm going to have point zero five and point zero two five, I'll multiply those. And then this one, I'm just gonna tag on at the end like I did before. Okay, so this is a little bit different, but I'm going to try and make this matrix here. And what you see is I have these lots of one minus thetas. And then what I do is I replace some of these with, so all of these need to be replaced with one for the first one because they happen at the same time. And this one I have, I'm gonna replace these two with one and those two with one, right? So this is what I wanna get at, right? And the rest is very similar. You can see it's just the pattern that I continue here. So this is a coding that does that. We just have lots of one minus theta and I take the ones that are less than ti and I turn them to one and the ones that are greater than t zero greater than or equal to a t zero. And I turn those to one and then I can just find the product. And of course, part one, I haven't included here. Okay, let's keep going. So one thing I wanna talk to you briefly about then that you need to think carefully about when you do this kind of vectorizing coding is the dimensions. And so I'm gonna introduce you to this function, the outer function. It takes two vectors and it applies a function to all the different values of that function, okay? So, right. And then we're gonna talk a little bit about going from inside to out again. I talked about that earlier and I told you you'd see it again and here it is. So what I'm gonna do is I'm gonna first address this, this function here because I think, you know, it's the right place to go. And of course, we're gonna go from inside to out and that is here, right? So I'm starting with this and I also have to put the derivative in. So I can set this seed and I have some gamma, some beta and I made this x beta, you know, very simple. I have the risk which is, and I round it, that's one of the things I like about tidy versus, you don't have to round it, doesn't give you 15 different, different decimal places like base R does. And then here I take this and I find all of these HRs. So all of the HRs are right there, right here, right? Now what am I gonna do with those HRs, right? I'm going to multiply them, right? So here I found this, it's the exact same coding we've seen before. This is the derivative here. And then I want to apply the product to this and I take off which gamma, right? And I multiply by the derivative. So you might look at this and think, oh my goodness, I'm gonna have to write, how am I gonna do that, how am I gonna code it? But you can see that this is, you know, like five, 10 lines, five, 10 lines of coding. Okay, so let's keep going. We're almost there. I don't know how many people had Halloween candy shoots in their neighborhood, but we had a Halloween candy shoot in my neighborhood and I thought it was a very good analogy to a function. So anyway, right, so let's keep going. So what I have here is this, oh my goodness, I tell you. Right, I have here is this and I'm, oh my goodness, let me need to go back. And I have this call come prod, so I'm gonna use this to take the product of all of these and I don't really have the time to talk about all of this in a lot of detail. But I'm just gonna show you the coding and I think I'm gonna be done. But what I do wanna show you about this for loop is that it goes through the number of columns, right? So I'm going through the smallest dimension that I possibly can because of course, you know, the data that I showed you only had four or five people involved in the trial, but most likely is gonna happen anymore. Okay, so what I do here is I have this, I have the derivative and then I make this the CC buying thing here. So I do this column, this cumulative product of the columns. That's this and I take off the gamma and I take off the one that I'm taking the derivative with and I take off the last one and then I multiply by the derivative and then one minus beta, right? So that's this one. One minus beta here and I replace those with the ones that I'm not interested in, right? From L, L plus one to TI zero, right? Which is which gamma, L plus one and TI zero becomes zero and then I just add them all together. Okay, so I think I'm done and the take home message is to really avoid for loops and to try to do calculations in big, huge chunks, right? So that's all I have to say. Thank you very much for coming to my talk and I am going to stop sharing now. Great, thank you very much for your talk. That was excellent. I always love a good vectorization. They really can make dramatic differences in your code and mix it more are like. So that's always pretty awesome. Okay."}, {"Year": 2020, "Speaker": "Mike Jadoo", "Title": "Creating Tornqvist Index in R for Production", "Abstract": "Michael introduces a set of functions for the R programming language to aid users constructing economic indexes for tracking trends in prices and quantities.  For productivity statistics, the Tornqvist index is a standard algorithm to aggregate over products or industries.  It uses a changing-weight formula that aggregates variables at two points in time using a cost/expenditure share approach to aggregate price or quantity indexes.  He also provides methods of aggregating measures by industry and by a group of assets for an industry sector, and a set of examples to illustrate their use for multifactor productivity statistics.", "VideoURL": "https://www.youtube.com/watch?v=M-chAQgJq9E", "id0": "2020_02", "transcript": "So our next speaker really loves to cook, read books, and meditate. So hopefully this year has been very good for him. He's had plenty of time to do all of that. Please welcome Michael. All right, thank you so much. And thank you, Jared. And thank you also to everyone at the Lander and Alex team for having me here today. Hello, everyone. My name is Mike to do. I'm an economist at the Bureau of Labor Statistics, Office of Productivity and Technology. Today, I'll be talking with you about my experience in creating a function in R that does a turnquest index. So the turnquest index is a method of aggregating index measures together for the purposes of analysis and reporting for official statistics. And just to let everyone know, any views expressed by me in this presentation is my own and not of the agency. So the motivation for this project was to provide a set of functions to help individuals to properly aggregate indices using the turnquest method in R. In the case, any staff wanted to use R for production. So before we get to that, I wanted to cover some national accounts economic vocabulary. So we have a good understanding for the rest of the presentation of what I'm going to be talking about. So here's a list of some of the terms I'm going to be covering really quickly. So the first time is productivity. So that's a measure of efficiency, which compares the amount of goods and services produced with the amount of inputs used. So we have labor productivity. So that's the efficiency at which labor hours are utilized in producing output for goods and services. Typically, typically called output per hours hours of labor. Then there's also multifactor productivity, also known as total factor productivity. It's the efficiency at which measured inputs are utilized in producing outputs of goods and services. Next, and you'll hear me say this a lot is the NAICS or also meaning the North American Industry Classification System. This is a standard used by federal statistical agencies in classifying business establishments. Then there's the cost of production. So this is the cost incurred by a business for providing a product or providing a service. Then the word measures. So this is a numerical value that represents an aspect size, scale or growth. Examples include measures of productivity, measures of output, etc. Next, here's the word index. So this is a statistical representation of a level over time for a particular measure using a base year. Next is the price index. So this is a measurement of prices movements in a numerical series. And the quantity index, it represents it's a real measure of a particular output or input. And at the bottom, we see like a calculation here for costs and prices, which I'll talk about towards the end of the presentation. All right, so now I recovered some productivity vocabulary. Let's go over the agenda for today. So first I'm going to provide some context to use the Twentechress Index method by providing you information about productivity measures and then cover some common index formulas that's used for official economic statistics. Afterwards, I'll cover how to use the Twentechress Index using Excel example and then I'll show how I did it in R. All right, so let's do a quick overview of some productivity measures. So why is productivity important? Well, here's some reasons why policymakers use it to help determine living standards. It's a source of gain, it's also indicative gains of national income. It's a key indicator for competitive across industries and nations. It's also used in employment projections as well. So here are some of the new releases that my office is produced on an annual and quarterly basis. We have quarterly labor productivity and also put out an annual measure. And there's also the annual multifactor productivity and is at the major sector level and there's also a multifactor productivity and labor productivity at the four to five digit NAICS industry levels as well. Okay, so we just cover some of our data products, what we use it for. Let's talk about some common index formulas. So an index number is the economic data of figuring reflecting a price and quantity compared with the standard or base value. So the base value typically equals 100 or 1 at the index number and it's usually expressed 100 times the ratio of the base value. For example, if a commodity costs twice as much as 1970 as it did in 1960, its index number would be 200 relative to 1960. So an economics index numbers are generally are the time series data summarizing movements in a group of related variables. So a good example of an index series that's used is the consumer price index. All right, so let's here just providing you some additional illustration about the economic, how we use these index measures. We have the economic circle of flow diagram. You may have remembered this from econ 101. Here we see the classical flows diagram, which represents the organization's within an economy, households, firms, and markets. And markets that produce factors of production, let's say raw materials, energy, other services. At each stage of the flow diagram, their economic activity is measured. And so we see here, there's different index formulas that's used to measure these kinds of activities. As I said before, we see the consumer price index. We see labor, multifactor productivity, producer price index, other things. But then here we have the different index formulas. We have the spares in a couple of different spots and also, pastures also used. But also the torque question index is another formula that's used in different spots to measure the economy. All right, so here's a quick illustration of these formulas. So the spares, we, the spares just shows, answers the question, how much will a set of basket of goods from a base period cost today. And posh, we have this, the posh equation, I see at the bottom, answers the question, what is the difference in the price of today's house in today's dollars versus the price of the same house in a base year's dollars? So next we have the Fisher index. So this helps answer the question, what is the unbiased value of today's homes being constructed in constant dollars? So here in doing this, it attempts to eliminate two kinds of problems associated with the previous two indexes, the tendencies to overstate inflation with the spares and the tendency to understate inflation with posh. So the Fisher takes geometric mean of those two measures in order to do that. All right, so next is the turnquests. So we have the turnquests index and also the Fishers actually comes out with very similar values. So these are also called superlative indices that produce similar results and are generally favored formulas for calculating price indexes. All right, so now I just want to go over a quick example here in Excel. So just covering the turnquests index. So just to give you a little bit more information, the turnquests index is a weighted geometric mean of the price relative to arithmetic averages of value shares in the two periods using the two periods as weights. So here in this example, I am wanting, I want to aggregate different assets. Here in this example, I want to aggregate computers, communication equipment and other equipment into one top line measure called information capital. And I'm doing this all for one NAICS industry sector. So what I'm going to need is those assets within that particular industry, their quantity index. So I have at the very top of the graph that we have the slide you see here. And towards the bottom, the second we see the log change and this right, that section, we're actually calculating the two year rate of change of the quantity index, because that's which is part of the formula. Then at the very bottom, for the weights, I'm going to be using the current dollar production costs for each of those assets. So the first step in constructing the weights, we want to use the average cost shares, we want to get the total of each of these assets. And we see the total at the very bottom. Next, we want to calculate it using the cost, we want to just do a check to make sure that we are doing the calculations right, making sure everything sums up to 100. And then what we're going to do is once we get the shares of each of those assets, we're going to calculate the average shares. So it's going to be two year average shares, as you see at the very bottom of the chart. The next step is to calculate the two period average shares to the log change. So here, this is in this calculation, we're actually coming up with each asset's contribution to the aggregate number. So we have computers, communications equipment, and other equipment, other equipment being, I'll say, fax machines and different things like that for a particular industry. So once we calculate that we have each of their contributions, and at the bottom, we sum all those contributions up of those three different assets to get information capital. And then what we do next is we want to have this numerical value in the presentation form. We want to normalize it first by using the community product. And next, we want to index it by the base year. So the base year in this example is 2009. All right, so we went over some common common index formulas. And then we also went over an example of how we can actually calculate the turn course index in Excel. So Excel shows a good way of breaking it down into more grammar information so you could have a better understanding about how to go about doing that. So how and why, and why do we use this? So there's literature on the theory of index numbers. So the turn course index has desirable, desirable properties is less prone to outliers and different things like that. And it's also used in a lot of different statistical agencies, as because it's not impacted by outliers. All right, so I just want to open that. Okay, so now let's talk about how to do it in R. So I'm going to show a demonstration with the example I just showed in Excel. I'm going to be aggregating three assets from a particular industry to form the information capital measure using computers, communications equipment, and also others being fax machines and different types of technology equipment. So yeah, so in this exercise here or in this project, I found that these two packages helped out a lot to create the set of functions that I had created to perform this task called calculate the turn course index. So we have the deployer and then tidy R. So deep wire is a package with provides a set of tools for officially manipulating data sets in R. The player focuses on data frames. The player is also faster and easier to use. It provides a consistent set of tools that helps us solve most common problems. Tidy R is helps us tidy the data. And it's easy to to munch with the player and other packages in the tidy universe. Some of the main functions in tidy R are the pivoting of tables and converting between long and wide formats. And so here's the main function for each of the packages that I use for this project. So on the left for the deep wire, we have the mutate, select the range, group by rename, and the different joins in tidy R primarily use the gather function because I needed help transposing the data into a usable format. So these are some of the tasks that was done by those functions. So mutate, I used it to create new variables, select, use to select certain variables, picture variables for subsequent steps, arrange sorting the data, group by and grouping it, and then the joins merging the tables. And I talked about transposing. Yeah, and so here's some additional information about how the basic format of those functions has used. And yeah, if you have the pipe operator, you could actually skip over the argument where you need to put in the data frame. But typically, it's the data frame and then you have the arguments on the far right. And here for tidy R, this is just an example of how I was using it using gather function. This is transposing it and to get into a usable getting the data frame into a usable format for subsequent calculations and combining tables. Okay, so like I had mentioned before, the exercise information capital, we want to aggregate the other, yeah, it could be some medical equipment, fax machines, different things, they have the computers, communications, you want to form the information capital measure. And this is the task. And at the very bottom is actually the functions name that I created. And I have a link to share with everyone, if in case you want to look at the code. So it's turnquest underscore index underscore M, just for this example. And we see on the right, we see the arguments that I have there. All right, so here's the data in its original format before, well, actually, after I have imported it into R, and this is what it initially looks like. And if for those of you who are interested want to download the data from the link that I have, you still get in this kind of format. But in this is how it looks at so on the far right, we have the cost different cost measures where they have the underscore C suffix. We have the sector in the far right. So that's the makes industry. And then the far left we have the year because we this is the time series data. And the computers, communications and others on the left side, that is the quantity index measures. So before we get into the main function, there's some pre processing that's needed. So what I needed to do in this section here, I needed to separate out after importing into our putting into into objects, DF underscore cost for the cost measures and DF underscore QTY for the quantity index measures. So that would be used in later steps for through this start. Combining tables. All right, so this is the main function of this for the to do the twerkers index for this exercise. Now I'm going to go over it a bit by bit. So it's not just going to work off this slide here. All right, so we just have a super impose a little bit. So we have the very top this formatting that's done. But so I'll just go over a block one. So I'm just going to go over a block one. So I'm just going to go over a block one. So I'm just covering it in different blocks. So in block one, we have. After we have imported it and we've called in the function, we had started into the different objects, or we're going to do, we're going to transpose both cost and quantity indexes into a usable format so it could be combined for subsequent calculations. Next, as you see, going down a little bit more of the inner join, we're combining both quantity and indexes and we're combining it by the next industry sector, which is labeled a sector and also by the assets computer communications and others, which is labeled as EMS. And next, we also combine it by the key is also here. So we see what the bias that those are the key. Those are the key there. It's there. Then we're going to sort it. And next, we're going to actually construct the aggregate cost for the very for that next step because that's following on. We need to calculate just like we did in the Excel example, the average cost shares for the weights. All right, so in this section here, calculating the average cost shares for each of the asset components. For your computers, communication equipment, others, and we also calculate the rate of change. As you can see, was the log difference towards the middle of the screen. And once we are so much we perform those calculations, we select those variables again for subsequent for subsequent processing. So after we've calculated the average cost shares and we multiply that to the rate of change of the quantity index and we get our contributions for each of the different assets, computers, communications equipment, and others, which is labeled as a cost. And then we're going to be able to get the data that's labeled here as the object product. All right, so in this step here, once we have performed the test before, we we sum up all of the different contributions for each of the assets to get our value for information capital, which is the goal that we're trying to achieve here. So we're going to be able to do that by performing a cumulative product, and we see that in the middle section of the slides here. And once we do that, we're going to index this value to get a quantity index by using the base year of 2009. So at the bottom here, we're we're calculating some additional measures for for this exercise, which is once we have the quality index, we could also come up with the price index as well, because we already have cost. That was done in the very beginning part of the program. So the slide here is showing the before and after picture so before on the far left, we have our data that we imported into our for the different asset components we have computers, communications and others, and we also have the quantity index and their cost, particular industry sector. And then the far right, we have the final product of this after we've performed this exercise, the information capital measure. So we just aggregated those three assets into the one information capital, which would be used later on for reporting purposes or possibly analysis. Okay, so I just showed a short demonstration with that. And just a review, I think I'm getting close to time here. We went over the different productivity measures, and we also covered the common index formulas that's out there and it's used. I talked about how turnquest is using why it's important. Why a lot of people tend to use turnquest index, and then perform the short demonstration on, well, both how to do it in Excel, and also how you can do it in our. And yeah, here's a link to my code, but I'm not sure we have time for questions but I can post that into chat as soon as I'm done. And if you have any questions, my contact information is here, and please stop by our website at www.bless.gov. Backside MFP to check out all of our other data products that we have available. Great, thank you very much. It was wonderful having that talk. Thank you. Thank you very much for that. So our next speaker."}, {"Year": 2020, "Speaker": "David Meza", "Title": "Developing a Knowledge Graph of your Knowledge, Skills, Abilities, Tasks and Training", "Abstract": "Understanding occupation elements and employee skillsets is essential to properly align your workforce, identify skill gaps, emerging skills and career/training paths. In this presentation we will explore using tidy models to augment a knowledge graph with inferred employee attributes.", "VideoURL": "https://www.youtube.com/watch?v=_QPO2I-2tKk", "id0": "2020_03", "transcript": "When I found out our next speaker was from NASA, I was really excited to have a talk about space. Then he said he's going to talk about people, not space. That's okay, I'm still interested in people too. And way back in 1977, our speaker was 15 years old. And his friend came over and he had an extra ticket to go see two bands. But our speaker's dad really wasn't in the mood, so he wasn't going to push it. So he asked his friend who the bands were. And the friend said ACDC and Journey. And our speaker didn't know who those bands were, so he said, okay, no big deal. Now he knows better. Everyone please welcome David. So what I want to do today is talk a little bit about it. And I appreciate your the introduction and it was definitely very interesting. That time in my life when I had to make decisions and unfortunately that time I made the wrong decision, but hopefully over the years I made better decisions. And when we look at decisions and we're looking at trying to figure out our ability to look at our skillsets across NASA, I was asked to take a look at how we're developing our skillsets across our information, across our different things. So let me give a little bit of an idea of what we're trying to do in developing a knowledge graph across NASA. Let me start here. So first thing is we take a look at NASA. We're going to get an idea of where we are and what we do. We've got various different centers, 10 different centers, another six or seven facilities. And we also have a research lab that kind of floats a couple hundred miles above the earth. Where we have to identify our workers, our abilities and our different skills. We've got a lot of different programs and projects going on. Of course now we've got a big mission and the way of trying to get back to the moon and on to Mars. And we have to identify the skillset across the organization. Each of these centers do different things. We have research centers that focus on medical issues. We've got other research centers that look at propulsion or heliophysics or planetary science and climate control. We also have our engineering areas like Johnson Space Center and Kennedy Space Center that work on actually getting our human exploration. So there's a lot of skillsets that are all across the United States and across different programs and each center does different things. But we really need to identify where they're all located and how we can better utilize these skillsets. So as we start thinking about this and I was brought into human capital about a year ago to take a look at this, I first had to understand what our architecture looked like. And as we take a look at our analytical architecture, when I first started this organization, we had the far left and the far right now with the body. We had some ways of acquiring data and we had ways of showing data but nothing in the middle. So over the last six to eight months, I've been trying to slowly integrate various tool sets into our pipeline to allow us to connect to our skill sets, to allow us to connect to our data actually to identify our skill sets and do various things across human capital with people analytics. But one of the major problems I had or not problems but an issue or concern was that we had everybody do doing different things. We had various skill sets across even our employees doing analysis. Some people use Python, some people use Ruby, other people use R. So I wanted to try to find a platform and connections of how we can do all of this together. So the example I'm going to show here today is really how we can combine not only various technologies to do this analysis but how to do this in a way seamlessly to be able to share data across internally that can help with across the agency. So we'll see how I connect from different data sources like a Neo4j graph database into utilizing both Python and R through some machine learning and eventually to a visualization aspect of that. So how do we create knowledge graphs? First of all, a knowledge graph is basically an understanding of a particular domain with various relationships across this domain. We ask different questions, we define the model, we do some analysis and the various tool sets that I'm using here today are going to be Neo4j, R, NAR studio and Python along with various algorithms with inside both of these packages. You can find my code for this at that GitHub repository and be able to update and actually do some of this work on your own because we have some open source data that's available with that. Why are we doing this? Well, when Gartner surveyed various different HR executives, the top two things, the two different surveys was critical skills, building critical skills and competencies as well as the emergence of new tasks and responsibilities for employees. So those are the types of things that we're really looking for, not only at NASA, but it turns out that most of the industry is also trying to figure out ways of improving their skills primarily because of the way things are going on now with the COVID situation and new ways of doing business. So this has really come to the forefront and it's really a way of looking at not only NASA skills, but across the industry skills. So what can you do with these things? There's a lot of different things we've looked at and what we can analyze with and one of the things is how do we push our actual skills across the workforce? Can we actually identify what project, what program, what theme or mission it actually has these type of groups of skills and it turns out we can to a degree. Can we identify adjacent skills? If somebody knows Cypher, do they also know SQL or Python and R, how can we do some kind of adjacencies on that? And if I want to change careers, what should I learn as we start infusing more things into our knowledge graph, we can add things such as training information, curriculum, objectives of that to identify skills and knowledge that may be attained in that. But now let's try to put this all together. We can start with a current database that's currently out there called ONET by the Department of Labor that actually has a little over a thousand occupations already developed into a database that they've looked at over the last 60 or 70 years they've created this and each of these databases has some elements. Those elements are made up of skills, knowledge, task and abilities and the data model looks something like this. So on the far left hand side, you have an ONET database that we've pulled down from the Department of Labor and we can break this down into a graph model. And in the graph model, the circles here represent no guarantees and the lines represent relationships across those no guarantees. So the elements that make up an occupation are broken down into things such as tool use, work activities, abilities, knowledge, task, skills, etc. Those can be found in occupations. Well, that's just part of the database. Knowledge spectra can be expanded to include NASA information such as what OPM series and that in itself is just the, our NASA government code or our government code as to what kind of occupation it is. But that could also be related to an employee. And how do we get information from the employee? Well, we've taken information from the employee's position description, runs a similarity analysis between the position description and the elements within the ONET database to try to identify which of those elements may also be found in an employee and build that back into the graph as well as provide an additional information about where they charge to to find out how we can be able to see what skills his employee has and how they're applying its different programs and projects. That kind of gives you a little semblance of a knowledge graph in itself in how we created this around a graph model to look at our skills, but how do we build this? How do we actually do something like this? So I developed this kind of a knowledge graph model of what we're trying to do just kind of real quickly showcase how this all works together. On the left hand side, you have occupations again from the ONET that we extract the different case that the knowledge, skills, abilities and tasks. We use the creative vocabulary that will train a Dr. VEC model that will be stored and allow us to utilize that and infer against the employee's description. On the right hand side, we have these employees. And from the employees, we have different information, the job descriptions, performance evaluation, resumes and CVs and training. And from that, we can use that information to infer against the Dr. VEC model and maybe pull out various skills, knowledge and elements that may be found in that database. Add all that together in a graph database, put it all together, run some graph algorithms against that and start to identify different things as skill gaps, career paths, succession planning, and maybe a strategic alignment model. Those are the types of things you can do with the graph model, but first we've got to get that all in together. And I'm going to show you really quick how we did that and how you might be able to look at that and pull out some information from this graph model. So we first start off. Here's where I start combining the world of Python and R and everything together. With the help of some interns and some different folks in my team, we're able to create a Python script that when run connects back to your Neo4j graph database and actually pulls the own that database directly from the website, run some algorithms, creates the constraints and search the graph database, make the relationship and within a matter of about 20 minutes roughly, depending again on your resources and what you have, you come up with something like this in the graph database. Now of course, yeah, this looks a little bit, it looks a little weird and wild and canned, but you're not like the graph model I showed you earlier, but trust me, it has all the information in the graph model. Everything here in the color are the elements that came from the occupation on the two nodes on the far right hand side of the occupations and the elements inside of that, the interest, the abilities, the task are all related back to the occupation, but you also start seeing other patterns and other connections here. On the left hand side over here towards the bottom left, you see that's the actual charge code and of course, somebody's paying to and we can start seeing those relationships which will eventually get associated back to an employee and back to an occupation. And I'll show how that works and give you an example of what that looks like in a little bit. So now that we have the graph database, we've got to create our Dr. Beck functions in order for us to be able to infer against the employees of information. So here are some of the functions that we've created to clean the test, create the model and I'll expand on a few here to show itself what we're doing to get this information. But we're combining, I think what I want to really want to get across here is we're combining various technologies here where we're combining some Python scripts connecting back to a Neo4j graph database and then eventually use some R scripts to do some major cleaning because it's an R, primary R user, I'm more familiar with doing a lot of the analysis in R than Python so we're having to mix these together showing how as a team we can work together with various technologies. So in order to create the model, the first thing we need to do is actually build a vocabulary for that model and we use a cipher query that we store into a Python script here to connect to the graph database and pull that information out of the actual graph database. So what we're doing here in simplest terms is with this query, I'm pulling out all the information about all the elements associated to occupations. And I'm using that as the vocabulary to build my model, my Dr. Vek model using the Jensen package within Python. And I'm looking at a vector size down at the bottom, you can see a vector size of about 200 using the skip gram model with the minimum count of at least two for the words using 50 epoch and a window of four across the words themselves to look at the context of the words and how they may be related to each other. So once I've built the model, I can then go back to R where I need to do some pre-cleaning and some changing of my employee information. It's very, you know, one of the things that always gets me all the time is how we've been around for 50 plus years here in NASA and we've got data all over the place and we've used so many different ways of structuring our data. We're still struggling how to get our data governance in play to make sure that we share information alike. Even within our employee information, some information in here in this particular case with what you see with the EPDS series up here at the top is our codes for our occupations come in three different formats. And so I needed to make sure I can relate them back together. So here I'm combining the codes and adding some information in order to all looks the same across all the different platforms. Then I'm creating here the doc.vec or the document sections for when I'm going to infer back into the doc.vec model. So in here I'm tokenizing the documents, the position descriptions of the employees into sentences. And I'm going to use the sentences at the individual document so that I can infer those back to a particular skill or knowledge or task or something within the onet database. Once I've done that I've saved that back into a CSV so that I can then utilize that back into the Python scripts to infer that information back into the doc.vec model. So in this particular case the top function here is basically tagging the position description information, tokenizing and tagging that I can utilize that in the doc.vec model. So each of those words are tagged with an appropriate sentence ID that allows me to use that information to figure out how to find a similar element, the knowledge, skill or abilities that may relate back to the position description. The bottom script there and I just noticed that it's name top 10 similar but if you really look I'm only pulling up the top three. We've played around with how many different similarities scores were pulling out. So this function basically takes that, infers those tag documents from the position description against the doc.vec model and finds the top three most similar elements within that information. And then it returns back to me a basically a data frame that looks something like this. It's got my position description which is the ID for that sentence. So in the numbers basically represent the UUPIC which is the individual ID of an employee. The position description number, the 303 in this top line is actually the OPM with a job code for that individual. And then the one represents the sentence that it came from. So then I connect those relationships back to that individual sentence. And here are the top three similar elements found. And as you can see within this one sentence it's saying it with the 46% similarities close to a basic skill entry requirement of active listening. It's also a 45% close to a competency of negotiation and 44% close to a work style of stress tolerance. I guess if you think about it, if you're doing a lot of negotiation and really listening you've got to have some good tolerance for stress when you're trying to do a lot of that work. But you'll see that this is how I was trying to pull up those top three things. But it's not very easy for me to utilize this information to put it back into the Neo4j Graph database. So I go back to R where I read that information in. I do some cleaning up, some rearranging, some separating of the position input numbers so that I can spread that across and actually identify these things. And once I do all of this, I then come back to a formatted output that has separated all and allows me to see the various labels for the note entities of knowledge, the titles of similarities and how they may be representative back to a particular sentence within an employee's position description. All of this information will get placed back into the Neo4j Graph database. I'll utilize scripts here to make the relationships between the employee and the individual node or knowledge or element node whether it be knowledge, skill, abilities and make that relationship back utilizing the weight of similarities so that we can try to play around a little bit with that to see how close the name may be related. Once all of that information back is put into the Graph database, I can start doing visual expressions like this out of the Graph database. So in this particular case, I'm looking at an individual here and actually myself, David, I pulled this out of the node and I have associated all these various skills and elements that were found within my position description that are also related to my current occupation, computer and information research scientists. So all of that is showing how related they are and what the most important elements or skills are found in that occupation as well as in my current position description. So now I can also associate back to where I charged to, not only what program or project or what mission and what theme and all the way back down to official division that I may be charging to. So now as a manager, I can ask the question, what skills do I have in my particular program or project or theme by aggregating all of this information back up the chain and identifying where my skills are associated. So it allows me to ask a lot of questions like do I have the right skills in my program, what skills are lacking, what skills are missing, allowing me to start really trying to identify what I need to do to change and maybe improve upon my organization but also help find individuals that may be useful for my program. So how do I do that? And I'll turn to going back directly into Neo4j with some different Cypher queries and if you're not familiar with grass databases, the Cypher queries, the equivalent of a SQL query within a regular relational database model that can pull out information. So in this particular case, after I've interested all that information, I've taken the information from Oned, I've added some more information from my employee information utilizing this Doc2Vec model and put that all into a grass database, I can start asking questions such as this as to what rights skills by occupation. One of the information pieces that comes out of Oned is that they do an assessment of all the skills of how important they are to a particular job. So in this case, I just pulled out the information for computer and information research scientists and looked at a few of the basic skills in the importance level and here the most important level, the most important thing that was identified by Oned was that understanding written sentences was the most important skill for computer and information research scientists. It's even higher than using mathematics to solve problems but even if you look at the top five skills here or more about communication and giving attention, then it is about actually doing mathematics and solving problems for your basic skills. It shows how important this communication is across any occupation but there are more things we can do with this type of information. What if I wanted to do a career change? What if I'm a management analyst, somebody that's doing just the research report or the research report and I want to see, I really want to get into maybe a business intelligence and doing some more things like that. Utilizing this query, I can pull out information about management analysts and other occupations that may be similar to management analysts based on the number of nodes that intersect across those. So the number of skills or knowledge that intersecting across management and other IT type skills and as you can see, business intelligence intelligence analyst is about 46% similar to a management analyst. So it tells me what skills are similar, what knowledge is similar but it also tells me what skills and knowledge I need to gain in order to obtain my goal of becoming maybe a business intelligence analyst or some other career. So it allows us to start seeing how we can move people around in different locations or if I'm a project manager and I need a business intelligence analyst, I can go research my employees and find out who's close may have an interest that I can upskill to make them a business intelligence analyst. So there's a lot of different things we can do as we move through this. And speaking as far as looking at employees, one of the things we can do here is find similar employees. If I have a program or project and I need particular skills, I can utilize a query like this to find employees that match those skills and knowledge for my program and project and start reaching out to them to see if they're available to join my program or project at that time. So another way we can identify skills and utilize these things across the organization. Last thing I want to show here really quick is as you try to combine all this together, it's how do we start visualizing these things. This kudos goes out to my current intern Madison Gibson who created this little dash app that we're going to put back into our Connects server to be able to utilize it. But I'll show this video really quick here which shows it connecting to the Neo4j Graph database. You start putting in information that you may be looking for and here again we're looking for computer and information research scientists and it clicks similar occupations in computer and information research scientists that can be found within the organization and how similar they are based on the nodes they share, the skills, the knowledge, the interest and things that they're shared across the occupation. I could do this, we could do this for employees and various different things also but for privacy reasons this is the only thing I could show but those are ways that we can share information across. So as you can see across this thing we've done a lot of work to this to try to develop skills across the organization. There's still more to come. We can do things as recommendation engines for people that are looking for maybe small projects to work on. I have a certain skill what jobs are available or vice versa. Can I find experts in the field? Also as we add training information we can develop training planning. I want to get to a particular career goal. This is what I want to do. What training do I need to take based on how those training objectives inform or add to the skills that I want to learn and it can help us outline career path. So in this 20 minutes or so what I really wanted to show is how we utilize various technologies to try to develop these skills, combine them together to be able to share across the organization ways of sharing information and building a possible what we currently have in order to be utilized all our information. So hopefully I stuck the landing here and you're welcome to reach out to me or to my intern, currently intern on some of this work we've done if you have any questions to be more than happy to connect and answer questions offline. At this point I want to thank everybody for their time and I pass it back to Jared. Thank you very much David. I wonder how much of that you use on astronauts. Coming up next is our time to do."}, {"Year": 2020, "Speaker": "Maxine Drake", "Title": "Modeling COVID-19 on a DoD Network", "Abstract": "Maxine was on a team that developed the U.S. Army\u2019s COVID-19 projection model. She will share lessons she learned developing this model on the DoD network. First, she will discuss the packages on which her team relied, specifically furrr, sharing a comparison of furrr with other iteration methods. Second, she will discuss how the team leveraged functions to make their code robust and flexible. Lastly, she will share what priorities and management techniques the team followed that they believe made their model influential among Army senior leaders.", "VideoURL": "https://www.youtube.com/watch?v=8UL5S_lxYT8", "id0": "2020_04", "transcript": "She is a, well, she was formally used to be a pull vulture. And she claims that she can still pull off a few pull-ups with her kids hanging onto her. So I think that's a challenge accepted. You need to do that. Please everyone, welcome to the stage, major vaccine. And I told Jared that I would be happy to show the pull-ups, but I'm in uniform, so it would be impractical. So that's my excuse. And I am major vaccine Drake. I am coming to you to talk about my lessons learned, my personal lessons learned from the one of the Army's COVID-19 models, projection models. Now, this is not the official Army model. This is one projection model of a few that we have in the Army that is a large scale, and it has informed the senior most Army leaders. Today, I will first, I'll tell you who I am, go over personal introduction. I will discuss our model briefly, describe to you what it is, and then go over some coding lessons learned that I hope will be of some use to someone in the audience. And finally, I'll discuss some project management and team management observations I've made over the course of this project. I am, like I said, active duty. I am an operations researcher. That is my career field in the Army. As an operations researcher, I am serving as a data analyst for the Center for Army Analysis here at Fort Belvoir, Virginia. Before being what we call an ORSA, I was an engineer officer. So I served as an engineer officer as a platoon leader, an executive officer, a company commander, all in combat engineer units. I served it in North Carolina, Louisiana, and Afghanistan. I went to West Point, United States Military Academy, and graduated in 2009 with a Bachelor's in Economics in Russian. Later on, I went to George Mason University and got my masters in operations research. That is where I started using R, so around 2017 is when I started. Our model. So I'm going to summarize it briefly, but first I didn't mention this is an iOS slide presentation and I'll give a quick shout out to Mallory, who really sold it last time when she gave her presentation using a dashboard and inspired me to use iOS, use R to generate iOS slides and dashboards. Our model, like I said, its purpose is to project COVID cases, estimate future COVID cases, and at its core, it is a CR model. The CR stands for susceptible, exposed, infected, and removed. It is a standard epidemiological model, and it's what epidemiologists call a compartmental model, which means the entirety of a population that you're modeling falls into one of, in this case, four bins. First, individuals are in the susceptible bin, which means they're vulnerable, have never been contacted the virus. Then those individuals move into the E bin, which is exposed, that means they come into contact and they're incubating the virus. And then individuals move into the infected bin, which is the, they are, individuals are actively infectious, contagious, and whether or not they're symptomatic. And then finally they move into the E bin, which is what we call the removed bin, which means individuals are immune to the disease and are virus and no longer susceptible. Of course, there are a lot of assumptions here. I'm not going to really talk about those because that would take a long time. I do want to touch on a couple other things about the model, which is we have what I would describe as a sub model. It's our primary sub model, which is an ex G boost model that estimates future are not values are not being the basic reproductive rate of. COVID of the coronavirus. We train the model on several data sets include about 30 different variables from data sets, including population mobility, state policy, population demographics, and of course, COVID-19 case trends. We have a couple other smaller scale sub models or calculations that we use to update parameter values and initial values for the model each day that we run it. We run this model for every county in the United States. And we run it for again, for every county and core based statistical area, which is effectively a group of counties. So I'll talk to you about some coding lessons learned. If you're a programmer out there, bear with me. You are aware of my academic experience. And I have not been I did not learn are through a programming lens. So some of this might not be relevant to you. However, I'm hoping that there's someone out there like me one year ago, who. Didn't fully realize certain benefits to what I'm about to talk about function and then later on some new packages that I learned. So function I realized function is old news. So I'm not here to talk about what function is and how to use it, but I do want to talk about the benefits of using function and modularizing code that I didn't fully recognize. Like I said, one year ago. So our model began as one script, one long run on script is how I would describe it. The what we did is we modularize the first thing we did is we modularize the code. So we organized it into purpose specific or task specific chunks and then translate those chunks into functions. So each function has a specific task. And at the end, I'll highlight and what I show here are our functions. You can't see what goes on in the functions, but you can sort of get a gist of each one of these has a task. The first one, not surprisingly downloads daily data as the title suggests or the name suggests. The last one I'll highlight is called sear modular. And I'll talk about this one later on. This is our wrapper function. And this is where we set all the default argument values that we want the model to use most frequently. And it dictates the sequence of those proceeding functions that we want the model to be routed through. So more on function, because I have a new love for this capability. So first of all, functions allow us to expand our mental model. And by that I mean, in the beginning when we had one wall run on script, we had to conceptualize the entire script. All at once, the whole model all at once. Once we modularized it and turned it into task specific functions, we really just had to conceptualize one task or one function at a time, which allowed us to make the model larger and more complex. Second, it made our model more flexible. So instead of hard coding values into our script, we could now use arguments to define to run the function using arguments using whatever parameter values or coefficient values that we wanted when we ran that when we run the function. So take this one step further. And instead of changing parameter argument value from, say, four to seven, we can also change it from four to a vector or a list of values, which made solving the, solving these functions or executing these functions iteratively possible. It also made performing these functions in parallel possible as well, using the fur package, which I'll touch on in a moment. We were able to replace entire functions sort of like swapping a car part. So if the structure of the input was the same and the structure of the output, what came out of the function was the same, then it didn't matter what happened inside the function. So we could generate an entirely new function if we were developing the model with some new capability. And then when we were ready, we flipped the switch and we told our wrapper function use function A instead of function B or vice versa. What we did was we took our core scripts, a couple of our core scripts, and we made them entirely comprised of functions. And then we sourced these scripts. This, and this was good practice. What I learned is it was good practice for a couple different reasons. First is that we were make, we were going into those core scripts less often because whenever we had to go into model development or run the model or troubleshoot, we didn't need to do it in that core script anymore. We just needed to source that core script and run those functions elsewhere. So it meant fewer changes to those core scripts, which made merging in Git a lot simpler. The second reason this making these sourceable scripts was good practice was that it allowed us to use these functions in other areas of our analysis. So now we were able to do validation verification by using those core model running scripts, but for a completely different purpose. And then finally, I can't go on without saying that we are, our objective is to make a package. We're in the preliminary steps, but we need a bit more bandwidth to finish that. So a couple packages, one of course, tidy verse. I'm not going to talk about tidy verse because I'm comfortable. I'm sure we are all comfortable with the role tidy verse plays in our coding. And if it's new to you, please look at what I will talk about as first. So fur allows one to execute a function in parallel over multiple cores on your computer. So what I've done here is I'm going to take you through just a very simple sequence of how to, how a simple function gets translated into a fur function. So the first example here is, and I'll go ahead and show all of them. The first function here is we're running that wrapper function I mentioned earlier, serimodular. And I'm running it in this first chunk using one singular FIPs, a FIPs is in this case a county code and one singular date. This is the simplest way to run a function. We're running the model with with these two inputs arguments. The second chunk runs the serimodular model. I can't talk right now. Please can't. Thank you. My, as you can see, I'm teleworking, so my kids don't appreciate that much. Anyways, so the next example is the per. So we're running serimodular over a vector of FIPs and a vector of dates. This per performs the executes serimodular iteratively similar to a for loop, but it's a little cleaner, I'd say, than a for loop. Then the last chunk is translating that into fur. So just like a for, we're solving a solving serimodular over a vector of FIPs and a vector of dates. We set a plan. So we tell for how many cores we want to perform the execute the functions over. And then we use fur to execute serimodular. And it's a simple translation from per. If you're comfortable with per, all you have to do is almost all you have to do is add future at the beginning of the function. And it's a simple translation. So for, because it's working in parallel rather than iteratively, it is a lot faster. And I ran at one chunk of our model using four different functions. One was fur using eight cores on my laptop. And it was the future IMAP DFR function. I also use per IMAP DFR, L-apply and a for loop. You can see the latter three are almost very close to the same amount of time per was slightly faster, but fur was just significantly faster. Yes, less than half the time to when using eight cores. So I talked about the technical highlights from our from our code and our model, but I also want to touch on what I hope will be interesting to this audience, which is some observations I had of our management in community in organizing and communicating our analysis and managing our team. I'll use a couple different terms. So there are a couple groups of people I'm going to refer to. One is our management, and that is our boss effectively. We had a couple different people who are leading our team who organized our analysis who gave us direction us being the modeling team. And then we're communicated that and briefed that to senior leaders. Senior leaders are the other group I'm going to talk about. Senior leaders are those whom we informed. They were our customer, if you will. And those are the senior leaders of the army. They include the vice chief of staff, the head of operations and planning for the army. And so I just don't want to get confused on those two groups that I'm about to talk about. The way I've been characterized our management's approach to informing the senior leaders was their attentiveness to the senior leaders decision cycle. So first was a willingness to a willingness to defer improvements in precision. The way I would describe that is a late answer to a problem is a non answer to a problem. And I'm sure that's not just true in the army, but I can really only speak for my experience in the army. So under duress, by that, I mean under duress senior leaders will always prefer a less less precise information to no information at all. This is difficult for us analysts to accept because we always want our work to be as precise and accurate as possible. So there's a but there's a trade off and this is what our management really thought hard about is the trade off between timeliness for the sake of meeting a decision maker's timeline and precision. Because that is our one of our goals as analysts. And they were able to understanding the model and our analysis as well as the senior leaders decision environment allowed them to make informed decisions and weigh those trade those competing priorities effectively. Second, eagerness to anticipate likely questions. So generally speaking, what I've noticed is by the time a decision maker knows what he or she needs to make decisions or make a specific decision, there may not be enough time to develop the informative and actionable analytics. Or in our case to develop the model accordingly. So our team management accepted some risk in developing and executing analytics before an official tasking or question came down. So basically this meant we could answer questions faster. For me and the rest of the modeling team, there are about three or three or four of us on the modeling team at any point in time. For us, this meant we were constantly in development mode and production mode at the same time. Remember, so, okay, so remember how I said, even when we modularized the code, we were able to swap functions like this. Like car carts, this ended up being very useful because we had our production model and we were able to develop functions that would replace a specific task function and then tell the wrapper function to route the model, we flip that switch route the model through this new function that we had developed. Of course, we also used get to create new branches when developing, but this was ended up being a pretty handy way of keeping the model flexible. Third was the attentiveness to the actual requirements of a decision maker. And so what I mean by this is decision makers often do not need complex and nuanced solutions to their problems. What they need is concise and focused and actionable and explainable results provided within their decision cycles. Before I mentioned the trade off between timeliness and precision, well, there's also the trade off that my management sort of helped me understand, which is the trade off between precision and simplicity, simplicity for the sake of the consumer. So the best answer to a decision maker is not necessarily the most precise one. Often it's the answer that, of course, is precise, don't get me wrong, but it's also understandable too. And that is the last point I had to make. So at this point, I'd just like to thank you all for letting me speak and your attention. And I'd be happy to talk during the social room periods that we have. Thank you, Major Maxine. It's always great having a service member speak at our conferences. So our next speaker is me. Thank you."}, {"Year": 2020, "Speaker": "Jared Lander", "Title": "Taking R From Hours to Seconds", "Abstract": "When facing a problem on a few millions rows of data, Jared wrote code that took hours to run, if at all. To speed things up he first split the data into smaller pieces, then did so in a smarter way. Still needing faster results, he wrote a custom function with a smarter algorithm, then sped it up further using Rcpp. All this took the runtime from hours to seconds, making it a feasible solution.", "VideoURL": "https://www.youtube.com/watch?v=c9Dp2HmtVSM", "id0": "2020_05", "transcript": "Hello everyone. I guess I have the pleasure of introducing Jared. He is the person that taught me everything that I know, or at least the foundations of starting everything I know. And so I've been very lucky to take my first data science class through with him. And without further ado, here's Jared, one of the organizers of this event. So I'm going to be talking about accelerating R code, dramatically speeding it up from something that takes forever to run hours, if at all, into seconds. This is going to be somewhat similar to a talk I gave at the first R conference six years ago. That was more of a survey. This will be more of a problem I had in walking all the way through it and all the different paths I took. So what was the ask? What were we trying to solve? What are we trying to accomplish here? Out of millions of points which are similar to each other. Essentially, which points are within a certain distance of each other? It's a range searching problem. So what's the problem about this? What's the big deal? It's slow. And the question becomes, how slow is it? I want to quantify how bad this is in terms of calculations. So this problem takes n times n minus one divided by two calculations. That's a quadratic relationship. For half a million rows, that's 125 billion calculations. That's 125 billion slots in memory. For a million rows, that's 500 billion calculations. I doubled the rows, but I quadrupled the calculations. The calculations are big O of n squared. And a million rows really isn't that much data. So let's check out the data for this particular problem. This is just a sampling of about 500,000 points and it's simulated data. The actual data set had millions and millions of rows. Now each row only has two dimensions, which is nice. It's not a high dimensional problem. It's two dimensions. And remember, we want to calculate how similar each row is to every other row. And then we want to find the ones that are closer to each other according to some definition. Visualizing the data, it looks like a Rorschach test. And we can see that there's more density in certain parts of the plot than in other areas. For my first attempt, I started with the obvious function, dist. It's built into base R and it returns the distance between every pair of rows. So I call this function and it did not go well. I don't know when it's going to finish. Crashed many sessions. So that didn't work. Had to go back to the data and look at it again. And I noticed something. When I looked at the ranges of the X and Y axes, I said something's up and I summarized them. The first dimension goes from negative 100 to 100. The second dimension goes from roughly 5 to 68. And I said, you know what? That looks a lot like latitude and longitude data. And if it looks like that, maybe I can use the tools in the SF package to treat this stuff like their geometries. So I use ST as SF to convert the data into geometries. And I treat VAR 1 and VAR 2 as latitude and longitude. It's now like points on a map. They weren't points on the map. There's no reason we couldn't use that technology to work with it. So I decided let's plot it on a map. And it looks like this blob trying to take over the Earth. Again, the map isn't really that useful because, you know, data aren't meant to be mapped. But I want to see what it looks like. So now that we're using SF, we can use a function from SF called ST distance. Now this is from SF, but it requires that the LWG on package be loaded. And this measures the distance between every point and every other point. And I figure it's in the SF package. It's meant for geometry. It's going to be great. It was a bit of a dumpster fire or rather a computer fire. It didn't fish. So I turn my attention to another function in SF. ST is within distance. It's exactly what we want. You'd give it geometry points and it tells you which points are within a certain distance of each other. Now this function is meant to be used with meters. I know there's a new way to do it with latitude and longitude data, but it's very new. So I convert it into meters. I'm taking my data, which I pretended to be lat long. I'm not converting that to be meters. I'm like two steps removed from the original data. And I had to convert my real threshold into meters, which I was roughly 11,000, which was not exactly a one to one crossover, but it was worth a shot. So I run this. And once again, it didn't finish. I needed a new idea. So I go back to the data, which is still being plotted on a globe. And I could see physically that there's some points which are just dramatically farther away from other points. Like the far left is nowhere near the far right. So why am I comparing a point on the far left to the far right or the far top to the far bottom? So I thought, let's split up the data into a grid. Let's break it up and analyze different sections of the grid separately. So continuing to use SF, there is a function ST make grid. This takes the range of the data and evenly fills it with equal size boxes. These boxes can be squares or in our case hexagons because they fit in better. Our goal is to get the grid cells small enough. So we're not comparing too many points at once, but not so small that ST make grid has a tough time doing the computations to make the grid. So we visualize this now. And we could see that the points fall into individual hexagons. So now instead of comparing all the points, I'm just going to compare the points within a single hexagon. Assuming the points are evenly distributed, I now have G, the number of groups, times n divided by G, times n divided by G minus 1 divided by two calculations, because assuming that there's n over G calculations in each cell, I just need to look at each individually and I do this G times. For half a million points and 600 groups, this is about 208 million calculations. That's roughly 125 billion fewer calculations than before. This assumes that the points are evenly distributed, which they're not, and we'll look into that. The difference is stark between not grouping and grouping. We're talking millions of calculations versus billions, hundreds of billions of calculations. But what do we do about the points near the edges of the hexagons? What if you have two points on either side of a hexagon border? What do we do about those? Because they wouldn't be compared, even though they might be right next to each other. So we inflate the hexagons. We use ST buffer to make each hexagon spill over a little bit into the other hexagons. This way, I'm counting a little bit of the points outside the hexagon. Yes, some points near the borders will be counted twice, but we can remove those later. It's not that big a deal. The goal is we now have overlapping hexagons, so no pairwise distances get missed out on. You make the buffer really small, just slightly bigger than your threshold. If you plot it again, you can't even tell the difference, because the buffering is so small, you can barely tell the hexagons are overlapping. Now we need to find out which points fall within which hexagon. We do a spatial joint using the ST joint function, and we say find out exactly which points intersect ST intersects with which hexagons. It's beautiful that we can do this using SF, makes it really simple. We plot this again now, color coding the points according to the hexagons. Remember, we're comparing points within a single hexagon against all the other points on a hexagon, not having to overlap. Well, the hexagons overlap, but we don't need to compare outside hexagons. All done for us. Now, ST make grid and ST join can be slow, and I'm trying to bring out as much speed as possible. So I looked into the hex bin package, which has an eponymous function, and this can figure out which points fall in which hexagons and it'll make the hexagons much faster than ST make grid. But it doesn't actually give me the borders of the polygons, and it can't deal these border issues. It can't create the buffer. And doing that after the fact, might actually take more time. It won't be much faster overall. So this was a bit of a false start, and I just went back to the grid we have. So let's examine that grid. I want to count how many points are in each cell. So I drop the geometry because if you count with the geometry, it has to aggregate the geometry, and it takes a lot longer. I get a count, and I remove any hexagons with only one point in it because there's nothing to compare it to. So my hexagons now range anywhere from two to a hundred thousand points, with the median number of points in hexagon being 201. That meant the median number of calculations was 20,000, which is manageable. However, the biggest group still has five billion calculations for the biggest groups. That's a lot. But if we add up all these different calculations, we see we now have to make slightly under 14 billion calculations as opposed to 125 billion. That's an order of magnitude faster. So now that the ID is labeled, we can split it up. So I first tried group nest, which will take the data frame, and rather than making a grouped table, it will make a list column of individual tables because I sometimes find those easier to reason about. I then use map inside of a mutate to iterate over that list column and apply my function to each of them. So I do this. I first transform it into meters. I could use ST within distance. I do the group nest to break it up into a list column, and I call ST is within distance on each of them. Our problem just went from impossible to solve to taking hours to run. If each group was small enough for ST is within distance. So we have two issues. Group nest can be slow. It's actually a slow process. And ST is within distance is slow and might not even finish. So we'll tackle one thing at a time. We'll first deal with group nest, then we'll deal with ST is within distance. So instead of group nest, I use the combination of group by and group map. Group by creates rather than a list column of tables, it creates a grouped table, and that is a faster operation. Then group map iterates over that group table and returns a list of results, sort of like DL apply from flyer. So I swap out group nest and I run this grouping by cell, then I do a group map applying this ST is within distance over each of the groups. We now went from hours to minutes, a dramatic speed up just by swapping out group nest for group map. If each group was small enough for ST is within distance. And we could have made smaller cells, but that would have added computational time in the cell creation and the joining. So I thought I could still do better. And I turned to data table because it excels at grouping. So I take my SF object, I convert it into as dot data table. I then pipe that into square brackets because data table uses square bracket notation. And you can pipe into square brackets if you put a dot in front of the square bracket. I group it by cell, there's a group by option in the Excel, the data table square brackets, then I call the function. But I, when you normally would do a group by and data table, you're calling individual columns. But if you want to use a non standard function that's not some or mean or something like that, you have to call the dot SD object, which is a special group object in that respects the grouping of the data table. And then ST is within distance is meant to be called either on an SF object or geometry column. So I extract the geometry column from the dot SD part of the group's data table. Our operation now runs in fewer minutes. And this matters if you're running this often. If each group was small enough for ST is within distance. So at this point, I realized I needed an alternative for ST is within distance. It just wasn't cutting it. So I turned to our fast. This is a package which has rewritten a bunch of base our functions to be faster. And it lives up to the name, including a function called dist with a capital D. This works pretty well. And it is fast. Until you get to 46,342 rows. And I have to dig into why. So I looked in the C++ code and I saw this function, which computes the number of calculations that will be needed. It's returning an integer. And that's the problem. Now remember, integers in R are 32 bits. We saw before that we need to make n times n minus one divided by two calculations. In order to figure out this number, we first need to compute n times n minus one in memory. So at 46,342, this comes out to be 2,147,534,622. And this, you could be thinking, is this bigger than a 32 bit integer? But two to the 32 minus one is bigger than this number. So what's the issue? That's because we use one of the bits for the sign to tell us if the number is positive or negative. So we really only have two to the 31 minus one, which is smaller than our number. So we need a bigger integer in C++. This is where our Xlint comes into play. It's not a 64 bit integer, but it's the biggest integer the machine can handle, which with most modern machines is 64 bits. So I made a change of the code from this first function returning integers to the second function returning our Xlint. So I made a pull request, but it hasn't been accepted yet because they asked me a question, and I didn't notice that until last week. So hopefully this pull request will go through soon. So then I talked to Brian Lewis, and he has a package called T-Core that's going to be on crans soon, hopefully. It's based on IR LBA. And it's really designed for computing threshold correlation matrices, but he told me it can compute, it can identify points that are within a certain distance of each other. So I do this for one cell. I convert the SF object into a matrix using ST coordinates, and I pipe it into T-dist. And T-dist can work in parallel if you register a parallel front and first. And I run this and it returns the indices of the rows that are close together and the distance. I then iterated over our data table to do this to each of the cells individually. A similar code to before, I get the coordinates as XY, I drop the geometry, I make it a data table, I then iterate over and pass the matrix version of the .sd object into T-dist. Now this is great, but it relies on the sparsity of the answer. If there's too many rows that are close to each other, this slows down. So I couldn't count on this always being fast. So I then thought about the torch. This is essentially a matrix algebra library on a GPU. It's pretty new in R and the R package is just R and C++. It doesn't rely on Python. So I was pretty excited about this. So I turn the geom object into a matrix using ST coordinates, and I turn that into a tensor and I tell it to be on the GPU by saying device equals CUDA. I then call nnfp-dist and this returns the pairwise distances. This was unbelievably fast. For 37,000 rows, it finished in 0.003 seconds. That's amazing. But my GPU only has 11 gigabytes of memory. So it's easy to blow through that and crash once you get past about 60,000 rows. And not everyone has a GPU. And it takes time to load the GPU, load the data onto the GPU for each group, and then extract the results and then clear the GPU memory again for each group. So there's a lot of overhead. So I then said, I need some custom code. So I huddled together with a few friends, Michael Bagelmacher, Casa Echomodo, and we came up with this algorithm. You first sort the data according to one dimension, one dimension only, the X dimension. You then do a double for loop. You go one row at a time and then do the inner for loop comparing all the subsequent rows. You don't look backwards because you've done that already, you just look focused. You first compute the distance along the X axis, X1 minus X2. If that is greater than the threshold, by the triangle inequality, that means the generalized distance of all the dimensions has to be bigger than the threshold also. So you know you failed the threshold. But the bonus is that since your data are sorted according to this X axis, you can then break out of the inner loop and skip all the subsequent points because they're sorted, all the other points are going to be farther away and you get rid of all those computations for that round of the outer loop. So you save a lot of time. If that passes the test, you then compute the generalized squared distance. You don't compute distance because that involves the square root which is costly. You compute the distance, the square distance. And that's on all the dimensions. You do it generalized for all the dimensions and this could be more than two dimensions. It could be a generalized distance. Now, if that distance calculation fails, you skip it. If it passes, you then take the square root and you keep track of the indices. Doing this in R, we first allocate an array for the indices and another one for the distances. I probably could have made them one array, but this was just quick and easy. I then do a double for loop and if your eyes aren't burning, you can see it's doing exactly what the algorithm says. You compare the X dimension first and if that breaks, if that's greater than the threshold, you break out the inner loop. If it's not, you compare the square distance. If that's good, you take the square root and make note of the indices and the distance. Running this on a single cell, it finishes 3000 rows in about a second, 18,000 rows in 21 seconds and 37,000 rows in 70 seconds. And part of the slowness of that is the, is allocating the array and there's probably a better way to allocate that array. But this seems manageable. So I'm using group by and group map to iterate over all the cells and do them individually. But all those individual timings could really add up and slow things down. And I can't really do it in parallel because that memory inefficiency could really hurt us because when you do it in parallel, you could really inflate the memory. I needed to go faster. This still wasn't good enough for me. So I turned to RCPP, which lets you write R like C++ code. This is the same function written in C++, which is more verbose. The first key thing I do is instead of preallocating an array for all the indices, I make a zero length array just to keep the indices and the distances. And as I add values, I can grow the vectors. And the vector, growing a vector in C++ is much more memory efficient than doing it in R. So we're not going to pay a penalty for that. I do the double for loop, which is much faster in C++. And I use Armadillo to do the matrix calculations. I first check the X dimension. I break out of the inner loop if that doesn't work. I then compute the generalized square distance. If that works, I take the square root and I push back my vectors. I make an adjustment because C++ starts counting at zero and I return the objects as a data frame. I call this on an individual cell. For 3,000 rows, it finishes in .05 seconds. For 18,000 rows, it finishes in half a second. And for 37,000 rows, it finishes in 1.3 seconds. This was an insane speed up. I didn't even do anything in parallel in C++. So you can see that the C++ code is going to, as you grow your data, is going to be much, much, much faster. So let's do this for all the different cells. We get the X, Y data, we drop the geometry, we make it a data table. The data need to be sorted. So I use set key in data table and that sorts the data. And I sort it according to the first dimension. I then call our function for each of the groups by using by, I say .sd calls so I convert just the X, Y into a matrix and I call my function. I get this beautiful data table back with my answers. For the entire data set, it took about 30 seconds. And I've tested this on millions of rows and it still stays about 30 seconds. We went from cannot finish to finishing in hours, maybe, to minutes to seconds. And this really matters when you're running this algorithm hourly. This really matters getting it down to seconds timing. And we did this by splitting the data into smaller pieces using a smarter algorithm and writing compiled code. And that made it a solvable problem by following these three steps. Thank you very much."}, {"Year": 2020, "Speaker": "Daniel Chen", "Title": "Learning Tidy Evaluation by Reimplementing dplyr", "Abstract": "The tidyverse has grown to be a widely used set of tools with `dplyr` as one of its earliest members. One can leverage people's familiarity with `dplyr` as the motivating example for going through the more complicated topics around tidy evaluation. By re-implementing the behaviours of some dplyr functions (e.g., select, filter, etc) one can see how `rlang`'s tools for quoting (e.g., `quo`, `enquo`) and unquoting (e.g. `!!` and `!!!` ) play a role in writing tidyverse functions. The audience may have already heard of \"passing the dots\", but this talk will take off one of the training wheels to see how users can use the tools to create their own functions by replicating some of the behaviours of the ones that many folks know and are familiar with.", "VideoURL": "https://www.youtube.com/watch?v=WoBbQ5gsbgU", "id0": "2020_06", "transcript": "I'm not sure how recently he found this out, but Yo-Yo Ma is in his family tree. I don't know if this is a 23andMe or you met him or how far away he is. Are we talking like second cousins or 30th cousins? I need to know that answer eventually. So please everyone welcome Daniel Chen. Okay, so welcome everyone. Hello, my name is Daniel. I am currently a PhD student at Virginia Tech studying data science education and pedagogy in the medical, biomedical and health sciences. And if you want to learn about some of the work that I've been doing, there is a website called dsforbiomed.tech, which is a data science book essentially geared towards people working in the biomedical sciences. So it's a work in progress. In the past, I was an internet art studio where I worked on the grade this package, which is a code grader for learn art documents. And recently I've been involved in the carpentry since 2014. So I've been an instructor since then, but recently I am a trainer. So I am someone who can train instructors. And as of the last couple of weeks, I am officially the community maintainer lead. So I help manage all of the 50 plus lessons through the carpentry. And like Jared mentioned, I have a book that is a blatant rip off of his book, which is called Pandas for Everyone. Today we're going to talk about tidy evaluation. And there's three main parts quasi quotation closures and data masks. And depending on which part of tidy evaluation we want to work with, there's a few topics that we need to cover like quotation closures and environments. So we're going to be using Deep Liar as an example because it's something we're familiar with. And I know the talk said we're going to talk about all of Deep Liar, but in reality, we're really just going to be using the Deep Liar Select function. And so the data set that we'll be working with is the famous Polymer Penguin's data set. And the first thing we're going to be covering is how do we select columns as a review? So in base are we can select columns using the regular square bracket method. So we can say row column and there's a parameter called drop. If we have a single column, we can put in our colony in string. And then if it's a regular data frame, not a table, we have to specify drop equals false. And that's how we get a data frame object back. Otherwise we get a vector. Tibils have this tables and data frames. If we need to select multiple columns, we can pass in a little vector of two things and then we can select multiple columns. Another way we can select columns is using a dollar sign. So we can use the dollar sign or a single column with drop equals true. And then we'll get a vector object back. In base are there is a subset function that sort of behaves very similar to the supplier in the sense that we don't have to put quotations around the columns that we want. We can just use the column names as is. And then the thing that we've all very accustomed to is the deep wire select function that I don't really have to go into more detail about. But one other thing that's really important is another way you can subset columns. It's not really advised when you're doing analytics work. But you can subset columns based off of the actual index position, like the number. And we'll see where that comes into play. So first things first, what is quasi-quication? And the thing we have to learn is actually what is quoting. So what is an expression? So the R code that you write actually is really composed of two things. It's like the code that you write, you hit enter, R takes that code, runs it, and it gives you an answer. So when we type three plus three, we get six. But there's a base R function called quote. And if we say quote three plus three, instead of evaluating it, we get back the code three plus three. And so we can save that to a variable. And then there's a base R function called eval. And then that's sort of the premise of lazy evaluation where we're taking expression and evaluating it later. So you can think of an expression like the string representation of your code, but it's not really because it's its own object, but it's a pretty reasonable approximation when you're trying to think about this stuff. So where does R Lang come into play? R Lang has a function called expert for expression. So we can do the same thing, pass in three plus three, we get three plus three, and then we can evaluate that expression using base R. Or in R Lang, there's a function called eval underscore tidy. And that'll do the evaluation on that expression through R Lang. And so expressions can be different things. It can be a call, a symbol, a constant or a pair list. Those are all really just implementation details under the hood. So if I say like three plus three, you'll see that I'll get a language object back. But if I put in three, then just a number, I'll get a number object. And then if I put in like something, you'll get like a symbol. So you can quote things, but under the hood, you might, it's like stored a little bit differently. And so how do we select column? So like I said before, we can pass in a direct string into a column. So we can pass in species directly, or we can save species into a variable and pass that into our data set directly. But if we pass in species on its own, you'll see that you'll get an error that says species not found. Right? And that's because it's not safe to a variable is trying to evaluate it and it doesn't exist. So in a table, this is sort of just a special thing with tables. But you can pass in these quoted name objects and tables and it will work. Right? So tables have this additional feature that doesn't work with regular plain data frame objects. So if we try to do that in a regular data frame, you'll see that like, for example, I'm using the iris data set. If I try the same subset on a data dot frame, it doesn't exactly work on the table. So these are things that we have to think about. So let's write our first selection function on our own. It takes in a data set and then a column. And then all we're doing, just like before, we're saying data using some regular square bracket notation and putting drop equals false. So we always get a data frame back. And this is nothing special, right? We passed in a string column, we get a column back, right? And it works in tables if we put in quotes. But like I said, it won't really work if we just pass in species on its own because it's trying to evaluate that, right? But I just showed you how to quote inputs, right? So let's use that quote function that we just learned about and let's quote the actual column that comes in. But then we find out that no, that doesn't exactly work because when we wrote quote call, what actually got passed into data is actually C-O-L, not species, right? So it actually quoted the thing that we told it to quote, which is exactly what we told the computer to do. So that's a problem. In our Lang, there is a function called nexper or enriched expression. And you use this function inside of a function to sort of handle that, like, hey, I want you to quote this thing, but don't quote the variable or the function parameter I gave you actually quote the thing that the user passed in, right? And so there's, we use nexper for that. So we can use nexper for our third attempt at this. So instead of using quoting, we're putting an expert. And then now if we put in species without a quote, it'll do that for us. And we can save this object to another variable and then pass that in along the way. But if we pass this into a data frame object, it doesn't exactly work properly because tables have that additional feature that data frames don't have. So the way we handle this more generally is we use the index position. So we take the, whatever the user passed in, and then we write some regular R code that essentially says, hey, look through all of the column names in our data set, find the ones that match and then subset on the index directly. And then we get the same exact features for both sides. We can say pass in species for a tipple or we can pass in species. That's the quote for data frame. We both get the same result. And that's weird thing of like using the index position. If you sort of dig your way and follow the deploy or source code, it pretty much does something like that. You'll see that there's this match call that ends up calling this VEC as location. So under the hood, the player is doing something very similar. So now we want to expand this. How do we get it to work with multiple variables? So before we showed you a function called nexper. So if we put an S at the end of it for nexper, it will be able to capture essentially dot dot dot. And then what we get back is a list of expressions that was passed in for dot dot dot. So what I did was we have a list of expressions converted to a character and then do regular R things to get the index position and then subset that way. And you'll see it works for both a tibble and a data frame. The thing that I noticed when I first wrote through this. I wrote which instead of match. And the difference between that is if I used which it didn't it preserved the same order of the original data frame regardless of what I used in. And if we use match, it'll if we specify the columns in a different order, it'll give you back the data frame in the order you specify. So that's the little difference between there. So what is exactly is quasi quotation, which is essentially quoting, unquoting and recording. So before our last attempt, we use nx expert to like handle the weirdness that's going on. We turned it into a character string and then we did regular R things with it. So variables can be both quoted and unquoted. So we can say like, okay, now we want column as species as the variable and we want to pass that in as one of the column names for selection. And you can see right now in our current function, we can do that in base R and we can do the same thing in the player using the player select and then we get a little warning that's saying that it's like evaluating a thing, but it's only a warning that's one per session. So we want to be able to mimic that behavior. So we can do that. Our function right now, if we take in penguins and pass in island and year, we can do that. But right now it doesn't want, it doesn't exactly work if we pass in call name, right? Because we can send species as a variable to call name, but it's not working properly right now because it's trying to pick up like COL NAME as a column in our data set and that doesn't exist. And that's sort of the error that we get because we're trying to subset on an NA. So the problem is we need a way to treat a variable as a variable, not as the quoted item. There is a train running across my window. So if you hear rumble, that's what that is. So if we want to unquote, so the thing is we want to actually unquote call name, get the parameter that the user passed in and then quote it back and use species in our subsetting function. And so that's where you see this bang bang operator, this two exclamation marks. And what that does is it actually unquotes the variable that we passed in. So now if we say call name is species and then we can use pass in bang bang call name and what that will do is it'll unquote it'll take call name, turn it into species and then pass that into our function. So that's what that bang bang operator is doing. So how do you know if a function actually quotes its inputs? So you first try to run the function as is and then try to run that parameter outside. So if we ran penguins in Ireland, but we just ran island on its own, you'll see that the first option, it actually runs without error and the second option can't find island. And we've seen this like in our normal day to day, our programming, if we library our laying, our laying is probably not a variable that you have. And so like libraries and other function that quotes its inputs. So at the end of the day, this bang bang unquoting is really just selective evaluation on parts of a quoted expression. So it's something that's quoted, but I actually want you to unquote that one little bit because I want to pass in some other user input. And so bang bang bang is bang bang for dot dot dot. So that is probably a sentence you didn't know was a sentence until right now, but essentially that is what the triple explanation mark is for. And so what is bang bang bang? It is how do we pass in or selectively unquote a list of things? And so we can have a bunch of expressions using experts and say species island year and then we get a actually a list object back. And then if we pass in that list object to our selection, it doesn't know what to do with that. So then what we say is, hey, take those expressions sort of unpack it and then like evaluate that list, right? So evaluate that dot dot dot. And then we can actually use our selection function to do that. The supplier on its own actually has one more check. So you can sort of have this mixed series of columns so we can pass in species as a column name and pass in column name there. And then we can also use bang bang a column name and it will selectively unquote that. So the player allows both sections and our currently our current function doesn't work that way. And then way for us to make our function behave more like the player, we have to learn a thing about closures, which is a quote and a closure. And so if I say closure, we probably all seen some error essentially along the lines of object of type closure is not subsetable. And typically the reason is because we are writing in our code something like df bracket one. And that's because df is actually an R function that's built into the stats library, the built in stats library. So what we're actually doing when we do this sometimes if we didn't specify df as a variable is we're trying to subset a function. And if you try to subset like select from the player, that doesn't make sense. And that's you'll get the same exact error. So what exactly is a closure? A closure is something like a function or an expression plus its environment. So what is an environment? Well we can create a new environment using the base R function called new dot m. And we can say in this environment, create a variable X assign it to three. And the thing is if we in our regular global environment, just say X, it doesn't exist, right? You'll get an object X not found. But and if we try to say like, hey, use that X variable, right? So like three plus X, it doesn't exist. But if we say three plus X and then also pass in this environment E that contains the definition of X, it'll work, right? So that's what environments are. And so formulas actually encapsulate an expression and its environment. It's not just for models. And so if you say like tilde three plus three, you actually get the expression three plus three back. And that's why you see the tilde being used throughout the tidyverse. It's sort of the quick way of quoting something. And so we can actually subset formula objects. The first object is always going to be the tilde and we can get different expression parts out of it. And we can evaluate them. So formulas is a form of an expression plus an environment. And so what we can do is we can take a formula, we can take a separate environment, and then we can run this eval, pass in the formula expression, and then pass in the environment of the formula and have it evaluate the formula expression in a given environment, right? So a closure instead of like that formula is really an expression plus the environment. So it's not, it's a different like object, but it's really a subclass of formula without the little tilde bit. And this allows quasi quotation. And so the user might use the tilde, but from the developer's point of view, you'll be working with closures. And so closures behave very similar to functions in that there is this part that's the expression and then there's this other part that is the environment. And then what you can do is run eval tidy on a closure object. And then it'll say, okay, I know the expression bit, I know the environment bit, I'll run those two bits together. And so crojors in practice, we actually don't use that new closure function, but that sort of exists in the world of our line for a complete mistake. So the last bit of tidy eval is this concept of a data mask. And so essentially data mask is a data frame or a list that essentially when you try to run code, that's where it starts looking for variables first. So it supersedes the actual environment in terms of variable lookup. So think of a, you can also think of it as like this is a environment that goes before the current actual environment, we're variables to look for. So what we can do is we can create a closure using new closure and then we can pass in a data frame that says y of one to five. And then in eval tidy, we can pass in the closure expression and a data frame or the data parameter. And so right now our expression is x times y, the closure itself has the value 100 for x. And when we say run x times y with x equals 100 and pass in a data object of our y values, we'll get the 100, 200, 300, 400, 500. So it's using that data object as a way, as a place to look for additional variables. And you can see at the bottom there, if we try to run it on its own without this infrastructure of dealing with environments, we get different, it either doesn't work. Or if we want to totally replicate our example, we would simply say 100 times df dollar y. So that's exactly what that eval tidy function is doing under the hood. So in our previous try, we handled the weirdness, we unquoted the arguments, we found the index positions and then we did regular R things to return our data set. But it didn't deal with the fact that we passed in call name by itself. And so we use the data mask for that. And so we still handle the same weirdness. And essentially, you can see here, I'm using set name sequence along to get the all of the columns of our data frame. And I'm running eval tidy to say like, hey, all of those inputs evaluate them in this context so I can get the actual column name back. And then, you know, just to handle the fact that sometimes a user might pass in a character string, we dealt with that too. But at the end of the day, select a range tidy select all of that stuff, all it's doing is getting a vector of index positions back and then returning that back to you using base R that's penguins, the index and then drop equals false. So there's a lot of things I didn't cover. For example, there's this colon equals operator that essentially lets you quote things to the left hand side of an equal sign. I also didn't talk about pronouns like data and dot M. So you can read more about those. But the TLDR is if you want the user to pass in a string and the function itself internally uses a string, that's something that we're all used to. So for example, G.D.P.A. has an AES string option. And we can use our function as is. If you want to use it to pass in a string, but the function takes a quote, what you want to do is convert it to an expression first and then unquote the expression. So we can use AES with bang bang there. If the you want the user to pass in a quote, which is essentially without the quotations mark, so the actual code and then the function itself takes a string, you capture the expression using an expert, convert it, convert the string to an expression. So there is an R laying as string and then you can execute that as is. And then if you want the user to pass in a quote and the function itself also takes a quote, you first have to use N quote to capture the quotation and then unquote it. And that's where you see this curly curly embrace location in R laying. It's essentially doing that quoting and unquoting for you. And so with that, there is this meta programming chapter in advanced R and I've probably read that like once a month for the last year. So just keep reading that until you eventually get it is sort of what I ended up doing. So thank you. Thank you very much, Dan. That's awesome. She started soon. It's awesome. Love dance technology is always always brings it both on screen technology and others. So that was really great with that, folks."}, {"Year": 2020, "Speaker": "Col. Alfredo Corbett", "Title": "Air Combat Command Enterprise Data Improvements", "Abstract": "Data is a warfighting asset, fundamental to how Air Combat Command (ACC) operates in and supports all five domains of warfare. With a rapidly growing data landscape, ACC is implementing major improvements to the way it manages, acquires, ingests, stores, processes, exploits, analyzes, and delivers data to its almost 100,000 operators. In coordination with the Department of Defense and the Department of the Air Force, ACC is pursuing six lines of effort to improve its data governance, data architecture, data standards, and data talent & culture.", "VideoURL": "https://www.youtube.com/watch?v=_OEXboIxE9o", "id0": "2020_07", "transcript": "He thought his hair would grow back, but I guess it hasn't. So please welcome Colonel Alfredo to the stage. Good afternoon everyone. I'm Colonel Alfredo Corbett, the Deputy Director of Cyber Space and Information Dominance for Headquarters Air Combat Command. Hopefully you enjoyed that fun fact that was set up of me. Yeah, I expected my hair to grow back after I shaved it, but I think it looks good now. I'm glad to be here and thank you for allowing me to share with you Air Combat Command's data enterprise strategy. We've come a long way in a short period of time, but still have a ways to go. And with that, I'll begin the presentation. So this afternoon, I'd like to talk to you about some of the actions that we are taking at Air Combat Command or ACC for short to better position us for information dominance. We have undertaken a comprehensive assessment of our entire data workforce, our data infrastructure and our culture in an effort to better position us for the future of warfare. This means improving data management capabilities, enhancing data access and sharing, especially with our mission partners and delivering advanced analytics, including through artificial intelligence. We're currently putting the final touches on our data operation plan to define how we will implement the task to align and comply with the Department of the Air Force, the Department of Defense guidance and especially as it's contained in the DOD strategy and the Air Force implementation plan. On the right-hand side of this slide is a memorandum from the Deputy Commander of Air Combat Command, Lieutenant General Christopher Wegeman. He signed this document earlier this year and issuing his guiding principle for us in developing a plan that implements the Department of Defense data strategy. General Wegeman's vision is fourfold. First, it establishes an integrated data governance capability at all levels. Then it creates a modern data reference architecture that aligns with DOD and Air Force standards. And then also integrates and delivers improved data management and governance practices by integrating DOD and Air Force data standards with an emphasis on data access, exploitation and sharing. And lastly, it establishes a modern ACC enterprise data platform that brings together data, data management and discovery and analytics tools. We'll talk more about some of these themes as we go through today's briefing. The bottom left-hand graphic is the Department of the Air Force and it outlines the vision of the Air Force Chief Data Officer, Ms. Eileen Verdream. Our data must be vault visible, accessible, understandable, linked and trustworthy. In developing ACC's roadmap to our data-centric future, we've relied on ACC, US Air Force and DOD principles and guidance. That same guidance has guided our approach. As I mentioned on the previous slide, we focused on conducting a robust and holistic assessment of our current data strategy, people, processes and metrics. We interviewed 30 executives across ACC and collected inputs from almost 700 airmen across our organization. Our findings and the resulting reports span eight different capability areas and will start at the top of the graphic and work our way around clockwise. Data governance is the foundation of effective data management enabling data to be integrated into an enterprise-wide data management solution by codifying and enforcing data and technology standards, developing a data-centric workforce and integrating data efforts across functional teams. So if data governance is the foundation of our data management success, data architecture is the foundation of the technology that enables data effectiveness. An effective enterprise data architecture, when it exists, optimizes the ingest, storage, processing, integration, usage, access, delivery and exploitation of data. In the Air Force, it is paramount that our data architecture is able to operate at what we call the speed of war. Delivering the right data at the right time to the right firefighter, a process that in combat operations can be fractions of a second. Metadata management creates a standardized framework for generating, controlling, classifying, and managing our data assets. It's the data about data. Effective metadata management is crucial to enabling data access, whether it be through data discovery, data classification at the attribute level, rather than the system or network level. And master data management ensures that there is a single and authoritative version of the truth and requires that common entities are represented using a standard consolidated framework. Now, data quality preserves the value of information assets by safeguarding it from corruption and ensuring accuracy. When we talk about information lifecycle management, we are talking about ensuring that collection, preservation, and disposition of data not only conform with federal laws and regulations, but also enhances rather than hinders operational successes. And information privacy and security focuses on security and access policies and tools that permit authorized access and prohibit unauthorized access. This capability area is especially important in a military context where classified operations and communications are common and where personally identified information is everywhere. Finally, enterprise analytics focuses on instituting reporting processes that are baked into mission processes, but also on developing common tools to analyze data. Now, on this slide, depicts the approach as we have tackled it at ACC, taking into account the eight capability areas that we just discussed on the previous slide. So looking at the slide from left to right, we established a clear vision that is part of the Deputy Commanders' guiding principles that we mentioned on the first slide. Then we conducted an assessment of our organization's maturity and developed a plan to set a data management office that can implement these changes over the next five years. We also developed a target reference architecture to be implemented by our nation data management office. The graphic at the bottom highlights what the architecture looks like, and I'll discuss more in a moment. In order to define and prioritize the organization's goals and objectives, we developed four mission-focused use cases that identified a real ongoing operational problem for the ACC warfighter across aviation, cyber, and other enabling communities, and highlighted methods that enterprise data management can solve. And lastly, we put pen to paper and wrote a data operating plan that assesses our current state, identifies our target, describes the gaps between the two, and builds a roadmap to get there. So what have we found out? So before I talk through some of our findings on the left, let's take a brief detour to talk through the concept of multi-domain operations. In the past, our battles were fought and won in a single domain, such as the Battle of the Monitor in a Merrimack, the Battle of Britain, the Battle of the Bulge, Operation Line Backer, just to name a few. Those days are gone. The future of warfare is conflicts that simultaneously engage maritime, land, air, cyberspace, and space domains, all of which are enabled by data. Multi-domain operations is nothing more than the recognition that in order to compete successfully, the US military will need to leverage more than one domain, and likely all five domains simultaneously. Because of this need, connectivity between the five domains of war is paramount, and must be enabled by a system of systems that facilitates fast, accurate, and reliable data flow between every node in the system. Join all domain command and control. What we call JAD-C2 is the multi-domain enabled iteration of command and control that provides unity of command and flexibility and scalability for the theater commanders across all domains, and it is the system of systems being developed by the Air Force to enable warfighting capabilities and replace the aging command and control platforms currently in use. The large graphic on the slide indicates some of the core components of this internet military things, each of which is identified by its function and the suffix ONE, or one, to indicate its universality as a source node of that function. So for example, the centralized data repository in ABMS or Air Battle Management System is data one. The open architecture moving target indicator is open MTI one, and the unified cloud solution is cloud one, and so on and so forth. Why does all of this matter? As the graphic indicates, the central node of ABMS is its integrated data cloud based architecture. Data drives the entire concept of operations. General Charles Brown, the chief of staff of the Air Force, hit the nail on the head when he spoke at the National Defense Industrial Association earlier this year and highlighted data as the most critical component of ABMS and JAD-C2. He said, we have a lot of data. We just don't publish it. We don't share it like we should. We must accelerate the change required to connect all our platforms and sensors and our weapons systems. We must develop the infrastructure for the digital backbone for the data. So where does this leave ACC? As the Air Force's largest matchcom and its lead matchcom for air dominance and cyber operations, ACC plays a leading role in enabling the Air Force's success through ABMS. But what we have is a ways to go until we realize that vision. Our goals and observations include the findings on the left of the slide. We need to be better at viewing data as a warfighting asset. This is a cultural issue as much as a technological issue, and we need to help our entire workforce understand that data is a commodity that enables success in every other area of our operations. Enterprise data management is critical to enabling our future readiness, agility, and effectiveness. Without good enterprise data management in place, our vast flows of data become a hindrance rather than an enabler to our combat operations. Data architecture improvements are desperately required to optimize our infrastructure consumption, maximize data sharing, and unlock our analytics and exploitation capabilities. ACC needs a robust data classification and access capability that's compatible with intelligence community and our joint and international partners, and that classifies data at the attribute level. This will avoid stovepipes and silos created when data is classified at the system, network, or data set level. And last, ACC will benefit from processes, automation capabilities, and analytics, SAM boxes. These capabilities will lead to greater precision, better tracking, higher output, and increased ACC's ability to scale to meet operational needs. So how do we get there? Our new data architecture will be key to enabling the points discussed on the previous page. This is a top-level view of the reference architecture with the flow of data into the architecture depicted from left to right. The flow begins with data sources on the far left-hand side. These various mission systems across the operational spectrum from readiness monitoring services and financial management platforms to sensitive ISR and air superiority fighters that generate data of all types. In the acquisition phase, those data sources are ingested into the architecture. This could be via an application program interface, automated ETL, streaming, or other method that that point is that architecture is open, flexible, able to adapt to the diverse technical capabilities of all of our mission systems. In the data marshaling, the data process vetted, integrated, arrayed, processed, and stored. This stage of the architecture is format agnostic and should include a variety of databases for all data types. At this point, the data should be visible, accessible, understood, linked, and trusted, vault, if you remember it before. The accessibility of the data will be evident in data access, through which end users and analysts are able to access the data through application program interfaces, native extract, transform and load processes, custom streams, and more. And in enhancing, understanding in the enterprise reporting and analytics stage, those same users will be able to leverage a robust set of information consumption capabilities that enable data science, data discovery, data dashboards, geospatial analysis, and more. Action takes place once the data reaches target applications. This is when the data completes its journey through the architecture, arriving at the target to enable terminal weapon systems to do its job. Undergirding all of these infrastructure services and enterprise information management services, we have metadata management, master data management, information lifecycle management, and others that we spoke about on slide three. This slide shows three of our use cases that we have developed to demonstrate the full scale of our data improvements. If you direct your attention to the legend at the bottom right, you'll see that this graphic highlights various theater equities and the role they play in a specific use case. The green dots show involvement in our dynamic targeting use case, the white dots show involvement in our joint cyber defense use case, and the blue dots show involvement in our readiness use case. The first use case supports ACC as the lead command for JAD-C2 dynamic targeting with enterprise data solutions that integrate and present data to the war fighter at the speed of war. The ACC mission requires advanced data sharing as the bedrock of next generation information superiority. Key programs of records such as air operation centers or AOCs and distributed common ground systems or DCGS offer water fighters siloed light capabilities in their proposed current framework. But if properly integrated into a unified enterprise architecture would become force multipliers within ABMS. AOCs will leverage the ABMS C2 or command and control framework. JAD-C2 dynamic targeting and DCGS' time dominant fusion capabilities to execute their find and fix capability in near real time. Ultimately data improves will demonstrate information superiority with integrated data driven operations through an entire kill chain, which starts with theater taskings from the joint force air combat commander to the DCGS target and intel reporting, then through the automation of near real time threat reporting to the AOC, which concludes with the transmission of the task and the target to the weapon system. The second use case will integrate the US Space Force's defensive cyber operations mission data with F-Cyber and to cyber commands big data platforms and additional data sharing technologies like the US CyberCom unified platform, the Space Force's unified data library, and the Air Force's vault to demonstrate the cyberspace domain awareness. The desired end state is that both ACC and US Space Force can leverage data from joint full spectrum ops into integrated platform to expedite counter adversary tactics, techniques, and procedures. And big data platforms like Elixir will be integrated into the ACC data architecture. The third use case will provide a tool set and methodology for air combat command to comprehensively evaluate the aggregated operational readiness of its work body forces at all echelons and across multiple measures of effectiveness. In sum, the current process in which we measure readiness is in a binary go, no go mentality and uses legacy readiness metrics for analysis with very little near real time insight into actual readiness down to the squadron echelons. In order to holistically understand readiness, our data must be integrated to conduct data discovery and arrange systematically for advanced analytics. The end state of this use case is improved squadron unit readiness with ACC unit commanders able to identify, analyze, understand, and forecast in near real time unit readiness and effectiveness for current and future operations. To move forward towards this realized state, we've developed a plan. Pursuant to the directives of the Department of Defense in Air Force headquarters, we've created an operational plan to implement key improvements across our architecture, governance, standards, and talents and culture. The blue lines of effort or L O E's on this slide are from the Air Force Data Strategy Implementation Plan. And our team has identified key tasks and actions for ACC within those L O E's. L O E 1 sets the foundation by focusing on building our ACC Data Management Office headed by the ACC Deputy Command Data Officer. We also intend to hire a Command Data Architect and associated support staff to grow and expand this effort over the next 12 months. L O E 2 directs how we are evolving the workforce by identifying data stewards who are responsible for managing data assets across ACC, as well as working on establishing training and continuing education plans for these stewards. Another item we're exploring within this L O E is setting up a center of excellence to aid data specialists to the data fluent. This center of excellence would also have a significant charter to grow and develop ACC's use of advanced analytics, especially artificial intelligence and machine learning. L O E 3 presents positions and protects data. In this L O E, we will build out architecture and process standards improvements like metadata management with a focus on driving interoperability by conforming to federal, DOD and Air Force standards and creating standards that don't exist today. In L O E 4, we'll focus on building, optimizing and operationalizing our architecture improvements by delivering them to the warfighter and incorporating them into broader frameworks like ABMS. A natural extension of L O E 1 is L O E 5, management and governance. In this L O E, we will empower the data management office and every data management professional across ACC to own and establish a partnership between mission, operations and IT to deliver a proactive data governance capability. This means developing a common governance framework for the data management office to provide toolkits and resources to stewards and always driving integration between our data management office and head of quarters Air Force. In L O E number six, converging means integrating resource and budgeting approaches between other match cons and the department of Air Force, while aligning our data management efforts across the DOD and with partner nations. We have a long way to go, but we're excited about the journey ahead. So with that, I would like to thank you all for allowing me the opportunity to deliver this presentation. These are exciting times for ACC and the Air Force and as General Brown, the Chief of Staff of the Air Force says, we must accelerate, change or lose. Everyone have a great Air Force day. Thank you. Thank you very much for that wonderful talk. It's nice to add to our second military member for this conference. Thank you very much Colonel Alfredo. So with that, we have"}, {"Year": 2020, "Speaker": "Andrew Gelman", "Title": "Election Forecasting", "Abstract": "Several months before the election, Andrew and his team worked with The Economist magazine to build a presidential election forecasting model combining national polls, state polls, and political and economic fundamentals.  This talk will go over how the forecast worked, the team struggles in evaluating and improving it, and more general challenges of communicating data-based forecasts.  For some background, see [this article](http://www.stat.columbia.edu/~gelman/research/published/jdm200907b.pdf).", "VideoURL": "https://www.youtube.com/watch?v=JAPSVOE-KP8", "id0": "2020_08", "transcript": "With that, we have a speaker who has repeatedly been at the New York conference, and this is the first time we have gotten him to the DC slash government conference. Really exciting. He does a lot of political science work and he does a lot of forecasting on the elections. So we're going to hear how he did, whether he nailed it, did terrible or got somewhere in between this year's election. There's so many things I could say about him from, I don't know, from the greatest living statistician, that's not going to make him nervous to a fun teacher to everything we could talk about. Everyone please welcome Andrew Gellman. It's a pleasure to speak in Washington, DC, even though I can't be there myself. I grew up in the suburbs of DC and my parents both worked for the government in non-supervisory capabilities, but nonetheless, they were, they did. And I just want to thank all of you for all of your public service. I would like to thank the previous speaker also for his service. I have never been in the military, but when I was a student, I did work at the Naval Research Lab in Washington, DC, right next to the sewage treatment plant for a few years. And I learned a lot of science there too. So today I wanted to talk briefly about election forecasting, and it was fun writing the title of the talk because I sent the talk title in October before the election had occurred. So I titled it how we succeeded brilliantly, failed miserably, or landed somewhere in between, which seemed like it covered everything except people don't really like the landed somewhere in between part. Like everyone wants things to either succeed brilliantly or fail miserably. And one of the messages we have in statistics is that things aren't binary in that way. While the elections binary, there's a winner and a loser. Public opinion is not binary. Science is not binary and a success of a forecast is not binary. One of the problems with people's perceptions of election forecasts is that they swing back and forth between thinking that a forecast exceeded brilliantly and that it failed miserably. So people are overreacting in both directions. Now before the forecast occurred, I was going around telling people that I hope that a forecast is perfect because it'll make us look good. But then after that happens, I want to explain to people how we just got lucky. And this is what I wrote at the time. I posted something after sending Jared the abstract that I wrote our model could fail miserably. For example, if Joe Biden wins Alabama, which we say has less than a 1% chance of happening, Joe Biden did not win Alabama. Or it could land somewhere in between. For example, if Biden wins the electoral college, but with just 51% of the popular vote, which is at the end of our forecast interval, Biden actually got about 52% of the popular vote. So it was within our forecast interval. But it can't really succeed brilliantly. Even if our model correctly predicts 49 states or whatever, that's as much luck as anything else as their estimates of margins of error. Well, the funny thing is our model did predict 49 states correctly. But and I don't want to go around telling people. So I was all prepared for the models going to predict 49 states correctly. People are all going to say our forecast is perfect. And I was going to have to explain to people that it wasn't. But of course, I don't need to explain that because everyone knows that the forecast wasn't perfect. And understanding what went wrong with the election forecast, I want to do two things. First, I want to tell you what went right with the forecast. And then second, I want to tell you the information that was used in constructing the forecast. Then we can think looking forward about how we can do better. So what went right about the forecast? Well, we said we thought Biden was going to get 54 and a half percent of the vote. And he got about 52 and a half percent of the vote. So our forecast was off by two percentage points at the national level. And it was off by more than two percent in some states and less than two percent in some other states. What was of course, you know, it's not going to be perfect. So what was right about the forecast was that it had a margin of error that accounted for things possibly going wrong, as indeed they did. Now given that, we have to say when information went into the forecast. So to forecast the presidential election, we use several sources of information. First is fundamentals. So predicting the national election outcome based on national political and economic conditions. So that would be the state of the economy. For example, as measured using per capita economic growth during the year preceding the election. Political fundamentals such as whether the incumbent is running for reelection. And the popularity, the approval of the president in the months before the campaign. Now incumbency is interesting here from a statistical perspective because incumbency comes into the model. Not so much as a main effect, but as an interaction. And what I mean by that is when an incumbent history seems to show and logic theory and logic also would imply that the impact in forecasting the election of economic and political fundamentals is larger when an incumbent is running for reelection than when he's not. So because the incumbent president was running for reelection, that suggested that the poor economy and his low approval would give him a, gives a low forecast for how well the incumbent was going to do, how well Trump would do in the election. Now the only tricky part about this is that over the years, in recent years, we think fundamentals are less important because of political polarization. So for example, in 1984 Ronald Reagan got 60% of the vote. Well, there was an economic boom at the time. But I don't think that if you were to have a similar economic boom now, you would get a candidate getting 60% of the vote because the two parties are just too isolated. And there are just too many Democrats who are not going to vote for Republican and too many Republicans who are not going to vote for the Democrat. So that's included in our model that the fundamentals matter, but not as much as before. The second part of our model is the relative positions of the states. So if you can predict the national election and you know the relative positions of the states, then you should be able to predict the election for each state. And of course, that was very important. Everyone anticipated providing to win the electoral college, he was going to have to win something like 52% of the vote, which is what happened. And we know that because from one election to the next, the relative positions of the states don't shift by very much. The third piece of information is national polls. So the fundamental space forecast has a big margin of error to it. The polls we can use basic inference to partially pool between the polls and the fundamentals. The next piece of information is a time series model because a poll is a snap-off. Shot not a forecast. So if it's September 1st and we're making a forecast of the election, we have polls up to September. Our fundamentals model is predicting what's going to happen on November 3rd. I think that's what the election day was or November 4th, whatever it was. The fundamentals model is predicting what's happening on election day in November. To link the data to the election, we have to have a time series model of how public if commune can vary from day to day. And the final piece of information that's in our model are the state polls. State polls are kind of overrated in importance in some way. We pretty much already know the relative positions of the states before the election is even going to occur. State polls don't provide as much information beyond the national polls, but they provide a little bit of information. Then in connecting, in a statistical model, we need to connect the data to the underlying thing we care about. The polls are measures of public opinion, but we know from historical data that the polls can be off. We found that the empirical error in a poll is about twice the stated margin of error. And not only can polls be off, but polls can be off in a correlated way. So having 100 polls doesn't mean that your standard error goes down by the square root of 100. I could have a million polls, but all the polls could be off. And indeed, in this election, the polls in aggregate were off by about two or three percentage points, more in some states and less in others. Now, there's some interesting statistical research here, which is trying to understand how our model is going to work. And so we haven't published any of this yet, but we've been playing around with ideas like refitting the model, where you take some data out, or where you take some data and shift the data, and to see what the inferences look like. And people sometimes will talk about the impact of one particular poll on the forecast, but that's not really important. One particular poll doesn't tell you that much, but what about all the polls together? So the reason why we landed somewhere in between, we did not succeed brilliantly because there was systematic error in the polls. And we allowed for the possibility of systematic error in the polls, but we didn't have a statement about which direction the error would be in. In previous elections, sometimes the polls have favored the Democrats, sometimes the polls have favored the Republicans. We followed a kind of economic argument, which is that the survey organizations are motivated to be accurate in their inferences. And so as a result, we wouldn't expect there to be a bias in one direction or another, or to put it another way we expect there to be some bias, but we wouldn't know ahead of time what that would be. We think the reason why the polls were off was because of non-response, differential non-response, that Republicans were less likely to, to Republican voters were less likely to respond to the polls and Democratic voters, even after adjusting for demographics. Some polls also adjust for party identification and how you voted in the previous election, but even then it seemed that there was differential non-response possibly having to do with people who vote differently than their party identification or with independence. The other difference is voter turnout, and so it seems, I think we still don't know the full story, but it seems that Republicans were doing more voter registration and in-person campaigning, and Democrats weren't, and some of that did show up in new voter registration numbers. Also, it's possible that the well-publicized early voting by the Democrats motivated more Republicans to then show up on election day to counter that. So of course, an election is a dynamic system. And finally, the reason why we landed somewhere in between, it did not fail miserably, is because we were historically motivated and allowed for the possibility that the polls could be off. Now, when preparing this forecast, it has all the moving parts, we thought a lot about how this information is communicated, and we did a lot of simulations, as I said, of our own forecast, and we examined others as well. And we learned a lot, so it's interesting, in the forecasting world, it's generally accepted that people have a motivation to be overconfident. And so I always give the example of the auto mechanic. So if you take your car, the auto mechanic, it's not running right, he'll say, oh, yeah, there's a problem with XYZ, and then you leave the car there, and then you come back the next day, and he says, well, it wasn't XYZ, it was this other thing, but don't worry, I fixed it, and you pay him, and everybody's happy. If you go the auto mechanic and you say, what's wrong? And he says, I have no idea, you might just go to a different auto mechanic. And then when you talk about doctors, sometimes people say you're actually paying for the certainty more than anything else. So there are a lot of motivations. This, as a statistician, I spend a lot of time fighting this motivation to be overconfident. It's just very easy to, people want you to be. So being overconfident is like saying, I love you. Like people, it just, you always, in any relationship, you're kind of in pressure to say, I love you, because you get immediate positive feedback. But yet, you know, you shouldn't just go around telling people you love them all the time, right? And so similarly, it's so tempting when giving a forecast to go into pundit mode and say, I am sure. And if you're a statistician, well, I know I'm not supposed to say, I'm sure. So going into pundit mode would be putting out numbers and saying, well, the probability is 58%, or whatever it is. And we have to be careful about that. So as a statistician, I'm always criticizing scientists for writing papers where they say, we've discovered this, we're sure of this. And like, then when things don't replicate, then people are like, oh, yeah, well, not everything replicates. And then like, why were you so sure before? We didn't really mean it. We're just saying that. So we don't want to be like that, right? You don't want to be that guy who's always overconfident. In election forecasting, though, forecasters got burned by some previous elections. People are surprised by things. So sometimes there's a motivation to be underconfident. Because so imagine what you're a forecaster, and what you do is produce 50% intervals for a living. And people stare at your 50% intervals, while every time the truth is inside the interval you succeeded, and what's outside the interval, you failed. So that creates a kind of economic motivation to supply 50% intervals that are too wide, so that they're more likely to contain the true value. If your 50% intervals have 70% coverage, then it looks like you succeeded 70% at time. Well, of course, someone could check, but with elections, there aren't that many ways to check. It was funny because the 538 website had a chart where they showed the calibration of their forecast based on hundreds of thousands of forecasts. Now, you might wonder where they got hundreds of thousands of elections. They've only actually been running for something like six elections. So they got hundreds of thousands by looking at all 50 states and getting the forecast for every day, for every state. But they're not independent events, right? You don't really have that. So we can't really evaluate the calibration. That's why it said your forecast can fail miserably, but it can't succeed brilliantly in the same way. That is, if my interval is this, and the actual election outcome is way out here, then I failed miserably, right? So if my 90% interval for Biden's vote share was something like 51.5% to 57.5%, standard of 54, something like that. And the truth was like 52, 52.5% the national level. So let's like, that's the way it goes, right? If that had been our interval and Biden had only got 49% of the national vote, or if he got 60% of the national vote, then you could say we were way off. So there's this motivation to always make your interval a little bit wider just in case. But that's okay, you should just be aware of that. The complicated thing where the math comes in is that to make, oh, I should say we did everything in R and stand. So that's why this is the R conference. We used R and we used stand. It's, the way a math comes in here, it's tricky, is that we don't just forecast the national popular vote. We forecast the state votes. Now I keep saying if you forecast the national popular vote and then you assume that the relative positions of the states are known, you're going to get like almost all the way there. But people want the separate state forecast. So that's fine. You combine, you can think of having 50 state forecasts, which you aggregate to get a national popular vote forecast and to get an electoral vote forecast. And we noticed something, which was that we knew we wanted a certain width to the national forecast. Like we had a sense historically of how far things could be off. Like we would not want our forecast to have been 54.5% of vote plus or minus 1%. That would have been too narrow. And so we also can get, we have inferences for the states from state polls. And if you assume that if you if you have 50 different states and you assume the state errors are independent of each other or close to independent of each other, then when you have reasonable state uncertainties, then your national uncertainty is way too narrow. So in order, if you don't want an unreasonably low national uncertainty, you have to either make your state uncertainties very wide, where you have to put high correlations between the state uncertainties. And a high correlation means that you just that if Biden did better than expected in one state, you expected to better than expected in other states. And we found from that was interesting. So we knew this because we had worked really hard and we did it wrong at first. We had to change our model because our intervals were too narrow and we realized that our correlations were too low. And they didn't truly affect the representer uncertainty. But then we did notice the forecasts on the 538 website and the national the state forecasts were way too wide. So they had Joe Biden having a 6% chance of winning South Dakota, which was really never going to happen. And they had Biden having it and they had Trump having a chance to win New Jersey and California and all sorts of other things. And we realized what had happened was they because their correlations between states were too low, they had to make the state errors really large in order to get a reasonable national error. And like the way I'm describing it, I hope it all makes sense. And it all comes out right. But when you're actually building it with all these moving parts, it's not so clear. And I think what happened to them was they kept adding new features to their model and they ended up with this wide uncertainty, which didn't make sense. But then like, so what if Trump is if Biden has a 6% chance of winning South Dakota, he's not going to win it anyway. Like it's it's so what I want to say to you, I want to end a message to you because I think that you and your work, I should have said this at the beginning. So pretend I said this at the beginning. All of you are forecasters, all statisticians or forecasters, even if you don't think that's your job, it is. Even if you're only doing exploratory data analysis, people are doing it because they're interested in what's going to happen next. Even if you think you're doing causal inference, causal inference is a forecast for what could happen, give them what treatments are occurring, right? Whatever you're doing, you're doing forecasts. And it's very hard to get all the dimensions of a forecast right. If I'm only forecasting the national election, that's kind of easy. I mean, it's not easy to get it precise. I have a big standard error, but I just say that's all I can do. But when you're doing a complicated forecast, like if it was a business and you were forecasting sales in many different divisions, or if you're a government agency, maybe you're forecasting recidivism among many different groups of people, maybe you're forecasting uptake to a new government program, you're forecasting something is multi-dimensional. It's very hard to get that all working at once. I think Bayesian inference is the way to do it, but it still depends on the model. And whatever you try first isn't going to work. Wherever you try second isn't going to work. Then you're going to find that the forecast is wrong in the dimension that you care about, and you have to fix other parts of the forecast. And then you do that and it makes other things not work. This is kind of what happened to us, what happened with 538. What I want to say to you to conclude is I want you to embrace that process. I want you to take your forecast and look at all the weird things. Look at the weird corners of the map and all the places where you made a map and there's a discontinuity across states and it shouldn't be. And that's because one of your state level predictors is wrong. Whenever it is, when you see a problem, you can't necessarily fix it because you're under a time constraint. So when you write your report, you should say, this is my forecast and I stand by it, but no, I don't really think Biden has a 6% chance of winning South Dakota. We just haven't fixed that part yet. Be open, embrace it because that's how you're going to learn. If you accept, this is like you are a sinner. When you accept that you are a sinner, then you can do better. If you come in thinking your forecast is going to be perfect, you're going to mess up. But once you realize it's going to be wrong and you can admit it, then you can learn from the places that it was wrong. And that's what we have to do. And part of that is going beyond it worked or it didn't work. How do we learn things with the things that have a problem is because first we're willing to recognize there are places where our forecast messed up, but we're not just saying, oh, yeah, it all sucks. There were some things that worked and some didn't. And I would like you to think of this talk as a model for how you can proceed in the problems that you work on. And that is all. Thank you. Another classic no slides talk from Angel Gelman. I hope everyone enjoyed that as much as they have enjoyed them in previous years. It was a good barn burner. It didn't get much controversy out of that one. We have to get some in a post conference interview. So that brings us up to our first."}, {"Year": 2020, "Speaker": "Marck Vaisman", "Title": "Processing LIDAR images for Forest Preservation", "Abstract": "This talk will showcase how the USDA Forest Service is using LIDAR data to support large-scale forest management operations, conservation, and landscape-level ecosystem restoration. Marck provides a quick introduction to LIDAR and its benefits and using the lidR package to process images, and how using cloud technologies accelerates the process.", "VideoURL": "https://www.youtube.com/watch?v=tTdMlEQEca8", "id0": "2020_09", "transcript": "He's actually one of the early, I guess, progenitors of the data community DC. He's one of the people he saw what was going on in New York, and he said, you know what, DC can use that. He helped build an amazing community in DC, which we are very proud to partner with to do this conference. They are, you know, a bunch of really good people. And we would like to give a big warm welcome to Mark Weisman as he comes up, one of the leaders of the DC community, to give us his next talk. Please welcome Mark. Stand up, show us your shirt. Yeah. Oh, yeah. Hey, there we go. Yeah, we're 20. And we even both have the same hoodie on too. My hoodie's on the floor over there. All right, have a good talk. Thanks. My talk today is about a bunch of collaborative work that I've been a part of over the last year. And I'm really, really excited to talk about this. This is a joint effort, joint work between an academic government and industry collaboration. So several folks are involved in this. First of course is the US Forest Service, which is part of the USDA Department of Agriculture. The second kind of, and they're all related. The second part is Florida A&M University and CSER, which is the Center for Spatial Ecology and Restoration. And of course, Microsoft. So they're my customer. The US Forest Service is my customer. As you know, I work for Microsoft. I'll get that in a second. But this is basically a joint effort between FAMU, Florida A&MU, created the center, which is a joint effort between the academic and government. And when I was going to give the talk, I told Jared, I said, I can talk about this topic about using lighter images for forest management by any task permission. Because the work really is, I am part involved in this, but the work is really not my work is really the work of the CSER folks whom I'll acknowledge in a second. And they gave me permission to talk about this. So I'm really excited. Thank you again. And again, I'm really excited. So a big shout out to CSER at FAMU, so the Center for Spatial and Ecological Research, to Jason Drake, to Paul Medley, to Joe St. Peter. He's probably online listening to the conference, so Pedro and Jordan Vernon. So I've been working with this team for the last year. And they're doing some really, really cool work. A lot of it in R. This is essentially what I'm going to talk about. So I'm going to talk about, just give you a quick intro to LIDAR. So I didn't know much about LIDAR before working with these folks. I learned a little bit more in preparation for this talk. I'm going to tell you, I'm going to show you all the time, I'm just going to show you what they're doing and really, really cool stuff. How they're using these LIDAR images for really interesting analysis and for management, conservation, preservation, and all these sorts of other things. I'm going to talk briefly about the LIDAR package, which is what they're using to process these LIDAR images. And then I'm also going to touch on a little bit on how we took a manual workflow that they had, moved it to the cloud, and accelerated that as well. So, oh, wait. Oh, and I'm going to show you some cool pictures as well. So a little bit about me. For those that know me, you know that I've been, as Jared said, I've been part of the R community and the Descience community here in the DCR for a long time. Started data community DC, ran the meetups, not as involved as today. I still live being a part of the conference and I wish, like, a step to be in person because I really, really live this community. I work for Microsoft and part of the Microsoft Federal team. I work with federal customers, helping them migrate their work into the cloud. My specialty is in data science, machine learning, artificial intelligence, R and Python, of course, all the open source, but really, you know, helping our customers move, use our platform to do their work. You know that I'm an R fanatic, right? Otherwise, I would probably wouldn't be here. So what I'm going to talk about today, again, the work itself is Caesar's work. The opinions are mine, mine alone, not necessarily representative of anyone that I'm affiliated with. The other thing I wanted to say is for those that actually don't know, another fun fact is I actually didn't, when I started running the meetup group 10 years ago, I didn't know R and today, quite earlier today, actually, I saw a tweet in the, I saw a thread in Twitter about people talking about why they had to learn R or what was the first reason. So for me, it was a GG plot, you know, just really, that was like the first thing I actually learned. The first question was when I started running the meetup, I was really interested in R in the community, but watching other people do really cool work with R, like really motivated me to learn it as well. So that was 10 years ago, of course, a lot has happened in between. Okay. So what is LIDAR? LIDAR is similar to sonar and radar. We all know these terms, right? Basically it's the sense that there's a signal that gets emitted, gets received, and it gets processed. So, you know, we know sonar, right? You know, like the bleeps and the cleeps, space balls reference here, or hunt for a vector, but like, you know, uses sound waves. And then radar, of course, uses radio waves, right? It emits the wave, it receives it back, and it does the processing. You kind of get a sense to what's out there. Well, guess what LIDAR uses? Anyone care to guess? Actually, I'm not watching the chat, but let me see. Uh, stage. Anyone care to guess what LIDAR uses? Oh, sorry. Drum roll. Laser. Yes, I'm being able to look goofball right now. No, but right. So LIDAR uses, yeah, so it's light, light, basically laser, which is a form of light, right? So LIDAR, it's, LIDAR means light detection and ranging. So basically what LIDAR has many applications, there's a lot of it out there, but in the context of what I'm going to talk about today, essentially is about collecting aerial LIDAR images from LIDAR units that are flown on planes. So these are planes or drones that fly over areas. So there's the ray of light is sweeping left and right, right? And it's bouncing the light off and collecting data and so on and so forth, so forth. So that's what LIDAR is. And that is the source of the data that I'm going to show you today. So the LIDAR uses electromagnetic spectrum. It uses the green light or the near infrared. And the reason why is because those are the ones that seem to reflect the best from vegetation. And what you usually see out in the field, right, at least LIDAR. Now LIDAR is also used, for example, in self-driving cars and a lot of new technology, right? And it's the same principle. It's just that it's mounted on a different thing. It could be mounted on your car. I actually think that a lot of the cars that have a lot of the safety things or the self-breaking. I don't know what technology they use, but I'm pretty sure that a lot of self-driving cars use LIDAR. Probably some of the commercial cars, like the Subaru's that have the two lenses, it's probably using LIDAR. I'm not sure, but anyway. But the point is that you can either emit LIDAR from the ground, emit it from satellites, or from planes, which is probably the most common thing. So what makes up the LIDAR device or the LIDAR system? It's the LIDAR unit itself, which is attached to the bottom of an aircraft or a drone, right? And it's as the plane is flying, it's scanning, it's emitting a array of light that's going left to right. There's the GPS, right, which pinpoints the position. It helps also understand not just the XY, but also the elevation using GPS locations. And then third, it also uses the inertial measurement unit. Because what happens is, as the plane flies, it moves. Also the light, because it's a ray that's going back and forth, left to right, there's an angle. So you need to know the angle of the plane, you need to know the angle of the light beam. So there's a lot of, not just the data that gets collected back, but there's also a lot of metadata that gets processed to create all of these images. And what happens when the laser beam is going left to right, it's hitting a lot of light on objects. But in the case of vegetation and probably other objects as well, it penetrates. So you actually get many responses at the same point. So when I talk about the images a little bit, I'm going to talk about the response or multiple readings. And what you get looks something like this. So when you get all that data and it's post-processed, so there's also a computer and software that takes all the stuff and creates these images, what you get out of the LADAR system, it's what's called a point cloud. So it's essentially a set of 3D images that is basically essentially a multi-dimensional array where there's many different dimensions for, of course, the response, the xy location, the measurements, like all of these things get packaged into this file type called point cloud, which is of the data file itself is of type LAS. So that's the, I don't know what it actually lasts down so far. I don't think it starts for anything, but it's a last file. So it's a point cloud and it's a last file. It's a standard. It's done by the American Society of Programatory and Remote Sensing. There's a GitHub link over here. This is just a part of the specification. This was updated fairly recently. Okay. So that's really what LADAR is. And those are the images that the folks at Cesar and other agencies are working with. Right. LADAR is actually, it is readily available. So from what I understand is there is actually a lot of LADAR data out there and a lot of governments, state local governments spend a lot of money collecting this data for other purposes. So and just to give you an idea, right? And these statistics were provided to me. So in 2019 and 2020, the state of Florida spent about, was it $23 million for about 38,000 square miles? So roughly about $600 per square mile to collect. There's another one here, no statistic, which is about $200 per square mile. But they actually, the point is that they fly these drones or aircraft and they collect the LADAR data for other reasons, usually for the purposes of what create and what are called digital elevation models. But the ancillary use is what I'm actually going to talk about. Right. So the folks at Cesar and other teams have figured out that they're using this data that has been collected for other purposes, for the purposes of forest management. Like I said, they are leveraging this data that has been already collected and get additional learnings about this. And one of the things that they're using is to calculate structural characteristics of forests, things like forest, and canopy cover, which when you get that, it's a strong correlation to things about a forest, about the amount of timber, the biomass, which is the amount of organic material that's there, the habitat quality. So they can use these measurements and also layer it with other data. I'm not going to talk about it, but you can also layer this with satellite data and these other things. But ultimately, it's about understanding the metrics about making decisions. And what they're doing with this, which is really cool, right, is they're building hydrology models. They're building forest inventories. So they're trying to understand the evolution of forests over time. They're trying to make strategic decisions, especially as it relates to event response, natural event response of using the data in pre and post hurricane image forest analysis. For example, identify areas in the forest for maintenance and restoration and or preservation. So there's a lot of different things that you can do with the product of these datasets. And ultimately, what happens right when you take these datasets and you leverage open source technologies and tools like our, for example, machine learning and the cloud, it gives you better accuracy for a much less cost than doing this manually. Before these lighter data, people have to go out into the field and collect this data manually so you can imagine you can't really scale a person to cover a broad area, whereas in flights, you can cover a pretty broad area in a short amount of time. So there of course, there are trade us, but ultimately the long term idea is to really leverage this data, fully integrate this into many processes and share this data across agencies, et cetera, to really build a full blown end to end process. So this is how you don't collect lighter data. One second. I'm sorry, Joe. So this was last year and I think it was October. I was down at the Apple Atchakola National Forest. So the folks that sees are a family, they're based out of Tallahassee. And we went out into the field. They were doing some testing of these fixed wind drones, testing multiple vendors. This one, I believe, is a sense fly. It's a fixed wind drone. That one actually did not have a LIDAR sensor in it, but I just thought it was fun to show anyway. But they actually do. There is a LIDAR sensor. It's not as sophisticated. The images I'm going to show you come from different devices, but nonetheless, I wanted to show you that. So I told you I was going to show you some cool pictures. So we're going to look at some what we're called point cloud files. So this is what a point cloud looks like. So this is an image that's coming from the Gator-I system. And the Gator-I system is a really high definition, high density LIDAR system. It gives you about 5,000 points per square meter, which is pretty dense. Whereas other systems can give you anywhere from 4 to 8. It varies. But this one is really ultra-ultra-high definition. Of course, the higher the definition, the bigger the file, the higher the definition, the shorter the or the smaller the area you can cover on a given point for the detail. So there's different levels of LIDAR. There's sort of lower resolution, bigger area, higher resolution, lower area, sort of everywhere in between. So this is an example. I believe that this is part of the Apple. This is Western Apple at Chacola. And these images were taken right after Hurricane Michael in 2018, if I'm not mistaken. So you can't really see it here. These are just a couple of different examples. So this is actually what happens. You see all these different points. And that's all the data that gets collected. As the beam gets thrown down, you get a response, and all this data gets processed. This is what's called the point cloud. And this is what is used sort of as raw data for downstream analysis, for building other products that they're using for doing other kinds of analytics. This one, in this image in particular, you can sort of see a little bit of an area in the blue, which is the air closer to the ground that's less populated. That's kind of an area where trees fell. So this was a flight path right after the Hurricane Michael, which was in the panhandle area. And this flight path was near the area where the hurricane went through. So just to give you an idea. So these are a point cloud files. This is a TIFF file. So what ends up happening is once you collect all this data, you create our... There's multiple levels here, but you can extract a lot of layers from the point cloud and also create a little bit, create TIFF files, which are used downstream. But these are multi-layer TIFFs. So if you open a multi-layer TIFF in a typical TIFF viewer, on your computer, whatever image you have, because it's multiple layer, you're not seeing anything. So here we're actually seeing as the flight path. And if I come here and I take my mouse and I kind of hover it over this... I guess you can see it. But essentially what you see here is the whole flight path and you see a wide strip, because that's the laser beam that's panning left to right as the plane is moving. So this is sort of that circular flight. And the images that we saw that I showed you earlier were actually a part of this flight. But this is looking at the entirety of the image projected until a single plane without any color, because it's just a bunch of data projected in a single plane. Well, how do these get used? So they work on creating what are called raster products from these lighter point clouds. And when we say raster, it basically means a grid. So it raster in a sense is two dimensional, but it's partitioned by grid. And those are the kinds of images that are really used downstream to produce further analysis. So I'm going to show you some of the inputs and outputs that are created. So for example, so this is a subset of that bigger flight that I just showed you. So this is just zeroing on a really small area, just for the sake of illustration, but just because also the image sizes are much smaller. And I can actually show you what much of these things look like. So this is an aerial photo. This just comes from satellite from the Esri platform, which is a GIS platform. So when you overlay the 2D point, this is just the raw 2D point cloud, but it's overlaid on the aerial image just so you get a sense as to where things are. This is the 3D point cloud of that same area. Of course, it's not look it's it's see it's the point of view is not from directly from the top, it's sort of coming from the side. But ultimately, again, these are point clouds, but then we start getting into what are called the raster. So this is where you're actually taking the from from the point cloud data, you can filter different bands and look at specific parts of this. So this is looking at the canopy at the canopy height, right? It's kind of like tree height or not. So a blue is more open canopy. Red is denser canopy. And if we go back to the satellite, right, you see that blue, right? There's in this area over here or kind of towards the bottom middle, there's less trees. You see that it's blue because it's open. The next one, sorry, that's three of them. So that's canopy height. This is canopy cover. So this is the height of the trees. This is actually the cover of the trees. And you can see that the reds are more trees, more leaves, blue is less leaves and so on and so forth, right? This is one version of it. This is canopy cover. This is another canopy cover here. Actually, this is shrubs. This is shrub cover, sorry, I mislabeled this light. And blue is no or low shrub cover, orange or red means it's dense. And then here, we're actually combining all of these different layers into a single image. So the green is canopy cover. The blue is the shrub density. So shrubs meaning lower trees, right? And the red is the canopy height. So the really tall tree is kind of where the things are. So here you actually get a multi-dimensional layer, like in a grid, right? This is a two-dimensional projection in a grid because every grid is, this is resolution of one meter, I believe, one square meter. So there are metrics or calculations that are done per grid, per grid cell to produce these images, right? But this basically essentially is a heat map for a lack of a better term. This is a sweep, right? This is kind of a sliced view of the longitudinal or lateral sweep. So it's the same image. We're just looking at it from here. So you can see that you can do, again, I'm not an ecologist or an arborist or something like that. They're actually doing a lot of, and cleaning a lot of information, actual information from these images. They're using the lidar package for that. So I'm just going to show you a couple of functions. Basically what you do with the lidar. So lidar is a package. It's an active development. It seems to be the best tool out there for doing this kind of stuff in our, particularly, but also they tried a bunch of different workflows and it seemed that this was the best. So below the library, the library does use Raster. It uses SP. You need G-doll as well for certain things, not for what I'm showing you here today. But the last file can be either part of a catalog or it can be a single file. So you can read in the catalog, which is just really in the metadata. Remember, each of these files is really big. So the big file, the GatorEye file, that's about the one I showed you. That was about eight gigabytes. The other one, the subset was less than about 500 megs. These are pretty large files. So when you can do a catalog and a catalog is basically a grid of images or a single image. But again, so here we're reading in a single image. You can see that that image covers about 4.0 square kilometers. There's 274 million data points in that particular last file. But then we can subset that. You can clip it, give it coordinates and clip it and subset it. And here we're looking at a smaller file, which is about 36,000 square meters, much less 7 million data points as opposed to 100 and something, something. So you can plot this directly with methods in R and get some of the images that I showed you and then you can post process this. But ultimately, what they've done is they've created a set of standard metrics, or not entirely custom metrics, that basically reads in the file and produces all these sorts of metrics per layer, produces a list output, and that's what they use to produce the downstream analysis. So I need to wrap up. So we actually migrated this workflow from local workstations that were using data in hard drives, 8 core machines with not a lot of memory into the cloud. So using larger scale machines with more CPUs, more memory. The data's already in the cloud, so the speeds. So we've really accelerated the process for being able to process many, many terabytes of data in the cloud. And that's one of the things that I've actually been working on with the team. So just to kind of wrap up, this really has a really huge impact because they're working on the scalable, repeatable methodology that can be applied over many national forests, used by other government agencies, and also build a workflow that folks that don't have maybe the skills or the computer researchers are going to be able to use and replicate. Like I said, this is the best workflow that they've been able to find in terms of trying a bunch of different things. And the raster images, those 2D images are actually used as inputs for machine learning models. That can be a whole talk on its own. And like I said, it really reduces the time to assess natural disaster impact. Thank you. This talk was actually made with Shearingen. I've been using Shearingen a lot for the last year in building slides for class. I mean, as you know, I teach at Georgetown, I've recommended that. I really like Shearingen. This was built in Shearingen in art market. So thanks a lot, guys. Mark mentioned that we were twinning because we're both wearing DCR shirts, the blue DCR shirts. But you notice he was wearing a hoodie. It was the N-Y-R hoodie. So we were twinning in more than one way. And Mark was supposed to introduce me, but he didn't show up so he didn't get his chance. And he said for his fun fact that I should just take a jab at him. So of course I had to be nice. But Mark's an awesome guy. We love each other. He's doing really great stuff for the community. So our next speaker is..."}, {"Year": 2020, "Speaker": "Wendy Martinez", "Title": "The Rocky Road to Using R at a U.S. Government Agency", "Abstract": "In this presentation, Wendy Martinez will describe some of her experiences (successes and failures) using the open-source statistical computing software R at several U.S. government agencies. By doing this, she hopes to inspire others and to pass along some of the lessons she has earned along the way. R is just one of the statistical computing tools available to us, and she believes data scientists and statisticians should have many computing tools ready to use. However, getting permission to use it in the U.S. federal government has been challenging.", "VideoURL": "https://www.youtube.com/watch?v=t3mePzEeMBw", "id0": "2020_10", "transcript": "Wendy is a former MATLAB user, he even wrote a book about MATLAB, then turned open source. Let's all give a big round of applause to Wendy for coming to the open source side. Everyone please welcome Wendy to the stage. Thank you for that wonderful introduction. Thank you for inviting me to be here today. I'm really happy to be able to share a journey that I've been on with R as my companion. So any journey I think has to have a destination and the destination for... I love you. On your screen, the viewer says stop sharing and hide. Could you click hide so everyone can see all your slides? Oh, hi. Like that? Hi, you're welcome. You're welcome. I wasn't sure if I was hearing things because I just heard this little voice. On your shoulder. Anyway, thank you. All right. So here's my destination essentially is to get our approved for production so we could use it for production purposes at the Bureau of Labor Statistics. So in terms of this talk, I'm gonna talk a little bit more about who I am and what is a federal statistical agency because I think that sets some context for this journey or this road that I'm on. And then I'll talk about some of the rocks and the obstacles that I've encountered along the way and let you know where I have now because I'm not quite at my destination yet. So a little bit about my education. I got my bachelor's degree. I'd had a double major in math and physics from a small liberal arts school in a lot in Oklahoma. From there, I got a research fellowship at this kind of off-campus school, part of the George Washington University, but it was at NASA Langley Research Center and I got my master's in engineering. From there, I went to work at the Naval Surface Warfare Center in Dahlgren, Virginia and they supported my PhD work at George Mason University where I got my PhD in computational sciences and informatics with an emphasis on computational statistics. And about eight years ago, I moved to the Bureau of Labor Statistics and a condition of getting the job was that I had to take some courses on how to work with data from a complex sample design. So I ended up going back to school as a much older student which of course was pretty challenging. So I said I worked for the Naval Surface Warfare Center. There I did basic and applied research in support of the Marine Corps. From there, I went to the Office of Naval Research and that's a funding agency sort of similar to the National Science Foundation, but there was a lot of fun. I wasn't able to do research or analysis or anything, but instead I came up with the research ideas and I gave money to people to have the fun of doing it. After a while, I went to what I call a dark agency still in the Department of Defense where I had the opportunity to re-engage with analyzing data. And then as I said about eight years ago, I left Defense and came to BLS. So here's where BLS is located. This is our building, it's called the Postal Square Building. Right now I'm coming to you, talking to you from my home because we're a full-time telework, but once we get back, this is where I'll be and it's right across from Union Station. And it's actually part of the Smithsonian, that door that you see right there is a, I guess if you can see my cursor, that's the entrance to the Postal Museum. So if you can get there, it's a great place to visit. So some of you may not know much about the statistical agencies and it wasn't until, of course, I came to BLS that I really even knew that they existed, I'm ashamed to say. But anyway, there's 13 of them, one of them is BLS. And you can see from this list that they span a lot of areas. So from economics, labor, crime, transportation, demographics, education, agriculture, and a lot more. Now the reason why this is important and why it impacts this road that I'm on is because each of these agencies are responsible for producing very important official statistics. These statistics are used by the public, by decision makers, lawmakers, really everybody. And so here's just an example of some of the ones that at least a BLS does. And one that you probably are familiar with, which is the monthly employment situation that those are produced by the BLS and also the consumer price index. So the reason why this is important is because by law, we have to produce and publish these statistics. And the production of these statistics relies on our IT infrastructure as well as the software that we use to produce them. So we can't just go to the public and say, oh, I'm sorry, we can't give you the employment situation this month because our software failed or we had some glitch. So we have to be very, very careful. Now a former commissioner of BLS, Dr. Erica Crotian, kind of coined this, created this acronym, which I think is, well really it's at the heart of what we do and how we produce data and the statistics that has to be accurate, objective, relevant, timely and accessible. Again, the software that we use to produce our statistics that we need to account for all of these important priorities. So what about this rocky road that I'm on? Well, I mentioned my destination, which was to get our approved for production at BLS, but what was the start of my journey? Well, I was first exposed to our, when I was working on my PhD and when I was working on these slides, I was trying to think if I really had used R back then and I'm pretty sure that I did because this was in the early 90s and around that time, you know, S plus was around so I know I used S plus, but there was also R there. And during my time at the office of Naval Research, I didn't have a chance to really do much research. So, you know, I didn't use R. But then once I got back to analysis and I was at this dark agency, I was sort of jumping in at the deep end of the pool, so to speak. So as you might suspect that, you know, an agency like this that we had, there were classified networks. And so we couldn't just, you know, go to crayon because these networks weren't connected to the outside world. So you couldn't just go to crayon and, you know, install our packages. But one of the statisticians there, what he did was like twice a year, he would go to crayon and essentially download everything off of crayon and get it checked out, make sure it was okay. They would install it on the network. So essentially on our networks, we had crayon mirrors where we could, you know, install packages and R and use it. The other thing I did when I was at this agency was I developed a two day intro to R course. And I'm not quite sure why I did it, but anyway, I did it and taught it several times there. And it was, you know, people liked it. The other thing that I came to appreciate was that they would set up these communities of practice. So these would be groups of, you know, colleagues that would come together and talk, you know, they'd have this common interest and, you know, they would help each other out. So I kind of took those ideas with me to BLS and I kind of revised the course and taught it. I've been teaching it at BLS probably two or three times a year and I think it's, you know, probably the most popular course there. And the other thing I did was I started a, our interest, our users group at the agency. And I have to say I didn't ask permission to do that. I just kind of did it. And it's been fairly, it's been very successful and it's still going very strong. The other thing that's important, you know, about this journey that I'm on is the fact that SAS, I mean, BLS uses SAS. Not totally, but it uses it in particular for the employment situation to get those numbers out. And many of the statistical agencies are, you know, use SAS. Now you could, as you might expect, you know, this is government, we can't just, you know, install whatever we want on our computers. And I have to say that BLS is pretty open to us, you know, installing things on our computers as long as we ask for permission and they go through this kind of a security check and an approval process. But the approval distinguishes between whether the software is going to be used for research or production. And so one of the problems has been, what does it mean to be production software? And nobody really had a good definition about it, it's pretty vague. But essentially, in my opinion, it would be whatever software you use to produce an official statistics. So here's a graphic that kind of shows the approval history for R and the green is the, so this starts from 2015 to current time. And the green means it was approved for production and the blue means it's approved for research. So you might be saying to yourself, well, Wendy, what's going on? You've already reached your destination, you know, it's been approved for production. Well, as you could see, it's not right now. And what's happened along the way is that it gets approved for production, I'm all excited about it. And then somebody says, oh, wait a minute, we can't have that. And so then it gets kind of kicked back to being for research purposes only. So I'm still not at my destination. Now my colleague Brandon Cobb developed this graphic as well as the previous one to show kind of what's the status of software at BLS for production use and research. And then by, you know, what's the software used for? And this first column, so the production is on across the top and green. And if you look under the languages that are approved for production, you have Python. And if you look all the way to the right, under the other block you see Anaconda and QGIS, those of course are all open source. And one of the issues or comments we've had about why R is not approved for production is well, we need to be able to just call somebody up if we have a problem. That's why we like to have commercial software, commercially supported software. So here we have examples where open source is used for production. So why use R? Well, this is predominantly in our conference, so I'm sure we all love R and we know why we want to use it. But here's some of the things that we've tried to use as a selling point. I think the fact that functionality or new research is often implemented in R and then it doesn't become available in commercial software for a long time, if ever. When we hire people right out of school, they usually know R or Python, but they don't know SAS, you have to have to train them. And I think there's this opportunity for innovation when we use R. And I'm going to give you just a couple of quick examples. So there's a survey we have called the Occupational Employment Statistics Survey. It publishes annually statistics on employment in wages for over 800 occupations and I think various industries and for different geographic levels. Now the maps that they produce, I'm going to show you, they're static and they're produced in using SAS. So here's an example of the user interface. You have these drop down menus and you could select the occupation and then what statistic you want. And essentially what they do is they use SAS to create a map for every combination of statistic and occupation and so on. And so then these are just kept on a server somewhere. And essentially these interfaces, these drop down menus are just used to access what file it is on the server. So it's really not dynamic at all. And then in 2015 and I thought to myself, well this would be a really, I mean we could create a shiny app that would kind of create a version of that that would be truly dynamic. So that's what we did. We had two economists, so these weren't even statisticians or computer programmers. So they had little to know our experience and almost zero shiny experience. And we asked them to do this, but the thing is that they knew the data. And so just for two days a week, and I think within two months they had an app built that would kind of do this dynamically. So here's some screenshots. The legend would change and the title and you could change I think color and there is you could hover over the areas and get information. The other thing is they would also create this table based on the data that you had displayed in the map and you could then download the data to use yourself. Another brand and cop actually did this one too. He created this app for our publications department. They wanted to sort of automatically generate text that could be used in new statements because a lot of times it's not necessarily boilerplate, but it could be somewhat similar and then you would just kind of put the right numbers in there. So in order to demonstrate what he could do, what he was doing, he said, well, let me just put this in our shiny app and he could show it to stakeholders so they could see what are the possibilities. So the point about these examples is the fact that these weren't created by our IT people. They were created by the subject domain experts. And that's kind of where I think innovation happens. Now I love this sign because this really I think illustrates what I feel my road has been like these past several years. So just some of the issues that we've encountered, first of all, one is that people do still want to use R for research, so they want to be able to just install whatever packages they want, whereas on the production side, they say, well, we have to have more control over the versions and so forth. There's also this idea of SAS versus R. When I started this, I think that people thought, especially in the IT department, but if we get R approved for production, that means we're saying we're getting rid of SAS and we have to convert everything that's in SAS to R. And nobody's ever said that because that would be a huge undertaking, right? Remember, our monthly employment numbers, they have to go out. And nobody's saying that. We're just saying, where it makes sense to use R, let us do it. And then the other one, the other sort of pushback that we often get is, well, you have people in the, you know, the subject domain experts developing the R shiny apps, what happens when they leave, you know, what are you gonna do with the updates or fixing it or whatever at that point? Well, you have that same problem with, you know, the Java programmers in the IT department, so there's gotta be ways that we could, you know, kind of mitigate that risk. So what happened about, oh gosh, I don't even know how long ago now, but maybe like two or three years ago, I was ready to get our approved for production. The executives said, no, we just need to go back and do a process to approve software in general for production. So we did that and presented it to the executives a year ago and they just, you know, massive pushback and they said, all right. So at that point, I was kind of ready to just quit, but somebody, our deputy commissioner said, well, no, let's just get a smaller team. We'll take this process that you developed and fill it out for R. So we did that. And that was a good exercise because we kind of revised the process and then instead of going back to the executives and saying, okay, give us the approval, we said, well, let's just have a little smaller approval team that has the authority to approve it and see what they think of our application. So it's in their hands right now. We're trying to, we want a quick turnaround. We don't want this team to be taking six months to give us an answer. So we're hoping to have an answer in, well, about a month. So we'll see what happens. So what have I learned in this process? Well, one of them is just be patient, you know, keep trying. I think as we've gone on, one of the folks in the IT department where, you know, five years ago, he was sort of pushed back with us. He's now completely on board and very supportive. And so I think you just have to, you kind of plant a lot of seeds and nurture them and you'll get there. So I'm hopeful. And I just want to finish with a plug for, you know, an advertisement that several years ago I started an interagency R users group. And we're just kind of informal. And if you're interested in, you know, R used in the government, then just send me an email and I could add you to our list. Great. It definitely seems like a bumpy road. And that's something I've seen at many, many federal agencies too. And some move at different paces depends, you know, what their mission is and who's there. So thank you for fighting the good fight for R. Especially someone that used to be a MATLAB user and come over to the open source. It's really amazing to hear."}, {"Year": 2020, "Speaker": "William Doane", "Title": "Building a Data Analytics Team at a Federally Funded Research and Development Center", "Abstract": "Building a data analytics team in any context can be challenging, especially given the rapid pace of new tools and methods, the compute resources required, and the varied backgrounds of team members. Building a team within a federally funded research and development center poses additional constraints and opportunities. This talk will highlight some of the technical issues that arise which then translate into challenges for analytics teams as they collaborate to bring value to research sponsors.", "VideoURL": "https://www.youtube.com/watch?v=K6b5G-iMjfQ", "id0": "2020_11", "transcript": "Assuming you could trust anything stored in a database, a little disconcerting at a data conference, but I get the sentiment, his family tree can be traced back as far as 600 AD. So I think we've got to find out where it's going at 600 AD, how much of his family trees, like one little line is like a whole big forest of trees. And I want to know where were you at 600 AD? What continent, what country, what were you doing? I'm very fascinated by this. So everyone, please welcome to the stage Will. Well, thank you, Jared. If anyone read Robert Heinlein novels at any point, my family always felt to me like the Howard Family Foundation, but in a good non-suicidal sort of way. There exists a Don Family Foundation, currently housed at Don University in Nebraska and their job for the past few hundred years has been to record who be got whom. So they have very good records, going back to pre-Plinov Colony and to our European origins. My name is Will Dohn. I am a research staff member at the Science and Technology Policy Institute, which normally I say most people have never heard of, but in DC you may well have heard of us. We are located in downtown Washington, DC, opposite corner from the Eisenhower Executive Office Building. We are a federally funded research and development center. I'll say a little bit more about what that means in a moment. We were established by the United States Congress back in the 1990s and are currently managed by the Institute for Defense and LSEs, IDA, previously we were managed by Rand up until around the turn of the century. Our task, our charter is to support the White House Office of Science and Technology Policy and the President's Council of Advisors on Science and Technology or PCAST as it's known. We also take research requests sponsored by other US federal executive branch agencies or offices or institutions, whatever the designation might happen to be, but critically we are not government employees, which puts us in a bit of a bind. It gives us many of the same constraints that BLS has and other agencies have with adopting tools, but the challenge that our email address isn't.gov. So where there are solutions where a.gov email address would get you out of trouble, we're not privy to those, we are a.org. Federally funded research and development centers, if you're not familiar with them, are a category in US code. They are intended to provide long-term support for research and development that is in some way beyond what normal contracting might get you. Most people know several FFRDCs, but may not be aware that that's what the organizations are. You've probably heard of the Jet Propulsion Laboratory, Los Alamos National Labs. Those are both FFRDCs as well. In our case, we're supporting the White House and their cases, NASA and Department of Energy. Critically, we get access to both sensitive and proprietary data. So we straddle government and private sector. We have access, we have security clearances from DoD, where granted access to government information that's for official government use only or classified, but we also get access to corporate data where that would be useful to our analyses. Information that government agencies might not normally have access to. There are a number of up... There's on tasks that often demand quick turnaround, sometimes on the order of hours, but many of our tasks run as long as a year or two years as well. We're working on topics where the answer probably isn't known. If it were known, we would not be the people to come to. We're working on cutting-edge issues where the landscape is still uncertain and we're helping agencies to understand what that landscape and what the potential for science and technology investment might be. Because we are working closely with the federal government, that imposes on us some constraints on the tools that we can use in line with what was being described previously, the national security and the data privacy concerns all come into play for us even though we're not.gov. And because of the kinds of rapid turnarounds that we're often asked to do, we need to be able to set up and tear down in our data analysis infrastructure fairly quickly sometimes. We're often sandboxed. The systems we're working on might even be air-gapped, meaning there are no network connections available to those machines, period. And so it's whatever we can do on the box that we have or on the isolated network that we have. So imagine a situation where you've been issued a new computer, you're not the administrative user on that computer, you have no network access and you're left trying to figure out how to establish your data analysis environment and how to collaborate with other people who are working on the analysis with you. That's the situation we're in often on a daily basis. That begs a range of questions that are really valuable to have answered but are often not well documented in the packages that are available on CRAN or on GitHub, licensing terms. Are your packages appropriately licenseable by a not-for-profit organization that happens to be working with government? Am I going to be able to install, configure and use those tools? Or do you have dependencies in the installation or the configuration that require network access? That can be a problem for us. Each new tool involves some technical debt that we're acquiring. We have to be really aware of those things. We have to be sensitive to the debts that we're taking on so that we don't build a stack that is more like Jenga than a tower of Hanoi. Are the tools documenting their data security handling? Do you make it clear and transparent when data is being transmitted out over a network connection? Do you attend to data privacy concerns and are you addressing sort of section 508 accessibility issues with the tools that you're creating? And critically for me, how are the results supposed to be communicated at the end? We'll say a little bit more about each of these as I go here. So again, thinking about the licensing, think about a lot of the tools that you use, particularly the ones that need to be purchased. We're not government, so we don't get the benefit of government contracting, but we are paid by the government and so we're making purchases with federal dollars. Most of our purchases are subject to the federal acquisition regulations. We need to be able to certify the data security and the accessibility compliance of the tools that we're using, particularly if the tools are being used to produce a data dashboard that's either going to be made public or it's going to be made available within a government agency. So many things these days have gone to subscription, software subscription models, that's a poor fit for us since we're working on research contracts. The contract that drives us to choose a particular tool probably won't be in existence six months or a year from now. And so when that comes due for renewal, we're not in a position to charge to that task any longer. We're not in a position to continue to support the license of that tool. And the reality is that if we're going to adopt a tool, if we're going to accept the technical debt, then we probably also want to be able to use the tool across multiple tasks, which for us means multiple government sponsors. It just, it makes licensing very complicated. And think about all of those tools that you use where the mechanism is to go to the website, log on to the page, put in your credit card number and pay for it. As an end user within the organization, that's not an option for me. I need to collaborate with our finance office to go through that kind of a process. Oh, but the signup will be very simple. We'll just send you an SMS message. That can be challenging. So some things that we take into account whenever we look at a new tool, we are in our shop and we are reliant on our studio and not just the tightyverse, although I've used just tightyverse hexes here, but our users may not be full administrative users, even if they are quote unquote administrative users under the normal concerns, there may be some specific Windows permissions or user group settings that are customized for the national security computing environment. As end users, we're not authorized to sign NDAs or end user license agreements on behalf of the organization. We have to run those through the corporate counsel's office. So do you provide an easy way for us to get those documents out of the software so that they can be forwarded to corporate counsel? And I'm not trying to call corporate counsel and read them the licensing terms over the phone. We're not authorized to act as government employees. So if you have particular set of sides that allow the government to use your tools, do they apply to FFRDCs? Something worth considering, I hope. We're generally prevented from using anything that depends on the cloud. So if I see a tool, a package or application that requires cloud use, that generally disqualifies it from use, not absolutely, but almost always. If you have a runtime requirement that the tool call home in order to register a license or verify authorization to use or retrieve updates, that's going to be a problem. If you have just in time dependency installation as so many tools do, right? You run a new command and that command says, oh, I also need this other package. The fact that we might not find out about that for days or months and that once we do find out about it, we're in an isolated air gap environment. It makes it very difficult for us to adopt these tools. So I'm coming at every new tool with a very critical eye. Much like BLS, we need to get approval for the specific versions of the tools and all of its dependencies at the time that we're going to install those tools. We have very limited ability to update those tools. Certainly can't do it on demand. You've probably run into a situation where you've discovered a post on Stack Overflow or wherever. And the suggested solution is simply to disable security. That's a bug for us. That's not a fix. That's not a solution that we can adopt. And more and more questions about accessibility are requirements, not optional considerations. So does your tool provide color blind safe, color palettes? And do you provide descriptions for the graphics that you're creating? How easy do you make it to integrate accessibility concerns into the use of your tool? This is my, there'll be dragons slide. Then magic happens. We've gotten through the setup. We've managed to get through all of the licensing. We've gotten through the configuration. We have the packages installed that we want to work with. Then you do your data science. You conduct your analysis. And at the far end of that, you have results that you want to convey in some way. At least during this talk, I'm not worried about the analysis. I'm worried about the before and the after. So what are the typical mechanisms these days by which results are being communicated? Well, you can publish it to the web or to the cloud in the ways that that's different from the web. You can push a repo to GitHub. Maybe you can generate HTML and JavaScript files that you can email to someone or set upload to a server, or you can produce an executable file or maybe a zip file that can be shared, except none of these are workable solutions in my environment in FFRDC. The first three, it's not appropriate for the kinds of government data that we're working with. When we're dealing with controlled unclassified information, CUI used to be called FOUO for official use only, or when you're dealing with classified information that's at the secret or the top secret or above levels, those systems are isolated. You're not going to have access to those cloud services. And HTML files, executable files, compressed files are all too likely attack vectors. And so delivery to sponsors is usually blocked for those things. You can't unless I put it on a physical media and sneaker net it over to a sponsor's office. I don't usually have the ability to bring those kinds of things into their environment. And even if I can bring them into their environment, they may not have the ability to plug in a USB stick. USB ports may be turned off. They may not have the ability to launch an executable that hasn't been verified by security and so on and so on. And so what are the typical delivery formats? Well, I hate to say it, but Microsoft Word, Excel, PowerPoint, with no macros and no VBA active in them are typical, PNGs for static images and PDF files, again with no macros involved. Or we can stand up on premises devices or services. So we can't use GitHub, but I can license the enterprise version of Bitbucket and stand up a Bitbucket server within our firewall. So just to check your thinking on this, these are some pretty common things that one does when you're setting up a new system with R. You want to install R. Well, that requires network access to download it. You want to install RStudio requires network access. And keenly you have to be an administrative user to install RStudio on Windows systems. You want to install the tightyverse again, network access. And if any of the packages involved in the tightyverse are most, the most recent version of them is in source code on CRAN, a sorry on GitHub. You may need to compile those tools from source. That requires installation of R tools, which requires that you have network access and be an administrative user. And if you've gotten past all of that, you've gotten all of those things installed and you're in RStudio, the first time you click that compose notebook button or the knit document button, you need runtime access to download the latest version of R Markdown. And you need access to download a version of tech to be able to produce PDFs or Word documents. Maybe as we saw earlier in the day, you'd like to use FUR or future packages for parallelization, but those trigger network access. And so you have to have, you have to be an admin user to grant firewall access to allow for the parallel computation to happen, which means even if you have that, it means that you can't write standalone self-running scripts that you're going to have to be there to click the authorize button in order to get those parallelized scripts to run. Perhaps you'd like to do some geo work using maps as we saw earlier today. Well, those packages typically require network access to grab the map tiles for the area that you want to plot. And if they're interactive maps with something like Leaflet, you need JavaScript access for interactivity on the plot. Shiny and R-Press, similar sorts of things. You may, because they're deployed using a local loopback, you might need administrative access to grant that network access. And again, runtime access to be able to download JavaScript libraries and don't, things like Shiny IO, GitHub, Amazon Web Services and so on, those are just out of scope because of their cloud nature. We have no way of certifying their security. So a big part of my job in building a data analytics team when I bring in new people is to expose the assumptions that are made about the tools we're using in building. People get very comfortable. We've gotten very used to the idea that we just click a button and it installs a new package on our system and we have a new capability and we're often running and we're not thinking in the moment about whether that package came from a reliable source. We're not thinking about whether that package has underlying dependencies on cloud services and so on. So we need to bring those to the surface, which often means getting people to step back when there's a bright, shiny new version of something that's come out and to think about those sorts of underlying assumptions. We have to manage that, the technical debt that comes along with each of these new tools that we adopt. And ideally, we want to be able to contribute to the free and open source software community, even though we can't necessarily give you a reproducible example or a reproducible environment in which the problems we run into occur. I can't tell you our networking infrastructure. I can only tell you that your package has a problem under certain networking infrastructures and suggest a fix. And given that we're not able to access most commercial cloud services, how do we then conduct reproducible research and communicate our results to our sponsors? So at the end of the day, I hope what you'll do is think about the packages and the tools that you're using and that some of you are developing. Hopefully you'll have an opportunity to inject these into conversations with people who are responsible for building the tools and documenting the tools to think about these kinds of issues. So what assumptions are we making? I come from academia originally, so I tend to think about what are we doing with our training? What are our assumptions when we're training our data science students? That setup is quick and easy and everything can just be done on a server in the cloud is the first assumption I think we often take. What are the assumptions we're making when we're building new tools? What assumptions are we making about our end users and what capabilities and permissions they have on their systems? And what assumptions are we making about the compute environments that our analysis tools need to operate in? Ideally, we would be intentional and explicit about the dependencies that we have and choose between those things that are absolutely necessary for what we're doing and those things that are just nice to have features and somehow factor those out. We will be explicit about the security implications of the tools that we're developing and explicit about the accessibility issues that come along with the tools that we're using. I provide a few resources for anyone who's interested in reading up on either FFRDCs, the complete text of the law that establishes the authority for FFRDCs is provided there. Information on the National Archives and Records Administration and the Presidential Records Act, which are the kinds of things that prevent, for example, acceptance of zip files. By certain administrations, by certain offices. And then a run of accessibility-related websites and articles dating back a few years about the kinds of features in software and in programming languages. It's worth noting here, the third item, our studio has done a phenomenal job the last year, year and a half in bringing accessibility features into our studio. If you are not familiar with those accessibility features, I highly recommend that you take a look at them. Accessibility tends, paying attention to accessibility tends to expose a lot of interesting insight about the communication and visualization and rendering of data in other non-visual formats that can make all of our data products better, I believe. Great. Yes, thank you so much for showing us all the panels and obstacles you have to go through. I really want to pack it in both you and Wendy in there to show the challenges of software and team building in the federal space and different agencies and different ways of getting around this. So thank you for showing us all that. First update on Twitter."}, {"Year": 2020, "Speaker": "Selina Carter", "Title": "Predicting Project Delays at the Inter-American Development Bank Using R", "Abstract": "How can we use R to predict project delays in international development? We'll walk through this applied example from the Inter-American Development Bank. Their pipeline of R scripts sources and cleans data from internal and external sources, then generates predictions using a decision tree (random forest) algorithm with confidence intervals (using the infinitesimal jackknife approach). Selina will display results in real time to end users in an interactive online viz.", "VideoURL": "https://www.youtube.com/watch?v=fWfSGI-pf0A", "id0": "2020_12", "transcript": "She was in the Peace Corps in Ecuador and she's a banana bread master and her side gig is painting and she is actually one of our artists who donated paintings. So everyone please welcome Selena to the stage. Thank you Jared for that great introduction. It's great to be here. Thank you so much for inviting me again to the our conference. I absolutely love this conference. It's so much fun. And I have here the nerdy outlet nerdy slash artsy outlet. I am into lately, which is this are plate and it's it's very durable. Actually, it's pretty, you know, sturdy and thick. It's microwavable and you can wash it, you put water on it. So it's great for serving cheese and grapes with your wine to talk about your your stats with your friends. So, so that's a good fun outlet for me. And so today I'm going to talk about a project we have at my work. I'm a data scientist at the Inter-American Development Bank. You can see my Twitter handle on the screen. And our project is called predicting delays at the Inter-American Development Bank using R. Please note this is my own presentation done in my own capacity. These are my opinions and not those of the IDB. So a little background again, I was talking about how much I love art. It's just one of my hobbies and I did a R pumpkin a while back. I had R cupcakes for one of my training sessions and R. And of course, the R plate, which is up for option. I'd like to introduce my team first, me. I'm originally from Maine. And that's a picture of me with lasers in the background, of course. Marcelo Carillo is my colleague at the IDB. He's originally from Ecuador. He's an amazing programmer. Alexis Estebes is also at the IDB. He's originally from Argentina and he's a great data scientist. And Pablo Riva also my colleague. He's from Costa Rica. He's really good at the business side and the data architecture management. And finally, Dr. Jonathan Hirsch, who is our external consultant from Chapman University. This is John in the chess club. And his earlier days, let's get a close up. That's him. Let's get a close up. Absorb, absorb it. Because you don't recognize him. This is him today. So that's our great team. So here's the overview. This, by the way, this presentation has a lot of animations in it. I kind of went crazy with that. So brace yourselves. I also try to make it somewhat interactive. So there are two parts. The first part is we'll look at what is the model. So we'll think about the business need, the data, the model itself, the challenges and the results. And finally, do managers trust the model? And it turns out this is a very frontier topic. So we'll look at some other cases. So what's the business need? The inter-American Development Bank, where I work, is the leading source of development financing for Latin America and the Caribbean with 26 foreign member countries, which are in green. That's the right. The IDB offers loans, grants, and technical assistance to governments. It aims to reduce poverty and inequality. And its main instrument is called sovereign guaranteed investment loans. Basically, that just means the IDB gives loans directly to governments. So a little bit about SG loans. The average loan size is $67 million. So that's pretty big. I mean, we're dealing with large infrastructure projects or major overhauls to a road system or to a environmental management system. The loans can go as big as $150 million or small as $25 million. Around 90 new loans are approved every year. And at any given point in time, there's around 500 loans in execution by the IDB. So an example project would be like a hydroelectric dam. In this case, this is a regional project. So several country governments are involved. And the total cost is $80 million. So as you can imagine, this is a big project with lots of actors and components. And it's going to take some time to execute. Typically, the projects are designed to execute within five years. So that's five years to disperse all $80 million or 100% of the funds. So what do you think a delay means? Well, a delay at the IDB means for whatever reason, the IDB could not disperse 100% of the funds. So that might happen for various reasons. It's not that the IDB doesn't have the money. It's because something happened at the project level on the grounds that made it delayed. And therefore the next milestone or the next tranche of funds could not be dispersed. So think to yourselves, what percent of the low do you think are delayed? What's the scope here? Is it 5%? 20% 50%? Well, 78% of S.G. loans approved since 2000 have some sort of delay. Oops, that's kind of a large number. What do you think the average delay is? It's 14 months. So over a year. So instead of that five year window, projects tend to have extensions to their original deadline of five years. So there's something going on here with the design of projects that causes these delays to happen. Moreover, 33% of projects had extensions over two years. So that's a big deal. So you're at the right. You can see a breakdown of the situation where only 22% of projects are on time and a third are significantly delayed. So what is the implication for the bank? How much do you think the IDB spend on supervising these extended loans over an eight year period? $50 million. So that's around six or seven million dollars a year that the IDB is spending to deal with these late loans. So those late loans costs around 60% more on average than on time projects. So as you can see, this is a problem for the organization in terms of cost and also in terms of the development goals for the country. If you're curious about the breakdown of the delays, then by country, then this is a map that shows you also made in R. So Mexico, the blue means the projects are on time are early. So a lot of proportion of Mexico's projects are earlier on time, whereas yellow and orange mean there's a larger delay. So Guatemala and Brazil, maybe Argentina, those are countries where a lot of projects have a larger average delay. So what's our objective here? First of all, we need to predict the delays. Why do we do that? Because we need to help managers develop a proactive approach to avoid the delays. Second, they need to know in real time. They need to know the risk of delays before they happen. And ideally, as soon as a project is approved. So right when the project is born by the IDB, we need to have an idea based on historic data of how long the project will last. And this information should be updated every month. Finally, we need a clear visualization. We make a dashboard so that end users can see the results in a simple way. So here's another one of my art outlets. I made this schematic of the data pipeline, if you will, for the project for the modeling process. So the first stage is the data itself. And in our case, we have lots of different data sources. We have the company data. We have lots of structured data. We have the funky data, which requires more cleaning. And then there's always cool data we wish we had, but we can't use for some reason. So what's the data we're using? Well, we just threw in as much as you could possibly figure out from use from our company system. So here's just a giant list of everything we're using. And there's over 100 features and we're still adding more. So we have six variables, which is the dark blue. So you have things like the country of the loan, the department, the approval year, the approved amount. And then you have dynamic variables, things that change over time. So things like the number of changes in the team leader. Is there a high rotation in the management team of the project? How much experience is the team you'll have, things like that? Obviously, this is going to be a nightmare as a data scientist and someone who has to deal with this data, because not only do you have to clean the data once you have to make scripts that are flexible enough and robust to slight changes in the underlying data structure. So you need to make a flexible pipeline so that one tiny little data error doesn't blow up the entire bottle. So the next phase is data cleaning. So what do we do? We have the outcome. We have a bunch of features and we want to make a model from those features that predicts the outcome. And then we're going to use the same model to predict onto projects where we don't know the outcome. That's the goal. So this is actually non trivial. How do we set up the actual data itself for the model? What we decided to do was create a long format where every row is a month of the project's age and then the project itself. So each row represents the month age of the project. And then of course, the outcome is fixed. And then some features are fixed and some features are time varying. One disadvantage to the set up is obviously the rows now are interdependent. So for one project age to the next one time period to the next. Obviously, there's a lot of correlation among those rows. That's one disadvantage of this setup. But for now we're trying to start with this is our baseline and it's working pretty well so far. And we're going to try another phase where we fix those problems. So next stage is what is the model itself. We want to do data exploration and test different ideas. So we have a lot of options. We have a wish list on the left where you have a bunch of different features of models that you want. Obviously, we want the model to be accurate. We also want to have confidence intervals around each prediction. It makes no sense to tell somebody the project is going to be delayed eight months and then not give them some sort of ballpark of how, how, you know, what the range of possible values is there. And then the next two boxes are the why we want to be able to understand the black box. What are these like predictions based on. And you want to have a way to kind of understand that beyond just, you know, a clunky black box model. You also want to have ideally a model that runs pretty fast and it needs to handle a large number of variables. So for our candidate models, it turns out the random forest is the most useful in our case, although it has the disadvantage of being somewhat slow. So the model itself takes around 10 hours to run. We do use a virtual machine for that. So there's some architecture points that you need to take into consideration when you have a lot of data. So we put the data into 85% for the training and 15% for the testing, the key our packages in the model are carrot and the ranger. And then we build confidence intervals using the infinitesimal jack and knife approach, which is sort of like a fancy bootstrap. So now what are some of the challenges in the actual process of running the model. So we have a ton of data and a lot of our scripts and then the model itself is its own beast. So this is a lot to manage. So the key here is you have to find a way to make that as simple as possible. So what we did and what we recommend is that you always have one R script. So the key is the master control panel if you will for the rest. So if I want to run this model, I just start with preparing the data, which takes about 30 minutes. And the script goes through each of the files that sources the cleaning. And then the second part is the model itself. So the key is state organized. You're going to have to build a lot of scripts and you're going to have to understand how they connect. So other people have to build it up to some day so you're probably going to have to make that user manual so that other people can figure it out later. So what are the results. So here's a predicted versus true plot on the training set. So on the x axis, it's the true, the true delay of the project. And the y axis is the prediction. So projects in the training set are concentrated on the 45 degree to mind, which is good. That's what we want to do. On the test set, you can see that there's a lot more dispersion. So that means we're over fit a little bit in terms of the level of the error. And then look at the mean absolute error. It's really small for the. 1.5 months on average is the difference between the, the predicted delay and the true delay, whereas in the test set, it's eight months. So I think that's a, that does show we have over fit, but I don't think it's unreasonable to say, okay, you're within this range of, you know, you're, we know if a project should be significantly delayed, or if it's just going to be sort of less than, you know, the level of delay is less than the value of the test set. So in my given the, the fact that we're starting with a baseline of zero with no prediction at all. I think this is okay. We're, we're still refining the model itself and adding the variable. So hopefully we'll get that mean absolute error down for the test set. So we have our results. We have our model. We have our scripts. The next stage is to build a visual so that other people can understand clearly what's going on. So we have here a dashboard that shows everybody, what are the projects in execution. What is the age of that project? And then what is the actual prediction, which is the, the dot with the confidence intervals. And then of course people can filter by region, country, department, anything they want, you know, change the colors, everything like that. And we also show people the performance of the model on the test and training set. So here's the predicted versus true plot where people can also manipulate this and see how it changes by country or their own division or department, etc. So finally, we, we, we do this and we expose people to the final result. And the question is, do managers trust the model? Well, we asked them. We sent a survey to managers in February of this year. We got 87 responses. And the main question is, after viewing the prediction tool, did the decision tool alter your belief in the likelihood of delay. However, most managers did not update their prediction of project delays. So you can see from this chart on the right that if each dot is a manager, their prediction of the delay before hearing the tool and then after is almost always the same. So this is like super upsetting because we want people to change their beliefs. That's the whole point. We want the AI to help them know more than they already do. So it makes us wonder if we did something wrong. Well, it turns out this is not a trivial issue. And it brings up the point of what's the level of exposure you let people see. So at this point, we sort of hide this machine. And then we have this thing called the veil of data ignorance. Those are John's words. And then we can say, okay, well, let's expose more. And then let's just show them the whole beast. I mean, what's the level of control that you give the user so that they can understand what the actual model is. And it turns out that's a very important thing. This is like a frontier issue that we just landed upon. And there's a lot of examples in the world right now on this issue of trust in the eye. So, for example, you know, vehicle piloting systems are people comfortable with letting a car drive them to somewhere or robotic surgery. Do doctors and patients trust a robot more or a human more to perform surgery robot warfare. Do we trust a robot more than a human in sort of the ethics of how to behave in war. And then there's the famous jail times case where, you know, judges are given an AI model to predict whether or not a defendant will we offend if they're released from jail. And what a lot of people found is that, you know, people often don't trust the tool. So we're trying to figure out why. And a lot of times it's because people see this noise and they think it's the signal and we as statisticians are as data scientists we get noise we're like, oh, that was just this random thing we know randomness. But people in the real world these experiences of seeing something happen might not translate into a tool, you know, that we want to do school. So what things impact algorithm trust. Well, according to Abdullah Alma to the MIT. There's a lot of different things transparency how well does the algorithm work interpretability can the model explain its own decisions in a ways that humans can understand performance is it communicated well is it performing well. And control can the unit these are actually modified the algorithm in our case I would say we fail in all three except for one case. We do communicate the performance well with our visual, but we're really not giving people very much control and we're definitely not letting them inside the black box of this algorithm. So what we're planning on to do planning on doing next is giving them a little more insight into this black box. So we're going to add these local interpretability measures at the project level where you can see, for example, for this project. Why does the prediction give us the result it does and it gives you a breakdown by by variable. It's not quite as straightforward as saying a linear regression model, but it gives you some insight into the black box of why a particular project has its prediction model. So what are the key takeaways first when designing a live prediction model state organized make it flexible so one error doesn't break the whole thing and document your entire data flow. So this is the very unsexy task of doing a user manual, which I always feel like I end up doing, but it has to be done someone has to do it. And then the next question is, how do you lift the veil of data ignorance. The interactive dashboard should mean, obviously filtering and changing colors and downloading the final data like that's what we think interactive means but it should be so much more than that. We should also be able to show the influence of certain variables on the model itself. So give people access to what's going on at the variable level or the future level. And then let people manipulate input data, hopefully interactive these maybe they can enter a fictitious project and then they're going to see the result based on them changing one variable at a time. Thank you very much. Thank you Selena. Excellent talk. I love the John Hirsch references. Those you don't know John Hirsch gave a workshop yesterday about machine learning for public policy. He's been a speaker of both New York are and this conference before. So it was a lot of fun and I love the before and after photos of him."}, {"Year": 2020, "Speaker": "Rose Martinez & Brook Frye", "Title": "Using Data to Improve the Lives of New Yorkers", "Abstract": "Learn how the New York City Council\u2019s new Data Team uses a data-driven approach to improve the New York City Council\u2019s policy making process. The New York Council Data Team answers policy questions and informs laws about everything from public housing residents\u2019 heat, to school bus delays, and marijuana arrests. The Data Team also uses data to conduct oversight of City Agencies. They source datasets, create data analysis and models, maps, and dashboards to assist Council staff and Council Members to use data to make decisions. Their unique strength is in marrying data with public policy making.", "VideoURL": "https://www.youtube.com/watch?v=kdYdt0otQrU", "id0": "2020_13", "transcript": "We actually have two speakers coming up at the same time. They both work for the New York City Council data team. Rose has a twin sister who also works for the New York City government and Brooke knows all the words to the rap in the movie Teen Witch. Definitely got to get some twin jokes in here or something. So everyone, please welcome to the stage Rose and Brooke. Thank you, Jerry, for the introductions. The New York City Council is the city's law making legislative body. We will go over some example of projects that show the role of data in the New York City's law making process. So our work leads to smarter laws and therefore that leads to improving the lives of New Yorkers. Here is our team or lovely team pictures of us. We are a team that has background in physics statistics, visualization, communication, policy and planning. The team is mostly in our user group. Next slide. So our mission is to promote a data driven approach to the New York City Council's decision making processes. This is accomplished by working closely with the legislative staff while they're generating laws and doing oversight duties and communicating our findings to the general public. So what we do. So we have three main data functions. The first one is data laws. Data laws are laws that require agencies to report data to us. And this feeds a second pillar, which is data analysis. And data analysis is a step where we make sense of the data and that will inform existing and future legislation. And lastly, we wrap up the results nicely and visualizations and communicate findings to internal staff and to the public. So now we will give some examples of our work in each of these three functions. And Brooke, you can please tell us what we do in terms of data analysis. Sure. So as far as data analysis goes, it's one of our core functions. We source data. We often source data from the open, the New York City Open Data Portal, which is an amazing resource if you don't know about it. We clean data, we transform it, we model it, and then we eventually turn our findings into actionable insights that will inform legislation and policy. Go to the next slide. So as Rose mentioned before, there's like some nice synergy between all of these steps and especially between the reporting bill or the data law bill and then the sort of the data laws and then the analysis side of things. So a good example of this sort of synergy is this climate mobilization act or local on 97. Local on 97 mandated or set sort of thresholds on how many, how much greenhouse gases buildings over 25,000 square feet or more can emit. And so in order to come up with these thresholds, we needed to understand the sort of universe of emissivity to understand what like, what characteristics a building had that sort of were predictive of emitting greenhouse gases. So this law relied heavily on a reporting bill law, local law 84, which mandated that buildings of that size report on energy usage. So we, in order to like understand and set reasonable and achievable threshold for these buildings, we had to really like investigate what features of a building were predictive of emissivity. So we used a lot of different kinds of models to interrogate that information. We use a CHT boost. We use a short cast to understand the seasonality. And what we came up with was that, and it's not surprising, but that it's not so much like the way a builder building is constructed or sort of like where the building sits on a hill, but it's really all about the work that is being done within the building that's going to be predictive of how much greenhouse gas it emits. So then we were able to use this information to group buildings like buildings within like buildings and sort of like create these buckets and then sort of reasonable thresholds within these buckets that we felt were fair because we set like sort of limits. The thresholds were like relative to other like buildings. So that was a really exciting and big law that we worked on and it will hopefully have a major impact on the city as the mayor and his administration have set the goal of an 80% reduction of greenhouse gas emissions by 2050. Go to the next slide. Okay. So a good example also of our work informing policy is what we sort of did with the taxi medallion task force. So this task force convened to try and understand and fix this sort of crisis that yellow cab drivers are facing. So just to summarize, if you're a yellow cab driver in New York City, you need to own a medallion. Medallions have been increasing in cost over the last 10 years about and the amount that you can sort of make being a taxi, a yellow cab driver has gone down. So we wanted to understand when and why this is happening. So we looked at historical data daily sort of daily earnings data for each medallion and tried to understand like, I mean, there's seasonality of course, but tried to understand like at what point did the trend start to really sharply decline. And we found that that and it's not surprising, but it's nice to have this confirmed with that as soon as, as soon as like around the time that these four higher gig economy, or lift and Uber cars came onto the scene and saturated the market, that's when sort of wages started to really, really decline. And so we incorporated that into our, the policy recommendations that went to the state finally. And I think recently the state released some recommendations on how to sort of fix and help mitigate this, this issue. So as we mentioned before data laws or reporting bills are really important to the work we do because they give us the data that we need in order to do the important analyses and sort of reframe legislative issues in like a more scientific way. So reporting bills require city agencies to report specific info on their activities to the city council and or the open data portal. So metrics that show their sort of what sort of work they're doing and how well they're doing that work. Next slide. So I'll go through the next, the next couple slides will be like contacts like, what is the process and how does the bill become a law and where do we sort of step in. So the process for creating a data law and more broadly legislation is that we sit down with central staff or elected officials and their staff numbers to understand the issue at hand and see if it can be reframed in a quantitative way. So we sit down with them and like, is there, you know, is there too much trash in a neighborhood and should we have more better service in that neighborhood or is that just one block and maybe some anomaly that doesn't require like a whole entire sort of lot to fix. Oftentimes we realize that we don't have enough data, but there is some data that like can describe the problem like a little bit, but maybe not enough to really like understand why the problem is. And so oftentimes the solution is we have to get more data and that requires the reporting though. We come up with this issue of privacy concerns as data people, we want high resolution, very granular space, you know, temporal data and we have to also be mindful of people's privacy, especially, you know, in the city of New York. Lots of, you know, concerns there. We also like an interesting sort of thing that we have to consider is that the laws have to be future proof. So we can't, we can't like specify or it's important to like make sure that the laws are flexible in the future. So if we specify specific technology that may eventually become obsolete, that law is going to be kind of like, in comfort by that specification. It's going to be hard to adjust. And so we want to make sure that everything sort of like we describe the technology, but we don't use specific sort of brands, which has been a tricky thing for stress. Okay. The next slide. So the process of a bill becoming a law. So a bill starts out as a, and I'm sure everyone is maybe a lot of people are familiar with this process, but it might be a little bit different at the city level, but an idea becomes a sort of legislative request. A legislative request is the formalization of like the idea. So it becomes something that then gets circulated within staff members, lawyers, and is true and is turned from an idea into a legal ease. So it's turned into legal language. This is where we often sit down with the lawyers and talk about how to make sure that like all of the quantitative aspects of this idea or of this issue are considered. And if it is a reporting bill, are we asking for the like, right, you know, level of granularity? Are we asking for, right, like the right sort of temporal sequence? Are we asking for the right geographic sort of unit? Are we asking for enough information that would let us sort of like answer the question or solve the problem or, you know, monitor the issue? So this is where we spend a lot of time at this step. The bill then, once we have these like sort of internal conversations, the bill is introduced. And that's when after that point, that's when the public has a chance and advocates have a chance to sort of wait in and say, you know, remind us of things that we didn't think of, which happens every time. It's a huge important step. And then, you know, eventually once all of that sort of like lots of editing and drafting and sort of reef formatting the bill, it becomes this thing that then is gets voted on and that's. That needs a majority vote from the council and then the mayor then was the mayor's desk and the mayor can either veto it, which I don't think he has done in his, in this administration, or he can sign it or he can just leave it be and then it becomes a law after 30 days. So Rose is going to give you an example of one of our reporting bills. That's very timely and we're really excited about because we're working on it actively. So the pay equity bill is an example where we provided a lot of support to the legislative stress. Legislative legislative staff that were drafting the bill. We help them think through the language. So the characteristics and variables that should be included in the bill. So the intent of the pay equity recording bill is for agencies to report and bring information to us. That way we can then investigate pay and equities in the city workforce. So during this process of drafting and negotiating the bill, we met with advocates and the agencies involved in this process, which is the Department of administrative services. This is the New York City agency that collects employee information for a lot of city of the city workforce. So the woman in the photo is council member Lori Kumbow. She is the prime sponsor of the bill. She was very excited and happy about this bill, as you can see. So when talking with her and with advocates, they were, they only had in mind three variables. Gender, race and age, they were very fixed that these variables were enough to explain. Pay and equities and it might be the case, but we told them that these three variables are not enough. There might be other confounding variables or characteristics that will do a better job and explaining pay differences. So we listed a whole bunch of them, as you can see up here. So some of them are like years of experience. Are you in a manager position, your education level, et cetera. So this helped significantly in order for us to answer the question that they were looking for to explain pay and equities. So, so this bill is one of the first of its kind as well in terms of how the data is being reported to us. We will be getting access to the data via an API. We're very excited. So the city council will have access to person level data. So there's privacy concerns here. So we only have access to this data set for a period of time every year. And the public so advocates really wanted this data to be available for the public to view and analyze. But due to privacy issues, those are good upon to make public and aggregated version and not all the variables. So this is already out there. If anyone's interested. I guess if we share slides. Or you can search on the open data portal for the local law. It's out there, but. We're very we just so this is the timeline before COVID of when we would receive access. To the API. So we just got access to it a week ago and we're in the middle of. Just digging into the data. We have no immediate findings yet. We are. We know that we have to deal with missing data. We don't know how consistent employee information is reported across all the agencies. But that's something that we're very excited to dig into and maybe at a future point or. I'm sure they'll probably be a release of the findings and we'll be in the news. Okay, so moving on to our last data function. It's data visualization. So. Communicating our findings in a way that is easily understandable by everyone and pushing our work onto GitHub or the council webpage. For others to add and collaborate on is the main goal of this process. So an example of a project. That does this is the victims in New York City. So this project we worked on it when the speaker of the council was held another position at the same time, the interim public advocate. The former public advocate went on to another role. So I guess due to law, he becomes the interim public advocate. So, evictions is a big issue area for the council. The council passed also another legislation first of its kind. Called right to council. So this legislation seeks to decrease or address the eviction crisis in New York City by providing. Free legal assistance to those going through an evictions proceeding. It's being rolled out in stages. So at one point. I 20 something and not too sure it will be available across the whole city. But also this remains an ongoing issue due to COVID. There was there's a moratorium on evictions, but at the end of the year, it will be lifted unless it's extended. So there might be a big rise in evictions. If it's not extended. So we put this page together to showcase the council's bill and to sort of to see. The facts of this bill. There has been decreases in evictions in the city and also just to show where it still remains an issue. As you can see in the Bronx. There's a lot of evictions there. So in this process of putting the charts and the maps together. The data source, the eviction data set on the open data portal, lacked latitude and longitude columns, despite having address information. So the addresses were very dirty. So if you were to put them in a geo coder, it wouldn't spit out the latitude and longitude for you. It would give you an NA. So what we did was did a script that cleaned the addresses. So using GSA, then regular expressions to clean the addresses. We were able to get the latitude and longitudes using the New York City's do your coding API. And that led us to be able to map at this fine level. Before that, the only maps of evictions in the city were at the zip code level. So advocates were really excited to see these maps out there. And the housing data advocates reached out to us thinking for our work and putting the code out there. It helped them. Cross check their own work that they've been doing on evictions and hopefully expand on our scope of the project at that time. So this sort of hits all our points and goals of data visualization, this project. So I guess to start wrapping up. Maybe Brooke, do you want to tell us what has been our success strategies and embedding data at the council? Sure. So I guess the main reason the reason why we exist is because we have support from leadership and higher ups and that our speaker of the city council does really believe in data and using data to inform policy and legislation and all of our bosses. And then we have to think about data and how we can do that and we can really do believe in that too. And lawyers who are writing the laws are really receptive to conversations with us about how to use data and how to think about data in new and different ways when it comes to legislation. That we have a strong like sort of core like visualization effort and this, this sort of like goal or this ethos of like. Making our findings really interpretable and legible to the general public. We strongly believe that everyone should be able to understand our results and that's really big, especially when you're dealing with so many different audiences like non technical and technical. Like everyone should be able to understand things and visualization, I think is really important for that. And then we have some really strong quantitative and technical people on our team that can like in creative ways reframe these. You know, legislative issues or these constituent issues almost into quantitative scientific questions. And so we're, I think we're really good at like incorporating the scientific method and just using our scientific backgrounds and making that really like integral into the legislative process. And that's, yeah, those are things I'm really excited about. Rose, do you want to talk about the things that are hard. There are challenges and that we're slowly chipping at and still any walls to climb over. So sometimes due to timelines or the staff are just too busy or maybe they forget us, they bring us into late. So there's a question, a problem that they want us to help with and they're like, can you do this by tomorrow? And we're like, well, probably not. So we don't have the opportunity at that time to go in depth or like in depth enough to give them an adequate and correct response to their question. And that also feeds into managing expectations. Sometimes they think we can do magic, but sometimes we're limited to the quality of the data, the data quality is poor. We can't give sort of estimates with a certain amount of certainty. Things like that. And then lastly, our own projects, most of the time our work is very reactive to what's in the news. Or agendas, the council and other council members. So long term projects that we have our time on them is limited. Compared to the amount of time we spend on these more reactive short term projects. So I guess to summarize our role at the council is new and I guess for the better for New Yorkers and the laws that are coming out of it before there was no scientific or quantitative thinking put into a lot of these laws. So like more thought is going towards like numbers that are being written into the law. So, or processes have gone through like some sort of check of what's happening, what's the data telling us. And maybe this sometimes this can't contradict what a constituent thinks is happening on their block. And they assume that that's happening everywhere in New York City. So I think we've in a short amount of time we've done a lot, but we still have some challenges. I don't know Brooke, do you want to add anything? I mean, we're a team that has grown from, I think it was just Rose and one other, maybe two other people. And like the confidence of the growth, like the confidence in what we do is like sort of represented by how much our team has grown and how. Yeah, like the like works that we've sort of done at a micro and macro scale, whether it be like, here's the data set that you need. And here's that like mean that you were looking for to like more sophisticated models to understand like really complex problems. So I just think, yeah, we have that and we work on a lot of different projects, a lot of different topics. So you're not working on one thing. Forever, you're working on housing and then you're working on greenhouse gases and then you're working on, you know, I don't know, a million other things. So it's. So thanks to everyone that's watching. Here is our contact info for our team. Our GitHub page. Our website and our Twitter handles. So you guys have any questions feel free to reach out to us. And thank you again for watching. We still have more ahead for you. And I'll show you that in a second. But I do think that when they talk about how to make a bill, did anybody else think that they should make a city version of schoolhouse rock. I think that would be great to see a thing involving schoolhouse rock. I think that would be pretty awesome."}, {"Year": 2020, "Speaker": "Simina Boca", "Title": "R and the Study of Rare Diseases", "Abstract": "A rare disease is often defined as a disorder that affects fewer than 200,000 individuals in the United  States. Over 70% of rare diseases are genetic and of those, 70% start in childhood and often lead to a  substantially reduced life expectancy. Together, rare diseases affect 25-30 million individuals in the United  States. Due to the small number of individuals affected by any one rare disease, their often progressive  nature, and the fact that they often affect children, it can be challenging to perform clinical trials in this  space. Because of this, the US Food and Drug Administration (FDA) allows for specific flexibilities when  evaluating new drugs for the treatment of rare diseases, for example by allowing the use of biomarkers  as surrogate endpoints in some instances. Simina considers the specific example of Duchenne Muscular  Dystrophy (DMD), a devastating X-linked disease affecting around 1 in 5,000 newborn males that leads to  muscle wasting, loss of ambulation and eventual death between the late teens and early twenties or thirties. She presents two vignettes related to the use of R in understanding DMD and setting research priorities in this clinical space. The first vignette concerns the analysis of the first comprehensive  metabolomics study for DMD, which adds to the list of possible non-invasive blood circulating biomarkers,  representing one of the first steps towards finding metabolic surrogate endpoints of disease progression (repository for analysis at https://github.com/SiminaB/DMD-metabolomics). The second vignette looks at  curating DMD-related clinical trial data from the government-maintained database www.clinicaltrials.gov,  with the eventual goal of developing a product to allow researchers, clinicians, and patients to stay up-to date with ongoing drug development in the DMD disease area, as well as to prioritize research focusing  on individuals who do not currently have many available clinical trial options.", "VideoURL": "https://www.youtube.com/watch?v=L5GnQfdklp0", "id0": "2020_14", "transcript": "Our first speaker has lived in four countries before the sixth grade. So I do think we're going to have to ask what those four countries were. Everyone, please welcome Samina. Hello, I'm really happy to be here. And thank you so much for inviting me and giving me the opportunity to talk to you about some of my experience in using art to study rare diseases. And I will be talking about Duchenne muscular dystrophy specifically, but I hope that this will be an opportunity for everybody to become familiar with some of the ideas and also some of the ways in which you can ask and ask your questions in rare diseases. If you're just in getting in contact with me, the easiest thing to do is probably Twitter. And actually, if you go actually my pinned tweet on Twitter is actually linked to this presentation on GitHub. So you can have all this information, including my email if you just go ahead and do that. Okay, so just a quick introduction to to rare diseases, which generally have this this zebra ribbon in case you've seen it. And sometimes it refers to specific diseases sometimes just rare diseases in general. So a rare disease in the United States, it's often defined as a disease that affects fewer than 200,000 people, which is a fair number of people but of course it's very small compared to the US population. And over 70% of them are genetic. Unfortunately, that means that a lot of them to affect children, especially rare diseases which are recessive so basically you need a copy of a gene that's, that has some issues from from both parents. The center of fact children more often. And they lead to a substantially reduced life expectancy. I think that they used to be called fatal but I think that the correct term that people generally uses is life limiting and some, some examples that people might be familiar with include cystic fibrosis, sickle cell anemia. Some of them if they're dominant like hunting turns, they might actually affect adults, but oftentimes they affect children and muscular dystrophy and I'll be talking about one of them. Do you shun muscular dystrophy or DMD. Now one thing about rare diseases that rare disease advocates really like to point out is that even though individually they're rare, as a whole they actually affect more than 10% of the population. Sorry, I guess I guess close to 10% of the population, they might actually be more than that because some of them of course might be undiagnosed or under diagnosed with their rarity. So the question is what are some approaches for studying rare diseases and of course since it's the our conference, you know, how can we use our and what are you know kind of some of the priorities in doing this. So just a quick little tidbit about this ribbon wise at the zebra ribbons so Theodore Woodward was a very well known accomplished researcher at the University of Maryland. So he said when you hear who feeds behind you don't expect to see a zebra and I guess this is especially because in Maryland, you know, where I live, for example, in Maryland they're not that many zebra's right of course it depends like where you are right context is everything. The idea is that, for example, if you're a if you're a doctor, if you see a patient even if they kind of seem to have rare symptoms they're more likely to have a common disease. Just because just because of the numbers right. And so the idea is that you know where does he have you can see want to point out that you know there are zebras out there so they sometimes think of themselves as you know medical zebras and the other thing is that each zebras also unique in terms of their district pattern. So just something to that I just wanted to share. So some challenges with with rare diseases there's very many challenges, including of course it can be very hard to diagnose them right because there is this issue of you know kind of being like a you know quote unquote a zebra. And it depends on what you know what the disease is I mean there's screening down newborn screening for some of them but of course that's a challenge, but even post diagnosis of course it's an issue in terms of just performing studies and performing clinical trials for example. So the obvious reason of course is that, of course there's a very small number of individuals affected by any one disease and so you're sample size by definition is going to be below so that's going to of course be a problem from a statistical point of view. So you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you. And if you, you know, you know, you've an opt. And if you know, you know, you've an opt. And if you know, you might want anything else just. So really one of the things that, you know, most people know, when they first saw the Slide. I think like they're centuries old things, you know, I think you look, like, they're, like, you know, a lot of people know, you want to see a story that people really,Seen. for six minutes, right? And because of some of these reasons, DF-GATE does actually have some specific flexibilities when they evaluate new therapies for air diseases. For example, by allowing surrogate biomarkers to be used instead of specific endpoints. So maybe looking at the impact of a drug on a protein or I'll be talking about the impact of a drug on a metabolite, which is a small molecule. And if you can show that that's correlated with a disease endpoint, then you can just use that instead of the actual disease endpoint in the clinical drug. So I mentioned I'll be talking about DMD or Dushan muscular dystrophy. So it's an X-linked disorder. It's a very devastating disorder. X-linked means that it's caused by mutations in a genome-d X chromosome. So this is why it affects about 1 in 5,000 newborn males, a lot fewer females, just because most males have XY chromosomes, of course, not everybody, but most of them do. And so they have a mutation in a genome, the X chromosome. They can't have another X chromosome, like females, usually due to kind of compensate for that. So it can happen in females as well, depending on if they have mutations in both, or if they have some part that's missing or sort of different situations. And it would actually work to see whether women who are carriers might have some of the symptoms, like maybe some cardiac symptoms as well. But in general, people think of it as being a disease that overwhelmingly affects boys. And even though it's caused by mutations in a single gene, it's actually the biggest gene in the human genome, as far as we know. And there's actually many different kinds of mutations that can happen in that single gene. And different mutations can actually lead to different phenotypes. So certain mutations can actually lead to a milder phenotype. It's called Becker-Muscular Dystrophy. They used to be considered two different diseases before people actually figured out the genetics. And there are therapies right now, some of which are in trial, some of which are actually conditionally approved, that only target individuals with specific mutations. So that's just sort of like, it's like a subclass of a small class of rare diseases. And it's an example of personalized medicine. And that's okay, but again, I think that's complicated situation, for example, clinical trials. And then I mentioned, it's at least muscle loss and generally unfortunately, it leads to death. It's pretty rare to live, I think, past like early 30s. Some people live into their late 30s, but it's pretty rare. And the mainstay treatment is still steroid treatment because that can help maintain muscle mass, but it can actually cause issues in the long term. And it's only good for maybe like a few years. Okay, so I mentioned, so there are a few therapies that are conditionally approved. There's many clinical trials right now actually for DMD. So that's great. But again, it can in some ways actually exacerbate this issue where you already have a small number of patients and you're actually having them be competed for even more in these multiple trials. And then typically I think families do actually try to enroll their children in these trials, but there is this issue where at a certain point, they can kind of like age out. So I mean, there's typically an age limit for various trials and again, depends on the clinical outcomes and on the clinical symptoms, it depends on the mutation type. So that is a challenge. So one thing that I've tried to get interested in is to actually look to see what trials are actually performed in this space. And eventually have the goal of developing a product that actually allows everybody who's interested in this area to stay up to date with the ongoing development. And also to try to prioritize research focusing on individuals who may be our greatest need. And so that you don't have like all the trials focusing on maybe the 13% of individuals who have a mutation that's amenable to Exxon 51 skipping, for example. And so I had a summer student been high last year who looked at clinical trials.gov, which is a repository of all clinical trials. Basically every clinical trial at least in the US has to be pre-registered. And it's all of the files are XML files. And so she downloaded all the XML files that were related to Dutian musculodistrophe. And the great thing about our of course is that there are all these packages that allow you to interact with different kinds of files. So even though, you know, usually I think about you like tab delimited or comma delimited files, you know, even if there are XML files, you can still use the XML package and you can actually pick out some of the blocks, the kind of the predefined blocks. And then of course perform data cleaning that includes things like redundant drug names. Every drug I think has released like four names. That's a whole sort of issue onto itself. And then she used our Shiny to design and create a web interface to explore these trials and actually look at, you can search by condition and you can get some summary statistics and so on. And again, if you go to my Twitter account to look at the pinned tweet, you can get to the slides and you can, if you want to play around with this and take a look at it. Okay, so the second thing yet that I wanted to talk about was using molecular data sets to try to help out with actually finding biomarkers of either disease progression or drug response. So I mentioned that there's a focus in doing this, especially for rare diseases. And we focus on looking for metabolic biomarkers. So basically metabolize, we trust small molecules. The study of metabolites is called metabolomics. They can be detected in various biofluids and tissues that includes things like blood, urine and so forth. And they have various factions. And the good thing about them is they can be influenced by both genetics and environment. So they're sort of like, somewhere in between. So it's good to have this opportunity to look for different kinds of markers. And I'll be focusing on this first paper that we worked on from 2016. So this paper looked at 51 TMD patients and 22 healthy controls. We detected over 2000 metabolites. The samples were all run at Georgetown in terms of the metabolomics, but they are from five different study sites. So one of the things that we were always very, we were careful to assess is to see whether there's any possible batch effects. And you can see actually the individuals are not balanced between the study sites. So we did also have also a subanalysis just looking at Calgary, Alberta Children's Hospital, which did actually have a more balanced number of patients and healthy controls. So the first step, I just, I always like to point out the first step in doing any sort of omeks. And first of all, it's being a lot of data exploration, making sure that everything sort of matches up. And then I guess the second step is to do principal component analysis, because I think that it's very revealing in terms of, you know, are there batch effects? Are there artifacts? When people see, for example, two clusters, sometimes they get very excited like when I discovered a pattern in my data. For me, it always just makes me feel nervous because I worry like, oh, doesn't mean that actually the samples are processed differently. So we were actually pretty happy to see this. It's just color codes by age and it's color codes by study sites. Now, if I was to read you what now I would read you it a little bit differently. I just wanted to kind of have this like as a parentheses because I feel like I have learned some things since 2016. So I do try to use a colorblind friendly palette because I'm not colorblind myself, so it's not something that comes automatically, but I think about 4% of the population is colorblind. But so there's some of these questions that, you know, how can we kind of better automate these things that you don't have to make these decisions, right, which is kind of like tiring. And like, I was, you know, I was like, you know, like going back and changing the colorblind friendly palette. I'm trying to get to get better at this. And then for PCA now, because it is, if it's all omics, so unless you're using different types of measurements of different units, there are combinations generally to actually not scale. So to use the default, for example, if you're doing a print comp in R and also to use the same distances on both axes. So you can see like here between zero, this is zero and 10 on the Y axis, zero and 10 on the X axis because the first principle component has more variability than the second principle component. So it's actually a deal to try to maintain that as opposed to having it be a square. And I learned a lot of these tips from this great and Gwen and Holmes paper, which is also a great example of reproducibility because they actually have the code for generating all of the figures in their paper. I just trying to just share that. Okay, so in our study, so basically we had, you know, over 2000 metabolites, we basically fit a regression model for each of them that had the outcome of transformed metabolite value, log transform, various other quantile normalized and so on. And then we looked at how that can be explained by age, study, site and then disease status, of course, and then also age by disease interaction. And we found 14 metabolites that were significantly altered between DMD and healthy control that a false discovery rate of 1% out of those 14 because of the technology that was used. We didn't know exactly what they were. We just had an idea of what their weight was and some other characteristics. So the lab that we worked with, which is the core at Georgetown, did validate them using MSMS and they found four of them creating creatinine, five ADHD and testosterone sulfate, we're pretty sure that, you know, this is actually what they are. And L. Arginine was likely validated. And this is actually, it doesn't seem like a great deal, but it's actually pretty common for these kinds of experiments. It's sort of like part of the course. Like most things actually don't get validated. Creatin and creatinine, we were happy to see because we know that they're actually involved in the creatine pathway, which has to do with things that happen in muscle and in blood. And you see creatine and creatinine here. So these are both metabolized, they're small molecules. And this reaction here is catalyzed by creatine kinase. Creatin kinase is a protein. So we didn't pick it up in our study because it's too large. But actually our team did do, some members of our team would actually do a proteomics study where they picked up creatine kinase. And this is actually a very well-known biomarker, a very well-known diagnostic biomarker of DMD. So it shows up being at very high levels in sort of children between the ages, maybe like four and 10 or so, because it shows that there's a lot of muscle damage going on. So typically somebody suspected of having DMD, they'll have a blood test where they look at CK levels. And typically those will be way, way higher than healthy individuals. But the problem is that with age, as there's muscle wasting going on, and so with age that actually goes down a lot. And so you can't really use it as a biomarker of disease progression or of therapy response. But in any case, it was good to see these as some of our, among our results. And we also considered in a post hoc fashion, the ratio between creatine and creatinine, because you see that like one of them leads to the other. So they're actually inversely related. So their ratio is actually even more, it's actually even more compelling to look at it. So you can see between DMD and controls, they're generally very, very different. And it had the very low P value. But I mean, this was kind of a post hoc analysis. But we were actually happy to see this result validated by another group. So that was good. Another thing I just want to talk about. So in terms of doing this work, I think it's important to keep in mind that there is kind of this, I call it like a study ecosystem, like what's going on beyond the paper. And you can think of this as sort of being related to reproducibility, you know, reproducibility powered by R. I think that's one of the big benefits of R. So I know some people have said that a scientific article or a report, it's sort of like advertising the research because you're only kind of getting maybe like a big picture view, finding out about like specific decisions that were made maybe in order to have a certain result or a certain publication. But there's a lot that's actually kind of going on behind the scenes. This is published in PLOS. And so PLOS always wants to deposit the data, you know, as much as possible. And so we deposited the raw data in Dryad, which is one of the repositories that they recommend. And then the process data along with the analysis code is on GitHub. And they're all linked, you know, ideally you know, you can get all the links. So they're all linked, for example, from my website. I think it would be great to figure out some better ways of the community to sort of connect all these things. And there's also like links, you know, in the paper, but it's one of those things where again, I think if you kind of incentivize it and sort of make it be automatic so that people don't have to kind of think about doing all of these things and kind of figure out, you know, like every time, you know, like I update my CV, you know, it's like you have to like kind of copy and paste like five different links. I think it would be great to have a more, a more automated way of doing that. Just like it would be great to have maybe like, you know, like color blind, friendly palettes or just be the default. So you don't have to think through it and make, you know, make more decisions about it yourself. So just some of the conclusions. So primarily for the, for the metabolomic studies of the 2016 study that I talked about, we're very proud that it was the first comprehensive metabolomic study for, for addition, Mescalistrafi. And we thought it was one of the first steps in finding metabolic surrogate biomarkers and adding to this list because people have looked at other kinds of, of, you know, like proteomics and looking at MR name markers and so on. And we also had a 2020 study which looked at the urine of mice. So there's a mouse model called the MDX model and we compared healthy mice with MDX mice. And then they either were just taking a control or they were on pernizolone, which is a type of corticosteroid. So we got some additional biomarkers from that as well. So some future goals in terms of also combining this with, with what I talked about in terms of clinical trials, I think it would be really great to have these types of studies embedded into clinical trials because clinical trials have very clean data. They have very good clinical outcomes. And so then it's a lot, I think it's a lot easier if you can just have access to some of these samples where people have already, you know, the people who set up the clinical trials are already measuring a lot of these, a lot of these outcomes on the study participants. And then you can also do kind of some omics sort of like on the side. I mean, it has to be embedded into the protocol but it's now to be sort of like the ideal thing to do. And then also try to connect this type of study with genetic data and focus it on some of the, you know, I mentioned the areas of greatest need in terms of maybe looking at people who aren't served by current trials in some way, right? Either because they have a rare mutation or because of their age or because of some other, some other aspects. Okay, so just this is just, I just wanted to acknowledge all the people who are involved in the study are funding, the study participants, including these are three students who helped me out. They weren't involved in the original publications because they started working with me on this afterwards, but Ben was involved in the clinical trials work and then Bo Yun and Shintang are looking more at the metabolomics data. We also did some metabolomics proteomics integration. Again, if anybody's interested, you know, definitely feel free to contact me afterwards. If anybody wants to talk more about it. And just this is just my final advertisement slide. So we have a master's program in health informatics and data science at Georgetown. It's a great program if I say so myself. I'm one of the faculty in it. We're in our second year right now. So we're open for applications for next year. Definitely let me know. It also includes a, so one year program that includes a summer sort of internship with industry, which is co-mentored by faculty member as well. And, you know, I recently joined our ladies DC as a co-organizer and I know that our ladies is also involved in setting up this meeting. So these are some of the Twitter accounts for our ladies global and our ladies DC. And of course, there are ladies chapters in many cities in the US and the world. So thank you so much. Thank you very much. That does remind me, thank you for the reminder. We are going to have an our ladies photo. Let me just check when that is supposed to be."}, {"Year": 2020, "Speaker": "Refael Lav", "Title": "Deployment of a Recommendation System Built in R at Scale", "Abstract": "Imagine that your analysis and model are improving with each additional user using your system! Enterprise Recommendation engines are powerful analytical techniques that benefit each user more while getting information from their interaction to benefit the next user. This presentation will cover the creation of a recommendation engine using R using individuals\u2019 data and site behavior. Once created, showcase the two methodologies used \u2013 On-prem and how we used docker, Plumber, and AWS to scale the infrastructure while allowing information to enhance the model. All of this to service a production-level website for resources with smarter recommendations.", "VideoURL": "https://www.youtube.com/watch?v=9eQWBuWFQYo", "id0": "2020_15", "transcript": "This year, he has dedicated 2020 to trying all different types of coffee and making more and more elaborate lattes at home. So everyone, please welcome Raffi. Good morning, everybody. Well thank you, Jared, for the introduction. I really appreciate that. Yeah, I've been doing a lot of lattes at home. It's actually a pretty exciting side business. I'm going to start, I'm going to have to start inside of R. Good morning, everybody. This is going to be a talk about something that I'm really passionate about. It actually was the same topic of the first DCR meetup is how do you take solution from R, not just the, it's not a statistical analysis, but a solution for clients. And what we've done around really serving it as a solution for client out in the real world. So a little bit about myself real quick. Again, I'm a lead data scientist for Deloitte. I run R for governments at the Deloitte GPS. And I do a lot of work around a different agencies, a little bit of commercial, really focusing on the advanced analytics part over there, again, using R in particular. And before I start, I just want to say thank you for two individuals who are building on their work, and Palmer and Docker. I think if you have an opportunity, really look at what they write about and the work that they're doing, a lot of what you're going to see over here really stem from there. So this talk is really not going to be about, I'm not going to show a whole lot of code. It's really about how I had a problem and how do we went about solutioning that using the tools that are available to all of you without needing for a huge infrastructure. It's not a Docker training. It's not an AWS training. It's the little that you know to make this thing work. And then you can go from there and stock overflow, et cetera. So again, I am going to focus on four aspects of a solution, how I deliver R at capacity to production to an entire state. And we're going to start with the left side of the equation of the R model itself for recommendation. We'll touch a little bit about that in the Plumber, how do you actually serve that, how we containerize that and really able to serve that as a whole. And then using microservices essentially, how do I use AWS capabilities which are allowing us to take this R model and really serve it at scale without the need to know a whole lot of cloud work. And finally, how do we communicate that out to the public? And again, this is a real solution. You guys can hit it right now. And I'll show you in a second half. So a little bit of a background and this is a text heavy slide and I'm not trying to know it's a Deloitte feature. But the solution that we have built under an umbrella called GovConnect is really how states and local governments are really communicating to citizens. The solution specifically over here in Kentucky is called is a resource engine. And what this is provide is an access to citizens to know what services are available to them in the community. Food bank, which is very, very important right now, education, clove, childcare and the like. So this is an entire, this is a website that you can go into and really search for based on your location and if we know something about you because of eligibility system and what we know what the state know about you, we can start giving you access to resources that might be benefiting to you. Specifically the work that we have done in the R work is really related to the personal intelligence, which is the items users like you, like Amazon, users like you who looked at A also looked at B or users like you who looked at A also considered B or will benefit from C. The reason that this is really important is because we know that most people don't actually know what's available to them. Not everybody is Google savvy. It's really important that we really enable people to either self sufficiency, enable people nudging them in the right direction. But more importantly, it's the community helping the community. Everybody like you will help you get to where you need to be. And that's kind of why this kind of capability is really important. What it's actually looks like is like that when the user get to a resource, they have some information that they can see around, for example, skills. And at the bottom, you can see frequently paired together and related services. And what you can see over here in related services are similar. It's the same thing just maybe in a different location. And the frequently paired, that's the complimentary. That's what the recommendation come into place. And that's really where you start getting other services that you may not know about, that you can actually get access to. And again, all this is live and in production. Now, the uniqueness of this situation of this is it's not like Netflix. It's not like Amazon. The reason that it's not is that because we don't have one to five rating, nor do we have anybody buying anything or purchasing anything or anything like that. The goal is really for individual to become aware, to actually know that this exists. That's essentially a success. Now we don't actually get rating. We know if you connected to something, but we don't actually get rating because only if you come back and say, yeah, that partner, not the service, but the partner was good, then we get some feedback back. Now it's also binary and implicit. So it is essentially the yes or no, did I connect or not. And finally, again, we only care about relevancy. Now let's go into the nitty-gritty of the solution. So what we have over here is, again, we have the R capability around the code, the get the data, the modeling, and we'll touch on that in a second. The delivery is feeding into plumber. All of this is Dockerized, sitting on a fair get infrastructure, which is a serverless serving of this Docker, and it can scale up, and that's how we can serve an entire state. And then behind API Gateway, so we have security. So the system, my R code and the model and the data is actually secure and only get through the API Gateway. So I will walk through each one of those items in this specific R component that matter. All in all the solution, we also have the ability to reproduce it in other location. So I'm going to talk about recommendation system, but it can be applied to any model that use API and the like. So deep dive into the recommendation first. The first aspect that I want you to think about is that we essentially segmenting the population, think clustering. And by the way, segment is not all pregnant woman. It's essentially people who will behave similarly. And it's essentially our first boiling it down from the entire infrastructure. We have the metrics that we created. And again, you can see it's a function of the information that we get. We created a metrics. And then using this wonderful library recommender lab, I'm creating an item-based collaborative filtering. So I encourage all of you to go and Google what item-based collaborative filtering. But think about this as a metrics where I have the items or the individuals that actually are selecting that and the items that are the resources, food bank and the like, that they actually selected. As you can imagine, this is a really, really, really sparse metrics. So we have to handle that. But the recommender functionality will create that real metrics that then create this outcome, which is essentially the base for the model. Now the second part, how do I actually make this? Okay, you build a model. How do I actually do something with that? So the second part over here is that I create the plumber functionality. So in three seconds or less, I'm creating. You can see at the top right over here, I'm creating the post functionality of plumber. So that means that it's not through the URL. I can actually serve a body. And you can see in a second why I need to serve a body. But this is information such as the user ID, where they are and what they're looking at. And what I'm passing in, what I'm passing to this RE-based model. So this is my example for the base of purpose of this demo. This is the base model that I'm serving. And you can see on the right over here that I am getting a function, I'm getting the user, the variable that I need. I'm using my function get ranking, distance matter, for example, how far you are from the resource that I'm trying to recommend. And I'm essentially the outcome of this function is a response out, which is essentially a list. This is important because again in the API, I'm serving a JSON file. And we'll get there in a second, but this is important. I'm serving a list. Now, in the next piece is actually the critical. And this is the, if you remember anything from this talk is the next piece. I'm serving the model. I'm creating the plumber router that's through the new PR functionality. A few months ago, a new plumber library was released. It's wonderful. I encourage all of you to take a look. But what's important over here is now I am using the PR run to start a server. I'm using a host 0.0.0.0 on port 5001. Now, this is really critical. How do I serve this? The reason, port is essentially, well, that's the port where we're listening on. But why do I need the host that 0.0? That 0.0.0. The key over here, and this is really key how to serve this in production, is that by default Docker has its own namespace. The namespace is essentially its own infrastructure IP and the like. If I want to connect the Docker to a different environment, I need essentially to forward the port from the 5001, the port that I'm actually opening. And the IP that the Docker provide, and you can see on the right, the bridge. I need to essentially listen to all of them. So to achieve this, I need to listen on an external IP inside the container. And 0.0.0.0 is all IP before. So it's listened to everything. That will allow me to have the connection from the Docker to the external environment. Now, once you do that, now you can create the Docker. And this is where it's very, again, this will be posted. This is the classic Docker for R. I'm using the base rocker environment. We updating the Linux. I'm creating a copy. That's copy everything from my directory, all the code, all the data, and all the functionality that I need in order to serve that. I'm running the script setup. And that's all the install library X and install library Y. And then I opened the 5001, which we discussed in a second ago. And then the entry point is the container start. It's served this SME base S1. That's my, I'm not really good at naming. I'm the first one to admit that. And that's where it's going to start. So it's going to start this server. Then essentially starts the plumber on 5001, which then show the RE base functionality that we saw over here, our base model. Now comes the fun part is the AWS infrastructure. I took this Docker, a Dockerized that. I put it on ECR. And that's the services from Amazon to where Docker can live. At a very high level now, and again, this is not a training on AWS, but I'm going to tell you essentially what we found was the right way to set this up. And a lot of it, you can use the wizard of AWS. You use FairGet for clustering. And we'll touch on this in a second. Behind application load balancer, behind API gateway. And there's some cloud watch for logs. So FairGet gives me the essentially the services that would scale up using the application load balancer. The method is how API gateway check is it a post? Is it a get? Is it part of the URL, the package? And the API gateway is essentially how AWS managed the connection to the external services. That is where the API will actually live. So in the next two minutes, I'm really going to talk about these components. And while you can read and learn about how to do this, and how actually to deploy that, where is the plumber and the R comes into place. So the first thing that you need to do is to create that cluster. So if you go to ECR and you create cluster, the first thing that you need to create is the task. And what's important in the task is that you need to link to the container. You need to say which container you're actually using. And very important over here is that when you open the container, when you link to the container, it wants you, you know, what report you're listening to. So the classic port 80 is, you know, open to port 20. Those are the classic one that kind of internal communication. No, use the 5001 that we open in the plumber. It's because that's how plumber is essentially sending out from the Docker container out to the environment, the recommendation. Oops. The second part over here is that now you need to go and create an application load balancer. You go through the wizard, you can see a configured load balancer, configure security and the like. But really important, again, when you get to the routing, that's the target, what it's essentially targeting. Again, you can see IP port 501. Again, you'll see that there's a path over here from the plumber that opened my function on 5001 through the task through the load balancer. The next step is looking at the service. So you go back to the clustering and now you set up a service and you select the service. It will ask you what you want to do, regular or fair get, choose fair get. The reason fair get, I should have mentioned that is fair get is essentially their serverless. They will manage for you how much you grow in compute power. So the more you hit it, the more Docker container will be spun out. I should mention that this is actually really, really, really fast with the new rockers. So this is actually, thank you. So you create the service and again, you select your virtual private cloud, etc. But again, what's really, really important is that when you create the service, when you get to the security group, essentially what IPs can go in and out, you look at your port for the application load balancer, you're using the application load balancer as a group ID and the port range again is 5001. Maybe we get to the application, the API gateway and this is an if the trick that I want to, I'm actually a little learned and I think this will be very interesting to everybody is that when you set this up instead of a post or get, you set it up as a proxy and then in the end point URL, you use the stage variable and I get it to real BT6. That's my stage, not going into exactly what stage is. It's essentially where you have all your variable that API gateway needs. The important thing is the curly bracket proxy curly bracket. This is important because that would say that when I hit it with a function name, it will go to the Docker all the way back to the Docker and it will rebuild the URL saying, you know, I need this location, etc, etc. That name of a function, so I can hit that Docker with any function that my R infrastructure actually serving any function, R based in this case, but it can be other ones. And this is essentially where this is the key over here is that means that I can control all my function through the R and API will just pass it all through, but only through the API. Essentially, it looks like that and you can see it coming from my client all the way to my proxy, hit the proxy and go back. I will finish this by actually showing you this is a so you I'll show you how that's actually work in practice. You can see over here, I set up a post, I set up over here the API. Yes, I know it's East one. I know there was issues over there, but that's why you need multi zone. But this is the name and you can see that I didn't name any what the name of the function. I'm naming it in the resource. And my my proxy will take this R based model and rebuild it and hit and it will spawn the Docker and we'll get a response back. And here's my post and you can see I'm providing the latitude and the user ID. So let's run this real quick. And what you can see is that we are hit we taking that of the this post request and I'm hitting that. And here's my recommendation. I just like that. I got a recommendation from my Docker that is based on our based on this first metrics that is essentially was built elsewhere. And now I can serve it to thousands of thousands of people all at once and or go down if I don't have to serve to anybody and and what a minimal minimal cost. This is really what we called in analytics as an API. Essentially, it doesn't have to be recommendation engine. Any sort of prediction that you want and you want to serve it at scale, you can use this type of ID of using Docker plumber, application load balancer, a fair get an API gateway. All right, that was a fire hose. It's really like to be a data scientist today. You need to be a full stack data scientist and understand the elastic load balancing and APIs. There's a lot to be doing. It's no longer just knowing how to fit a good model."}, {"Year": 2020, "Speaker": "Alex Gold", "Title": "Easier, Better, Faster, Stronger Data Science with R Packages", "Abstract": "For #rstats enthusiasts working in or with the public sector, it can be hard to promote the spread of R across your organization. Based on his experience working at think tanks, in federal consulting, and with a wide variety of organizations at RStudio, Alex will share patterns for treating an R package as a tool to promote better data science and more use of R. Daft Punk references will be plentiful.", "VideoURL": "https://www.youtube.com/watch?v=QspEWEcwpOw", "id0": "2020_16", "transcript": "We can say lots of things about Alex. He's a long time friend of the conference. I think you were even here for the first one, the first DCR gov one. And I can really feel you what's going on in your life right now. He's got it into a lot of home improvement projects. So last week, he's got a new electrical circuit board in the bathroom. He has a new electrical circuit to the bathroom so you could get a vent exhaust, which is a good project to do. And the fan is working great, he says. But there's also a whole delivery of ceiling where he accidentally stepped off the joist and threw the ceiling. This sort of seems a little bit like a National Antoons vacation going on here. So I hope that you are safe and that you eventually patched that hole, which is another home improvement project. See how one home improvement project leads to another. So with all his great home improvements, please welcome Alex to the stage. Thanks, Jared. Cool. Thanks so much, everybody, for being here. My name is Alex. I'm a solution engineer at RStudio. I just want to start off with a huge thanks to Jared and the Lander Analytics team. I love this conference. I love the Lander team. So it's just a delight to get to be here. And thanks to all of you all who are tuning in. My Twitter handle is down there at the bottom. So feel free to tweet at me if you like what you see. Or if you don't, that's cool too. Love hearing that. And I will tweet out a link to these slides after the talk. So the title of this talk is powerful for going along with my theme song. Easier, better, faster, stronger. I figure after my fun fact, I need to write some evidence. So this is what my living room looked like on Saturday, where I stepped right through. But this is what it looks like now. I started patching it. Like Jared said, I just made more home improvement work for myself. I have two more codes to do. We'll get there. That's going to be this weekend. I just wanted to introduce myself a little bit, because I think the credibility of what I'm going to talk about really depends on whether you think I'm credible. So I just want to share a little bit about who I am. So as I said, I work at RStudio now as a solutions engineer. But this is sort of by far the least DC job I've ever had. My background is in the public sector. I was in the DC think tank world for a long time working on economic policy research. After that, I went into politics and did data science in the political realm, working on voter outreach experiments. And I was a data scientist and a data science team lead for a federal consulting firm. So I do have a lot of experience working in the public sector and with the government. And during that time, several of those teams I went through transition. So I went through a state of TAR transition. For any of you who's background is an economics. I'm sure you have lots of fun stories about data like I do. I also did some Excel and Tableau to our kind of transitions. And now at RStudio, I work with customers that are going through that transition. And so that's a little bit of what I'm going to talk about today is sort of how to use R packages in that transition. So let's get started. So over the past couple days, we've heard a lot about like getting R into federal agencies. We've heard about sort of like some of the certification difficulties, how like getting the software prepped is hard. The security requirements are high, those sorts of things. I'm going to totally ignore all of that for the purposes of this talk. I'm going to assume that sort of like somebody else paved that road for you. Like Wendy and William gave those great talks yesterday about sort of how that process works. But even once that is done, you're not finished yet. Like even once you have R that's allowable in your organization. And that's true whether you're in like a federal organization, a consulting organization, or like something totally unrelated. You actually need to get people to use R. You need to get buy in. And because you're here, I'm going to assume that this applies to you. Like you probably want to do more R. You probably want your team to do more R. And so I'm going to talk a little bit today about like some strategies to get there. And of course, I'm going to talk about it using my personal favorite tool, which is of course the R package. And not just like any R package, we're going to talk about the power of building your own R package. So I'm going to start off talking a little bit about why you would build an R package. And I'm going to talk about the house or the strategy and also the mechanics of building your R package. For those of you who've built lots of R packages before, the mechanics piece is going to be pretty basic. But I think it's worth going through because the tooling has gotten so good, it's really easy to make an R package. And there's just no excuse for not doing it. So let's start off with why. Why would you for your team want to write an R package? And the answer of course is that it will make your work easier. It gives everyone access to the same set of tools and provides a pathway to get everyone sort of onboarded those tools, to get everyone using those tools. It's better. It gives everyone access to elite level tools. Everybody gets to take advantage of the knowledge of the most knowledgeable person on the team. It makes your work faster because your iteration will be shorter. If you do something this time with a project, you turn that into code. Next time that gets faster, it's this virtuous cycle. That's the whole benefit of using code to do data science. And of course, stronger, right? Because code in a package can be validated, can be tested in ways that are hard to do and just sort of like a vanilla analysis. It can make your analysis more bulletproof. You may even have requirements that your analysis be validated in certain ways. And a package can really sort of aid with how that all works. So of course, we're going to talk a little about like how to make our packages and use them to make your work easier, better, faster, and stronger. So you're here at this conference. So I'm going to assume you're like the R expert on your team, or at least like the R enthusiast, the R evangelist. And so like very often this story goes, and I've heard some great talks here at DCR talking about how to do an R package. And like Jared said, I've been coming to this conference for a bunch of years. I love this conference. And I think the first one of these conferences I came to, I went home and I was like, let's write an R package team. And so like the idea was like, I'm going to write this R package. It's going to be amazing. Then like team, everybody is going to use it. Like that's great. The problem is that like it totally didn't work because like this is what I call like the expert package development pattern. And this can work. There are cases where this works. But there are a couple of problems here, right? First of all, like adoption is really hard. So like maybe these people on your team are really familiar with R, but like maybe they don't like the way you wrote the package. Maybe like you didn't write things that were helpful for them or the API isn't good. Maybe like this person is like really a tableau person and like they don't really know R and don't particularly care that you've written in our package that doesn't really help them. Maybe this person is an Excel person. They're like, you know, doing whatever they're doing, doing all their stuff in Excel. Maybe this person is like a new grad and like they don't know what they're doing. They're just like figuring out, you know, what is this whole thing. And so, you know, adoption here can be really hard because people are comfortable with different tools. But the other big problem here is that like it assumes that the only relevant expertise is our language expertise. You know, but the other kinds of, there are many other kinds of expertise. You might want to sort of encode in your package like data visualization or like visual design, right? Like maybe you want to have custom GG plot functions in there. Maybe you want to have custom R Markdown templates or CSS for your shiny apps, right? Like that probably should go in your R package. Maybe you have like custom machine learning or stats things that want to go in there. Like for example, there was a at a previous job of mine. We had to derive our standard errors for estimates in certain ways. And like the person who understood the standard errors best and like understood the derivation of those standard errors was not the best R programmer. On the team. Or like this is one that comes up all the time, I think, especially in like federal context, but really like every place ever, which is that like the data model, right? Like the data access is sort of complicated. And like there's like that one person on the team who like knows how to actually access the data and the rest of you want to take advantage of that. But like, you don't all know, like this came up in a previous job of mine. One of the guys on the team, he was really, we were doing hospital data. And so like we had all these different data sets with like the admissions were separate from the diagnoses and like you had to match them up to do anything meaningful. But like there was one person on the team who like understood how they matched up. And so we really wanted to encode that into a package, but like that he wasn't in our programmer. And so like that was really hard. So I want to suggest to other models of package development that can be way more effective at actually getting your team to use those R packages. So the first one is, you know, particularly if you're a team where you have people using all kinds of different tools is like you going home all jazzed up from the DC are conference. You know, your job is to start the package. Maybe you write the package skeleton, but then the idea is that you get everyone else to contribute code to the package, right? Everybody adds their own code to the package. And the idea here, right? The goal is both that along the way, maybe you work with some folks, they learn some more are they get more comfortable with the R package. But the other part, which is even more important is that they get excited, right? This package then is theirs. Dave helped design the package. They helped write the package. This is just an incredible way to get buy in from your team on being like, yes, we are going to use R because it is awesome. Like we had experience writing this package and now we're going to use it. It's just it's a great way to get everyone involved and get everyone bought in on using your R package. I call this the everyone package pattern because everybody is contributing to the package. The other pattern I want to talk about is a pattern that is maybe for teams where sort of people are more comfortable with R, but you're trying to figure out how to make your projects more verifiable, more reproducible. And that is the sort of package per project. So like for every project you're working on, like folks work on a package that corresponds to that project, the per project package. And this is really a great strategy when like your primary goal is to increase the sort of testability and verifiability of your analyses. One thing I do want to remark is a lot. Sometimes this gets interpreted as make your project into a package. I am personally not a huge fan of that pattern. Maybe you are. Maybe this is where I'm going to get yelled at on Twitter, but I'm okay with it. I don't love the project as package. There is an R project and there is a package and they are distinct entities. And I really like to use them for their intended purpose. So I recommend that folks sort of adopt this kind of strategy where they have an R project. Oh, sorry, sorry, just a project directory that that's where the Git repo is. Then you have separate package and project subdirectories. They're parallel, but they're not the same. And then your functions go in the functions inside the R package, your tests go in the tests, and then you have separate like actual code that builds your report, does your app, sort of whatever you need it to do. Again, my recommendation is to keep those things separate because I find that this is a cleaner division of labor. It helps with separation of concerns better in terms of writing good clean code, helps with testability. If you happen to be using RStudio package manager or RStudio Connect, it just works much more cleanly in terms of the deployment pipeline. And so I really recommend this. If you want to read more about this actually, Miles McBane wrote a great blog post on this topic. I don't have the link handy. I probably should have gotten that, but he wrote a really good blog post. He's like an R community kind of guy and he also thinks the project package is great, but not the project as package. So anyway, that's my soapbox and I'm going to get off it now and just talk about some less controversial things. So now I want to do a quick demo of how this works in reality, how you can create an R package in really just a few minutes. So I've got about eight minutes left in my talk and I think I can get this R package written easily in the time we have left. So I'm going to pull up RStudio server here. And so here's a function that I wrote. First, let me just load it and I'll just show you what it does. So this function gives me a random lyric from the song, Harder, Better, Faster, Stronger by Dafpunk. I can also ask for a specific line from the song. So like that should give me back the same line. By the way, this function makes use of the genius package, which is a cran package written by my colleague, Josiah, at RStudio. It's very fun if you want to get song lyrics. So let's just talk through like what the code here is doing. Oops, on one. Where did my source pin go? There it is. All right, let's talk through what the source code is doing here. So we've got my function definition here. I'm grabbing all of the lyrics for this song using the genius package. Then if somebody, if this line was specified, I'm taking that or I'm going to sample a random line. And then this is just sort of pulling and returning that line. So again, I can show you that sort of to zoom in, it's hard to see everything. I can show you know, that's how that works. That's pretty straightforward. So now let's turn this into a package. So with the use this package, creating a package is so trivially easy. I'm going to do use this, if I spell it correctly, create package. I'm going to specify a name, daf package, great package name, Alex. I'm going to open my session. It's going to take just a second to load up. And what you'll see is that what use this does is it creates the whole package skeleton for me. This makes it super easy, right? Like for any of you who know about package creations, it's created in description for me. I can fill in like what this actually does. And it's given me my name space as well. It's actually empty. Unlike if you do new package up here, it doesn't have that hello world function, which I actually prefer. So anyway, I recommend you use this over up here. That's if you already create packages. And so now what I'm going to do is to now add my function to my package. I'm going to just copy and paste in my function definition. I'm going to save it, daf function. And so now what I'm going to do is I'm going to write some documentation for this function. I can go up here to code, insert oxygen skeleton, or I can just hit what does that command alt shift R to insert it. And I'm going to a good song. I'm going to write my documentation, which line defaults to NA random returns a character. And I'm going to put an example here. Right like that. So here are a couple of awesome things about this thing. One is that if I have an example here in the examples, I can just run it if I had loaded up my function. The other great thing is that now when I do dev tools document. So this is my favorite moment, the moment where I feel like I've written a real thing, where I go dafed lyric. And like, there is real function document that's taken on my credit. My moment, you're ruining my moment. Here we go. That like this is real function documentation that got auto generated from my comments over here. And this is like whenever I'm writing a new package, this is just like my favorite moment where it's like, wow, I did a real thing. That's so cool. Anyway, so now that I've now that I've rendered it, I'm going to load up my function and I'm going to run my function. Oh, it doesn't work because I didn't have the pipe here, right? Like this is actually not from my package. Same thing with deplyr and genius. Like those are outside packages that get referenced. And so I need to make sure I include those. So I'm going to do use this, use pipe. And it's that easy to add the Magrittor pipe to your package. And similarly, if I want to use this, use package, if I want to add deplyr and genius, I don't have to remember how to add them. I just like use the use this package. And in case any of you are curious where that goes, it gets added here to the description so that it now knows that it imports those. And if somebody were to load up my package for the first time, they would be auto installed. Those those packages would be auto installed for them. All right, so now if I do document again, load it up. Huzzah, it works. Do it best. Their work is over. Right? Like these are classic lyrics here, real poetry. Okay, so we have about two minutes and 30 seconds left. We're going to add some tests in that last two minutes and 30 seconds. So this is again, almost trivially easy to make this verifiable. We're going to do use test that. Test that is an R package that allows you to add tests really easily to an it is a package that makes it really easy to add tests to a package. A little bit of a mouthful. Now I'm going to run use tests. Daffed test. I'm going to open up my test file and let me make this a little bigger. I'm going to test lyric getting. Then what I'm going to do is I'm going to use these expect functions. These expect functions are going to make it really easy to get the expectations. I wrote out mine ahead of time here just to make this go a little faster. So what I'm doing here is I'm getting the lyrics sort of ahead of time. And then I'm checking with this expect true that my lyric that it returns is in this this lyric, which by the way is just a data frame that looks like this in case you're curious. Right. I'm checking that it's in this lyric vector. And so now with dev tools test. Going to load my package run my tests. I am a coding rock star. Wow. That's so exciting. It even gives you encouraging messages. It's one of my favorite features of of test that here. So anyway, so that's it. We created an R package. We added tests all of a sudden that work that I did to write this function has become so much more verifiable. It can be reused across different analyses. Like that was almost trivial to do. And so, you know, I cannot emphasize strongly enough how much this can be a tool for you and a tool for you to onboard your team to the awesomeness of using R. Anyway, that's what I've got to share. Thank you so much for tuning in. If you are interested in more about writing R packages, there is an R packages book written by Hadley Wickham and I believe Jenny Brian. It is free for use online. It is fantastic. Really just a great read if you're curious about our packages. We talked about using the use this and test that packages. If you're curious about then, like, once you've created the package, how do you manage it? I'm going to be doing the public webinar on this with RStudio. So stay tuned on the webinars, this webinars.rstudio.com. If you want to sign up, you can get alerted to our upcoming webinars to learn more about how you sort of manage your package environment. But the very short version is that you can use the R and open source package to manage a set of packages you have installed in your environment. And RStudio package manager is a product that our studio sells that allows you to manage the packages in the repository environment, including if you are offline or air-gapped. That's it. Thanks so much for tuning in and feel free to tweet at me. Thanks, Jared and Lander Analytics for having me. Thank you for presenting here. And if you want to learn more from Alex, he's doing a Meet the Speaker during lunch. So I believe lunch starts at roughly 12.25. Go to the RStudio booth during lunch and meet Alex and talk to him about writing packages or about how you patch holes in the ceilings of your house. Because both are useful skills, depending what you're doing. He did mention building packages using RE and V and RStudio package manager. And I've been using RE and V a lot. It's a nice way to keep individual packages for different projects. And he also mentioned Miles McBane has this nice blog post about project management. And so we've had both Miles McBane. We had that Miles McBane's talk of the meet up a few months ago. And he gave a whole talk about using Drake, which is a great way to manage projects. If you're not into the package based project, you can do the drink based project. So we go to nmyhackr.org. You can find Miles's talk there. But this week, literally two nights ago, three nights ago, on Tuesday, however many nights ago there was, three nights ago, we had the author of the Drake package, Will Landown come give a talk about the successor to Drake, which is targets. So you want to learn more about Miles and you want to learn more about Will, check out nmyhackr.org. Miles's talk is already up there. Will's is not posted online. I don't think you have to just cuss me straight from the meet up into the conference. It'll definitely be posted by next week. Those are great resources for you. I've learned a lot from Miles's blog post and from Will's talk and everything. Oh, Will's talk is up. I just heard him out. He just told me Will's talk is up. So you go to nmyhackr.org. You can see talks from both Will and Miles McBane talking about project workflows and everything. That's really fast work. Thank you, Omata and Michael, for getting this stuff up there so quickly."}, {"Year": 2020, "Speaker": "Anna Mantsoki & Imane El Idrissi", "Title": "FIND SARS-CoV-2 Test Tracker", "Abstract": "When the COVID-19 pandemic began, the Foundation for Innovative New Diagnostics (FIND) developed an interactive data platform to build a global picture on testing coverage. Imane and Anna will showcase how their team in Geneva, Switzerland built a comprehensive dataset on testing coverage across 179 countries, using automatic data mining tools (Selenium, R, regular expressions and GitHub actions), minimizing the needs for manual intervention and maintenance. They will also preview their user-friendly Shiny application (SARS-CoV-2 Test tracker) which allows users to visualize and compare the number of tests, cases, deaths and of positivity rate across countries and inspect changes over time. The FIND SARS-CoV-2 test tracker is, to their knowledge, the only source that updates world-wide COVID-19 testing data daily.", "VideoURL": "https://www.youtube.com/watch?v=N81JbR0BXeA", "id0": "2020_17", "transcript": "Out of the two speakers, Anna. She's been 2020 juggling being a data scientist and being a mother to a toddler, which anyone who's having to have a toddler has one knows that's no easy task. Full of energy and full of disruption and destruction. All right, and Eman, this is a story back when she was gonna be an exchange student on college, kept looking to get a secondhand bike, but she couldn't find one. It wasn't until she moved there that she realized why she couldn't find one. And I'm going to guess that answer is because of all the hills in Hong Kong. Hong Kong is very famous for the hills, which makes for dramatic skylines, and it's awesome, but it makes it very hard to bike. So please welcome to the stage Anna and Eman. So hello, thank you very much for giving us a chance to present our work here. I am Anna and Claudia, I'm a data scientist here at Eman. And I am Eman and Idrisia, a junior digest who is also at Eman. So today we're gonna talk to you about our fine SARS-CoV-2 tracker for our testing data. It has been really unprecedented here, if we might say, because it certainly feels that our world has stopped on its tracks after experiencing this pandemic. Certainly this is not the first time that the world is experiencing pandemics. There was the Spanish group, for example, in 1918, but I guess there are not many people here in the audience of that they have a first-hand experience on this. But back then around the 1.3 of the world's population actually was infected at the time. But one might argue that there are two key differences right now. We actually have a truly global economy. We're international travel and, you know, habits when we're getting our food from all over the world would allow COVID-19 to spread rapidly, much more rapidly. And also we actually have the tools like modern medicine and data access to be used in sophisticated sort of modeling techniques in order to fight this and allow the countries to track the pandemic. But even though this is a global pandemic, unfortunately, CO was tackled in national levels, as we've seen. And sometimes it was heavily politicized and science was sometimes ignored as well. So since the beginning of the pandemic, the WHO director, Dr. Edwards, which you might be familiar with now, has actually spread the message of test, test, test. But why is recasting really important? Testing is actually our main tool in order to combat this pandemic. It can actually help us do contact tracing and isolation of the confirmed cases. It has been actually preliminary results show that one death of around a thousand health workers screens can be inverted and actually using the right numbers. It can be more policy decisions. But cases, hospitalizations and death data, unfortunately, unfortunately, they're easy to obtain, but because they're quite visible. But testing data is not as if they are a bit more abstract. And even though they should be used in order to inform the metrics, the activity rates or infection rates in order to help us reopen our economies. So at a very, very, very stage of the pandemic, it became clear to everybody that testing was important, from patients to healthcare workers, including policymakers and governments. However, it was clear that there was no alternative taking care of the worldwide testing coverage reporting and neither guiding all the countries of the world on standards on how to record their testing data. So you've probably seen many of these dashboards describing the global pandemic. And this is just an example of the World Health Organization dashboard. They display many different epidemiological data describing the pandemic. You have cases and death numbers, but unfortunately, the test numbers are missing. This is a global pandemic and we live in a globalized world. Look at this here in Geneva. We basically cross the border between Switzerland and France several times a day without even noticing these. It is really important to be able to have a global overview of the testing coverage for community learning. And this was missing and we really identified a gap here. But fortunately, there are solutions and we have people believing in those solutions. And this is why, and I'm near here today. So we both work at the Foundation for Innovative New Day and Elastic. It is a non-propere organization in good health that was established during the 56th World Health Assembly in 2003. So our mission is to make the agnostics for infectious diseases available everywhere in looking for people in low-income countries. And in order to achieve that, we partner and we build projects by partnering with the public and private sector. So we work on different diseases, for example tuberculosis or malaria. And we also work on pandemic preparedness in general. And this is why we got involved in the Act A, which means access to COVID-19 tools accelerator. So it is a global collaboration to accelerate the development, production and equitable access to COVID-19 tools. And there are three pillars, the vaccines pillar, the therapeutics pillar and of course the diagnostic pillar, which is where the fine intervings. But in order to properly fulfill our role in the Act accelerator, it was important to have to know how well the countries in the whole world, how well they were doing in terms of the testing coverage. And since that was received, we tried to investigate country by country. So some countries decided to group their data. For example, we have here the dashboard from the Africa CDC. And they showed a very good example here. So we have epidemiological data on COVID-19 for all the African countries, including the number of tests. Other countries, unfortunately didn't decide to put together their data. And it almost looked like a competition for the fences and the most color for dashboards. So we have three examples here, Qatar, Switzerland and Sri Lanka. As we can notice, they report daily numbers for COVID-19 testing. And they report in a precise and structured way. And also we can see that their dashboards are actually in English, although their official languages are not necessarily English. Other countries didn't think of that. So there is clearly great effort put in for relating David before it, describing the national situation about the COVID-19 pandemic. But unfortunately, we can't really see that the people behind these reports, they didn't think of the accessibility of their data. So it's all unique to me. It's basically the sentence I told Anna was desperately trying to localize the testing number in this PDF all written in the week. And fortunately, she is from Greece. And you know, this is quite representative of Geneva and the foundation for innovative new diagnostics because it is a multicultural environment. And we have people coming from many different countries and speaking so many different languages. And thanks to this great diversity, we were able to basically find all the testing data from the different countries of the world. So at this stage, the data team have found decided to build a global tracker for the testing coverage of COVID-19. And Anna will tell you more about the whole story. Yes, thank you, Anna. So initially, having this very ambitious thing, we tried to show the global picture and we had to put together a dashboard like everyone was doing at the beginning of the pandemic in a very short time. Unfortunately, things, decisions were not taken that's, you know, thinking the outcome. And that picture was taken a long time below. Some of the problems were that because of the data were scraped manually from each website's report, Twitter report, Facebook report, you might imagine. And we were putting lots of time and effort on this every day. Then all the data sets that are used in the Shiny app, they were actually generated by the Shiny app, actually slowing down the application significantly. And then like countries, sometimes we also didn't think of the end user and that some users might need to use our data for their projects. So it was not very easy to retrieve them for the programmatically. And last, it was not very user friendly from the feedback that we got because there were too many paths and users were getting quite lost with all this information. So we decided to actually fix some of the problems and we developed and designed fetch functions in our community to parse these websites and reports from the countries that were actually at least reporting and things in a place that we could retrieve. And then we actually generated an R package where we put together the generation of the data sets. And we created an GitHub repository where we were pushing every day all the data sets produced. Also, we got an article like our filling and we're shaped application. In one, we go further with the fetch function. So from the diversity in the different ways the countries are three reported data. We found some similarities and we then grouped them and we assigned different types to the different groups. So some countries were assigned to type HTML, other is the type PDF and for more complex countries we decided to use Selenium. So we have a Selenium script that scrapes the data of some countries and then we generate a text file containing the scraped data. We have R script that goes country by country and according to the type, the country has been assigned. The R script uses what we call fetch functions and it uses the corresponding fetch function. So for example, if the country has the type HTML, the R script will use the fetch from HTML function and the pixel and you'll be fetch from Selenium and then leave the text file. So we end up every day with the file containing all the testing data, so the cumulative and the new tests per day. And then we put this file, you can push it and hit head and I will explain you how then from this head this is actually put in our test tracker. So as soon as we run pushes the file on GitHub with these triggers and automated workload for with GitHub actions, that actually it's a used three main functions of our package, which is process J boot data, which is actually a function taken together, the cases and best data from the John Paul Pins repository and creates the coronavirus on the short cases file. Then we have the process test data which creates a coronavirus on the short test. So it says, and the create shiny data which combines both of these data sets in order to create the data shiny data set which is then used by the Shiny app. All these three data sets are updated daily and they're found on our fine code 19 type repository on GitHub. Then we actually reshaped the Shiny app. As we said before, we wanted to be a bit more like to have an article live feeling so the user can go through all the different visualizations et cetera, I'll show you in a bit. We basically used the taper dash R package in order to encode the fluid row layout and we used or the E charts for R functions in order to render our visualizations for the map and the time series. Also we use a global data aspect where we put together our helper functions in order to read the files from our GitHub repository and filter it in order to be used in the Shiny app. So I will show you very, very fast sort of preview of our tracker. It's found on our, it's embedded on our main client website. Here we, you can see that we have the testing coverage and some, sorry, we have detailed instructions on how to use the application. We, the user can actually select the different outcomes, test cases, test, the taper terrain, their doggles for daily and committed data and for capital and total. So for, to show you how a user can get the very class inside or the data, since in months that before that our nation is to show data, especially if coverage is enough and equitable of testing. We want to show that there is a clear difference of testing coverage between high income and low income countries. You can clearly see it here on the map. They're in Africa, South America and Southeast Asia. Daily less capacity is far less, far lower than we actually in North America, Europe, et cetera. And the user can get actual visualization of this straightaway. Here we compare the data by country, continent or income group. The user can actually sort and choose the different categories that we put together. I will quickly select high income countries and low income countries, which is also an input for our track change over time, time series graph. It's an HR graph. So you can clearly track the pandemic and you can see that there is a striking difference between the number of tests performed in high income countries and low income countries. So we wanted to get across the semester really, really clearly. And then we can have the data downloaded even for the selection that was done in the mall or the user can actually go through the state relating to our data repository. So now what are the next steps for us? Tomorrow in more countries are actually decoying the use of rapid diagnostic tests, which is different from PCR tests. And it's really important to be able to discriminate between those two types of tests. So because we really want our instructor to respond to what is needed to tackle the pandemic, we want to go and evolve along the pandemic. And we will definitely start soon segregating data between rapid diagnostic tests and PCR within our tracker. We would also like to include the number of IDPs procured for countries and it is also a very important information. And at this stage, we still have some countries for which the testing data is taken manually. And it obviously would like to automate also for those countries. Probably we'll use CLI, Selenium, and GitHub actions because it's easy to maintain and less resource reliance. So all in all these tools have actually enabled us to access the data that were very scarce. And there were all over the place basically in multiple websites unlike the case data. So they allowed us to actually have access to these when we had really many constraints of time and resources here in clients. And we needed to produce a very high standard web application to put their out there in our website in order to add value on the importance of the testing data during this pandemic. We would really like to highlight that this testing data as a drug should be really important and should have their open access sort of repository. No matter if you have an outbreak or pandemic but in general in order to sort of inform surveillance and epidemiological efforts to inform research and have really evidence-based policy decisions. And also we need to have orchestrated efforts from governments, other authorities such as ministries of health, et cetera to reduce this sort of machine-readable data and open access in repositories where researchers can use them very, very fast. And we would like to help our colleagues here in mind. Christine, Stefano and Orena. And of course we would like to help Patrick and Christoph from Cinfra that have helped us reshape our application. And last Andrew that helped us put together the slides in order to get across our message better. Thank you very much. Thank you. Excellent talk. We're continuing along our day and our next speaker."}, {"Year": 2020, "Speaker": "Graciela Chichilnisky", "Title": "Financial Innovation to Reverse Climate Change", "Abstract": "How can financial instruments resolve climate change? Indeed, an interesting question. Here, Dr. Chichilnisky can show how this can be accomplished quickly and effectively by using existing capital markets and benefiting high- and, especially, low-income groups. The process Dr. Chichilnisky proposes is simple and can lead to a transformation of our capitalistic economy in the direction of human survival. Furthermore, it is realistic and is profitable along the way, supporting the transition.", "VideoURL": "https://www.youtube.com/watch?v=m0B6cCROaGQ", "id0": "2020_18", "transcript": "So she was my professor back in grad school. And it was actually a really small class, only five of us, the mathematics of economics. And I didn't realize this at the time, but she was one of the key authors of the Kyoto Protocol. And she was also a author on a paper that won the Nobel Prize. So when I asked her to speak here, I emailed her and she replied within an hour and I was super excited that we were gonna have the Kyoto Protocol offer and one of the authors of a Nobel Prize winning paper here. She also sent us her fun fact, which is actually something I didn't know about her at the time either. I knew half of it and not both of it. She never went to college. But then she got a PhD in math. And then she got a second PhD in economics. I have friends who have two PhDs and I have friends who have a PhD without going to college. This is the first time I seen an intersection of no college and two PhDs. So I figure I'm having no college, two PhDs and being on a Nobel Prize winning paper, we can give a video talk here. Thank you very much for being here today. Particularly thank you of course, for the heroes of the medical profession that are taking care of everybody and in this horrific pandemic situation that we're going through. I want to thank to all of you, not just for being here, but also for being part of the solution and not part of the problem. My name is Graciela Chichiniski and I am the co-founder and the CEO of Global Thermostat. It's a company that started in 2010 and we have a technology that can remove CO2 from the atmosphere and transform it into bio-fertilizers in foods and beverages, building materials, synthetic fuels, seawater desalination and enhance our recovery. In fact direct air capture, which is changing the way energy is produced the world over. And we're doing this with extraordinary partners that include the Coca-Cola company, ExxonMobil, early kid include Aqua, the biggest desalination firm in the world and Saudi Aramco. And more recently, AME and Siemens, the large engineering firm for the production of synthetic fuels that essentially are like gasoline without producing emissions. In the year 2019, Global Thermostat was chosen among the 10 breakthrough technologies in the world by MIT technology review curated by brigades. So direct air capture and Global Thermostat technology removes CO2 from the atmosphere and it stabilizes it, it embodies it into projects that stabilizes CO2 on Earth. So at the end of the process, you end up with less CO2 in the atmosphere and more on Earth. And it's accepted by National Academy of Sciences and the IPCC that direct air capture can indeed remove CO2, is feasible technologically, can remove it from the atmosphere as it's needed now according to the United Nations. It's not sufficient to limit it anymore, it has to be removed. But it can do so at low enough cost that when you sell it to what McKinsey says is projected to be a trillion dollar market, you make money out of this. So you improve the economy, you create jobs and you clean the atmosphere all at once. That is the purpose of Global Thermostat. First let me tell you where this is originated. This whole thing started 20 years ago, actually 23. When the nations of the world and 160 of them voted for the Kyoto Protocol and the US was one of the voters, and the Kyoto Protocol was a way to prevent catastrophic climate change. In 1997, I wrote into the Kyoto Protocol, I designed and wrote the carbon market of the Kyoto Protocol that works by putting limits on emissions and allowing those to go above the limit to compensate those that are below the limit. Just before this horrific pandemic started in December of 2019, physics today published an article about the fact that the carbon market is considered to be by economists in terms of the price of carbon as the necessary solution to climate change. And in fact, that carbon market that we created then, the first one in the world, the European Union emission trading system succeeded in its purpose so that as of now, 20 years later, and after becoming international law in 2005, it has decreased the emissions of 15 nations in the European Union. Those emissions are now 20% lower as they were when we started in 2005. That means the carbon market succeeded, but it means more. It means that that amount, that decrease in emissions of these 15 nations of the European Union, if extended to all the countries that signed the protocol then, particularly the United States, would have resolved climate change. That's what it means. So it means that we have really had a solution over 20 years ago. It was not implemented, and I want to talk about why. 40, but the most sufficiently implemented that 25% of humankind is now governed by carbon markets, which are the descendants of the carbon market that I wrote in the Kyoto protocol in 1997, and became international law in 2005. If we had decreased entire emissions of the world, by 20% below the level of the 2005 emissions, the situation now will be essentially resolved, and we would know how to go forward. But instead of that, there was a lot of concern about the impact that limiting emissions would have on the economy. Those concerns were particularly in the United States, because the US was the largest emitter of CO2 at that point. And because of that, in law, emerging in the United States, the World Seagal law, which passed in 1997, unanimously in the Congress, saying that the United States would not accept any limitations or emissions that hurt the global economy, or that hurt actually the US economy. And that World Seagal law, in fact, detained progress because there was the fear that limiting emissions as required by the Kyoto protocol would have such a negative effect in the US economy. So then you were saying the Kyoto protocol, but didn't rectify it and we didn't execute it, the US was the largest emitter. The European Union, 50 nations did, and they solved the problem, but we needed the burden to be more general. Now we have carbon markets in China, carbon markets in California, carbon markets in several states of the Eastern USA, and of course carbon markets in the European Union. But there was that fear that it may hurt the economy. So I was then the lead author of the IPCC, representing the US, and I know that if we didn't take action then something different would have to be done. And as part of that plan, I created the law of thermostat to agree on the solution that was requested, if you wish, by the Bertie de L'Eckt. To show that decreasing emissions does not hurt the economy, it can be helpful to the economy. And indeed, that's the purpose of global thermostat is to resolve the problem of climate change in a way that is commercially feasible through the production of all those services and goods with our partners and clients that I mentioned before. It is actually a global transformation because now there is a technological solution that technological solution has two levels. And one level, the company, global thermostat, created a breakthrough technology that removes your two derivatives of the atmosphere and it can do so in such a way that the carbon that is removed is commercially valuable and is used for those purposes. That technology, which is also goes by the name of direct air capture directly cleans the atmosphere, directly captures the CO2. And our firm can do so at sufficiently low cost that when you sell it, you make profits, you create economic development and you create very good jobs. The second technological aspect that is very important is that out of the Kyoto protocol, hundreds of billions of dollars traded were used to be uncent to developing nations including China, particularly China for clean technology projects. And China used their allocation for the scaling up of photovoltaic technology. And that means that right now, thanks to the Chinese effort and to the funding for that effort that was created by the carbon market in 2005 when it became low, we now can be sure that the kilowatt hour, electricity produced by solar energy is lower cost than the electricity that is produced by coal. And this is important because everybody agrees that right now, the world economy in the next 20 years is going to double this consumption of energy. So the solution here has two technological parts. One is a solar revolution, the photovoltaic revolution where you can produce solar energy much cheaper than you can produce it by using coal and fossil fuels. The second part of a third must have technology that can remove CO2 directly from the atmosphere. So the initial solution that succeeded because it took down by 20% the initial CO2 from the original level as of now, amazing. That solution was only partial because only the European Union adopted it so far. But the, I mean, that solution now was prevented globally by the vision that limiting emissions would hurt the economy. And now the new technology shows the opposite. It can be done in a commercially feasible way. It can be done while making money, while growing the economy and growing jobs. So that's in a way the second time that we will restore climate change. The first time it goes for the carbon market that all economists agree, the price of carbon is needed to restore climate change. And that was the price of carbon and it could do it. The second time is to show, and you can go through that solution in a way that helps the economy, in a way that produces jobs, in a way that leads to clean development, the clean materials, clean energy, held by the fact that the funding from the Kyoto protocol, the billions, the hundreds of billions that went to developing countries was used to scale out the photovoltaic technology leading to very inexpensive solar energy today, the undercuts fossil fuels, the production of electricity. This is a successful story, but it's not finished. We know what we have to do. We're doing it, and we have a fantastic extraordinary, I should say, technology partners, and clients that I already mentioned in that process. This is, we know we have to do that, but I wanted to say finally that, even the Pope, as well as many other thinkers, and other religious leaders perceive that the oil and gas industry plays an important role here. And the proposal here is to work with our technology partners, with two folk heroes. First, the creator of Earth Things, why we are here today. Tom will grow. Thank you so much. He has a vision of a consortium of oil and gas companies perhaps sitting here in Texas that can achieve the transformation, perhaps using our technology, but it can be done, and they can become part of the solution. Very important. That is supported by experts, by our company for sure, and our partners and clients, but also by experts like Ted Rossiver and Barack Lismar, all of us have this vision. We think that the oil and gas company can transform itself to part of the solution. We did it once over 20 years ago, and we can do it again. We're going to reverse climate change and overcome catastrophic problem with the space in humankind. We need to do it. There is no other choice, because if we don't do it, we won't be here to tell the story. So let's do it for the second time. Thank you. All right, everybody. I hope you enjoyed that talk. It's very fascinating to hear about the carbon markets and the effect it had and the ongoing effect of the photovoltaic market. So I hope everyone enjoyed that talk. Got a lot out of that. I wish Dr. Trims-Linnensky could have been here, but hopefully that video gave you all the information she wanted to present to you. So I enjoyed that, but also I'm a bit biased. So all right, so thank you across the yellow. And now we have-"}, {"Year": 2020, "Speaker": "Tyler Morgan Wall", "Title": "rayshader: Bringing Spatial Data to a New Dimension", "Abstract": "Data visualizations are no longer afterthoughts destined for the supplementary material section: Learning how to create beautiful data visualizations is a key skill to influence decision makers and engage the public with your research and results. In particular, 3D visualizations are a powerful tool to attract attention to your projects and draw people into your research, and R has become one of the best language ecosystems for reproducibly generating high quality 3D visualizations. In this talk, Tyler will show how you can use the rayshader package along with several other tools to generate stunning 3D figures, entirely in R. He will also demonstrate how you can combine your data with free and open spatial datasets to create these figures in only a few lines of code, directly from the source data.", "VideoURL": "https://www.youtube.com/watch?v=gBfmFZKudNY", "id0": "2020_19", "transcript": "He made a spur of the moment decision to jump on a bus to New York. Now I don't know where the bus is coming from. I'm guessing it was from the DC area, but I don't know where he was living at the time. So this could have been like from California to New York like, you know, one of the movies. I say, the spur of the moment, I don't know how to bust to go to the US Premier of the final Harriet Potter movie and camped out overnight in line. That's commitment, but he was rewarded for that commitment because he met both Rupert Grint and Daniel Radcliffe. So let's applaud him for his dogmatic desire to see Harriet Potter and welcome Tyler to the stage. Hi everyone. Thanks for the introduction, Jared. My name is Tyler Morgan Wall and I'm a researcher at the Institute for Defense and L.C. here in Washington, D.C. I took the bus from Baltimore, so it wasn't California, but it was still a fun time. So I'm the author of the Ray Shader package, a package that allows you to generate beautiful maps and 3D data visualizations in R. So it's an incredibly exciting time to be working with spatial data in R because the ecosystem for spatial data has evolved immensely in the past few years. So no longer is mapping purely the domain of experienced cartographers and closed source GIS software suites. Nowadays, you can create maps with just a basic knowledge of R and a few lines of code. So in this talk, I'm going to be showing how you can use Ray Shader along with free and open data sets to create engaging 3D visualizations entirely in R using no outside software. So I'm going to show you where these types of visualizations fall in your data as toolbox and how you can use them to bring attention to your projects and analyses and win the hearts and minds of your audience. So first things first, what is Ray Shader? Ray Shader is a package for 2D and 3D data visualization in R. Specifically, Ray Shader uses a technique called ray tracing to realistically simulate how light falls on landscapes and 3D surfaces. On the left, you can see how Ray Shader calculates the shadows cast by buildings in Philadelphia as the day progresses, using LIDAR data from the city along with the SunCalc package to calculate the position of the sun in the sky. Ray Shader can take these 2D maps and project them into 3D, as we see on the right. Ray Shader also includes functions to apply post-processing effects to enhance these 3D maps. So here, I've added a depth of field effect, which blurs areas far away from the focal point and a camera-than-yetting effect to add all subtle darkening by the edges like you get in a high-end camera. So some people would say that visualizing spatial data isn't special, that you just use the same techniques you use when you visualize non-spatial data and any distinction between the field of cartography and the field of data viz is more of a historical artifact. But I think there is indeed something special about working with spatial data. So normal data visualizations live in an abstract world of well-defined axes, usually situated in a pristine blank void. Spatial data, on the other hand, exists in our world. You just can't place an X and Y axis down, dump your data in and expect it to stand on its own. So these data here on the right are the major highways around Monterey Bay, California. But there's no way you would know that just from looking at the road data itself. You actually have to build the world that the data lives in so people can contextualize and interpret those data points. So spatial data visualization actually combines two distinct fields, cartography, which builds the map on which the data lives and what we consider to be traditional data visualization, lines, points, polygons, et cetera. So race shader has tools to support both of these fields. It provides functions that allows you to build the underlying map and also to add data onto that map. So race shader does all of this with a straightforward, pipeable interface where you use elevation data to generate shadows and maps. So here the Hobart object at the top of the chain is just an R matrix, a 2D array of elevation values at every point. To the right, I've plotted the data with base R's image function with terrain colors, which in my opinion doesn't really represent the terrain in a very understandable way. So we can use race shader to produce something much nicer. We can first generate a base color layer with the sphere shade function and race shader, which colors in the map using the angle and slope of the terrain. So we can already see how this is far better than the pure height to color mapping because what were once blobs of colors are now much more obviously mountains and valleys. I can add a water layer with the race shader function detect water, which detects large contiguous flat areas on the elevation matrix and colors them in. I can add some realistic shadows with the race shade function. I can add another layer of shadows to darken the valleys with the ambient shade function, which all in all generates a very nicely 2D map entirely from an R matrix and just a couple lines of code. So this is nice, but the real magic with race shader is how easy it is to transform this map into 3D. So we simply just take this output and pass it along to the plot 3D function, along with the original matrix of height values. So this function generates the 3D mesh represented by the Hobart matrix and paints the surface of it with the output of everything above. So you can generate either still snapshots of the map with the render snapshot function or call the render movie function to generate a spinning 3D gift like this. Now everything in race shader is customizable. That's the cartography part of the package. Changing the color palette is incredibly simple. There are seven built-in palettes, but you can create your own with the create texture function as well as customize the water color. When you create your 3D plot, you can either explore interactively by dragging the 3D model around or control the camera programmatically with the render camera function. So this is useful if you want to create a complex animation where the camera zooms in and around for the views of your data set. And this is also where working with 3D models in a programming language shines. So you can program an entire animation in a single for loop like you would script any other repetitive task. So on the right, I've plotted the camera angles phi and theta, along with a rendering of a diagram of the scene and a view from the camera itself. So the code showing how I generated this camera view is on the left and which shows how complex animations don't necessarily require complex code. Now, our planet's climate is currently going through an enormous transitory period. And due to the scale of this problem, many of us who work in government and in the public sector are often in the best position to tackle the problem of climate change. However, in order to have buy-in from the public, we need to have the proper tools to communicate these issues to decision makers in an engaging and intuitive way. One of my main goals with Ray Shader was to make it dead simple to make beautiful informative visualizations at this critical interface of land and water. So here all a user has to do to visualize a changing coastline is to set the depth the water level in plot 3D or actually just call the function render water. So Ray Shader handles all of the 3D rendering and slicing and you can customize the color and opacity of the water. If you aren't just interested in a map, but rather are interested in data overlaid on the map, Ray Shader also has the ability to add other types of spatial data. So here I have generated some fake bird tracking data and overlaid it in 3D on a map of Monterey Bay. So I can change it to either be displayed as a continuous path with the render path function or as individual data points with render points. Here these data are simply lat long altitude values passed in as individual numeric vectors. This interface of passing x, y, z data is fine if you're passing in small data sets, but most spatial data in R comes packaged in spatial data formats. So R has a vast ecosystem of packages to work with spatial data, but one of the most popular and fastest growing in the community is the simple features package SF. So simple features are a standardized way to encode spatial vector data and Ray Shader includes support for both spatial line and spatial polygon overlays. So all you have to do to pass in the simple features object is pass it into the corresponding ratio function, generate line overlay or generate polygon overlay and it will add it directly to the surface of the map. So you can also specify column in the asset object map to color like an aesthetic function in Ggplot. So to the left, I've again plotted the major highways around Monterey Bay, but this time in 3D. And to the right, I've plotted the buildings in Seattle where the polygon is colored by the height of the building itself. So while flat overlays are nice, Ray Shader goes one step further and allows you to integrate fully 3D data from SF with the render path and render polygons functions. So adding 3D data to both of these scenes is a simple one liner performed by passing in the simple features object into the corresponding function and specifying which variable maps to the third dimension. So to the left, I plotted the trails in Zion National Park and to the right, I've again plotted the buildings in Seattle, but this time as a 3D extruded polygon rather than as an image overlay. So you can easily add labels to your map using lat long coordinates at a scale bar or even just at a compass to give context to your spatial data. So all of these features combined to enable you to bring your viewers on a cinematic tour of your data set in a reproducible way all within a single R script. And one of the best things about working with spatial data is how much of it is freely available and easy to access. So the USGS provides massive amounts of data on the entire US, such as gridded elevation, building footprints, tree cover, water and streams, water tables, road and trail information, the list is endless. And usually the only thing you need to do to access this data is to create a free account with the USGS. So I recommend especially the USGS National Map website because it provides a nice point and click interface to search for and download multiple types of data. And you can really get a taste of what's available just by browsing that. Additionally, the Landsat program provides free up-to-date worldwide satellite imagery. So as an example earlier this year in the spring, I read a news article that was discussing the annual poppy bloom in the Antelope Valley California poppy reserve. So the bloom had just started earlier that week. So I went to the USGS Earth Explorer website and downloaded satellite imagery that was just taken a few days per year and downloaded satellite imagery that was just taken a few days prior and combined it with elevation data also from the USGS to create a 3D representation of the landscape with this year's bloom, which you can see here on this race shader map. So if you're working on a project that involves some form of constantly evolving spatial data, like let's say wildfires, it's an incredibly powerful tool to be able to quickly generate and automate the creation of these types of visualizations that represent current events, especially if you're dealing with the public. So if you've seen all this and you're thinking, well, I actually don't really work with spatial data. Are there ways I can use race shader to plot, let's say, non-geographic data? The answer is yes. Race shader can also create 3D GG plots directly from a GG plot object. So here you can map any variable that you'd normally map to color to 3D. What's nice about using GG plot to power these visualizations as a user versus creating entirely 3D, plotting a new 3D plotting interface is that you don't have to learn anything new. You can bootstrap off of your hard-earned, pre-existing GG plot knowledge to produce an entirely new type of data visualization. And this API for 3D GG plots is incredibly simple. So race shader just simply takes as an input the existing GG plots and then it extrudes at the 3D, which in my opinion can lead to some pretty awesome results. And I think it's far easier than any other 3D data vis software out there. So it's simple enough where you can actually play an experiment with your visualization, which as an end user, I think is really important because often you don't really know what you're doing ahead of time. You kind of play around with it. And with 3D data, with 3D data vis, that can often lead to kind of like a big bowl of spaghetti. But this provides a nice kind of more limited method. You don't get floating points, lines or labels, but you often get something right out of the box that works really well. So all of this together, I think that most people would agree that R is best in its class when it comes to 2D data vis, thanks to GG plot and even just the robust base R plotting system. But I wanted to make sure that it kept that title when it came to 3D data vis, and that meant R needed to compete with professional 3D rendering software, think less like a PlayStation 2 and more like something out of Pixar. So this required a bit of work, however, since there is nothing built into R to support high quality 3D rendering. To support these types of visualizations, I wrote the Ray Render Package, which is a path tracer written entirely in R and available on the CRAN. For those of you who are talking yesterday about the difficulty about getting outside software on government servers, this is key because while it might be a hard sell to get IT people to approve a standalone program like Blender, or many other 3D rendering software out there, if you are approved, this package comes for free when you create your local copy of the CRAN. So Ray Render is a powerful, flexible and extremely capable package for 3D rendering, but that complexity does make it somewhat more difficult on its own to pick up than Ray Shader. However, acknowledging this, I've written an extremely simple function in Ray Shader that integrates Ray Render and just requires a single function call, render high quality. So this function takes your current 3D scene and renders it using realistic light transport algorithms, resulting in a beautiful realistic rendering entirely in R. And you'll see a lot of my visualizations in this talk were made using this interface. So in my opinion, it's by far the most accessible way to start making beautiful 3D maps. You know complex software build instructions or hour long YouTube walkthroughs required, just call a single function and get a beautiful data vis as the output. So after all this, you might be asking yourself a question, well, when should I actually use 3D plots like these? What purpose do they serve? Where do they fall within my toolbox? And you're right to ask this because some uses of 3D aren't great. The 3D pie chart is the classic example. Pie charts encode data in the size of the wedge and usually the 3D adds nothing. Generally superfluous use of 3D is to be avoided, even if you make it look really good. And even non superfluous 3D plots have some downsides. So generally data can obscure other data that's behind it. The use of perspective can make comparisons between data points harder. And generally you lose the ability to pull out the exact value the Z axis is representing. However, there are some cases where the positives do outweigh the negatives. So 3D offers some perceptual advantages over color. Color perception varies from person to person. So it's an obvious difference to one might not be to another. And 3D can be far more engaging than a 2D plot encoding the same data. So here's an example. We have two plots that show the same data with one plotted in a traditional 2D color plot and the other plotted and animated in 3D. So this is the classic John Snow color data set here visualizing the number of nearest neighbors per infection site. Now these two plots aren't interchangeable in purpose. If you were submitting this data to a journal, you likely wouldn't use the 3D plot because the 3D projection makes the exact values difficult to read. The fact that the 3D plot is more engaging doesn't do anything for you in a journal since the hook in an academic article needs to be in the abstract, not buried in the paper. However, if you are trying to communicate these results to the public or highlight them in a talk, particularly to audiences who might not be intimately familiar with the data already, the 3D plot is far more useful. 3D and animation is a far better hook than a paragraph of text to readers who aren't already familiar with your topic. And Ray Shader provides you with the tools to easily generate these visualizations directly in R. So here's a real life example of 3D plots being used as a hook. So these animations were created by Dr. Robbie Bishop Taylor of Geoscience Australia and were used to promote their project of mapping the entire Australian continents coastline using 30 years of Landsat images. So the animation on the left is a traditional figure that you'd see in a journal, so informative, but kind of abstract and definitely geared towards experts as it assumes the reader is familiar with concepts like color ramps and contour plots. The animation on the right, however, was designed for much wider consumption. It uses the research to show something that people will be much more familiar with. The tides going in and out represented in a way much closer to reality than a color map. So this visualization actually ended up being featured on NASA Earth and got a lot of very good attention on Twitter when it was posted. I think the original post got like 300 likes, which was pretty good. And the 3D animation definitely helped bring in readers that otherwise wouldn't have known about the research and the work that this government agency in Australia was doing. And finally, the 3D toolbox provided by Ray Shader and Ray Render allows you to produce some truly stunning and informative visualizations to bring attention to your projects. So this past year I was tracking the Arctic sea ice and saw that we were having yet another year of record melts. So I'd seen polar ice barrel plots on the left before, which show the Arctic sea ice as time progresses, but I always found that they were a bit too abstract because it's hard to understand what a number like 17 million kilometers squared truly represents. So I use the Ray Render package, along with open data from the National Snow and Ice Data Center to show the actual shape that these values corresponded to and animated these values for the past 30 years. So this visualization ended up getting to the front page of Reddit and people generally agreed that adding the globes in the right made it much more understandable, which was only possible by the great spatial ecosystem and are this involved SF raster. So I'm going to show you the angle or packages. It's just a lot of a lot of different packages went together to make something like this, but it ended up being just a kind of a short script. And if you're actually interested in seeing how I made this visualization, I took a blog post to my website, TylerMw.com, where I explain the process and include all the code. So check it out if you're interested in the details or would want to produce some sort of other global maps like this. So I'm going to ask you a little bit about the information about Ray Shader. For examples, actually recommend going on Twitter and checking out the Ray Shader hashtag. So the R community has created a lot of fantastic visualizations, especially recently because November is the 30 day, 30 day mapping challenge. So that I saw, it must have been at least 100 different visualizations made using our and ratio that were just really great. The website, Ray Shader.com has all of the documentation and lots of examples along with a fairly extensive walkthrough. And on my GitHub, I actually have materials available from a masterclass. I taught with a pen use of program on how to make maps and 3D visualizations with Ray Shader. And I know there are some other talks about using LiDAR data. I actually go through that and show how you can use raw LiDAR data to create some maps that show the effect of your rising sea levels. I think in the example, I use Miami. But there's really a lot of really cool things you can do with our, and I have some examples of that. That's freely available. So thank you so much for Landry Analytics for inviting me to speak and thank you for listening. Thank you very much for that talk. I think we all enjoyed that. Whenever Tyler speaks, it always gets a lot of wows. He does some really cool stuff with his package and enables so many people to do it. So thank you for that fun talk. We have a few."}, {"Year": 2020, "Speaker": "Kimberly Sellers", "Title": "Analyzing Count Data Expressing Data Dispersion", "Abstract": "It is natural to consider a Poisson model to analyze count data, however such approaches maintain a constraining underlying equi-dispersion assumption (i.e. that the (conditional, when applicable) mean and variance equal); this assumption can lead to spurious results and inferences. Instead, much work has been conducted developing flexible alternative methods stemming from the Conway-Maxwell-Poisson (CMP) distribution -- a two parameter distribution for count data that contains the Poisson model (among others) as a special case. To illustrate the impact of such contributions, this talk focuses on CMP regression models and related R packages available to perform such analyses.", "VideoURL": "https://www.youtube.com/watch?v=VkpjB0bCmL4", "id0": "2020_20", "transcript": "So our next speaker is a member of Alpha Kappa Alpha, the sorority incorporated, I'm not sure the incorporated part is, but it's the same sorority as our vice president, elect Kamala Harris. So everyone, please welcome Kimberly. All right. Hello everyone. And thank you to the organizers for inviting me to speak to you today. As you may wonder what could possibly be the connection between my talk and Snoop Dogg of all things. But as you can see, the topic for my presentation today is analyzing count data, expressing data dispersion. And as you can see, this is done through my joint partnership at Georgetown University and the Census Bureau. Okay. So notice Snoop Dogg was counting one, two, three, and to the four. So he too can appreciate count data. All right. So for context with regard to regression analysis, of course, we're all too familiar with the idea of this being a fundamental means by which to establish the relationship between associated variables. For example, with the Census Bureau, one could ask about change in geographic units with regard to housing growth or decline. So if we're interested in the number of new homes in a region, then obviously most neighborhoods are stable. Right? So you would not expect there to be any new homes in that area and that influences the ideology of considering a zero inflated count model. And so with that in mind, of course, there are various models that are already in existence and popular in use. With the ZIP model or the zero inflated Poisson that underlies a Poisson distribution, which assumes equidispersion in the data, which means that the mean and variance equal. If we allow for over dispersion, I'm sorry, if we allow for over dispersion, then a zero inflated negative binomial is popular. More broadly for cases of where over dispersion, I should say, implies that the variance is larger than the mean in a data set. More broadly, if we consider over or under dispersion, where with under dispersion, the variance is smaller than the mean, then one such choice to allow for such flexibility would be through a zero inflated generalized Poisson distribution. However, it's worth noting that this model can express some peculiarities and issues under certain conditions. And so it promotes the idea of me introducing yet another alternative flexible model for count data, namely the zero inflated Conway Maxwell Poisson. To offer some background with regard to the CMP distribution, as you can see, it's based on a two parameter count model that generalizes the Poisson as a special case. The Lambda parameter here serves as a generalization of the Poisson rate parameter. And while the new parameter serves as a dispersion parameter. So for this value nu, and this Z function serves as our normalizing constant, it's important to note that when nu equals 1, this simplifies to our Poisson model, where, as I said, the nu captures the dispersion and the data. So if nu is less than 1, then it captures over dispersion. For nu greater than 1, it captures under dispersion. What's nice about this distribution is that it not only captures the Poisson distribution as a special case, but two other classical distributions as well. When nu equals 0, it captures the geometric distribution, again, where lambda is constrained to be less than 1 now. And when nu goes to infinity, it captures the Bernoulli distribution with success probability lambda over 1 plus lambda. When we think about a regression model underlying this distribution, the parameterization that I'll be working with here is that which originated by the original Conway and Maxwell paper and was revived in the Schmulli et al paper referenced above. However, there are other parameterizations that exist. And the interested reader can access my wires review paper with a student of mine, Bailey Permue. One of the nice properties of a CMP distribution, though, is that it has an exponential family form, where I present the log likelihood to you here, such that we have the ability to recognize the joint sufficient statistics for lambda and nu. And as you may recall from your statistics training, this exponential family form allows for a lot of nice statistical properties to exist for this distribution. Some other properties include the ability to a representation for the moment generating function and a form for its moments, where, as you can see here, even just focusing on the mean and variance, that form is not of a closed form, because of its dependence on that Z function, that infinite summation that I presented to you earlier. As such, where I present the representations for the mean and variance, they do have approximations that exist, where those approximations hold for the cases where the data are either over dispersed, hence nu is less than or equal to 1, or lambda is sufficiently small. Moving forward to thinking of a regression model, then we now have this framework where we consider the log-linear relationship between lambda and our covariates. Now, what's important to note here is that lambda is not necessarily the mean, except for the case of the Poisson model. And so more broadly, what we're looking at is a nonlinear functional relationship between, if you will, the mean of the data and the covariates. However, in the Sellers and Schwollie paper, we stand by the use of this relationship, because it allows us to clearly recognize the generalization that's attained from the special cases to this broader CMP. Again, for the case where nu equals 1, we are considering the case of a Poisson model, in which case this log-lambda relationship is precisely the usual or typical relationship that we consider for our rate parameter. As well, I remind you that another special case was the Bernoulli distribution when nu went to infinity. Here, the underlying logistic model, then, is likewise captured. If we move forward to consideration, excuse me, of a zero inflated CMP, then the idea now is that we are incorporating a generating process that allows for excess zeros. So with the underlying CMP model as our motivation, as you can see, we incorporate a third parameter, P, to consider the probability of having extra zeros in the data. And so we can write out the formulation for this zero inflated CMP model now, where now that we're going to broaden the scope to consider a regression analysis for it, we are again allowing, excuse me, a log-linear relationship for lambda and nu respectively, and meanwhile, a logistic model for P. So that said, it's worth noting that the zero inflated CMP, of course, captures the analogous special cases, as did the original Conway Maxwell Poisson. So for nu equal to one, we recognize that we capture the ZIP model or the zero inflated Poisson. When nu equals zero and lambda is less than one, we capture a zero inflated geometric regression. And if you think through our special cases, remember when nu equal went to infinity, we captured the Bernoulli. So one would imply that this is a capturing of a zero inflated logistic model. Well, what does that mean here? Because of course, in a logistic, the response values are either zeros or ones. Well, all that means now is that we maintain a logistic framework, yet we adjust in our success probability for P of having more zeros. So with all of this in mind, and presuming the log-linear relationships for lambda and nu and the logit for the P, the question now becomes how to estimate the corresponding coefficient values in those models. And we suggest doing so through the method of maximum likelihood estimation. And we're here because of the complex nature of the underlying score equations, one has to do this numerically, hence the need for R. And so we can perform optimization procedures such as the nonlinear, the bounded nonlinear minimization, or the direct nonlinear minimization, or simply a general optimization form, where in all of these cases, it's recognized that the function of interest is to be minimized. And hence, the function that one would supply would actually be the negated log likelihood form of the function of interest. And with this optimization, the advised starting parameters, or starting values would be those values with which say we consider under a Poisson model. So be it either regular Poisson regression or the zero inflated analog depending on your model of interest. And so with those starting values in place, then the optimization procedure can occur and we can obtain our estimates. And meanwhile, we can maintain the form of the underlying Hessian to establish our Fisher information matrix and thus obtain our standard errors. Next, with all of this in mind, a popular question that usually arises is how to perform a test to assess for the existence of potential significant dispersion in the data. And so a natural approach here would be to consider a likelihood ratio test, since much of the information is already contained under the hood, if you will, within our regression object. And of course, we remember that under the proper regularity conditions, the transformed likelihood ratio test statistic has a chi-square distribution with one degree of freedom. Alternative we can perform parametric bootstrapping as a means by which to conduct our test as well. Meanwhile, it's also worth noting, if one is interested in one of the other special cases, say the geometric or the Bernoulli, just recognize that since new is in the boundary in those conditions, we have to consider a chi-squared mixture for those instances. So with all of this in mind, my package, Compasson-Reg, is available on CRAN and can handle everything that's been discussed here. So it can fit the, excuse me, the CMP or its zero inflated analog model to data. And it can provide one with estimates, perform hypothesis testing or model diagnostics. It can handle constant or variable dispersion. And to illustrate this model and our package, excuse me, I provide us with the following illustration with regard to maternal health and mortality data. And so here, we are considering an example where we have collected data across various provinces in the Dominican Republic. And we're interested in understanding various aspects about maternal health and birth outcomes. And so as you can see here, we pull the data from online regarding information such as the number of deaths across these different provinces and variables such as the number of beds, the number of vaginal or C-section births respectively, the number of abortions, et cetera. And so the idea is to consider an association between the number of deaths that have occurred in each of the provinces versus some measure regarding these various explanatory variables. Okay, and of course we're going to do this using. Oh, since we're assuming or finding the data that they don't seem to present any excess zeros in this data set, we simply went with the basic C&P regression for illustration. All right, so what we see here is the call GLM.C&P is the functional call to perform a C&P regression where the usual formula is as one would always expect where we have the response till the explanatory variable of interest. And here we're just running a simple out illustrates a simple regression to associate the number of deaths with the number of beds. And so when we run a summary of the model output, you get what you would expect, namely, and I should be clear, the output is stretched over this and the next slide, where what's initially presented is the, of course, the natural coefficient table that exists for the coefficients associated with lambda and that is associated with nu here. And so as you'd expect, we have the corresponding estimates, standard errors, Z values and P values. In particular, I focus your attention on that P value for the, if you will, the gamma term that associates with nu. We notice that it's statistically significant here and that is reiterated in the output directing us with regard to nu. So we have a direct estimate for the constant dispersion nu. Notice it appears to be statistically significantly less than one. In fact, it's on the closer end to zero here and we provide the corresponding standard error. More output, as you see, which is presented, includes the sample size, various information criteria and convergence information. Focusing our attention on the AIC and BIC, we'll touch on this later. But it's important to note that because the data set was over dispersed, one of the reasons that this can happen, here I'm just pointing out that the optimization did, in fact, converge. One of the reasons for over dispersion is that the model may not be complete enough. In other words, we should consider more variables in our model. Doing that here, we find that the coefficient table now presents an output where the gamma parameter is not statistically significantly different from zero. And what this implies is that the value for nu is not statistically significantly different from one. What does this mean for us here? That under this fuller model, we have something that can be achieved apparently with Poisson regression. And that is more settling when we look at the information criteria and recognize that the AIC and BIC values have now been reduced, which is what we would hope for a more optimal or preferential model. And so with this information in place, we can now make the informed decision to consider, oh, sorry, for completeness, I'm noting that the optimization did, in fact, converge again. But now for completeness and more precision in our estimation, we can now go to the usual GLM and conduct our Poisson regression accordingly. And so now when we run it, what we're obtaining is more information with regard to the usual suspects. We have our coefficient table, we can look at statistical significance, we have more refined measures of the estimates in place here for understanding. And with that said, because I have now reduced my model consideration to the Poisson, I have reduced the associated AIC accordingly because I've removed the extra dispersion parameter. So while this is an illustration and the analysis is deserving of more study, one of the points of note here that came of it was the question of potential incorporation of offsets into the model. And the current form of the compost on reg does not allow for that. And so this is ongoing work that's in place now to try and incorporate offsets. So in conclusion, I hope you've learned that the ZICMP serves as a powerful regression, if not for explanation, but also for generalization, should you have over or under dispersed data, particularly under dispersed data. It contains common regressions. And the compost on reg package is a very helpful tool for addressing this matter. More work that has since been done with regard to regression further expands to what I call a zero inflated sum of CMPs, which contains a zero inflated binomial, Poisson, and negative binomial of special cases. The CMP doesn't, my work in the CMP does not stop here. It spreads onward to work in control charts, distribution theory, et cetera, as you can see here, a wide array of methodological developments. And so in closing and recognizing the time, I just want to thank the co-authors or collaborators that were involved with various aspects of the projects that led to this talk today. Thank you for your time. Thank you. And I noticed in the chat, it is nice seeing someone pronounce Poisson correctly, which I still can't do, despite my last may helping me on Twitter. Thank you for pronouncing that correctly. Because that was awesome. And like I said to, I was saying earlier, that's my favorite distribution. So I love seeing this talk really spoke to me a lot. And there's a nice, like, a lot of old fashioned statistics in there. The Census Bureau, I think as a conference full of data people, we can all say how much we love and respect the Census Bureau and the great things they offer to this country for over 200 years now, giving us this, not just the deciduous census, but the rolling ACS is so valuable as statisticians. I think we should all be very thankful and go thank Kimberly for not just Kimberly, but the whole Census Bureau for this. Very much tag Kimberly on Twitter because that was awesome. All right. So now we come to."}, {"Year": 2020, "Speaker": "Tommy Jones", "Title": "Exploring Diagnostic Test Accuracy with Plotly Dash", "Abstract": "Epidemics \u2014 as we have all come to see first-hand \u2014 are full of uncertainty. Diagnostic test results are no exception. Some tests are more accurate than others due to specific characteristics of the test, and in some cases it may be important to consider the trade-offs of various tests before deploying diagnostics to a population. Additionally, the accuracy of any test depends, in part, on estimates about the prevalence of infection, i.e. what percentage of people in the population actually have the disease. While knowing the true prevalence of infection is extremely difficult, statistical strategies can inform helpful estimates. It is important for decision-makers to understand these strategies and their limitations.", "VideoURL": "https://www.youtube.com/watch?v=Wo7hpYd6WN8", "id0": "2020_21", "transcript": "He spent $800 on Star Wars toys. I'm hoping those are baby Yodas and I want to say one baby Yoda or multiple baby Yodas. And does this have anything to do with your super fancy ironing board you told us about last year? I'm very intrigued by your spending habits. So everyone, please welcome government stooge, Tommy. Thanks Jared. So I'm Tommy Jones. I'm a member of the technical staff and an organization called Inkutel. Inkutel is often described as the venture capital arm of the CIA. And if you ignore the fact that we're not a venture capital firm and we're not part of the CIA, it's actually a fairly accurate description of what we do. What we are is a nonprofit strategic investor and we invest in technology startups on behalf of U.S. intelligence agencies. And being a nonprofit, whenever these investments make money, we then reinvest those initiatives that further our nonprofit mission of public service. One of those initiatives is BE Next, which is sort of like a biodefense research organization. And so I'm sure you can imagine over the last year, almost, they've been pretty busy with coronavirus stuff. And so since I'm a statistician by training, I got pulled into some of their work and I'm here to present today on some work in diagnostic testing that I did with my BE Next colleagues. So COVID-19 testing has been all over the news. Are we testing enough? Who should get tested? I'm not really going to talk too much about these things that you might have been hearing about. I am going to talk about testing errors, however. I'm also going to talk about the FDA's emergency use authorization. So there's been an EUA in place for COVID-19 diagnostic tests since about February 4th of this year. And if you want to go research on these tests, what you would do is you would go to the FDA's website, you would scroll to find the tests that you want. And then if there's particular information about the tests, say what sort of platforms it's compatible with or what sort of clinical evaluation data did they report, you'd have to scroll through this PDF to try and find that information. And every PDF is different because the manufacturers themselves produce them so they're non-standard as forms. So we built an application to help with sort of researching these tests. I'm going to talk about that. But first, let's back up a little bit. We'll start with a primer on COVID-19 diagnostic testing just so we're all sort of on the same page. And then I'm going to talk about, you know, why tests fail? Why do we get false positives? Why do we get false negatives before turning our attention to this application that we built? The front end is in Plotly Dash. The back end is, at least for the calculations, is programmed in R. So there's three major types of coronavirus tests. The first and most common is the molecular PCR test. You probably heard a lot about it. That's the one where you get the cotton swab shoved up your nose to collect a sample. They ship that off to a lab for processing. PCR tests measure the presence or absence of the virus directly by measuring the presence or absence of viral RNA. The two types of tests are antibody and antigen tests. I know there was a lot of press about antibody tests, particularly earlier in the pandemic. We haven't heard as much about antigen tests until the last month or two, I'd say. And antigen is a substance that's around the virus that induces an immune reaction, and the antibodies are that immune reaction. So antigen like PCR tests sort of measures the presence or absence of the virus by detecting something directly on the virus, whereas antibody tests are indirect. They're measuring their body's reaction in the presence of some sort of infectious disease like the coronavirus. So PCR tests and antigen tests can be used to diagnose current infection. Antibody tests are used to diagnose recent past infection. They can in theory be used to diagnose current infection, but current CDC guidance says that antibody tests for coronavirus are not reliable enough to make such an important diagnosis decision. You know, the PCR tests and antigen tests are also something that can detect the presence of the virus or come back positive sooner because it can take the body a while to ramp up its antibody reaction before that can be detectable. But why do we do diagnostic tests anyways? So in a clinical setting, we probably want to answer two questions or one of two questions. Does the person tested have the disease or does the person tested not have the disease? To answer the first question, you need a test with a high positive predictive value, meaning if the test comes back positive, it's likely that the person has the disease. And to answer the second, you need a test with a high negative predictive value, meaning if the test comes back negative, then it's likely the person doesn't have the disease. And unfortunately, a test can be good at one and not so good at the other. Or the test could be good at both, but the conditions and context in which you're testing, which I'll get to in a little bit, can affect the accuracy of that test. So this third question about population monitoring is one that's incredibly important and statistically fascinating, but it's also out of scope for my talk, so I'm just going to move on. But now we're going to turn to why tests fail. So why do we get false positives? Why do we get false negatives? And I'm going to handle each of these in turn. So this chart has got a lot going on. What it's plotting is over the cycle of infection for a particular patient that's on the x-axis time, what is the likelihood that we might be able to detect the presence of whatever the substance is this coronavirus test is testing for? And I'm not going to walk you through every little line here, but needless to say, you can get false negatives even if you have the virus if you test too soon, because there's just not enough viral load to detect, or if you test too late in the infection cycle, because again, not enough viral load to detect. Another reason we can get false negatives are from poor sample collection. So there may you may have the disease, but whatever sample we took, be it of the cotton swab or blood or spit, there just wasn't enough viral load there to be able to detect it. PCR tests seem to be particularly prone to this as that Q-tip is very uncomfortable for you, it's very uncomfortable for the person giving it to you. False contamination is a reason you can get false positives. So your sample may itself not have any presence of the substance that you're trying to detect, but it could be run through equipment that does have that on there, and so you can get a positive result from that. But when we turn to the statistics, this is where things get a little bit more nebulous. So first, there are four accuracy metrics to measure how accurate a test is in particular circumstances, right? And they have four distinct definitions. I already talked about the bottom two a little bit, although we'll come back to them, but sensitivity and specificity are what you're more likely to hear about when people are talking about accuracy of diagnostic tests. The reason for that is because sensitivity and specificity are calculated during some sort of clinical evaluation of performance. And if they're measured well, they're just sort of immutable characteristics of the test. This is how well the test performs given these definitions. So we know what they are. But if you remember from a few slides ago, what I said was for the clinical questions that we actually care about for diagnostic testing, what we want are positive predictive value and negative predictive value, because they're that direct measure of accuracy to the thing we care about. The unfortunate problem is that PPV and NPV are calculated as interactions between the characteristics of the test and sensitivity and specificity, and the infection rate and the population that's being tested, or in the case of an individual, the prior probability that that person has the disease. It's very rare that we actually know with some accuracy what the infection rate is in a population being tested. And it's impossible to quantify somebody's prior probability of infection when they walk in the door for the test. That doesn't mean that all was lost here. This is a pretty statistically literate audience. We've probably all had the example when you were learning Bayes rule about this curious result from diagnostic testing, where when you apply Bayes rule, the probability that you have the disease given that you got a positive result is maybe a lot lower than we expected it to be. But I think where we've failed in sort of statistics education is to draw a line from that curious result to what actually happens in the real world and why are diagnostic tests still reliable under certain conditions. On the other hand, if I can wag a bit of a finger at some in the biomedical community, I've had conversations with folks where they're just blindly accepting the test result and seeing it as this is the gold standard and my clinician judgment or other factors don't matter if you've got a positive result. What I wanted to do is spend some time here and give us some intuition about what these numbers actually mean and how they play off each other. So looking at this chart on the left, we've got positive predictive value on the y-axis. On the x-axis, we have prevalence, the infection rate, or for an individual, it's that prior probability of infection. So for a population, it's the infection rate and that PPV is going to be basically an average of the positive predictive value across everyone who you gave a test to. That's not really helpful again in that clinical setting because you care. I care about me and I want to know, do I have the disease given that this test result came back? So think about it this way. If I have no prior reason to believe that I've been infected, I've been staying home, I've been seeing people, nobody that has been around me has been diagnosed with coronavirus, I'm showing no symptoms. My prior probability of infection is quite low. I'm on the left side of this chart. However, if I've been going out, huge parties, a bunch of people I know all got sick and I got positive diagnoses and I'm starting to develop a cough, I'm now moving way right on that where that test diagnostic is going to be pretty conclusive. I actually lived through this recently. I went in for a physical, my doctor ordered an antibody test and I told him, hey doc, I have very little prior reason to believe that I've been exposed to coronavirus and I walked him through all these statistics and he said, that's all fine and good, just humor me and I said, okay, fine, if it comes back positive, we're going to do another test because I think it's going to be a false positive. So lo and behold, I get a positive result for the IgM antibody, but a negative result for the IgG antibody. And so taking it face value, if IgM is positive but IgG is negative, that could be indicative of current infection, not past infection. Okay, so I started out very close to zero on the x-axis. I get a positive result. So what that means is now my posterior probability of infection is probably about 40%. I'm going to go schedule a PCR test and if that comes back positive, I'm starting at about the 40% line. So if it comes back positive, that is really strong evidence that I'm infected. In my case, the PCR test came back negative that was in fact a false positive, but I hope the little anecdote has helped sort of marry the math with what's actually happening in the real world when we do diagnostic testing. Very similar result for the negative predictive value, except now when the prevalence of the disease is low or the prior probability is low, negative results in the absence of other information are pretty strong evidence of not being infected. But when you start getting into areas where lots of the population has the disease or you've got a patient who has a very strong prior for being sick, then the negative results themselves can be suspect. So turning our attention back to the FDA's website to our public health official, you want to know about these tests. You don't want to comb through all of these PDF files. We built this portion of the application so that you can sort and filter that information. I'm not going to spend a lot of time demoing that just for the sake of expediency, but I am going to show you this part of the tool that focuses on test accuracy in terms of PPV and NPV and lets you interact with all of the tests that have received FDA emergency youth authorization for molecular PCR and antigen, but we haven't done antibody tests. And so with that comes the most nerve-wracking part where I have to do a live demo and hope nothing crashes. So here, like I said, covidtestdb.com is this sort of a list. If you're a public health official in the biomedical field, there might be a lot of relevant information here about which tests are compatible with equipment that you may have in your lab. But I'm going to go straight to the performance. So covidtestdb.com slash performance will drop you into this application. It'll start off in a document that we wrote that covers a lot of the material that I've already done in the presentation so far. But if you go into this results tool, here's where you can start browsing tests. This picture should look familiar. I just want to say that something I glossed over before is this shaded region here. That's a 95% confidence interval. So like I said, we pulled the data from the FDA's website out of these, the data they report, the manufacturers reported. These are finite sample sizes. So we didn't just calculate the expected PPV and NPV, but we could also put a 95% confidence interval on it. And so here you can select a test. There's dozens of manufacturers. I'm going to go with Abbott here. They're pretty popular. And then from there, you can select whichever tests they've submitted to the FDA. Here you'll notice we've got one test, but then multiple sources where the sources find DX. So a couple of speakers ago, we had a couple of speakers from Find. We've been supplementing the data that we're pulling out of the PDFs with data that Find is collected that's often independent evaluations that maybe have larger sample sizes than what the manufacturer reported to the FDA. Here I'm going to go with this Abbott AG card. This is an antigen rapid test. It's received a bit of press recently. On the left, you've got this positive predictive value chart for the Abbott antigen test. On the right, we've included data from an influenza rapid diagnostic test. The purpose here is so that you can see how does my coronavirus test compare to a flu test that is used and trusted regularly. So you can see the overall shape of these curves are the same. Like I said, the main properties here are properties of testing for relatively rare conditions, not anything unique to coronavirus, but the coronavirus test has a much wider confidence interval because there's more data backing up the influenza test. If you scroll down for those that may be less mathematically inclined, we've got these text boxes here that say for the selected prevalence level, if you give 100,000 tests, here's how many we would expect to come back positive of those. Here's how many we would expect to be true positives and false positives. We've got the same thing for negative predictive value, including the text here. And you can update that dynamically with this slider. So I went from 5% prevalence to 10%, and the text here has now updated. So you can then use that to explore how you think these tests might perform based on the data that's been reported and conditions that you believe may affect you as an individual or the population in your jurisdiction. And the last thing I'll point out is that you can also download the data that's backing up these charts so that you could do your own analyses with the data that we calculated across the whole range of possible infection rates here. So with that, I've got a couple minutes left, so I'm going to bring us home. You're going to see a lot of charts like this in the tool. And basically what happened is in the rush to get these diagnostic tests out to market, you know, we're in an emergency, people need to move fast. What the manufacturers have gone with have been very small sample sizes and often what are called like spiked samples. So they're contrived samples rather than being taken from a person that they've confirmed has COVID-19. They've then spiked a sample with it. And so, you know, we see 30 positive samples, 30 negative samples all perfectly classified. It doesn't mean the test is bad or the data is bad. It just means that we don't have enough data to really say for sure how accurate this is. This project is still in active development. So, you know, I'd encourage everyone that might be inclined to get involved. Check out covidtestdb.com, covidtestdb.com performance. If you have questions, comments, feature requests, want to help with data, you can email us covidtestdb at vnext.org or you can check out our GitHub repository there, you know, forecast on GitHub, open up a feature request. Big team went into this that cut across several business units that didn't you tell. So big thanks to them. And that's all my time. Thank you so much. Thank you, Tommy. And he didn't disappoint with the tweet jacket. You followed through any word. All right. I think we could all agree. COVID is a very important thing we've all been discussing back in April when we went virtual for the meetup. Very first talk was Noam Ross talking about some of the math behind epidemiology. And the next month was Rami Crispin talking about the data sources. So I think it's a data community. We could all hopefully be helpful with the coronavirus outbreak. So that brings us up to the."}, {"Year": 2020, "Speaker": "Yvan Gauthier", "Title": "Using R for Advanced Analytics in the Department of National Defence", "Abstract": "Yvan is a senior defence scientist with Defence Research & Development Canada (DRDC).  Since 2017, he leads a data science team supporting the Chief Data Officer of the Department of National Defence (DND). He will present how his team has leveraged the R ecosystem to deliver advanced analytics in support of different DND initiatives, such as COVID-19 response planning, financial forecasting, survey analysis, and predictive maintenance modelling.", "VideoURL": "https://www.youtube.com/watch?v=j2s18Ua80LM", "id0": "2020_22", "transcript": "Since the beginning of the pandemic, he's been teleworking, so he's been doing it from a restored 1976 trailer that he calls his mobile data science lab. But he lives near Ottawa, so it's getting kind of cold in there. So I'm not sure how much longer he's going to last. Everyone, please welcome Ivan to the stage. Thank you. As Jared just mentioned, I work for the Department of National Defense in Canada. And today I'm going to talk to you about how we are increasingly using R for advanced analytics and also show you some examples of projects that my team has done recently. But before, I know it was a recurring team in some of the previous editions of this conference, but the military and defense organizations in general, in many nations, have a long tradition of data analytics. This is a picture of Dr. George Lindsay. He's considered to be the father of military operations research in Canada. And he's been leading the organization. I work for almost 20 years back in the 60s and the 70s. And just to give you an example of his work, during World War II, George Lindsay was working on predictors. Predictors are the fire control systems that were processing signals from the first generation of radars and using that data to predict the motion of German aircraft and shoot at them automatically. So predictors were initially mechanical systems before they became electronic. And that's how George Lindsay described them in a paper he wrote a few years before he passed. He said today's such problems would be classified as artificial intelligence. And he was right. I think some people would call that prescriptive analytics or automated decision making. But in fact, this was done in the early 1940s is, you know, pretty impressive. Another interesting fact. George Lindsay is also one of the fathers of saber metrics. So in the late 50s, he was already publishing papers on metrics that can be used to compare baseball players. That's 20 years before Bill James started to publish books on this 45 years before Moneyball. So that's also very impressive. All that to say that data analytics is not exactly new to the defense community. Now, of course, what constitute data analytics is something that is still a little bit debated. Some of my colleagues in D and D see analytics as this new great big thing. Others based on examples like the one I just showed you are asking, well, isn't it the same old thing that we have been doing for 75 years? And my answer to that is always yes and no. Because traditionally when a personal analyst in D and D apply their statistical expertise or their scientific expertise to a problem, they tend to produce one of analysis that gets to be published in a report or a paper or a briefing to a client. But data analysts in D and D rarely produce models that are deployed in our enterprise systems to be reused on a regular basis or models that will transform new data coming in without having a scientific experience. Without having a scientist in the loop to process it or interpret it. And to me, that's what modern data science requires. That's what modern data analytics requires being able to leverage a fairly large amount of data that we have in our in our systems, develop data products that will ultimately live on those systems. Very often using cutting edge tools and cutting edge statistical techniques. And that's exactly what my team is doing. You can see us here in a picture taken before COVID. Since then a few more people, including students have joined the team. And I will show you in a minute, some of the great work that they have been doing recently. Just to be clear, we're not the only data scientist in D and D. And we're not the only scientist in D and D to do machine learning. But we are the only team to be embedded with the chief data officers organization and we're the only team focused on developing models that will ultimately be deployed in our enterprise systems for routine decision support. We don't want our models to be used exclusively by scientists. We want them to feed dashboards and applications that will be used in a return fashion by planners decision makers. And that is really a it's a bit of a culture shift for us in D and D. It's also a culture shift from an IT standpoint to be really effective at doing that kind of enterprise data analytics. You need the right tools and the right infrastructure. And most of the tools that we have been using historically in D and D for data analysis are not very well adapted to modern data science. And this team was formed. A few years ago we made the deliberate choice to use almost exclusively open source tools for a number of reasons. But I can summarize these reasons maybe into one. As as when Griskey said that one of our great Canadian heroes skate where the puck is going to be not where it has been. So that's true for hockey. That's also true for IT as well. In the past most of the data analysis that was done in D and D was done using commercial tools like these. But when it comes to developing and deploying advanced analytics models these days open tool open source tools like our Python spark are really the way to go. And I know I'm preaching to convert it here. But I think it's a message that we need to keep relating within our government organizations. It's not just a question of getting access to the most cutting edge tools. It's also a question of getting the best talent possible. And if you want to recruit good talent you have to make these tools available within your organization and make them easy to use. That's easy to say of course but in practice it's not so easy. There are often IT roadblocks as a few people I've already mentioned in this conference. What has been a game changer for us in D and D is getting access to commercial cloud. Access to cloud gave us amazing data science virtual machines, sandboxes, more computing power, get lab servers to share our code. And also a means to deploy some of our models into production. And the good news is that in the government of Canada. We have recently adopted a cloud first strategy in which cloud is the preferred option for delivering IT services. So that will hopefully make the life a lot easier for all of the data scientists out there across the government. Okay, so let me now show you three projects that my team has been working on recently. I selected projects for which we have used our, but I also tried to select projects that I think will resonate the most with other government departments that are represented in this conference. Starting with some of the work we have done in support of Operation Laser. Operation Laser is the Canadian Armed Forces response to the pandemic and a good part of it involves supporting other government departments and provincial governments requesting military assistance. So for example, over the last few months, the Canadian Armed Forces have assisted remote communities in the north of the country. They were also asked to provide assistance in over 50 long term care facilities in Ontario and Quebec to do all kinds of medical support and logistical logistical support task. But of course any good military operation starts with good planning and good intelligence. And at the very early stages of the pandemic, our medical intelligence staff was already trying to predict how the pandemic would progress and where requests from requests for assistance would come from. And although there are quite a lot of models and dashboards available to do that these days back in February, March, there was not that many tools available to provide that kind of information beyond maybe the dashboard from John Hopkins. So what Talia Beach in my team did is to produce this animated map of the virus progression over time and space. And this was done using R and using open source data collected by the University of Toronto open source shape files with postal code areas from statistics Canada. And that map was probably the most geographically detailed picture of all the virus was progressing in Canada. So that map became very popular very quickly within D and D and even with other government departments. And it hasn't formed some of the planning decisions related to a blazer. Initially the map was just an animated GIF. And a few weeks after we released up Talia embedded the map into an R shiny app, which enabled users who have more control on it, picking an exact date, zooming in on a specific region. And not only that, Talia added some predictive component to the maps so the users can get an estimate of the number of new cases in each region up to seven days into the future. So that's, that's, that was a pretty cool project. My second project that I want to show you is about March Madness. Unfortunately, it's not about basketball. It's about finance. Because you have to know that in the government of Canada fiscal years are ending in March. And at the end of the fiscal year is when a lot of contracts with the government are ending. That's when a lot of goods and equipments are received. So it's not uncommon to see managers spend up to 20% or even 30% of their annual budgets during the very last month of the year. And that's why we call this March Madness. So, if certain managers cannot spend all of their remaining budget in March, because a contract may have been delayed or because of piece of equipment, as not been delivered on time. And when that happens, it's often too late to relocate the money elsewhere. So, in an ideal world, you would like to predict early in the fiscal year, say after just five of five or six months, which managers will understand which managers may actually require more money. And reallocate your resources accordingly. Problem is, it's not that easy to predict how much money managers will spend early in the fiscal year. So, what we have done here is try to leverage the huge amount of historical financial data that we have in D and D to better understand some of our spending patterns and use these patterns to make predictions. There are probably different techniques to do that like time series analysis, for example. But the method we initially looked at is to build a regression model that predicts the amount spent at the end of the year based on a number of potential variables. For example, if you plot on much a certain manager has spent at the end of the fiscal year against all much he has spent after only five months. You see that, you know, there's clearly a relationship between these two things over the years. And you can exploit that relationship. You can do the same thing with other variables, how much money was committed, how much money was allocated, and you can build a pretty good linear model out of that, which is exactly what. As we have studied hunasicara in my team and he's on on this conference. That's what he has done. He built a regression model that he implemented into shiny app, right now this shiny app is not directly connected to our financial system. But it can take data extracts from our financial system and generate pretty good forecasts in red glare with the 95 comparison, 95 confidence interval fairly early in the fiscal year 360. This is just a prototype, but we are working with our financial branch to test it on different funds with a view of deploying that kind of statistical model directly into our financial systems in the future. My last example is about the public service employee survey. This is a survey conducted every year in Canada to get the views of all the employees of the federal government about their workplaces. So it's a fairly comprehensive survey with over 180 questions that covers different teams like employee engagement, leadership, workplace, compensation and so on. And the goal of the survey is really to allow the government departments to identify what they're doing well and what they're doing not so well in terms of managing their people. So all the departments have a close look at these results every year and they develop action plans to address issues that are raising this survey, which is good. But most departments at the moment are still analyzing the results in a very traditional fashion. So for example, they build dashboards and they look at where they're scoring high, where they're scoring low, almost their scores have changed since the last survey. And obviously that's a natural place to start, but it's not necessarily the optimal way of exploiting the data because this survey has a lot of questions. So it's often hard for the departments or the department branches to figure out what part of the results they really need to focus on. So one approach to that problem that we have successfully used in my branch of D&D is to do peer comparisons. So for example, if you take the score obtained by a branch to a specific question and you compare it to those of all the other branches in the government, you can quickly find if you are in the bottom 10% or in the top 10% of all the branches. So that's a very simple approach, but it tells you very quickly the kind of results that you should be have a closer look at. And we have done dashboards to allow departments to do that kind of analysis very quickly. But even with that kind of tools, the results of the survey are not always easy to interpret or address because of how certain questions are framed. For example, one question of the survey is, I am satisfied with my department and people have to answer on a numerical scale to what extent they agree or disagree with that statement. So if a department gets a low score on that question, can you interpret that? How can you explain what causes employee dissatisfaction and more importantly, what can you do about it? So that's something we've tried to answer as part of a project we did in collaboration with the National Research Council. We developed a new set of dashboards using R that first have a look at how the survey responses are correlated with each other. So in this case, we use a firstly out algorithm in R to visualize the correlation matrix between the responses. And that allows us to spot very quickly what are the really central questions of the survey. For example, question 44, I am satisfied with my department. That's right in the middle there. Question 66, my workplace is psychologically healthy. Question 26, I am satisfied with the quality of the supervision I receive. These questions are strongly correlated to a lot of other questions in the survey. And you can just look at them in isolation. What you can do or however is to, if you get a low score to these questions, you can try to understand what are the main influencers. And again, there are many to do that in R. You can build a causal graph that estimates the strength and the direction of the influence between different questions. So for example, in Q66, my workplace is psychologically healthy. You can see that influence by question 65 responses about awareness and mental health. It's influenced by also responses to question four. I have support to at work to balance my work and my personal life or question 54. My department works hard to create a workplace that prevents harassment. That's really interesting because it's a little bit more clear and it's more actionable. I'm going to give you another example that is even more telling. You can use a machine learning approach to do the similar kind of analysis. If you look at question 26, I am satisfied with this quality of the supervision I receive, which is a very central question in the survey. You can train a random forest model to determine what are the top predictors of responses to that question. And if you look at the results, it tells you that the most important predictors are the responses to Q22. I receive useful feedback from my supervisor. Question 24, my supervisor keeps me informed about issues affecting my work. Question six, I receive meaningful recognition for work well done and so on. So again, my point here is that this kind of insight is useful and it's actionable and it can inform action plans that will ultimately improve the general satisfaction of employees. And we again, we develop dashboards in R and also using a combination of R and Power BI that make that kind of analysis very easy and very quick to do. So just before I finish very briefly, I want to mention some of the other projects my team has been doing recently using R. For example, we've done work with the Royal Navy about personal enrollment and selection modeling. We've done work with our material branch on procurement cost forecasting. We're doing some work with our NATO allies, including the US on predictive maintenance. And also, we did some work recently on supply chain forecasting and a bunch of other exciting things in other languages like Python and and and and and commercial tools. But, you know, that there's a lot to be done in data science and we're not we don't have any kind of shortage of problems that we can work with. So on that note, I'd like to thank you again. Thank you, Jared, for the invitation and have a good day. I love that we got to see the trailer then there's if the statistics wasn't good enough, we got to see his trailer. So that was that was pretty awesome. Okay, as promised, we are now going to do the."}, {"Year": 2020, "Speaker": "Abhijit Dasgupta", "Title": "Accessing and Analyzing Health Data from Government and Intergovernmental Sources", "Abstract": "Data on the health and well-being of populations is increasingly available through open data initiatives at various government and inter-government agencies, including WHO, the World Bank, and different national agencies. This real world data is accessible to anyone to understand trends in disease prevalence and effects of policy change. This year, the power of open data has been seen in tracking the patterns of incidence and death in the COVID-19 pandemic. In this talk, Abhijit will describe different ways in which the world's data repositories can be accessed using generic and specialized packages in R to enable visualization and analyses in the R ecosystem.", "VideoURL": "https://www.youtube.com/watch?v=A3x5ztenmw8", "id0": "2020_23", "transcript": "Our next speaker was once referred to by a head of state, particularly a national leader, as the good doctor. And it's true, I've seen it. And I do believe President Obama also said, I can't pronounce your name. And that's why he referred to you like that from then on. So with that, everyone, please welcome to the stage the good doctor, Abigit. So I'm going to talk today about health data that is available relatively freely and relatively copiously around the world through open data initiatives with various governments. So first of all, many of you may not know who I am. Yes, I did that little tidbit. I had an opportunity to be on TV with President Obama about five years ago. But that limits everything claim to fame. So I'm just going to introduce myself a little bit. I'm a local here in DC. I know this is no longer our data. It's no longer our conference DC, but nonetheless. I co-founded Stest Programming DC with Mark Weisman, who you guys heard yesterday. I've been using ours for God knows how long, over 20 years. I think my first version was like 0.3 or something like that. And I'm an occasional blogger. I've got my hands in a few pies. I do data science at a startup locally across the river from DC in Arlington, Virginia. But I also, for most of my time right now, do biostistics and machine learning, currently at AstraZeneca. And no, I do not know anything about the vaccine. And I was at NIH for a decade doing sort of the same stuff. I also teach R. Again with Mark. We tend to do a few things together over the last 10 years at Georgetown. And I'm also adjunct to George Mason, which are two universities that are relatively known. And I teach R and Python both in the university setting, as well as for corporate trainings and the like. So stay busy in terms of both learning R, teaching R, using R. I'm not quite as fanatic as Jared. I think, but I'm probably pretty close. Because I actually still also use Python. So Jared still. So why am I talking about open data in healthcare? Well, I've been interested for a long time in global health. And the sustainable development goals that the WHO put out several years ago. And sort of trying to understand why some countries do better than others. This sort of been a personal interest. And that's when I got used to using World Bank and WHO data and discovering sort of the packages that were there in R for accessing those kinds of data. Going into pharma, there's more of an interest in what's called real world evidence, which is basically everything that's not a clinical trial. So it's the publicly and some non-publicly available data that's from observational studies or from other kinds of studies that you want to leverage try and figure out who you want to address and what are the problems that you need to solve. And then in Murakayov was dealing with questions about hospital utilization among Medicare recipients and got to work with the American Community Survey data once again through our packages. For those of you who don't know, the American Community Survey, it is a survey that's annual, but it's run by the Census Bureau and it is published every year in either one, three or five year aggregates. And it covers sort of a subset of the questions that are asked on the Census. So thinking about open data, there's plenty of things out there and there's plenty of ways of getting at it, right? So if it's on the web, we can always, you know, web screen and there's great tools like HUTR and RVEST within the R ecosystem that allow you to do that. I'm not gonna talk about web scraping today. There's people who do it much better than I do. There's also just straight up availability of data sets on websites. So think of places like data.gov where you can just download data sets after some searching and so on and so forth. And that's great. And then also, time you can get data sets in CSV format or Excel. I've actually seen data sets from MIT that are actually in our data format, which I got a little kick out of that they'd actually do that officially. And then the other form, and then I lost my place in my desktop. So there we go. And then there's APIs. APIs are fantastic and they're actually more and more available. If you want to learn more about directly about APIs and how they work in R, go back to last year's DCR and Mark's talk on APIs that he gave last year, which was a great introduction to how to use APIs through R. And there's video and slides and all of that available out in the, in the internets. And then finally, of course, there's R packages. And of course, we're in our conference, ideally we want to talk about our packages at the end of the day. So there's plenty of things out there to access data. So all that motivation and then 2020 happens. And what that means is this, right? We're sort of in the middle of all of this. And I started way back in March to start accessing the John Hopkins data and start doing sort of these basic visualizations that sort of the financial times does as well in the New York times. But just to play around with how to grab data, how to do these kinds of visualizations playing around with with Ggplot. I haven't done these for several months because there's professionals who are doing these kinds of things and they're much better than I am. But it also got me really realizing the power of open data to aggregate all of these different disparate datasets into one place so that we can do these kinds of visualizations. Both the Hopkins people as well as the New York times. The COVID tracking project, which is an astonishing project because they, at least when I last went deeply into them, we're doing things literally by hand, of aggregating these open data sources and putting them into one essential repository and sort of aligning things so that we can use it. And for the COVID world, you know, the three of these four data sources have GitHub repos available for the data. And so we have the tools in R to just download these repos and access the data. And for the COVID tracking project, there is actually an API available which will give you either JSON or CSV files. And it's just standard REST API that you could potentially access via a HDR, for example. So open data is great. And really the COVID, the last seven months, eight months, whatever it is now, really showed us how much open data can be leveraged, which is fantastic. There's obviously a caveat with open data and open government data in that there's a radio variability in the completeness and the quality of the data. And there's other various problems in terms of ascertainment and how you call someone a patient, how you call something, a death, the COVID related death and so on and so forth. Which is absolutely not even around the world. And so data quality is still an issue but the fact that you have access to at least reasonable amounts of data that are contemporaneous is fantastic. So for open data, this is actually from data.gov, they have a map. And open data's gotten, these are government sources and it's pretty widespread, at least in the OECD countries, in India, China, Russia. Those are the country-wide ones and then the markers there are actually city or state open data portals. So it's not just the national level data that's available but also state and local data, which also is fantastic because it gives you some granularity. So in the federal government is required to publish their data online with open standards under the Open Government Data Act of 2018. The biggest sort of catalog of this is data.gov, it has over 218,000 data sets. There's also a health data.gov, which is more interesting to me, and it provides data from different agencies under the Department of Health and Human Services, as well as partner cities and states. And it's really focused on healthcare and allied subjects. So APIs in government data have become much more prevalent, which is a good thing. And there appears to be three general API infrastructures, if you will, that are in common use for open data if they're not actually having custom REST APIs available. So the three are the Socrata Open Data API, or SODA, CCan and DCan. And true to government, there's no one API to rule them all. The CDC uses SODA, Data.gov uses CCan, as does the UK government. The European Union used to use CCan, but now use a custom REST API. Health data.gov, which is you would think aligned to data.gov uses DCan. And then just to put a spanner in the works, the FDA just did their own thing. So you need to know a bit of different APIs to API calls and digging in to get at different aspects of health data, even within the US federal system. More globally, the World Health Organization, which of course is looks at data and collects data from various countries around the world, provides not one but two APIs to its global health observatory. One uses the Open Data API and the other uses the Athena API. And the World Bank, which everyone thinks of as an economics powerhouse, but they actually have great data on national health statistics, has its own API at API.4 Bang.org. So we've got a bunch of APIs. Most of the APIs will give you JSON data. And so the basic tools are HTTP to grab the data and then JSON Lite, XML, or other packages to parse the data. But I'm lazy. And I love R. So I don't want to do all this hard work of figuring out the APIs and actually scraping them and putting the calls out and pulling them back. So what do I do? What do I do? I look around for R packages to do this stuff. And so fortunately, the R ecosystem is fantastic because there are interested developers who have R packages that just wrapped various APIs. So for the World Bank, there's actually two packages. WDI is a little older, WBCS is a little newer, slightly different syntaxes, but fairly straightforward. You can basically put the indicators that you want. The World Bank indicators are very cryptic. And so it's worth looking at the World Bank site to figure out what these indicators codes are. But once you have them done, the data is a one-line command. And then you can do what you will because this comes down to a data frame. The WHO also has an API. The R package, also called WHO, is not on CRAN, it's on GitHub. And it gives me a little more control because it also allows me to download the actual codes, the indicator codes. And so once I have that, I can actually use R to do my searches and filters to figure out the codes I actually need to ask for to actually get the data that I'm interested in. So in this case, I'm showing you an example of getting life expectancy at birth for different countries over different years. The FDA, like I said, has its own API and it does a bunch of data, makes a bunch of data available on drugs, devices and food. What's actually often really interesting is that there have a lot of information about drug recalls or food recalls and adverse events. And these are things that are often really, really interesting to look at simply because it goes to the safety and whether I should use a food or a drug. So there's a lot of data out there, it's a really big database. There is an open FDA package that uses that FDA API. It's very thin wrapper around it. So it's almost like piece by piece going through and just recreating the API call. I'm just showing you an easy example here where I'm looking at all the drugs in the FDA database and at what age patients got them. So this is almost patient level data and you can see that we have, on the Y axis, we're going up to 150,000 counts. So most drugs obviously in the US are in the 40 to 80 age range and of course there's much fewer that are in the pediatric ranges. So it's mentioned to crowd us he can and decan. So all three have wrappers in our, which is cool. The also credit package is the one I'm highlighting simply because Socratic seems to be the most widely used across a lot of government databases. I'm just giving an example here for the CDC. I'm grabbing the number of COVID deaths in Maryland by week. I actually went to the CDC website and found out what the API call was. So I can just get that API call, put it into Rita Crata and I get a data frame that then I'm just using tighty verse to sort of filter it out for Maryland and then showing a plot showing that we had our last really bad peak in what is it May, June to early summer, but we're going up the uphill again as we go to the end of the year. And since it's 2020, there's plenty of COVID data packages. These were the early ones that I played with, which were COVID-19, coronavirus, tighty COVID. I'm sure there's more data pairs that have mushroomed since then, Google will be your friend. And so I guess the overall point is that there is data that you can use from government sources that are available, that are easy to access from our and allow you a great deal of power to do your own analyses and do your own sort of understanding of the underlying data for either your own benefit or your works benefit. And we can do it all in R, which I think is the best part of this story. Not all the APIs out there have wrappers. So obviously there's work to be done by the community to help us out lazy people like me to make more packages so I can get them. So thank you very much for your time and the slides are actually available on GitHub. There's a URL on GitHub pages for all these slides if you want to access them again. So thank you very much for your time and your attention. Thank you very much. And that actually brings up a great point. We get this question is asked all the time. All of the talks will be posted online at rstats.ai within about a month. It takes a little bit of time. As I mentioned yesterday, we actually have a former Nickelodeon double dare and a Phil Phil's producer handling off our video. It takes a little bit of time, but as soon as they're ready, usually that amount because it's a lot of very high res images videos. And we'll post it up at rstats.ai. And at rstats.ai, you can find videos of all the talks in the past six years of conferences both in your conference and this government conference. So hop on over there to see videos. So just a few..."}, {"Year": 2020, "Speaker": "Maureen Mo Johnson", "Title": "Launching a Pilot Program for Partnering with Cities' Vision Zero Initiatives", "Abstract": "Insight Lane is an open-source platform that assesses and visualizes crash risk across road networks. Started by a group of volunteers in 2017, its main goal is to inform and benefit residents and city governments for the purpose of improving car, bike and pedestrian infrastructure, using available data. This talk will focus on the use of data and participatory democracy to inform planning of city streets, to achieve Vision Zero goals by reducing crashes, and to facilitate collaboration between cities, advocates and the general public.", "VideoURL": "https://www.youtube.com/watch?v=ZlQQvjcbmcg", "id0": "2020_24", "transcript": "So our next and last speaker says that she is a mainstream millennial. She likes bicycles. I hope it's a fixie, not just annual bicycle, but a fixie. Beer, mountains and pub trivia. Though something that sets her apart from all the other millennials is that she didn't get into baking during the pandemic. So please everyone welcome Mo. Hi, hi everyone. Thanks Jared. I actually, I have a gravel bike. So the second best bike for a typical hipster millennial. I'm really delighted to be here. And I know that I'm the only thing keeping us from kind of socializing in half the hour and I hope to meet a lot of y'all there. I am Maureen, but most people call me Mo and I've been a part of Insight Lane for the past year and a half or so. And I'll be talking about recently launching a pilot program to partner with cities. Vision Zero initiatives. This talk might be a little bit more kind of policy focused than then code focus, but hopefully we'll have a bit of fun with that. So I often live in cities, cyclists, like I said, but I'm also a driver, civil user, pedestrian, park level, and much, much more. My background is in public health research. So most recently for the past 10 months or so I've been working in COVID and kind of getting back to my roots. I've also been involved in the tech ethics and data equity groups. And I'm primarily a qualitative analysis, but I'm excited to be here because the past 10 months in particular have been learning are to unlock some mixed methods research opportunities and this open source project is one of the ways that I'm hoping to use some of this newfound skill and knowledge. So Insight Lane. It's a volunteer driven completely open source platform that assesses and visualizes crash risk across road networks. I'll go into it a little bit more exactly how we do that. But really it was more not as the idea that we wanted to inform benefit. Any residents or city governments or any other stakeholder for the purpose of improving car, bike, and pedestrian infrastructure, which is part of the reason I emphasize that I do all of those things in cities and I suspect a lot of y'all do too. We really also support volunteer skill building. I can speak on that first hand to the main core volunteers that have been involved with the project for almost four years. Ben and Jenny have really become good friends have supported things that I'd like to do. And I've learned a lot as we've shared knowledge and contribute and built kind of community as well, which is really important during this time when we're all really mainly digital and trying to say as safe as possible. The big goal for Insight Lane is to really provide a tool to assess crash risk. We want it to easy. We want it to be deployable. We want it to be used. It's based on open source technology. We use open street maps. Everything's on GitHub. We also want the tool to be able to be applied to any road network for a couple of different instances is somewhat limited to cities because they're bounded. We would also like people to be able to initialize their city and add features with minimal technical overhead. So the platform is widely can be widely applied, but with individualized data from city, which I'll go into a little bit more. And then, of course, this is kind of the policy piece or the advocacy piece is that at the end of the day, what goes to the platform if it doesn't provide kind of interpretable actionable results results, even if it's fun to build. Yeah, so this is the basic kind of process of how we do it. So any city can provide crash data. It goes through our we configure it. You can add point based features, cut, which to any sort of crash data set. It gets processed, then we model it. We create kind of individualized segments that are consisted of different open street segments. We create a model which generates a set of risk scores for each road segment. And then we also have a visualization that I'll show you in the end. This is kind of a repeat of that, but some of the kind of cool things that I like about it is that each segment is basically analyzable and you can add features to each one of those. So that makes it a little bit different than some of the other. Vision zero kind of analysis that often look at density, rather than kind of segment based analysis. We also split into bike pedestrian and vehicle. So you can see risk for each one of those kind of transportation modes along a road network. So we're going to get a model based risk score. Across all the segments that goes from zero to one and is city based so a point five in Boston would not be the same as a point five in Philadelphia, rather you could look at it within city. So it's really, really individualized. We also can make different recommendations. Let's say that you want to get to. You want to see like basically two different routes getting to see city hall, which one is a higher risk route and that might tell you some things about like the accessibility of that neighborhood. To a particular point on a map. And then we also visualize it and have an interactive map, which I would encourage y'all to check out on our website, which I'll share at the end. Here's an example of kind of the route level risk scores. So this is in Boston. And we basically looked at two points arriving at city hall. And the blue zone is the lowest risk for a particular one mile or two mile segment leading to city hall and the red is the highest risk score. Just note that because it is a risk score. This is a mean like that. It doesn't correlate one on one with crashes rather it's predictive risk score. We are in multiple cities. One of our first volunteers was from Boston, which is why we started there, but then we had several volunteers come in from Australia and have a couple cities in Australia to this is an example of what you can see on our website where we generate help basically say what are the highest risk segments in a particular city. And also generate risk scores. In this case, I screenshot a map from the bicycle. Risk segments in Brisbane, Australia. So what led us to launching a pilot initiative to partner with cities. You know, at the beginning, this would have been back in 2016. We were a bunch of us were part of kind of an open source group called data for democracy and people were very excited about using data to support different policy initiatives or even just to kind of geek out about it. One of the volunteers was the chief data officer of Boston and kind of brought the idea that there's traffic crasters are increasing vision zero initiatives are gaining popularity. When Boston took on a vision zero initiative, they sought help from its community in order to accomplish some of its goals. So the team got together 2016 really focused on Boston initially and that really still is where the. A bunch of most of our data analysis is. It was initially called crash model project, which we expanded to insight lane in 2018 we expanded to many other cities about 10 which I'll show in a minute. And then in 2019 it was a little bit of a pause for us. It was about maintaining the pipeline and kind of reflecting on what this would actually do. I joined sometime in 2019 when I was trying to find an additional project to really seek my teeth into you. And we kind of came up together that like one of the things we could do to revitalize and relaunch the project was to actually kind of show cities, how we could potentially help them. And I'll go a little bit through the process on how we had to think about doing that because it's, I think there are always some really interesting like communication goals when you're communicating with maybe a government or a stakeholder who may or may not have the same kind of coding background and with a coding project. So one of the things this may seem really basic, but I think it's also important. We were initially part of kind of like a consortium of different open source projects. We decided that if we were going to be reaching out to city, we needed to do some communication and branding work. So we made a logo logo. We got our own Twitter account and we have a website and we just felt that that would be kind of along the communication. Like expectations of being able to reach out to cities that might be looking for some sort of data driven analysis to support their policy work. Again, started in Boston, the platform was built to be used in any city. I listed some of them here. Most recently as part of the new initiative we've reached out to Charlotte's Phil Virginia and Austin, Texas. I'm actually from Texas, but north of north of Boston. So that's been really fun. So I'm going to try an interactive experiment. And this is part of kind of maybe the meta that every city is is a little bit different. So I am going to tweet real quick. What I wrote here. And I would love if you can either share what city you're in, either in the chat or on Twitter where I just tweeted where or something that you love about transport or particular memory you have, or maybe if you think your city might be interested. And having the crash modeling platform that we've built be used. I'll go into some of the options for cities in a couple of slides. And maybe if if people make sure. If people are actually respond, then I'll try to read some out loud at the end. So a couple of things that values that drive the partnership to think about like while we're thinking about like the different cities that we have chosen is that cities have multiple stakeholders. You're a stakeholder in the city where you live. You might be part of a advocacy organization. You probably voted or participated somehow in the election that happened a month ago. And I think it's good interesting to always think like what what questions might you have about your neighborhood or your city. What might you change or create. We really want to support participatory collaborative democracy because we have a platform that can be broadly used each city, but at the same time city has its own data constituents and concerns. Most of us live in either Boston and I live in Texas. So we don't know what's going on in New Orleans, for instance, and would really want to support the kind of local volunteerism and participatory action there. And then also like really emphasizing that volunteers and stakeholders are welcome to co create and build on what we do. So we have had people come in. Initialize your city with the crash data and do a few things, but then we never see them again. But as long as it's getting used, we're really happy. So as we started to do this outreach really in the past three or so months, we've kind of noticed two different scenarios that seem to be popping up, both in our previous work that we've done with Boston and other cities and then also with what we were doing now. So some city governments already have done crash analysis and really have a digital platform similar to ours. That was the case with Boston, but they look at crash density while we look more at road features. So we can we think that's an opportunity to offer a different perspective. We can also potentially add an additional feature that they might be looking at because of the way that we do the analysis where every feature is kind of snapped to a road segment and isn't as density dependent. It's also really fun. This one we reached out to Austin. We were able to compare goats and share learn and might even be applying for some type of grant together. And the other thing too is whenever a city already has a digital platform that's similar that's in line with kind of a vision zero policy goals, we are there. The crash data is usually open and available. Some of the cities haven't had the resources or in house tech expertise to create this type of platform. We view this as another opportunity and that's actually where we're focusing a lot of our outreach. I think one thing to emphasize is that a city government partner does not have to be the government and multiple stakeholders may add value to contextualizing the model output. So if you're part of a bicycle coalition, if you're part of like kind of a moms or in father's group. One thing that we have run into is that the crash data may not be as easily available. We created a form that people can fill out on our website that I'll also link in some of my tweets and put in the chat later that people can fill out to see if we can try to help them. One kind of funny story with that is that somebody I think they were in Florida was very interested, but all the crash data was on floppy disk. So they were not as accessible. So I think this is where we like also really support value wise, this idea of like open data and also this idea of local sustainability. So when a local group also like kind of partners and joins our Slack channel and it's really involved in their particular city, we think that that's really exciting. So this is kind of the the meta part, but if we think about what a city is. We're the city, right? So it's not just a city government. I had a little bit of fun looking up the origin of the word city in Latin and then also because it means citizen. We also really support residents of cities acknowledging that not everybody has had the privilege of citizenship in the US and other countries. And I really loved that in Greece, they really in ancient Greece, which is also kind of the seat of where like at least we perceive democracy to come from, then we, the citizen was also like the most basic unit of the city. And on that note, especially because we grew out of a group called data for democracy and really still support some of the values of Democrat democratic participation is that we. Support the local participation and hope that the expertise of the public and civic participation and democracy is practice and open source and open data projects are really part of that. We hope that this provides kind of an open transparent way to link data and data analysis to advocacy and policy. That being said, this is a little bit of a kind of interpretation. It also means that when we seek a government partner, then one of the things that is part of that partnership is that there has to be trust in the expertise of the public support for Pacific participation and understanding that democracy can be an innovative practice, which often includes like seating some of the power of having access to data or even access to analysis of policy. So that accessibility is really, really key. If your data is on floppy disk, it's harder to get access to. I really want to emphasize too, I admire a lot Taiwan is a leader in digital and collaborative democracy, particularly with COVID they've really opened kind of their technology doors and let citizens really directly hack into many different types of support systems. I'm going to just list the menu of options for city partners. One thing that we found when we were reaching out to cities is that it was also helpful for us to say like exactly what we could offer instead of having kind of an open conversation and not assuming that a city could like immediately look at like our website and know what we could potentially provide. So one is that cities sometimes we have this form where they can let us know they're interested. If they already have crash data, one of the options we say is we can actually load it, initialize it, run it through our platform and generate the data and the risk scores, which is one of our outputs. If there's a particular policy question, we really love it. If they ask the question and we see if our data can actually support answering that and kind of thinking through that. And then the other thing specifically this picture here is in Austin and we were having a little bit of fun reaching out to Austin. The information like the gray like node is a voting booth. So this is around the time of the election and we were curious what the like higher risk routes to get to the voting booths or not where and we were thinking about this in kind of terms of like accessibility. So like if a polling location is not accessible to some neighborhoods because it's more dangerous to get there. This might tell us some of the ways that like things can be just resources can be distributed throughout the city. The other thing too that I think is really cool the output is basically a segment and a risk score and a lot long. We can combine that particular segment and risk scores with other data. So we've begun to look at census data and understand equity concerns. And those are some of the many projects that I'm getting to experiment a little bit as I grow my my coding knowledge in our as well. Again, we are a volunteer run organization. So I told my team that we also are actively would share that we're actively seeking volunteers. One thing that I think has made this project so sustainable for such a long time. I mean, it's really been three or four years of people having near weekly meetings. Is that we try to create a really sustainable practice so it's less based on hackathons and more based on kind of like kind of the slow pace work of getting a little bit of done every week. Also, I will say the documentation and this is not on me. This is really, I have to think, Jenny, our data engineer is excellent. And I think one thing with the pace matching with partners is that compared to maybe startups, it may appear that governments move more slowly but a lot of this is has to do with just like kind of the. Structure with which it's set up and we really are happy to like support and match that type of consideration and cycle. And let me check time. Okay. And so this is to get in check. The best way to communicate with us is really our stock channel which you can find on our website at the bottom. You can also write us or fill out a Google forum to try to onboard your city and we'll get out to you Twitter, which we set up in order to pilot this launch with cities is a great way to you. I managed the Twitter account and then our email will get forwarded to to all of us. So I do appreciate that. All right. I don't think I can see the chat as a speaker, but hopefully people wrote some stuff in there and kind of shared their city. Full disclosure, if you shared a city, I might be reaching out to you to see if you would want to onboard it or work on the project in any particular way because I think that's can be really exciting and give a lot of insight into basically future analysis and then also. Different policies or supporting kind of the vision zero initiative that your city may or may not have adopted. Yeah, so that's about it. Let me look on Twitter. And I was given a recommendation to talk to the pednet coalition. So thank you, Stas. Appreciate that. Okay. That's that's it. So thank you so much and have a lovely day. Thank you very much. That was wonderful. I like imagine a lot of people here are city dwellers and make good use of alternate modes of transportation will call it whether it's subway bus or bicycle and why that's called alternate I don't really know why cars get priority. You know, I get around primarily by a foot bicycle and subway. So that's great for me. And we have come."}, {"Year": 2019, "Speaker": "Refael Lav", "Title": "R Cloud and Google \u2013 Practical Tools for Data Science, NLP, API, and Maps", "Abstract": "In this talk, Rafael, a lead data scientist at Deloitte, discusses practical applications of integrating Google and cloud services to enhance data-driven solutions. Building on his previous discussion about solution development, Rafael focuses on using external data and cloud capabilities to support decision-making processes. He highlights four practical use cases, such as providing social service information through a website for low-income individuals, prioritizing inspections with limited resources using visualization tools, leveraging Google\u2019s speech-to-text API for monitoring prison calls, and automating document processing with OCR and corpus-based correction techniques. Through these examples, Rafael demonstrates how to effectively utilize external data and cloud resources to create comprehensive, scalable models and solutions in R, enhancing insights and decision-making capabilities.", "VideoURL": "https://www.youtube.com/watch?v=pJnthUVtQLU", "id0": "2019_01", "transcript": "My name is Rafael and I really wanted to, again, I have the pleasure of being over here for the second year in a row opener, so we'll start with something light, not too much code. And a little bit about myself, I'm a lead data scientist at Deloitte in our federal cognitive practice where I really cross over multiple agencies and a little bit of commercial. I also, I also the art champion for Deloitte Federal. So everybody, everybody around agencies and the work that we do around R, I'm involved to a degree. What I really wanted to talk about today is, in a way, continuation of last year. So last year, the conversation was really around the components in order to develop a solution, the different lego pieces when you try to develop a solution. And a solution to me is not, I developed a graph or I developed an analysis, but rather something end to end scale that a client, those are the people I'm working with, can really utilize to make decisions. That's really what this is about. So I wanted to take it one level lower to a more practical aspect. I'm a practitioner. Everything that I work on R&D or whatever has to have a business sense to it. Otherwise, I'm not getting paid. So in that regard, what is the, I wanted to talk about how you can utilize essentially Google and a cloud to really get some additional data, additional information in order to support the data that you already have or the decision that you already have in order to develop a solution. And we'll go through our four practical use cases. And this is, again, this is not a discussion about cloud or APIs. Mark is going to talk about this later today. This is more of a, okay, you can Google how to do that, but what do I need to Google? And not just Google Stack Overflow. So this is really about the third bullet points over here, really about the data inputs to your model. And going through those, I really want to focus for a second around what is the current use of cloud. So we all know about scalability. We all know about compute. I want to develop a shiny. I can put it on a shiny server. I can put it on AWS, and GCP, et cetera. And I can deploy it to a client. Great. But really where we are seeing a lot of work right now happening is really on the fourth aspect, which is really I have an on demand capability that I can deploy. And this is really a modeling capability. This is somebody already did some of the leg work for you. So how do you use that in order to make your life a little easier and really allowing you to get some insight from that one? So I'm going to spend the next roughly four minutes on each use case and really give you an idea of what's happening in the marketplace and how we utilizing R in order to achieve all of that. We'll really start with one of our biggest work right now in the state of Kentucky and politics aside. Finally there's an effort around trying to support a individual low income individual who needs social help. How do we support individuals like that in providing them with social services? Now there's a very famous Kaiser research saying most people and I challenge all of you, most people don't even know what social services are available to them, especially on the lower end of the spectrum. If something happened, you don't even know what it will look. A lot of people, especially in the older population, don't even know how to Google that kind of stuff. So how do we nudge individuals in order to get what they need? So we worked on a website that a civil self force that actually allowing people to search for that one. We'll get to the second, how do we use Google? It provides individuals, in the foster system, veterans, etc. to start looking for resources that might be available to them. So you can really start from here and then you get to the point where you actually can connect to a resource. And these are the type of resources that are available, a dress for success, or that kind of resources. Now how do you know what to look? Yes, you can do a search, but there's a better way to do this. So one of the things that we're working on is really around providing individuals with a suggestion and nudge in the right direction. So we want individuals that don't know about something, hey, by the way, there's also this that you should consider based on your information, based on what I know about you. And I know about this, about you from multiple sources, and I can have that that's really aggregated and you can get into a clustering analysis and all that stuff. But really what's cool about this is that I can, like Amazon, provide you with some related resources. I can provide you with complementary resources. Now how do I know what to give you? So we really start with our capability, our thinking around people like you, also look at somebody else. So I will say, listen, I'm not going to make any determination, but I'm saying you are like you, and therefore you should also consider something else. And I think all of you know where I'm going with this one. It's like the Amazon case, especially complementary. And complementary with people like you really should be considering the other things. So this is more of a knowledge base approach. So where do we get into the Google and R in all of this? This is a classic case where I can actually utilize, and these are the models, similarity and complementary. And this is where we're getting into the use of data, enhance my data based on current activity on the website. So we all know about, you can Google Analytics, I'm sure we all know that Google track everything that we do, but you can actually use this for good. And how do you do this? Because I can actually recognize users, and I can recognize their path around the system and around the website. And I know who those people are, and I know something about you. And therefore, I can kind of chart this for all of them. And this is where we, practically use case of R. There's a great package. Well, actually, there's a couple packages, but this one is using Google Analytics R, which is really an API entry to R from Google Analytics. And this is really where it allows you to take information out of the behavior. And again, the code is up there. It's pretty, nothing, I'm not going to go too much into code. But you can really see how you can start getting information, how people navigate through the system. And because they navigate through the system like that, I can now create association rule. I can create clustering, et cetera, et cetera. This is one use case that behavior can actually nudge other people behavior. Another use case is really around prioritization. There are efforts around agencies, different agencies in how to get, I have limited supply of resources of people, what they need to do, inspections of, from the FDA inspection. And this one is around transportation. How do I get inspection prioritization done right? The limited resources, this is what I know about what's happening around, there's a lot of statistics around that. But how do I use that in order to prioritize? Now, I'm sure all of us kind of thinking, okay, well, you get all the data, you build some patterns, you build, you can create a different type of analysis and start graph and all that, et cetera, which is all good and great. But there is an additional path that we are thinking through and how to really chart out additional data into your analysis. And one of the things, one of the way to kind of think through that is through animation. And there will be a talk later on about the history of GG Plots 2 and around descriptive statistics. Really important over here is two things. One, you can actually access Google Map and really start visualizing with an R, the patterns that actually happen. Now, yes, we can have a column for when things happen, you have a date and you can design a column that creates the lag. But when you actually visualize that and what you see in the give over there is really, and this is just a really, really small subset, accident. If it's red, it means there was a human event over there. The size of the data is obviously the damage. But it was really interesting over here and you can notice that it's hanging out in a particular location. It's kind of moving from Houston, it's going up, and then it's hanging out over there for couple of days. So you can start visualizing that there is a break in the data. Now you can kind of thinking about why. For example, weather related. Okay, now I know I need to bring this out of the data. So this is how this kind of analysis enhance your enhancing analysis. Next use case that I'm actually encouraging all of you to really explore the GCP and the NLP capability and this is really around the APIs that we'll discuss later is really voice the text. And this is really, if you remember a few slides ago, I talked about the fourth column of utilizing existing algorithm and really providing it to you. To create a speech to text algorithm, yes, you can do that, but you need massive amount of data. It's really useful to utilize API and the Amazon, Google, Microsoft of the world that actually invested in that and yes, there's a cost associated with that. But the important thing over here is that you can start taking calls and in this case, this is a prison use case where you can really utilize and identify if there are any risks associated with a particular call. This is not in order to monitor people and what they're saying, but really do I even need to worry about this is really around public safety. And in order to, now there's no way to do this manually, but how you can do this using Google is really you can start passing it through the API and get that. And here's some example around 50 calls and the word error rate that we calculated. And you can see that overall, we're getting a really good accuracy. It's over, it's roughly 70% accuracy on our calls. And you can start building models around the machine learning model. This is an extra boost model that really can start a, extra boost on top of extra boost, but you can really start analyzing and utilizing the text. But it's not just the text. What Google gives you is really just who says what, when you can start getting the text, you're getting the confidence, you're getting what it is about, you can start analyzing if there are any lexicon that you need to account for, which you can start really working with the, it's not just I have audio and it's not just text, it's the combination. You can start calculating, for example, if I have a significant break. And if I have a significant break, that is important to me because now I know that somebody's trying to communicate something unintentional or intentionally, I don't want them to listen to. And really what it looks like, and this is really, again, this is all generated in R, this is all, it's a shiny dashboard actually, that you can start visualizing through the API that you get, who said what, when? You get the language, you get the wording, you get the timing, and then to an R you can actually utilize that to create the frequency and the speed and essentially the audio component. And putting it together is really where you start, okay, now I have a complete data set. And this is just, again, another example we can utilize services that are beyond your local laptop in order to create data sets. We have information about people, we have information about the call or just a high-level daytime, et cetera. But what you can really get out of Google and out of services like that is really the capability of aggregate, additional data that now give you a complete picture of what's going on. And once you have this complete data, you can add other data to it. And in our case, I just want to, just a word about that, you can start creating additional, you can start adding additional statistics, zero crossing rate, for example, which is really the change of speed. Now you know who said what, when? You can imagine how you can really incorporate the speed in which people are speaking, if they're taking a large break or not. Only then, only then when you have this entire data set together, you can start, that's where the machine learning, that's where the data set's really applicable to where you can use that. Again, this is a really great use case of I'm not going to develop a model that I know, I mean yes, I can develop my own speech to text model, we actually did that. We got a really nice results, but nothing like the Googles of the world. And then you can start thinking about security, etc., but that's a different discussion. And I want to finalize this with a really interesting use case that we built for a banking institution, and there will be a later on discussion in the business school and the whole hour of this. How do you take a process which has variation within the process? You have documents like the one on the left, which is an audit report. How do you take that and really automate that? So we all heard about robotics and natural language processing, but there are aspects of it that you need to solve, that God is in the details, right? How do you solve the need ingredient? How do you take a document like that and really transform that that the code can start making decisions, doesn't have what I'm looking for it? Was the report state that the auditors comply with the particular standards, that the numbers matches what I have on my own ledger, etc., etc. So these are a lot of like needy, greedy, essentially automating the boring stuff that individuals are doing right now, but theoretically you can test for them. Now the example over there from the audit report, that's nice, right? That's a really clear document. It's digital. You can read it, no problem. But the problem is that it doesn't always look like that and it's not always looking, it's not always in English. And this is actually, I was trying, at the bottom right over here you have a really nice table, but you can see that there are dual lines over here underneath a couple of them and no lines in others. How do you adjust for that? Some information like on the top right you have a lot of noise in the system, for lack of a better term. The one on the top left is almost unreadable. So the problem that now you're facing is how do I solve for an OCR problem where I know my data might be wrong, but I do need to make a decision over that. So there are multiple ways to approach that and we can have a whole discussion around OCR, including a document enhancements and you can do a neural network or any trying to get a better results of that, et cetera, et cetera. But one cool way to do this is really around getting the data and building a corpus of correction. So one of the things that you can do is enhance your text saying in my corpus, and this is actually not computationally extensive, it's, I can take my text, my corpus and I can run it to a hand spell, et cetera, that really can identify misspelling. And then what you can do is you can scrape Google. Now this is an example of the front end. There is an API for this as well, but I really wanted to show a piece over here from our crawl package over there where you can really start doing it in the front end. This is just as an example that you can get, you can spell DCR conference wrong. And using this type of tech code, which is essentially taking, essentially taking what Google says that I'm actually looking for something and scraping the website, identifying within the string and then it's turning into a string manipulation. What's nice about this is that you can do this in multiple languages and really create the text, the corpus correction for your data that then you don't have to process every document through neural networks and all kinds of fancy, schmancy thing that takes compute and takes time to correct an E, a C to an E and vice versa in a particular wording. If you know that this is a common issue in your data set, you can create that and really this will take a second. This is kind of like a workaround, but this is where Google is really powerful. And you can see at the bottom right, there are some examples over here where we actually, this is a common in our corpus, common issues and the correction that really took place. And we know that this is in our corpus, this is not a right word, but we know that what's the right word now is. Now, on a few of them, yeah, you don't want to do that, but if you have thousands of thousands of those, that's when it becomes valuable. So those are just multiple cases, now you know what to Google, of utilizing external sources and Google in particular to make your data sets more complete to make decisions. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you."}, {"Year": 2019, "Speaker": "Samantha Tyner", "Title": "Pagedown: in HTML We Trust", "Abstract": "Following in the footsteps of rmarkdown, pkgdown, and bookdown, pagedown provides a brand new way to create web documents with R. LaTeX has been the master of paged documents for decades, but it is not meant to produce for the web. Enter pagedown: the R package which paginates the HTML Output of R Markdown to create documents suitable for printing. pagedown eliminates the need for LaTeX: to get beautiful PDFs, all you need is a modern web browser! In this talk, I'll introduce pagedown templates and some easy web magic so you can create beautiful paged HTML documents in R.", "VideoURL": "https://www.youtube.com/watch?v=G9YPMyqJ_20", "id0": "2019_02", "transcript": "One, but I want to thank you so much for coming here today. I want to thank Jared and Lainer Analytics for having me. I want to thank my friend Dan Chen for pushing me to submit something to this conference. So I'm really excited to be here. My name is Samantha Tiner. I'm a AAAS Science and Technology Policy Fellow at the Bureau of Labor Statistics. If you want to know more about this awesome program, please come up and talk to you about it afterwards. It's really great if you are a PhD when you're looking to get into government. So today I'm going to be talking about PageDome. And I have a few disclaimers before I get started here. First of all, I want to talk about my background. I didn't write this thing that I'm going to talk about. It's the work of three guys at our studio. So I know that there's some people from, I think there's some people from our studio in the audience. So if I make a mistake, please forgive me. But basically what I'm going to be talking about is this thing called PageDome. And it's this really cool R package that allows you to write PageHTML. So you can print off a PageHTML document so that it looks like you made it from a PDF, but it's actually from HTML. So anyways, I'm also going to make some assumptions about you in this talk. So some assumptions about you. Firstly, they're not going to be accurate 100% of the time. So I apologize in advance for that. First, I'm going to assume that you're at least familiar with Latec as a thing that is used for making PDFs and writing things in math. Then I'm going to assume that you're familiar with an R Markdown document that you know what YAML is, what R Markdown is, and you know how to make an R code chunk. And finally, I'm going to assume that you'll at least enjoy some of my pop culture references. And if not, I apologize for that, you know, millennials, right? So before I talk about PageDome, I obviously have to talk about Latec, but why would I spend time talking about Latec when I'm trying to talk about this cool new thing called PageDome. And the idea is, I've got to give you some background, right? I have to give you some background on why we need PageDome, and I need you to trust me, and I can tell you that I use Latec a lot. I loved it for many, many, many years until very recently, when R Markdown started becoming a thing, and now I use R Markdown pretty much exclusively. So let's start at the beginning. So this is me in 2011. I'm the one with the banks. And this is my REU, which is a research experience for undergraduates. I was at Bell Perrezo University, and this is me with my friends, Mike and Casey, and our advisor, Alara Pudwell. And what we were doing was we were doing math for fun over the summer, as one does, when one is a giant nerd like us. And so this poster was the culmination of our entire summer's work. We spent hours and hours critiquing this thing, putting an H space here, putting a V space there, one block here, one block there. So that was the summer of Latec. I totally fell in love with Latec. I loved it after that. I used it in all my math homework assignments. I used it in all my graduate school homework assignments to take notes and classes, everything. So I totally fell in love with it. And I even, because I am a huge nerd, I even had a favorite command, which is Phantom. Anybody familiar with this command? Latec uses? OK, a couple of people. So this is kind of a crazy command. Don't know where it comes from, but I discovered it when I first learned Latec. And hence, every time I use Latec, it might show up a little bit. I'm a little bit better now. So the idea with this thing is you can add any amount of white space exactly where you need it, regardless of the template you're using, of any of the fiddly Latec rules, whatever. So you can just add white space at random. And so sometimes it looked like this, Phantom, with two little As in there, just a little bit of space. And then sometimes when I was really frustrated, it looked like this. And then, quite often, it ended up looking like this. It's just Phantom's everywhere. And who knows what space is coming from where? In the poster I showed at the beginning, there were only 16 uses of Phantom, which I thought was pretty restrained, actually, looking back. But dealing with Latec, you know, templates is kind of just an annoyance, really. They're really nice and they're really beautiful. But sort of some of the tricky behaviors is really more of an annoyance than anything else. But there are some larger problems with Latec. And that's what I'm going to talk about now. So first and foremost, communication has fully shifted almost completely from printed methods to web-based electronic methods. Latec is for printing and HTML is for the web. So we need to switch to something for HTML. Again, getting tables out of PDF is a huge problem for me in my experience. So essentially, there are tools that do this super, super well. I haven't found one that does it well consistently all the time. So that's a problem, in my opinion. And finally, Latec is actually inefficient. So here's this paper from a few years ago that found that typing a document in Latec is actually slower than typing it in Word. So this is the abstract. The link is there if you want to go. But what you're seeing here is that the office concluded that even experienced Latec users, so people like me and other academics, they may suffer loss in productivity when they type of use relative to other document preparation systems. So there are validances of this paper out there. But the point is pretty compelling, right? We think this Latec is this great, awesome tool, but it's actually slower than we think it is. And I still switch back and forth occasionally between Latec and Markdown to do things. Markdown does feel faster to me. And although I'm still fairly anti-Microsoft Word, I use it every day because I work in the government now. So six and one half a dozen of the other. If that doesn't convince you that Latec is inefficient, that it's taking up so much of your time, consider the space that it's taking up on your computer. The last time I looked at McTuck, it was 200 gigabytes for Windows and 50 gigabytes for Mac. So that's a lot of space, right? So it's inefficient. But that being said, Latec does have some obvious benefits. First and foremost, it's beautiful at time setting. And that's what it was made for. It was made for time setting books with lots of math. And it's really as excellent as that does that really well. It cannot be beat, right? It's pretty ubiquitous. So in more quantitative fields, especially, everybody in academia uses Latec, it's in escapable often. Many journals do have custom Latec templates. And then it can just seem like it's not worth the trouble to avoid it. And also you kind of get this feeling of belonging, right? And there's a, the belonging is a link to an article that talks more about this. But when you learn Latec, and this has been at least my experience, is that, you know, you have all this hieroglyphics on your screen, right? And all of a sudden you press this button or you press a couple buttons on your computer. And voila, you get this beautiful typeset document with all this beautiful mathematical formulas. And you just get this rush of academics superiority runs over you, right? And so that feels good, right? But maybe all of this is more of a, you know, occult. But you know, we're only human and belonging feels good, right? So this is an advantage of Latec, right? It does what it does well. It's pretty much everywhere in some, in some circles. And you know, you feel like you belong to something. Okay. So now that we've covered that, let's talk about page down. So first we have to talk about why are we going to use page down? So hopefully, and what motivates it. So you've been paying attention for the first part of this talk, despite all the typical possibilities. But why are you going to use page down? Well, the main reason is that the web has taken a much faster to share things over the web, more eco-friendly to share things with the web. But printing is not ever going to go away. We're still going to have it. I mean, we still use facts and things all the time for other purposes, right? Printing is never going to go away. So how do we move these two arenas closer together? And that's the motivation behind page down. And now most people would be pretty intimidated by this research. But not the guys who wrote page down, right? So they're saying, you see this question, can we print a book with HTML and CSS? And these guys say, yes, we can. Another motivation for page down is that printing from the web can get pretty ugly, right? So you have this printed off version with all this stuff on the top, scroll on the bottom, and then you have all these boxes moved all over the place, and it looks nothing like the beautiful web that you saw. And so that page down is a way to make printing from the web prettier. So that's another motivation. And how are we going to do this? And in this section, I make no claim to be an expert in understanding things that are HTML, CSS, I'm not a web developer. I'm just somebody who really, really, really likes R, and I like to use it for everything. I've used it to send emails, everything. So this page.js is an open source library that is going to paginate content in the browser. So it's a fairly new thing. And it's also with CSS, right? CSS is what controls the style behind all things HTML. And then in an HTML page, you can sort of see that these are, if you look at all these boxes, those are all different CSS classes. So you can get, it's pretty strict structure. But you can see that you can control each one of these little boxes here with CSS and with your in-page zone with HTML. So how do we use it? This is two lines of code. There are less than 10 in this talk. So you just install it from cran or from the GitHub repo for the development version, like any other R package. And then you can use it right away. So the easiest way to do it is to go into our studio, just create a new R Markdown file, and you pick this page HTML document that's like the default, that's like your standard R Markdown document for paged HTML. And so like all things in R Markdown, a page-known document is going to start with a YAML header, and it contains Markdown and code chunks. And that's it. You know, all you have to do, really, the only difference here between a page-known document and any other R Markdown document that you might be using is just these three lines of highlighted YAML. That's it. So there's also lots of templates. You can, if you go back a few slides, you can see there's a post-circle, post-er options, there's a paper option, a few other options. So you can just use the templates. So in my experience, the templates are super friendly. You just edit the YAML, put your own name, your own date, whatever. And then you change, maybe there's some loremates in there somewhere. You change some of that, and you're good to go. The really tricky part comes when there's any sort of customization that you want to do with CSS. So that does get really tricky. And again, I'm not a web developer. I'm not an expert in this area. So do go to the page-down website. And the page-down website is a, I'm just going to take this off. The page-down website is itself a page-down document. So go see that. Again, to render a page-down, one of the things that you want to do is, oh, that's fine. This is one of the trickiest things. So if you go to use page-down, just remember this slide. That's really the only thing you need to know. So it needs a web server to view it. So instead of hitting knit and opening it up in Google Chrome, you can't do that. You'll see nothing. So instead, run this thing in the command line. So what you do is you have your R Markdown document open. Here I have my CV that's in page-down. And then you run this infinite Moon Reader thing, line of code in the console, and then it opens up in the viewer. Done. That's it. And then you can open it up in Google and do all of that. OK. So now we can do some fun things, which are page-down examples. So first, this is my web CV. I like it a lot. I think it's really, really pretty. I like it so much more. Oh, whoops. I like it so much more than all of my other than my other latex CV, which it was like. I had spent hours on this thing making it beautiful, right? And this was like 20 minutes just typing things. It's really, really nice. My favorite part was you can see these little icons next to everything. That's my favorite part. It's being icons. I'm particularly proud of, let's see, this one right here for leadership and service. I'm really proud of that one. But anyways, I mean, all you do is do silly little things like that, and it just does it all for you. So that's really, really nice, right? That's one of the major advantages of page-down. Also, you can do business cards. So I happen to make this business card for my partner. He's on the job. And right now, he moved out here for my new job. So he needs a job. So if you know anybody hiring lawyers right there. But this took me all of three minutes to do in page-down. It's pretty easy. But we have to conclude with some of the disadvantages of page-down. Like I said, you know, Laitech is very, very superior in these things. It's very dominant. So we do have to talk about the disadvantages of page-down. So first of all, it's brand new. I'm not going to sing the song, but you know the song. It's not well developed yet. This thing that it's based on the page.js is still pretty under development. The page on itself is still under development. You had just premiered it at RStudioConf earlier this year. And Laitech has about 40 years, 50 years on it. So it's a baby. A baby chart. Another disadvantage of page-down is that it's very strict. So it's very dependent on exactly precise CSS classes. And I like how I shared with all those little boxes on the page-down document. And it can be fiddly with the zoom level. So if you're zoomed in too far, things will move around, all sorts of things. Customization is really hard. So that's what another problem is with page-down. So you can picture all of your content. It's a cute little girl with pigtails. And page-down is mistrunchable. Just launching her around wherever she feels like it. I actually did try to recreate that poster that I showed you at the beginning that was made in Laitech. It didn't work. I don't know if I just didn't set enough time for it or I was trying to fit this squirt of Laitech into this beautiful round hole of page-down, but it just didn't work. But so templates are very strict. That's another disadvantage of page-down. You have to design specifically the poster. You have to design it with the page-down template in mind. And finally, it changes hard. People love Laitech. Researchers are using it for years and it does the job. People don't like having to learn new tools. Case in point, why does our iPhone keyboard look like this? It's because many, many years ago when we were doing typewriters, all the bars got stuck together, so they had to put them like this. And if you switch to a more sensible alphabetical order keyboard, it might be faster, but it's just not worth the hassle to switch. So this is how we do things now. It's kind of the same thing with Laitech. This is just how we do things now. So I'd like to conclude here by saying that I still like Laitech. It's still beautiful. It still does a really good job. It is bulky and it can be inefficient, so keep that in mind. But I really hope that you will start switching to page-down because I really like it. It's forward thinking. It is flawed, but it's really a lot of fun. And I hope that as we move forward and markdown becomes more and more popular, especially with reproducibility, I think that's really important. So I'll close by saying that. And I just want to say thank you so much for putting up with the technical issues. And thank you. And you can see the slides and find me online. Thank you so much."}, {"Year": 2019, "Speaker": "Tommy Jones", "Title": "Optimizing Topic Models for Classification Tasks", "Abstract": "Topic models are hard to evaluate as the variables they measure are latent, i.e. unobserved. Most topic model evaluation has focused on coherence metrics to mimic human judgment or preference. Yet topic models are often not used merely for the pleasure of the human eye. Instead, topic models are often used to support classification tasks, where ground truth exists. In this research, I compare LDA and LSA on how well they support a simple classification task. I use a Bayesian optimization service\u2014SigOpt\u2014to aid choosing the hyperparameters for each model, allowing each to be at its best. I optimize for both coherence of the topic model as well as classification accuracy on held-out data. All code is performed in R using primarily the textmineR, randomForest, and SigOptR packages and available on GitHub.", "VideoURL": "https://www.youtube.com/watch?v=k6kCSzWigiI", "id0": "2019_03", "transcript": "All right, well, thanks guys. Hi, I'm Tommy. I've been studying topic models since about 2013. Topic models and their newer cousins, text embeddings are latent variable models of language. Latent means that you don't see what your model is trying to estimate, and that makes it hard to know whether you have a good model or a really, really bad model that just looks good or a really, really bad model that you don't know if it's good or bad or not. And I am among other things a PhD student in my research centers around ways to make topic models better. In that vein today, I'm going to talk to you about a little experiment I ran. So before we get to that, let's take a step back and just review things. So first of all, a quick show of hands. Who here has heard of latent Dirichlet allocation? And it's a good number of people. Who here has used it? So that's probably about a third of the room, actually. So that's pretty good. So LDA is basically probabilistic matrix decomposition. You have three hyperparameters. So K is the number of topics. You have to fix that yourself. Alpha controls the concentration of topics within a document or what's in that theta matrix there. And then beta controls the concentration of words within a topic or that phi matrix. And anyone who's used LDA or talked to somebody who's used it will probably find somebody who is freaking out about what do I pick for these things, especially that number of topics. I have no idea what a right or wrong answer is. And a little note on vocabularies, since I know it's in our conference, there's lots of statisticians here. So in machine learning land, they use hyperparameters. The definition is slightly different than Bayesian statistics. So in machine learning land, a hyperparameter is anything that you have to set as a modeler rather than a parameter that is learned or fit while you're training this algorithm. So that's how we're using that today. And so the foil for my LDA in this comparison of this experiment is latent semantic analysis or LSI. Quick show of hands, who here has heard or used this method before? Actually fewer people than LDA. That's actually kind of crazy. So LSI has been around since about 1989. So it's not probabilistic. It's just a matrix partitioning algorithm. So you take a document term matrix or a TFIDF matrix. And you are going to factor it using a single-value decomposition. It's not what you get isn't probabilities, but in general, you're still getting a distribution of topics amongst a document and a distribution of words within topics. It really only has one hyperparameter, and that's K, the number of topics. You could get fancier with this, but for now the simplest version, it's just that one. And like I said, topic models and embeddings, they're latent variable models, they're hard to evaluate. The overwhelmingly most popular evaluation method is something called semantic coherence. So there's a bunch of different measures for this and formulas, but the idea here is that you can look at a distribution of words from a topic, and as a human being you're like, oh, that makes sense to me. And so the one that I use often ships with text minor, it's a package I wrote, that tries to look at the probability of words co-occurring within a document controlled for statistical independence of those words. Don't worry about that now. So coherence is a really popular way to evaluate these models. I'm not going to talk about goodness of fit, I can bend your ear about it later in the break. And then task specific accuracy. So a lot of times people are not using topic models or embeddings because they actually care about the model itself and want to interpret it. Instead what they're doing is trying to make some sort of numeric features of text and then feed that into some downstream task, like document classification. So maybe we can say a topic model is good or not good if, depending on how well it does on this downstream task. So the two things to keep in mind as we go forward and I tell you about this experiment is coherence. That's sort of like how interpretable is this accuracy. That's sort of like how well did this do for whatever that downstream machine learning task was. So I've actually been introduced as this is Tommy. He still uses LDA, but other than that he's okay. A whole chapter of my dissertation is just going to be this GIF on repeat. Last year especially has seen an explosion of deep learning based text embeddings that have really advanced the state of the art in natural language processing. But here is my argument for why this still matters. So first of all people still use this model like a lot. So knowing how to do it better is very helpful to a large number of people even if we're not state of the art. Second of all I think that studying these comparatively mechanically simpler topic models can actually give us some insight into these much fancier, more complicated deep learning embedding models that are arguably trying to do largely the same thing. And third we stopped the machine learning community really stopped studying LDA when machine learning for image recognition came out. So we never actually learned how to fit a good LDA model before we stopped caring and moved on. So I'm actually not convinced that a well specified LDA model wouldn't be fairly competitive with some of these newer methods. But watch this space in the next couple of years and see if I try and write that paper. We'll see how right or wrong I am there. That's what my gut's telling me. But if you insist. So in this work I use the neural network to write our code to use a Bayesian optimization service for topic modeling. So I've already really spoken to this a bunch today. But topic models are hard to evaluate. But think about what people use them for. That's either I want to interpret something from it or maybe there's this downstream task that's a little bit more objective than this fuzzy latent variable thing. And so maybe we can optimize a topic model for these things and see what the mechanics were that led to doing well or poorly on that task. So here's basically the goal. Take a topic model. Use it to embed some documents and some latent topic space. Use that embedded text to train a classifier. So for this I use the 20 news groups data set. And if I can date myself just a little bit for the younger folks in the audience. A news group is like a subreddit. But from the 1990s. And so the task here is to predict from which of these 20 news groups or subreddits the document came from. And so the idea here is that using my optimization service, and I'm going to talk about in a little bit, I'm going to create a bunch of observations around what hyperparameter settings gave me what values on these evaluation metrics so I can get a sense of what are the underlying mechanics of these models and how do they work. This is an art conference so I don't have a ton of art code that's just not my style. But here's my tech stack here. I have a policy against reading slides. So instead let me justify the selection of random forest. Again, neural networks are eating the world. Random forest just works. It's like an 85% solution. And since I wanted to spend my time studying the topic models, I didn't want to like pollute that with a more complicated model that has its own hyperparameters that need to be tuned and optimized. I wanted something I could just use out of the box. So I just straight up used the defaults from the random forest package. So I talked around this a little bit. So I used a hyperparameter optimization service called SIGopt. What this is is it's an ensemble of Bayesian optimizers. So every time you report an observation of with these hyperparameter settings, I got this evaluation metric score. It says, well, then try these new ones. And you're basically creating a data set on how well your model searches through this space. Ideally, you're fitting fewer models than like using random search or grid search so you can get to something optimal quicker. But you can also still create this data set of how well my model performed and study it. So basically you take your training data, you train a model, you report this to the service, it then suggests new hyperparameter settings, you wash rinse and repeat, do that in a loop, and then finally you get what might be an optimal model according to that metric you're trying to maximize. Full disclosure, my company has invested in SIGopt, so I'm not just a client, I'm also an investor. But I've used it here, I've used it for my deep learning class, which is the last class of my PhD, which I did last year. So investment status aside, I think it's a pretty nifty tool, it's pretty useful, it's helped me. So at a high level, this is the process. Take your documents, clean them, then you're going to train a topic model, then you're going to train a classifier and stick the SIGopt optimization loop in the middle here. Pretty standard cleaning and curation process. I used unigrams, I removed both stop words and words that appeared in fewer than five documents, nothing crazy there. But because of the scalability issues of LDA, or at least the Gibbs sampler that I wrote, and using SIGopt, I ended up with some pretty wonky training and test splits. So I took my split, the data set in the thirds at first, so you have your training set, then you have your validation set, and then finally you have this test set. And instead I took my training set and then split off the first thousand observations, use that to train the topic model, then got topic predictions for the remaining 5,600 observations, used that to train random forest, and then for that middle validation set there, I then got topic predictions and then random forest class predictions, and then reported those metrics off to SIGopt, so that was all out of sample. I haven't gotten to that bottom set here, this is an ongoing work for a blog post, I'm writing, but what I'll do for that last one is just take whatever optimal model I choose and get a true out of sample thing. So then here this is that whole optimization loop. So SIGopt has an R API, it's called SIGopt-R. It's pretty standard if anyone's used cloud services before. You get an API token, you're going to set it as a system environmental variable. You're going to declare an experiment and hand that off to that API so that it actually knows what the heck it's doing. You're going to declare a function to train your model and calculate those evaluation metrics and report them out, and then you're going to loop over that process that I described before. And so here's the only R code in this presentation, so up there at the top is setting that environmental variable. You don't get to see my SIGopt API key because I don't want that in the public domain. Set this experiment here, so I've got K, I've got alpha, ask me in the break why that says beta sum, not beta, and then you'll regret asking me that. But I won't let you escape, I will make you learn. And then I ran four parallel workers and got 100 observations, so I get a small data set of 100 observations of this hyperferometer space for these models. This big old function does that train test split that I talked about, trains a topic model, applies it to a held out set, trains random forest, applies that to a held out set, calculates the metrics, and then spits those out at the bottom of the function. And then here in step four is that main optimization loop. So there's the system sleep up there. What I discovered is that when I was running it in parallel, my computer got ahead of SIGopt server and then it would just hang and I would never get any results. So I found that by putting the system to sleep, it would like SIGopt server could spin its workers up. I've reported that to the SIGopt guys, like hey, maybe you should document that or fix it. Their engineers are on the case. And then the bottoms where you get the optimal experiment. So that's all fine in theory. How did this work out for me in practice so far so good? Let's use Google instead of my wussy little Mac book to run this. Don't let your code run for a week without checking on it. I can think of better ways to spend $40. I should have seen this coming. This wasn't a huge data set. I don't know what I was thinking. But eventually I fixed it and got results in a few hours. And now I have this nice little issue that I'm going to open up on github that says go fix your Gibbs sampler. So as I said, I optimize for both accuracy and coherence. And what this plots here is how you get accuracy versus coherence for a whole bunch of hyperparameter settings for both LSA on the left here in purple and LDA in green on the right. The orange lines are what it's called a pareto frontier. So basically what that's saying is you can't at that point, you can't do better on one of these metrics without doing worse on the other. So these would be considered like optimal points and then you have to decide do I care more about interpretability or do I care more about classification accuracy. So a couple of things jump out to me right away. So one, LDA just across the board way more accurate at that classification task than LSA. LSA only produces more coherent topics for like terrible accuracy scores. So you can interpret it but the model doesn't seem to be very good at predicting the class accuracy, at least not on this data set. The trade off between coherence and accuracy for LDA on that pareto curve is really tight. So I don't have to give up too much in interpretability to get some pretty good prediction accuracy. And then all of those points inside of the pareto curve would be inefficient points. And so another thing that jumps out at me is that I think because LDA has a much more complicated hyperparameter settings that you can do, you get a lot more inefficient points. Which goes back to my earlier thing of we got distracted before we even knew how to use these models. You know, there's a lot of ways to specify not very useful LDA models. But done well, you can get something that's reasonably good, at least on this data set for this task. So SIGOPT, they've got a fancy user interface that I'm not going to bring up now. But they report hyperparameter important scores that's sort of like variable importance from random forest or something like that. And so what I can see for LDA at least is that K is by far the most important parameter. So the number of topics, by far the most important in terms of your accuracy. But then all three of those parameters have to work in concert together to get interpretable models, to get reasonably good coherent scores. Again, fitting to this, these are, this is a complicated space. You can fit a model that isn't very good pretty easily. For the sake of brevity, I'm not going to show you all the graphs I made. But in terms of number of topics, here's comparing LDA and LSA. So again, we see in terms of both coherence and accuracy, LDA is a lot better. But another thing I see is that accuracy is pretty high and I get pretty tight groupings across this whole range of the number of topics for LDA. But coherence is a whole lot noisier, again, fitting to, like owning to this, like getting all three of those parameters right when you fit your model is what's going to get you a good one or a bad one. All right, and, all right, so what? So I guess if you can afford the compute, I would say use LDA, not LSA. I can't really speak to how it performs against these neural network based methods. But someday I'll test that out. LDA is pretty computationally responsive, but trying to fix my Gibbs sample error has, like, led me to realize that there's been some new algorithms in the last few years where people have tried to fit LDA in a much more scalable way. So let's watch this space. Maybe I'll implement one in text minor. Maybe I'll get distracted. Who knows? I have ADD. But one thing to consider here is that I got 93% accuracy on this classification task on this data set with LDA. Pretty much for free, right? Like out of the box, no matter what my setting of K was, like it was just really high on only a thousand documents. And then I had held out sets of about, you know, it was about 10,000 or more of the documents there that I ended up predicting on. That's pretty good for such a small sample to generalize that well. Maybe a property of the data, maybe a property of the algorithm, probably a fraction between the two. And so, like I said, I used SIG-OPT in my deep learning class. I've used it for this product. Yeah, my company invested in it, but it's a pretty useful tool. I'm also still bullish on LDA even in 2019. Come find me in the break and I'll talk your ear off and, again, make you regret ever having asked me any of these questions. And since I've got about a minute left, I want to do a quick pitch. So we're hiring here at Inkutel. You know, I've been there about two and a half years. I'd reached a point in my career as a data scientist where I'd gotten up into management and I realized that, you know, management, but I was still writing code for half of my day. But I realized that none of my problems were technical. They were all human problems. So, like, convincing my clients that the data that they had was insufficient for the question they wanted to ask and convincing them to go collect a sample and then they would argue with me, no machine learning is supposed to fix it. And I just was not equipped to, like, win that argument without pissing people off. And so you just end up sort of being like, I guess I'll muddle through and build you a model. So I came to this investment firm, which is a very person driven business, to specifically work on those skills. And at first I got my butt kicked and had to have my colleagues bail me out on my first negotiation, but bit by bit I got better and better and better. And more recently I just had a project where I was able to convince our customers, like, no, you need to focus on the data. You can't just, like, take this black box machine learning model and get what you want out of it. So I feel, you know, taking a couple years out of direct, you know, doing tech has helped me. So when I go back into the world, I can get the projects that are the right ones resource because I can convince people to do it. So if you want to talk, let me know in the break. And then that's the end of my time. So thank you very much, guys."}, {"Year": 2019, "Speaker": "Emily Robinson", "Title": "funneljoin: Defining a Tidy Grammar of Funnels in R", "Abstract": "In this talk, I introduced Funnel Join, a package developed with Dave Robinson and Anthony Baker to create a tidy grammar of funnels in R. Funnels track sequences of user events over time, such as page views or conversions, and can be applied to various domains beyond web analytics. Traditional approaches to funnel analysis are cumbersome and error-prone, requiring multiple steps and manual code adjustments for each query change. Funnel Join simplifies this process by providing a suite of functions to define and combine various funnel types, such as first, last, and any events, using a streamlined syntax. I demonstrated how Funnel Join can be used to analyze Stack Overflow data, showing how quickly users transition from asking to answering questions. The package, available on GitHub, empowers users to explore new types of data inquiries easily, ultimately fostering creativity and reducing errors in data analysis. More information and examples are available on my blog and the package's vignette.", "VideoURL": "https://www.youtube.com/watch?v=-n4XaYHDlG8", "id0": "2019_04", "transcript": "All right. Hi everyone. Really nice to be here. Choosing a walk up song is always a little bit difficult whether it should be thematic with the talk is what I went with this time. But last time I went with Mrs. Robinson which was a big hit when it landed on the Mrs. Robinson and everyone realized what song it was. Yeah, so but today we'll be talking about Funnel Join which is a package that I've created along with Dave Robinson and Anthony Baker to do define a tidy grammar of funnels in R. So what do I mean by a funnel? A funnel is a set of events by users over time. So let's take a few examples. Let's say you're an e-commerce company redesigning your homepage and you want to understand the behavior and who are these users visiting this homepage. Some things you might want to know about them include what were the pages the last page people saw before coming to this homepage or you might say and said what happened afterward what product pages did they see? If you're an e-commerce company you care a lot about conversions people actually buying. So what is that conversion rate and what if you limit that to a two-day window? So these are all questions looking at users in this case visitors to your website understanding their behavior whether that's page views or conversions and over a set of time but it doesn't just have to be people. What are some other questions we can ask in this way of this kind of first this happened then that and we have an identifier for who did it and what the event was in the time? Well I solicited some potential questions from people and I got some interesting answers like which salmon migrated to station one then station three to station two this is a question that ecology researchers might ask and understanding the migration patterns. What drugs did people take in the last month before starting another drug to make sure that you can capture any interactions? What was the last ad people clicked before they registered or what companies had their stocks had a hundred and then dropped to forty. So right these are companies, salmon, etc. It's not just people and they all haven't common this type of structure of the question. So let's take a specific example. When was each user's first landing and then the first then their registration first registration afterward? So we have two fairly small symbol tables here which just has landing registration and the user who did it in the time. Now currently if I asked you to answer this question your workflow might look something like this. You'd first have to filter to get the first landing per user. Then you need to left join with the other table registered on user ID but remember we only want those registrations that happen after the landing. So you've got a filter to make sure that the registration time is greater than landing or in some cases you want to keep those who don't have a registration. And finally we need to filter for the first row of the registration because we only want their first registration afterwards. Now this was a little you know this was a fair amount of code probably something most of you could code up but let's say then I switch up the question a bit. What about who registered for the first time ever after their first landing? So not I don't want to keep people who had their second registration after they landed I just needed to be their first ever. So now we've got to change it up a bit. We have say let's filter now for their last landing because I'm saying I only want their last landing and then first time ever registration. So we change up and add the descending. Then I need to join again with registered but I need to only join on the first people's first ever registration. And finally again I do that filtering. So this is a place where a lot of errors can come in right? You're copying and pasting some of the code but you have to remember oh I got to change it to descending. Oh I have to make sure I only get the first ever registration and there's a lot of room for error here. And it's also can be a little difficult to think about it. It's not super easy to switch between these two types of funnels. And that's why we created the funnel join package. You can find it on GitHub it's not yet on crayon but we've been developing it for about a year and a half so it's pretty mature now. And this is meant to solve these types of issues. Let's take a look how it does that. Because the goal of this talk for you and funnels is maybe before now this was you. This is Cinder Block the cat who's trying to lose some weight and as you can see is not very enthusiastic about this prospect. So maybe with funnels you're like yeah I guess I can do it. I really don't want to be here. But right after this talk this is going to be you. You're going to start taking your first step. You're like I can do this you know this is all fine. And then finally tomorrow because I promise you funnel join really is that simple. You will just be going and leaping over any problems that you encounter. So that is what I hope for you to all take away from this talk. Let's dive into funnel join. What is this magic package? Well let's take that question we had. What was that first landing in the first registration afterward? So here's how funnel join addresses this question with the after join function. Let's break it down piece by piece. So first we start with the table one which of these tables is the first event. In this case landed. Then we take table two registered. We want to do a left join so we could add it as an argument but funnel join you can just write after underscore left underscore join just like you could have right in there etc. Then we say what are the user columns that we're joining? What's the unique identifier for it could be people again company salmon? What are the time column names? How do we know when these events are happening? And then finally the type of after join. The first four arguments here probably look pretty familiar. Right we're used to this when we're joining any tables we have to tell it what the tables are what are the columns we join it on. But what is this type thing? So this is new and this is what funnel join is contributing here. So a type here is saying we want the first landing and the first after registration. We can look at some different types of funnels. Let's say this we have this user gym and this is Jim's behavior on our website with ad clicks and conversions. These are all happening over times we see he has a conversion that an ad click another conversion three ad clicks two conversions. What are the types of questions we might ask about Jim's behavior? We could say all right what was Jim's first ad click and his first conversion afterwards. So here we go we highlight his first ad click and we go and link that to the first conversion. This is the first first after join we just saw. But then we might ask instead what's his most recent ad click what's the last one and then all the conversion afterward. So here we highlight his last ad click then we link it to the conversion afterward but we link it to the second one as well. This is a last any join. We take the last ad click and any conversions that happen afterward. Finally maybe we want as big a funnel as possible so we want to link all the ad clicks and all the conversions that happen afterwards. So here everything is highlighted except that conversion that happens before any ad clicks because again we're defining all conversions must happen after the ad click but here we end up having many links. So then this is an any any type of join. So what we see here is that we can specify all of these different types of funnels. A first first after a last any and any any because depending on the type of question that we're asking funnel join comes with 16 types of funnels that are any combination of first last any and last before and first last any and first after. So you can combine you can have an any any as we saw last before first and so on again to really give you flexibility and easily switch between different funnel types. And to show this a little bit more I'm actually going to do a live demo based on a blog post I wrote using Stack Overflow data. So let's switch over to that. Can we switch up there? Then maybe if not I can see I can just go this way. There we go. All right. So let's take a look. Here we have of course we always start off loading the tightyverse and then funnel join the package. Here I'm going to be loading in these answers and questions from Stack Overflow data. So if I go ahead and take a look at this what this is I'm just going to zoom in a little more. So these are people all these questions tagged with R you can download this data from Kaggle. So we have here the exact type of thing we need. We have an owner user ID who is the person answering this question and what was the creation date when did they answer it. And similarly we have the questions data set which has the same type of thing. So one question we might ask is how many people ask a question later answer one. So how many people actually here have ever answered a Stack Overflow question? Hi we got a small amount of people. How many folks have asked them? Yeah a good bit more right. So not too surprising more people ask questions and answer them. So how do we quantify that? So we're going to go and take the questions and we're going to do an after left join with answers because we want to keep all the question askers. We're going to find that the user column is owner user ID the time column is creation date and now we have the type of join. So in this case we want how many people so I need this to be one row per person so I'm going to do a first their first question and then who answers a question afterwards. So if I go ahead and run this we'll get a data set of 60,000 rows which is a number of unique people and we can use another funnel join function summarize conversions that will take something that is and use that we identify as the column that indicates whether or not there was a conversion and calculate some statistics. Before I do that I want to use a little trick that I actually wasn't familiar with but you can before making this package but is present in dplyr you can add a suffix so if I don't want these dot x's and dot y's to indicate for columns that are common between the two tables but weren't joined on which one it came from I can instead say I want that to be underscore question and underscore answer just to make it a little cleaner and now I'm going to say all right well I know that creation date answer is only going to exist if someone answered a question so let's see all right we find that of the 60,000 users in this data set who asked a question tag with R 22.7% of them later answered one but let's change it up a little bit how long did it take those people to answer their first question so I'm going to go ahead and copy this and now I can I can add the gap call argument and funnel join so gap call says it's going to create a column called dot gap which will give you the time difference in seconds between the events so in this case we see of course we have n a some people never answered a question someone else answered in 734 seconds someone else many more seconds so we actually do a quick visualization I'm going to mutate to get the hours so I'm going to do dot gap divided by 360 and then I can make a quick ggplot so I can say let's visualize the hour gap by and we're going to do a geom histogram and now we can see this this of course doesn't look very nice or it does it looks fine but it's not super easy to tell what's happening so actually I cheated a bit I pre-wrote a little bit of code just to make the scale better so I'm going to change it to be a log 10 scale with breaks that makes sense with the time and we can see that we have some people answer within an hour a good chunk answer within a day some a week and some wait as long as a month so if I want to say I can get an idea from here like okay roughly what percent of those who answered a question answered it within say a week but let's say I wanted to answer that right away with funnel join so I could use I could take funnel join and I can add an argument the max gap argument so here this is saying I only want to join questions with answers that have a maximum gap in time of this much so I'm going to it can take either an argument a second or I'm going to use as diff time so I'm going to say let's do seven units equals days and run that and we're going to do summarize conversion on creation date answer and we'll see that here before we had about 22.7 percent ever answer a question and we see here that 8.8 answer it within a week so finally let's switch it up a bit you know here we've been looking again always okay who asked a question then answer it but there might be a sort of strange population of users perhaps that answers a question before ever asking one do those people exist and how many of those do we have so here if I take the code that I've written I can now go ahead and switch it over so I have answers going first then the questions because I want to know answers before questions change it to a right join keep all the questions and now I change to a first first join because I want people I want to get their first answer ever and did that happen before their first ever question so here I don't want to just know who who asked a question after they answered I wanted I want to know specifically it must be their first ever question and then if I use that I can see that and we use again the creation date in this case question and this still answer and I can see that in this case they're in fact 4.6 percent of these users answered a question before they ever asked one now let's go back to the slide show here and I'll wrap up the talk so here we have in conclusion there are a couple joins of the funnel joint a couple goals the funnel joint package beyond taking you from cinderblock being really reluctantly exercising to the cap cap leaping over defense and the first goal is to bring the impossible to the possible now I put impossible in quotes right because the funnel joint is written in R these were all things that you could have done but maybe felt impossible to you you weren't really sure how to do answer these types of questions and code it and code it up but I think more impactful is taking the time consuming an error prone to the quick and easy again if we go back to that first example of switching up the type of funnel you could start copying and pasting your code and remembering oh I have to do descending because I want the loss instead of the first and I want to change it to this join but it really could end up being that you might introduce some bugs that way but finally going from having limited creativity to asking and answering new questions so the biggest benefit I found and I use funnel joint a lot now is that I am asking a whole new set of questions that I wouldn't have even thought of before just because it used to be kind of time consuming and slow and I didn't think of this way. Hadley Wickham's often talked about one of the big goals of the tidyverse is to help you fall into a pit of success to make it easy and also to let you iterate quickly that's why ggplot2 is so powerful because it has this grammar of graphics it's really simple to switch back and forth between different types of graphs which is very important when you're doing an exploratory analysis and here that's the same goal we had with funnel join that by making it so easy to explore these types of funnels you'll start asking new questions of your data. If you want to learn more I wrote a blog post on funnel join that used the same sacoverflow data and asked a few more questions. You can look at that on my blog on hookedondata.org there also is a vignette on the package down site robinsones.github.io that you can go to and find and of course as I mentioned it's on github under my github and finally with that thank you all so much you can find my blog as mentioned on hookedondata also on twitter and I'm actually writing a book as Jared mentioned on data science careers which you can find at datacicareer.com thank you all so much"}, {"Year": 2019, "Speaker": "Jared Lander", "Title": "Raising Baby with R", "Abstract": "While babies are commonly called bundles of joy, they are also bundles of data. Being the child of a data scientist and neuroscientist my son was certain to be analyzed myriad ways. So in this talk I'll discuss how we used data to narrow down possible names then look at using time series methods to analyze his sleeping and eating patterns. All in the name of science.", "VideoURL": "https://www.youtube.com/watch?v=CPNphrraLsw", "id0": "2019_05", "transcript": "All right. So, now, I recently welcomed the newest R programmer into my family. He was born almost on the 10th anniversary of the New York R muteup. So that made me really, really excited. We missed it by just a few days, but, you know, it can't be perfect. I want to thank Vivian Pang and the Lander Analytics team for making these hex t-shirts, because you'll see they are baby-themed hex t-shirts. Yeah. Gigi Poops, probably my favorite. And yes, these are true to life I've discovered. Messier, tiny, versatile. All the baby spitting up. All happens. So, what is in a name? We needed to pick a name for this child. And this is high stress, because this sticks to them for the rest of their lives. You wanted to be a good name. You wanted to be not too common, but not too odd. Now, for us, it was a constrained optimization problem. We needed an L. It's a tradition to name your children after, to cease family members. So we had an L. At least we had it narrowed down. There were some very obvious choices that came up right away. Lando Lander. I almost got this. My wife was a bit out of it after labor, and she signed a blank birth certificate. And I was really sitting there like, should I put in Lando? But then I've heard that the court system will actually strike down names or they're ridiculous. So the next name I really wanted, which fit that we needed an L, was Lunar Lander. She also said no. So we had to turn to baby name books. It turns out Bruce Lansky has written a lot of them. But we're more digital, so we turn to Tinder for baby names. You literally swipe, you and your partner swipe through names, and if you both match, you get notified that you both like a name. But then it dawned on me. How can I not use R? Anyone knows me? I met my wife at the army at up. R has to be involved. So I loaded up the baby names package. This is maintained by Hadley, and it tracks baby names in the US since 1880. So I went into this. I filtered on just more recent names than 1980, and I looked for boys. And I saw, I did accounting based on the popularity in each year, and came up with an overall ranking since 1980. So I took that ranking, and I made a distribution. So we just take the ones that are greater than a certain percentage, and we make this histogram. This is a histogram of all of the traditional boy names since 1980, with the red line being the middle of the pack. It's a left skewed graph, as you would expect, there's a lot of very common names that a bunch of very infrequent names. But we needed an L. We needed an L name, so remember that. Or an L sounding name. So it's L is a bit constraining, we figured anything that sounded like an L. So I went back to my query. I took the recent names, but I also did a red jacks in there for the names that either started with an L, or started with a vowel, then an L. So we went through there, and there was about 587 names that fit the criteria. So we worked our way through those 587 names, and we narrowed it down to two that we liked. Elliot and Lev. So I wanted to see how these stacked up with the rest of the population, so I made a histogram of just the L names and see where they landed. And predictably, Elliot is the 32nd most common name in red, and Lev is the 175th most common in blue. And while they're both good names for us, I very quickly ruined Elliot. My wife couldn't handle a lifetime of this. I was going to do it every day of his life. So, Lev won out. It means heart, and when we're in the hospital, it kept sounding like love, so my wife really liked it. So we went with Lev. So now that we have his name, the baby arrives. And we're like, OMG, we have a baby, what do we do? We had no clue what we were doing at all. We barely managed to get him home safely. So fortunately though, all babies really do is eat, poop, and sleep. Right? That's the majority of what they do. And they're often called a bundle of joy, but they're also a bundle of data. We have this app called Hatch Baby, which lets you record all their activities, being sleeping, diapers, and eating. And you can talk to Alexa to add items. So you're collecting all this data, and then you can export it to a CSV. Now you can see the CSV, it has all the pertinent information, but it's a little messy. They delineate sections with these three equal signs, begin feeding, and feeding. It's a little messy. And I try to figure out why it was messy, why would they delineate the sections? Every single column is the same in the whole dataset, why they make in these sections. And I thought, because Excel. So we're going to read that into R, but first we need to clean it up a little bit. So we're going to use the pipe function to call grep on the file as it's being read into R. And grep-v means search for the opposite. So I'm searching for any lines that don't have these three equal signs, and those get piped into R. So it's a nice way of doing pre-processing at the command line right before you even read it in. Then we go ahead, we specify the column types because it was a messy data. We specify the time zone because they didn't include the time zone, which leads to problems. And I restricted the data to just be when he was sleeping or eating. I didn't do the diapers because I wanted him to maintain some dignity. So once we get it into this nice format, we have to do some cleaning. We have to say, what was going on, clean it up, we had to fix them, again, fix more time zones, restrict the data to certain time periods. We had to deal with a time zone issue right here, convert the duration. The durations they use weren't necessarily great, so we converted it to durations based on some math. And we made it into a sibble, which is the new version of time series data in R. There's so many versions of time series data that there's a new one from Rob Heinemann. Importantly, we had to dedupe the data. There were a lot of duplicate entries because parents are tired. Notice my wife about to pass out in the corner. So we're really exhausted. But now that the data is clean, we want to see what was he doing. I want to figure out when was he sleeping, when was he not sleeping. So we took the data and calculated for every minute of his life, was he awake or was he asleep. You get it all ready and we also converted the time into a factor. And there was about 300,000 minutes of his life. You take this data set, which takes a little bit of manipulation, you pipe it into a ggplot, and we use geom tile to make a heat map. And then you get this type of plot inspired by quilt I saw on the internet, like this. The beige is when he was awake, the blue is when he was sleeping. And you can see he was very erratic at the beginning. Then he eventually stabilized and let us sleep through the night because we were so very lucky. But you notice there's a little blip right here. A little blip where he started getting a little weird. I was away at a conference in India in Singapore, and he got sick and kept waking up my wife. And I had a very untactful response. He missed me. That's why he kept waking up. Yeah. And of course he ended up getting sick a few more times, getting us all sick too. So I said, you know what, baby's also eat a lot. So I redid the code. I had to make it a little bit wider to make it just the table a little bit different for different plotting. I went ahead and plotted his eating on top of the sleeping in green. And you can see right here. All those green dots are when he was eating. And you can see he ate a lot when he was really young. As he got a little older, he started eating a little more consistently, a little more quickly. So things a little bit better. So then I thought, you know, let's fit some models on his sleeping and eating. So I go back to the data, start manipulating it again. And for every day of his life, I calculated how many minutes he spent sleeping and eating, the number of times he slept in eight, and the average amount of time he spent sleeping and eating for every single day. So we take that, and I also calculated his monthly milestones. Every time he gained a month in life to add to the plot. So we have those information about his sleeping and eating habits, his timing, and number of them in a sibble. So since it's in a sibble, we could just use auto plot, and it makes a very nice time series graph very quickly and easily. And we can see that his number of naps and his number of eatings have gone down over time. He's doing less and less than them of them. Similarly, his time spent has diverged. So I guess not similarly. He has spent less time eating and more time than we radically sleeping. And then we look at the averages, and we see, yeah, the average time corresponds to what we expected to see in the graph. So now to fit a proper model, we create a training and test set using filter index, which is from the sibble package, and allows you to take the, all sibbles are filtered, are indexed on a variable, so you know which variable to use. The dot means from the beginning of the data till day, till all of September, 2019. I didn't specify a day, I said a month. The test set was just all of October, or the days in October that we had. Then we fit a model, or models, using the model function. With this one function, we fit seven models. A mean model, a naive, seasonal naive, random walk with drift, an ETS, and a RIMA, and a dynamic linear model with RIMA errors. And that last model there, we're trying to see if I can see, if I can see what impact feeding has on sleeping. So we fit the models, and then we forecast it against the test data, and get the RMSC for accuracy, and we see which models perform best. And it was the dynamic linear model by a thin margin. And you can see that ETS that surprisingly poorly. It was the fifth best model, I was very surprised by that. But look how easy this is with Sibyl, and Fable, and Feasts to get all this time series information. This came out about two weeks ago, or three weeks ago. It's really amazing what they did. So let's look at the models. ETS was a multiplicative model with an additive trend, and no seasonality. The dynamic linear model has a coefficient for feeding, and that negative coefficient means that as he eats, as he spends less time eating, he's sleeping more. But there's a very wide confidence interval, so maybe it's not worth the trouble. Maybe we just go to a straight ARIMA 111 model. And this does almost as good of a job of modeling his patterns. And with Feasts and Sibyl and Fable, we can plot the forecast against the true test data and see what's happening. And it actually captures it fairly well, gives you a nice confidence interval. And it works. But now that we've seen that, less detect change points, also known as regime changes. And yes, he was the regime change in our lives. So we need to prep the data to do this. We need to use pivot longer, which replaced, gather, and tidy AR to make into a long data set. To do our regime changes, we are going to use profit. But we're not going to call profit directly. We are going to use a package called fable.profit to call profit in the fable framework. This way you don't need to know how to use profit, which requires a Y variable and a DS variable. This handles it all for you. We will then go ahead and get, oh, and the growth function says, hey, look for change periods in these predetermined change points. See if there was a regime change there. We also got out the forecast from this, and we found out which change points are detected were significant. We then go ahead and get the fitted values from the training data, and we get the change points. And there's a lot of pure code. I filed an issue on GitHub to have them integrate this into the functions. Within the next day, they had made the changes. But I had already written my slide, so I stuck with it. So we plot these change points for each of the variables here. And we can see there are quite a few regime changes in this patterns. Mostly falling on his monthly milestones. Which you kind of expect. That's what all the baby books tell you. Every month, I'll have a change. What I really notice is that the first milestone for the feedings, those changes in how he was eating, actually corresponded very nicely to his first pizza. He goes, he stares, he loves his pizza. He's my son. He's my son. So nice thinking, well, was there a difference in breast milk versus formula? So we go back to the data, because we're tracking it. And we assume that if the number of ounces was zero, it was breast milk. And if we had a number of ounces, it was formula in a bottle. So we take this data, we go ahead and we plot it like this. We have auto plot once again. And we see it's a little bit noisy, and I prefer a bit of a smoothed out time series. So I'm going to fit in a Rima model and plot the fitted values from the Rima model. So it's a little smoothed out. I could have used exponentially weighted moving average, but a Rima model worked very nicely. And you can see right when my wife went back to work, we started changing patterns here. And eventually bottle feeding became the dominant source. So after we looked at all the types of feeding, I was like, how long is he napping? And I'm not talking about sleeping overnight. We already looked at that. His naps during the day. So we go back to the original data. We find when he started napping, after 7 a.m. before 6 p.m., we numbered them and just looked at the first three. And we started in June, gave him a chance to stabilize, to see what was happening. And then we go ahead, since again this is a sibble, we plot it. And aside from a little blip in August where the data went a little haywire because I was in other time zones, we see it does look like he is sleeping more in his first nap. But I'm like, let's see this a little more smoothed out over time. I calculated the average for each of the naps for each month. So we just do a index by, which is sort of like group by, but a time series equivalent. And we plot it over time for each month. And we really do see a trend here that in the first nap of the day, he is sleeping longer and longer. And the second and third naps, he's sleeping less and less. And sometimes he doesn't even take our third naps, which is really, really awesome. No. Then I'm like, let's just look at this entirely over time and see how he's going. Just roll it all up. And when we make box plots, there's a lot of variance. And we put G.G. B. Swarm over it to get a sense of the distribution. He is sleeping mostly in his first nap right when his parents don't need a nap also. So thanks for the contribution. And you can see that his second and third naps were also shorter than the first one. So we get a lot of out of him. So what do we learn from him? What are our key takeaways? First, he really likes our t-shirts. And yes, I got him a DCR hoodie from this conference. But his patterns are changing each month. I'm sorry, wrong one. Naming things is hard. Much like off by one errors, you've got to be careful naming him. His patterns are changing with each month just as the baby books said they would. So people have been raising babies for thousands of years. They noticed some patterns. His sleeping has gotten better over time, which has been amazing for the parents. It's really helped us out a lot. And his eating barely predicts his sleep. We thought we would really see something about his sleeping patterns and eating patterns going together. It's a very small marginal effect. And he naps most in the mornings, which again is not awesome for the parents. And of course, he really loves pizza. As evidenced by this. So thank you very much. Thank you."}, {"Year": 2019, "Speaker": "Daniel Chen", "Title": "R and Python Coexisting in the Same Development Environment", "Abstract": "In this talk, I share my experiences and insights as a PhD student at Virginia Tech focusing on data science education for medical practitioners. I discuss my work on the \"Grade This\" package, which functions as a code grader for LearnR documents, enabling users to create educational content in the style of DataCamp or Codecademy. I explore the distinctions and complementary strengths between R and Python, particularly in data analysis, web development, and machine learning, illustrating how packages like reticulate facilitate interoperability between the two languages. I demonstrate how to leverage Python's versatility in R through reticulate for tasks like machine learning with scikit-learn and integrating Python models into R Shiny applications. Additionally, I touch on tools such as R Markdown for live code execution and communication. Lastly, I provide practical advice on using Conda with R and discuss emerging technologies like Apache Arrow for efficient data handling across languages.", "VideoURL": "https://www.youtube.com/watch?v=rRLhsjZHfVk", "id0": "2019_06", "transcript": "Hello everyone. I'm Daniel. I am currently a PhD student at Virginia Tech. Doing pretty much switched my thesis topic a couple of months ago. Doing data science education mainly focused for medical practitioners. So if you work in medicine, if you're a physician or nurse, let's talk about how you can learn R or Python. This past summer, I was an intern at RStudio and I worked on a package called Grade This, which is a code grader for learn R documents. So if you are in the education space and you want to create something like a data camp lesson or code academy lesson in that type of style, you can do that yourself. And then this is an autograder that tries to say, hey, I expected you to use the function called SAM when you use the function called MEAN. And I learned very quickly that to generate that sentence is very difficult. So I am also, like Jared said, I wrote this book about this other programming language and this is what this talk is about because we are at our conference and I'm going to talk about Python. So, our Python, all I did was just Google Python in R popularity and I just took the first link and this is really just a slide that says maybe Python is something to look at and pay attention to. I just saw a tweet this morning that pretty much said Python just surpassed Java as the second most popular language in something because I don't remember the rest of the tweet. But it's, if it passed Java, I mean, that means something especially for the enterprise world. So, so what is Python? Well, it is a general purpose programming language depending on where you are and who you are talking to. It may not be the best at everything or anything. But one of, at Python, one of the things I did here was it is probably second best at everything and that's pretty good to be the second best language at everything. What Python does better than R, which is the only updates I've made since I finished my talk, was updating a few things where are how Python handles environments. So if you've tried to ever pin packages or our versions, it's a little bit more difficult than how you would do in Python. Just the other day, a package called RM was released. So, fingers crossed, that might make life a little bit better. And one thing objectively, I can say Python does better than R is in the field or in the world of web development and hardware. So if you ever see this, this is a little circuit board by Adafruit and it's running Circuit Python. So that is my name tag. Code in Python. So what I really do like about R is the world of communication. It does a really great job at if you have done a piece of data analysis and you need to present it to other people. So stuff like Shiny and R Markdown are probably, you know, that is the reason why if I do an analysis in Python, it comes back into R if only for Gigiplot or if only for like putting it on a dashboard for Shiny. So the inspiration for this talk came from over the summer when Max Cun told me to say, hey, you're an intern, so you have to give this talk for me now. And I pretty much gave a 40 minute version of a four hour workshop on like, this is the difference is between R and Python. So you can see those notes there. It was written in Jupyter Notebooks and it uses the rise reveal plugin. And it was pretty slow. And what I realized right before my talk was I could just, I could have just use R Markdown to articulate to create my slides. And so that's what this talk is using. So R and Python. So let's just give an example. We can use R to load data sets. So here is the billboard data set from Hadley's tidy data paper. We can use the player and select to rename, to select some columns or rename the artist column. We can tidy the data set using tidy R. Right now we're using pivot longer. So that is the equivalent of spread in the previous version. And then we can use something like per to do some additional data cleaning. One thing that if you didn't know, when you write something like function X, some kind of thing and then X like that, that is exactly the same thing as this notation, this tilde formula notation. So if you've ever seen like why people use this formula notation in a per statement, just know that those two lines are equivalent. That period essentially is a placeholder for the letter X. So we can actually repeat that same analysis in Python. So we can load up pandas which gives Python the data frame object. We can load up free for regular expressions. Emily showed the janitor package in R. Well, there is the equivalent package in Python written by Eric Mark, called janitor that tries to implement the same things. I wrote a package called PyProd root and implemented the here function in Python, just like the version in R. And then we can load up and do our day analysis just like we did in R. And people usually give me a lot of grief saying that I like R over Python because Python doesn't have pipes. Well, Python kind of has pipes. It's called dot method chaining. And so you can see you can pretty much write your code and look down just like you would a deep py or pipeline. So how do you interact with R and Python? There is a package called reticulate. It lets you call Python from R. It gives you a translation layer between R and Python objects. And it lets you load up individual Python environments. So how do we use this? We can load up the reticulate library. I am in the data science field and a lot of data scientists use anaconda. So we are using conda environments. You can load, we can, this is just code because I was jumping around between windows and a Linux machine. But I'm just using my default mini condo environment. And we can use the use Conda M and that environment. And then what you have done is registered your Python environment in R. If you do this at home, realize that once you run use environment, if you want to pick a different environment, you have to restart your R session. Just running the command again won't actually change your Python environment. So we have this Python script. We have a data set, a data frame, this is exactly the same thing as before. And we can run something like a group by and a mean to get another value. And you can source this as a script if you want to R using reticulate source Python. And you will notice that you can actually just have those variables straight into your R environment. You source the Python file and all of a sudden those exact variable names just exist. And here we get a vector mean ranked by weak. And then we actually get a regular data frame. And this is an R data frame, even though it is a script written in Python. And there is an entire translation layer between R objects and Python objects. If they source, they just come into R. So for example, you just saw a pandas data frame object being loaded into an R data frame object. So let's go through another example for machine learning. So there is a library called scikitlearn. The API for scikitlearn was sort of the inspiration from Axq and put together karat and parsnip, which was just a unified API to fit your machine learning models. scikitlearn has a function called load breast cancer to load a breast cancer data set. And the weird thing about Python is there isn't really a consistent way of loading data from packages. So in scikitlearn, you get this thing called a bunch, but you can use like seaborn to load up the tips data frame, tips data set, and you get a data frame object. So things are a little bit inconsistent, but you have to check documentation for it. In the cancer data set, there is a column where there is a value called target, which is the response variable or the y variable. There is also a variable called data, and that is the actual matrix. So scikitlearn runs everything on a matrix. It can handle data frame objects, but by default, everything is a matrix. We can use scikitlearn to train test split our data. So pre-processing is done through scikitlearn. We can use a minmax scalar to scale our data, and then we can transform our X training set this way in Python. And then we can create a support vector machine for classification using svm.fit, put in our training data, put in our response values, and then we will fit a model. And then by default scikitlearn has the ability to score our data set, in this case the default metric is called accuracy. So we can do that same exact thing in R. We can load up, reticulate, for example. You can see I'm loading up my Python environment using reticulate. If I really wanted to, I can use reticulate to load in that breast cancer data set from a Python library and create our data frame out of Python libraries. So you can use it for that. And we can use a combination of R sample and recipes, which is the new way of creating machine learning pipelines in R, to essentially set up train test split and do our data transformations in R. And then we can use parsnip to say, hey, this is the support vector machine that I want to fit, and you can get an entire table here in your official documentation for parsnip. So the support vector machine comes from current lab, so you have to make sure you have that installed as well. And then in R there is a library called yardstick that does the model evaluation. So here we can say yardstick accuracy to get the accuracy score. And it should be pretty much the same minus the third decimal point. So I talked about communication. So we fitted a model. What about communication? So this presentation, it's written in R Markdown's Errogant slide text. And all of the Python code you see were actually just live executed in my presentation. So all I did was in one of the earlier code chunks, I library reticulate, set up my environment. And then I had a code chunk and instead of R as the driver, I changed it into Python and it will load up and execute my Python code. So all of the Python code you just saw was not copy and pasted, was actually live executed. And I hit knit on this thing two days ago and not as I was plugging in the projector like I did last year. So sharing objects between R and Python, so in an R chunk, if you are using R Markdown, you can access all of your Python variables using py dollar sign. So you get a variable called py and you can say py dollar sign and access everything from Python bar. Likewise, if you're created variables in R chunk and you want to access, sorry, if you create a variable in, yes, in R chunk and you want to use them in a Python chunk, you get a variable called lowercase R and Python and you can access all of your R stuff using R dot. Notice it's a period, R dot and that's not the end of a sentence dot. So in a Python chunk, I can, for example, this is the test data set. And I'm just saying, let's just get the first row of our data set. And this is our Python data set. And in R, we can say, hey, py dollar sign, give me that variable from the Python world and save it to a single observation in R. So now I get our object from a piece of code I was executing Python. So you can do that and create variables in one language, create variables in another language, fit models in one in the other and just pass stuff around. Right? So we created this SVM object in Python and we can say, hey, in this SVM object, just run a prediction. Right? So in this example, it's kind of like nonsensical. I'm using my training, my test data to do a prediction. But we can say, hey, here's this Python object, go make a prediction. But here's also this R vector and pass that into my Python model object and they both work. So you can do the same thing the other way around. You can make in our prediction. The way we make prediction of models in R is running the predict function. So R, we had a support vector machine object in R and then we can pass in the R data. Remember that R data came from a variable that we captured from Python. So you can have variables or inputs from Python world and then plug it into an R model object. And then of course, they're shiny. What I've shown so far were all the pieces that you can then input into a shiny model. Right? You had an R variable that can now just be a reactive thing. And then you can create a shiny object out of this. So the first thing you want to do if you have a Python model is you need to save this out. So there is a library called joblib that is essentially like pickling or creating binary format. You can save out your Python model. And then in R, you can load in that Python model. So here's an example. I actually have two slide decks this year. So this is a R markdown document using shiny as a back end. So this is my way of mocking up a shiny app. So somewhere in your shining app, you're going to library or load up reticulate. You're going to find the environment that you want to use and enable that Python environment. We can get our data from scikit learn. So this is the exact same code that we did before because this is an R markdown. All I did here was wrap this around a Python chunk instead of an R chunk. And then we can load up the two libraries that we need to load that Python model object. So my previous slide saved out this Python model object. And in this slide deck, I am loading in that Python model object into an R in this case, a shiny app. And then here's just the use case. We can say Python model dot predict and give it some vector and it'll just give us a prediction. Whatever value of this row just said that no, you do not have rescues. So we can do the same thing in our py dollar sign model, dollar sign predict. So we can get the Python model from the Python world called the predict function, pass in variables that way. So that's all the pieces, right, for shiny. So now in shiny, we have to have some kind of widget object. So we have an input panel just so you see something and we have a slider input. This slider input just, all it is really doing is giving us how many rows in the test set that we'll plug in to our model to run a prediction. And so this is a py dollar sign Python model. So this is an actual Python model. We are taking the Python test variable. But then here you're saying you see input dollar sign, that is a shiny variable. And so now if we move the slider and we go to nine, we get nine predictions. This is a variable. This is a shiny object that we captured using all of the shiny framework. And we're pumping it into a Python object. Isn't that cool? Please go, wow. So the down ecosystem, so I showed you shiny bits of our markdown. This pretty much works for everything. And I haven't tried it yet. So there is like that book that I wrote and it's written in LaTeX and like Sam said, let's not use that. So there's something called book down. So maybe I'll just write everything and book down. But even though it's a Python book, fingers crossed, everything works. Blog down, the Hugo academic theme actually supports Jupyter Notebooks. So you can actually blog with Jupyter Notebooks. And it will render Jupyter Notebooks as a blog post. Some stuff in the Python world, I guess they realize that, hey, this stuff by RStudio is pretty cool. So this year I learned of a package called nitpie or nitbee, which is the nitter implementation in the Python world. And if you are in Jupyter Notebook world, there is a system called Jupyter Books that lets you create, use a Jupyter Notebook for individual book chapter. It sort of works like book down, it is a different system. You can create, reticulate our packages. I'm not going to show you this, but just know that the R-caris packages really reticulate in Python, just wrapping around the Python API. So if you load up caris, you probably have realized you that we really need an entire Python stack installed. And this is really just, hey, Python has all of these problems solved already. Let's just, you know, write a nicer API on top. And that's how caris in the R world was created by the folks at RStudio. One thing about installing Python, if you try to come back here and follow along, is I am a software carpenter instructor. And so I always point people towards a software carpenter instructions for those of you who took my Git training yesterday. You would have noticed that I also just sent you Git installation instructions and things got a little bit confusing in terms of the syllabus. But most people in data science world use anaconda to install Python. So if you want to install Python and not board the rest of your system, give anaconda a shot. The way you uninstall it is just deleting a folder. So it's pretty harmless in that sense. One thing about Conda, so there is this wonderful website created by Jenny Bryan and I think Jim Hester, I'm not sure, folks at RStudio, it's called rstats.wtf. It's got a great URL. And it's about things they forgot to teach you about R. There is a section in there about using Conda with R. And the main thing is when you install anaconda, you have the ability to install R and R packages using the Conda package installer. And if you are trying to follow along, the main suggestion is pick one of those systems and stick with it. And the one I recommend is if you are going to install R packages, just stick in the R world. So don't mix and match Conda install with install that packages. You will be very confused where things are if something works to happen. If you follow my talk last year, I talked about project workflows, West McKinney, who works for Ursa Labs, runs a project called Apache Arrow. So if you are sharing objects between R and Python, give Apache Arrow a shot. It is a very fast and efficient way where you can store and pass along data frame objects between R and Python. So if you do have large data sets that eventually need to be loaded into Spark or Hadoop or read and write parquet files, look into Apache Arrow and it will integrate very well with your R and Python world. And so that is my talk. If you want my slides, they are at the bottom of every slide deck. But if you need to take a picture, here it is. Thanks."}, {"Year": 2019, "Speaker": "Marck Vaisman", "Title": "Managing Your Cloud: Working with APIs", "Abstract": "In this talk, I discuss interfacing R with APIs, focusing on web APIs for data consumption and resource management. I clarify what APIs are and their types, emphasizing web APIs, commonly used with internet platforms like GitHub. I explain the HTTP protocol and REST architecture, highlighting the significance of statelessness and key HTTP verbs like GET and POST. Using R packages like httr, curl, and JSONlite, I demonstrate making API calls, including authenticating and handling JSON objects. I also mention the importance of pagination in API responses. Finally, I present Microsoft's involvement with R, highlighting tools for programmatically managing Azure services and encouraging leveraging APIs within workflows.", "VideoURL": "https://www.youtube.com/watch?v=Bi-QGGG5aN0", "id0": "2019_07", "transcript": "So, I am here after lunch, so you're either going to fall asleep, which I hope doesn't happen, but no, seriously. So I'm here to talk to you about APIs. Couple of speakers this morning talked about APIs. Raffi spoke about APIs this morning. He's actually beating up at the Georgetown Business School right now. A couple other people mentioned different APIs. What I am here to do today is to show you how to interface, how to use R to interface with different kinds of APIs. Well, not different kinds, but to interface with APIs, mostly for the consumption of data, for the consumption and or command and control of resources that you connect to. Someone was asking me earlier about Plumber. So if you don't know about Plumber, Plumber is a package that allows you to build APIs from the R site and serve them, we're not talking about Plumber. We're talking about the consumption of APIs and how to use R to do that. Okay, a couple of things. So first of all, I wear many hats. My work hat is Microsoft. I work for the Microsoft Azure team. I'm a Cloud Solutions Architect. I've been with the team for about two years. I work with the federal government and actually some of my customers are here. And I help them transition into Azure. So work with them side by side, help them come into Azure, teach them, work with them, etc. I teach here at Georgetown as well. I'm an adjunct faculty in the analytics program and actually many of my students are here. So hi. Thanks for coming. Then, DC2, DC2 and the meet up stuff. So I've been involved with the committee for a long time. That leaves a little time for other stuff. But anyway, this is really talk about my personal opinions, not affiliated to anybody else. We're going to start with the primary APIs. And I'm going to give you a quick walkthrough through this because I'm pretty sure that most of you, if not all of you, have at some point or not interface with APIs. You may not know that you're actually interfacing with an API. But what is an API? It's like this big, it's three letters. So API stands for applications programming interface. What does that really mean? It means that it's just a way for two different systems to talk to one another. So it's a connector between system A and system B that exposes some kind of port or connection or object. And then you can actually talk to it via many ways. There's many kinds of APIs. There's open APIs. There's partner APIs, which are, I guess, sort of, you know, private to some extent. Internal APIs are APIs that are perhaps used internally in an organization. Are not exposed. Composite APIs are kind of a mix. But our focus today is really on web APIs, which is probably what you are most used to. Now, when you're using your phone, like your iPhone or Android phone, like a lot of the stuff that you're actually doing, it's actually calling APIs. These are internally APIs that you don't know that you're talking to. But the software is actually talking to the cell providers API, whatever. The applications are using the OS APIs for doing, of course, for kinds of things. You know, again, it's this idea that two different systems talk to one another. When we talk about APIs, I think, collectively as a community, most likely we're talking about web APIs. So these are APIs that are from platforms. So, you know, things come to mind, like Facebook API, Twitter API, meetup API. I mean, the list goes on and on and on and on. So that's really the focus today. These are web APIs. Why, you know, these are APIs where the communication between these two systems usually happens over the Internet, whether it's public or VPN or whatever, but it happens over the Internet. Many are open. You know, the fact that you can actually connect to a system without any form of authentication or authorization. However, many of those still open APIs, open in the sense that they're exposed, still require authentication or authorization. So, for example, you know, if you want to do something with your GitHub account, there's stuff that you can access publicly, but there are things where you actually need to authenticate. And I'm going to show you an example. So, you know, you can communicate using the HTTP protocol, right, which is the ubiquitous web protocol. That's really what it is. And the APIs that we're, that I'm going to talk about today are really this thing called rest APIs. Well, what the heck is rest API? Representational state transfer. That's a mouthful. That's really an architecture. Now, again, from your perspective, this may not necessarily matter too much, but I'm just telling you because I think it's just useful to know what's happening. And this is really an architecture that meets, that has to meet these six criteria. So, first of a client server separation. There's a client, there's a server or system A system B, blah, blah, blah. The second one is really important. Statelessness. The idea of statelessness is that when you're interface with APIs, you know, you have to execute an instruction that is self-contained. That instruction needs to tell the receiving system what to do. And it needs to have all of the information. Usually these, if an API, you know, we can have a whole discussion on API design. That's not what we're going to do. But if an API is well designed, those calls are what's called it-important, which means that they're stateless, you know, that if you call it multiple times, if you call it once and it does what it needs to do, and then you call it a second time, it's important, so it's not going to change. You know, so this is really important. And then these are just different characteristics about restful APIs. So how does this stuff work? Well, you know, again, you have your clients here on the left-hand side that are executing some kind of HTTP call against the API. That there's the rest API in the middle. And then that rest API is usually it's this interface layer, right? That goes ahead internally and does something. So it could be a database access, you know, it could be a platform application, whatever it is. But that API, what the API does is it, it just, it, it, it's what I'm looking for. It abstracts the exposure of the back end system and makes it generally accessible to you. These are the verbs. So HTTP, if you don't know, uses, there's kind of, when you, when you navigate in your web browser and you go to a URL, that's what's called a get call. Or it's a, it's a, it's a method. It's a get, which means just retrieve, retrieve an object, right, from somewhere. Now, again, APIs usually have a URL associated with them that hopefully is accessible openly via the Internet. But these are, these are the six main verbs. So you have post get patch put, actually put twice delete. So usually you're doing it, you're doing a get. A get is retrieving information. So after the end of my talk, what I'm hoping you'll be able to do is use a couple of our packages to go and get stuff from the web that you would normally go and point and click and download. Or execute stuff as well. The other two important verbs to know about are post and put. So post is to submit something, right, posting is you're taking something from your local machine or your local client and you're pushing it to the API so that the API knows what to do with it. And then a put is something that gets, I think it's changed. So, I'm going to show you an example of a get and a post, which are probably the two most important things that you'll be working with. This is how APIs work, really, right? So here's your consumer. So, you know, when you issue the API call, again, all of information is self-contained. The API, you issue the API call, the API receives your call and it says, hey, can I have x, y, z? And the API struggles that information and makes it do whatever it needs to do. You know, again, this is kind of, and it does it via the get command. There's usually a URL with a path. You've seen these, right, exposed, even when you navigate the web or URLs. And then the API returns something. And that something is made up of a header, which includes a response, a status code, and the HTTP response. And that returns back to the consumer. And pretty simple. This is what a URL, you know, this is what a API call usually looks like, right? So, you've got the method or actually the request URL. This could be really nested. Sometimes you have like a, you have a, think of it like a main URL and then forward slash different types of activities, right? Whether it's, you know, for example, the meetup API, you can do like events, you can do members, you can do, and each of them has a different functionality. Now, it's up to the API designers to design it well. Again, you've got the header, the body. So, when you're, when you're getting something, a get call, so by definition, a get call is just to retrieve something and a common HTTP calling your browser is just a get call. There's other tools that we're going to talk about today. But then there's all this other metadata and the body of your, of your API call. Well, how does my data come back? Who knows what this is? Hmm? Yep. So, most modern web APIs are designed to work with JSON objects. And if you're not familiar with JSON object, JSON object, I think the JS comes from JavaScript. It is not really JavaScript, but it takes that sort of idea of key value pairs. It starts with a curly brace. And then, you know, you can have actually, you have a key and then a value. This is an array. You can just have keys and values as text, but it starts and ends with a curly brace. When you submit an API call, most likely you're going to submit some kind of JSON object. And what you get back is going to be some kind of JSON object. So, here's what we're going to do. This is my GitHub repository page, right? So, when I navigate to my GitHub repo, and again, I'm logged in, but if you go to my GitHub repository page, you're probably going to see most of these. I think most of these are public. So, we're going to create a GitHub repo in two different ways. First of all, I'm going to use curl. Curl is a command line tool that invokes a library called libcurl, which is a C library. And then I'm going to show you how to do that with R. Okay? So, rather than going to the web and pointing and clicking and whatnot, I am issuing a curl. So, this is the API call, right? So, when I do curl, this is actually retrieving my repositories. So, it's just querying me as a user and it's returning this JSON object with, actually, this is about my user information. There's a lot more here, but this is what a JSON object looks like. It's just plain text. This here is the same call, but now I'd add an I, which just means return the header. The header contains a lot of metadata. This, the first one here is probably one of the most important things to know, which is your status code. It's going to tell you whether your API call was successful or not. So, the famous 404, actually, this popular mechanics has an article that came out earlier this week. It talks about the 404 error and its history. And it's actually, I didn't read through it, but it comes from printing. I don't know. It's actually a really important read. But one of the things you get back is an HTTP code, you know, 200 if it was successful, 201 if you did a put, and it was successful, 404 if the resource was not found. So, you guys are familiar with that. This is an example of an authenticated post. So, now, in this case, in previous ones, I did curl. I didn't have the dash X, which means, but by default, is a get, right? It's to retrieve an object. This is actually a post. Now, there is redundant information in this call because I'm saying post. Now, here, I'm actually authenticating, right? So, I actually have a personal API key in my account. But, and then, as you see here, the D, I am actually submitting a JSON object as part of my API call. So, I'm actually invoking the GitHub API to create a repository. It's just going to create an empty repo. Now, when I run this, I didn't get the header back, but you get a 201, I think a 201, which means the post is successful, and then you get a bunch of metadata about your repository. So, this is a screenshot, but that repo was created in my GitHub account via the command line. Ooh. Right? Okay. Well, great. That's all great. Now, that's curl. That's the command line. Well, so what? Well, let me show you how to do this with R. So, there are three packages that you probably want to be familiar with. Okay? The first one is HTTR. It's part of the tidyverse, written by Hadley Wickham. Enough said. The second one is curl. This is actually a fairly new package. It's called curl. And, like the curl command line option, you know, both of these actually still use that lib curl under the hood. So, if you're in a Linux machine, curl is probably installed. If you're in a map, you need the curl library for the stuff to work. If you're in Windows, everything gets installed automatically, but you still need to have curl under hood. And then the last one is JSON light. JSON light is a really, really lightweight package that converts lists to JSON and JSON to lists and vice versa. Both of these packages are actually calling JSON light under the hood, even if you're not aware about it. You can use JSON light individually. There used to be another package called R curl. But this tweet was from May from Jerome. He is actually the creator. And he's actually telling people, like, that's old. Don't look at it. Use the new package, which is called curl. And they work pretty well. Now, if we go back to the previous slide, you know, this says here that for a more user-friendly HTTP client, look at the HDR package. So you really, for HDR, it actually abstracts a lot of this stuff and it makes it very easy. Curl is a little bit more complicated. You do have to have a better understanding of the headers and all these sorts of things. I haven't even talked about authentication, which is a whole other topic in and of itself. So this was the command line. This is the call that we did before, right? I created. So I'm doing a post. This was the command line. This is how you would do it in R. So I first, I specify the endpoint, right? These are just different three different strings that I declared. Here, I'm actually defining the body of my call, but I'm creating it as a list. So as you see here, right, the original JSON had name and then the name of the repo that I'm creating. This is name. You know, I'm creating a list, so I don't need to put quotes here. But this is the name of the repo that I'm going to create. And then this executes the command. Now, as you can see here, I'm doing a post to the endpoint. Now, the endpoint may have additional, like, you know, you can build URLs depending on what you want to do. I am authenticating. If this was an open API call, I wouldn't have to authenticate. It would just be open and that's it. I'm submitting the body. So the body is this piece over here, but I'm submitting a list, which I'm telling it to encode as JSON. So this actually does the encoding for me. I'm going to do it myself. What do we get back when we use htr? So we, you know, you know, what I did here is I actually executed the postcode. Sorry. But I actually assigned it to this create repo object. But if we inspect that create repo object, there's a lot of things that are in there. So the object that gets back is a list and it has all of the metadata from your headers. It has the body of your response. It has all of the associated stuff from the API response. So as you can see here, I got the 201 response, which means that it was successful. You can actually look at this content, but this is actually in raw format. As you see, it's a bunch of bytes. So there's a couple of ways to look at this. If I actually want to see the response, like the contents of the response, I use the content function on the object that I created, and then I can see it as a text. But if I just do content create repo, it's going to default as a list. So this and this are the same. It's just one is shown as a list, one is shown as a JSON object. There, you know, and now I create with R, I created a GitHub repository. Now, why is this important? Because I'm actually executing instructions against another system. So I can create stuff programmatically. Use these APIs that are well designed and well constructed and enable it into my workflow. Aha. There's one gotcha here. A lot of times API responses are, they come back in multiple pages. So you actually have to paginate. And you have to build some code that reads the response or actually get some, sometimes the header tells you how many things you get back. So for example, if the API is restricted and you're pulling back a lot of information, you may, you know, the whole thing might be a hundred thousand objects, but your API may only limit you to 200. So guess what? You're going to have to loop, right? You're going to have to figure out, okay, I'm trying to get a hundred thousand objects. I only can do 200 at a time. You know, maybe the API, if you pay, it gives you bigger bandwidth, whatever. But the point is you have to paginate through this. So go a couple of gotchas. The other thing is even though JSON, the JSON light object works really well. If you have a lot of nested nests in it, you still have to kind of, you know, so this is some code I wrote a long time ago. I wrote a package to pull data from the meetup.com API, you know, because of the meetup stuff that we do. I wrote this like four years ago. It is using JSON light, but as you can see, you know, I'm kind of using L apply and I'm nesting and blah, blah, blah, blah. The R ladies team actually just wrote, and I called my package meetup R. It's actually on GitHub. It's on a cran. Our ladies wrote also a meetup R package, which allows you to interface with the meetup.com, but they did a much more elegantly. They're using per and they're using table and all these sorts of things. And this stuff over here is taking the JSON object and converting it to a data frame. All right. What I want to end up here is, well, why is this important? So I work for Microsoft, right? And we have a lot of great tools, a lot of APIs, a lot of SDKs that allow you to do stuff on the cloud programmatically, not just, you know, but, yeah, so this is the point click way, right? You log into Azure or any other cloud provider and you do stuff manually. Programmatically, there's command link tools, which are usually a wrapper around an API, SDKs, which most likely are not native to R, a lot of them are in Python, but there is also REST APIs. There's this project called Cloud ER project, which has a whole slew of packages for R that allow you to interface with their different cloud providers. And, you know, rather than you defining the JSON object, it does all that abstraction for you. So there's one called Azure R. This is actually fairly new. This was published maybe about two months ago. And it has a lot of different things. So it can do authentication, it can do storage, it can do key vaults, it can do virtual machines and stuff like that. The important thing is down here. It's really not, it's not still not yet intended to support all of Azure. So for example, we saw an API, an example, a lot of people talk about the Google API for text analytics. So we have one in Azure. If you wanted to interface cognitive services, you would actually need to create a REST, like a full REST API call, but you could use HTTP for that, for example, as long as you know how to construct the JSON object. Last, you know, Microsoft has put a lot of investment into R. Microsoft acquired revolution analytics many years ago. Microsoft is putting a lot of stuff out in the R community and open sourcing a lot of of the tools. There's a new API, a new tool for the Azure machine learning services coming up. But there's a website called, if you search for our developer's guide to Azure, it will show you all of the services that are kind of our native, which are, which just work greatly with stuff. Thanks a lot."}, {"Year": 2019, "Speaker": "Marlorie Hughes", "Title": "Dashboarding Like a Boss", "Abstract": "In this talk, I share my passion for Dashboards and how they can transform data presentations into organized, interactive, and visually appealing formats. As a data scientist and consultant, I integrate my analyses into Dashboards to avoid the clutter and inefficiencies of emailing plots and CSVs. I demonstrate the ease of creating dashboards using R's flexDashboard and highlight the benefits of interactive elements with tools like highcharter and data table. By using these tools, you can effortlessly produce dynamic, scalable Dashboards that present data in an engaging way, encouraging others to embrace Dashboarding for effective data communication. Additionally, I provide resources and templates on GitHub to help you get started and inspire you to explore and incorporate Dashboarding into your workflow.", "VideoURL": "https://www.youtube.com/watch?v=yott4quKN6s", "id0": "2019_08", "transcript": "All right. Hello. My name is Mallory, and I really love Dashboards, and I'm so excited to tell you guys all about them. So I am a data scientist. I work full-time as a consultant for Amazon through a company called Onika. I was previously at NPR and Amplify Education before that. And we're going to talk about Dashboards. So I am kind of addicted to Dashboards, and every time I make a project, I put all of my analysis in a Dashboard, and my goal here is to inspire some of you to love Dashboarding. Like I love Dashboarding. I know all of you might, you know, maybe not. It's kind of like a therapy that you can insert in between the pain of constant data manipulation, and you get to make something pretty. So here we go. Also, this is a Dashboard, and I've got my slide deck in a Dashboard, and you'll see you can put Dashboards in Dashboards. I haven't, I mean, Dashboards all the way down, you know? All right, so why bother? So first of all, they're pretty and organized. So if you have a whole bunch of stuff, like a whole bunch of plots and CSVs, and let's say you have a set of slides and some other HTML output, and you want to put all this stuff together, you should never email that stuff. No one wants to look at CSVs and PNGs in their email, and things get lost, and it's terrible, and you forget the last state of things, and then somebody asks you for something, and you have to go find the PNG that, oh my god, and it's just terrible. So this is a way that you can keep your stuff organized and pretty. In a similar way, you know, you can make things that aren't Dashboards, you can make our markdowns, like regular notebooks, but it's a pretty easy transition, as you'll see, to making it really pretty, and you can convert between the different formats fairly easily. So it's risk-free, and there are templates galore, and by risk-free, I mean, so unlike when you're trying a new modeling tool or a new package that manipulates data, you have to know what you're doing, and you have to check a lot of stuff to make sure that what you intended to do, you actually achieved that goal. But with Dashboarding, like all you're doing is organizing your stuff and making it pretty, so you can take someone's template and copy and paste the CSS that they wrote, and you don't have to understand anything about it, but did it make your Dashboard prettier? Cool, you can use it. So it's a very fake-it-til-you-make-it kind of thing, you don't have to understand anything, and you just mess around and go, oh, okay, that blew everything up, or oh, that made like where's my output now? So you really don't have to know what you're doing. You're not going to mess up your actual analysis. It's just like a fun, like, play space. Also speaking of templates galore, all of this is on my GitHub, everything, all the tutorials within it, so. But my main contribution here is not that I'm going to be able to teach you a whole bunch of stuff right now, and mostly, like, trying to inspire you to bother to go to the GitHub, and then you can just take all the code and, like, put your own data in it, and then there you go, that's how it starts. So I know this is a bit controversial, but with Dashboards, you can have, you know, interactive stuff, like with HTMLs. So unlike in PowerPoints or other still forms, like PNGs, in a Dashboard, you can have a have everything interactive, like hover with, like, information that pops up, and you can do so much more with an interact, a single interactive plot than you can with a static one. So, yeah, sorry, Gigi, but we'll get there. And to redeem myself, you get to use R, so that's, you know, an upside. And it's flexible and reproducible. So just like with an R Markdown that you could supply arguments and then render the code, you can do the same kind of thing here. So, for example, if you had 30 different podcasts and you wanted to make a Dashboard for each one of those podcasts, you could write one Dashboard script with some arguments and then have a little render function that, you know, put a little loop around it, and then you could just build, render and publish 30 different Dashboards with just a single template. That'd be awesome. And best of all, people will respect you. So I have to say there's like nothing more satisfying in the world than when you, when some Python fanboys that you've overheard talking about how R is dead, see your Dashboards, and then ask you how you made it. And you're like, with R. Okay, so let's get to it. So I've created a still 12-step program to quit emailing plots and CSVs. I can't get through all 12 steps today, but I'll try to get you at least part the way there. And obviously I just wanted to make it 12 steps. I could have made it some other number of steps, like 10 probably was easier. I had to kind of work at it to get it into 12. But basically you have your basic layout. Actually making the Dashboard layout is not hard. That's the easiest. There's so much information about that on the internet. We're going to be using flex Dashboard as you'll see. And then there's tables, interactive data sets, interactive plots, exporting your data, and then how to deal with CSS, which mostly you'll copy paste. Somebody else is CSS, like all the CSS templates that I provided at my GitHub. And then you have branding, hosting. Hosting is one that you're going to want to do if you're using this for internally at your work. So you can talk to somebody that assists admin, find assists admin, and if you don't find somebody that runs the website of your company. And then automating and replicating, and then maybe making it shiny, which you can totally do. It is a very easy way to turn some, you can turn Dashboards into shiny quite easily. All right, so we're going to talk through the first five things here. So we're going to talk about the layout printing, interactive data sets, interactive plots, and exporting. Okay, so first, the basic layout. So with a regular RMD when you open up a markdown, it says output, HTML, HTML document. So to just turn something into a flex Dashboard in the most simple fashion, like all you would change is from HTML document to flex Dashboard. So there's a couple extra arguments that you can add and my go to for my basic setup is to have the vertical layout scroll. So it doesn't try to compress everything into a single screen. And instead, let's you make a dashboard that you could scroll through and orientation rows, which I'll show you what that means. So here's a basic layout. So this tab, I'm actually, this whole thing is a flex Dashboard, and this is a page within the flex Dashboard that I've built in like the main dashboard. Some of the other tabs are like I've inserted within it. Anyway, so this basic layout, this is done with rows. And how you can tell is that centerpiece is not, you don't have exact columns lining up if you want an orientation like this where something can span across two columns, then this is, it's a row orientation that you want. So those little things at the top are fun built in stuff that flexboard allow that gives you, this is a value box. So it lets you, if you just want to give a number, you can do this value box and then little gauges and that can be fun too. And then also you can add favicons, which you can go nuts with these. There's a whole bunch of free ones and you can make your own and that is, you know, just makes things fun. So here's a basic layout, nothing too exciting. This part is not hard, resources, this part's easy. So I'm not going to spend much time on that. All right. So now printing, where print is simply unacceptable, because the point here is to make things pretty. You want someone to want to look at this. You want someone to want to use it. You want to like make something where you can reference people to the dashboard and then they'll go and everything's pretty and nice and user friendly and they're like, why would we pay for X dashboarding tool and like you can just spit these things out. So oh man, I am so in love with this. Look at this, like this is in my CSS that's, you know, this like star background of the, ah, it's so wonderful. Okay, so now let's look at a few different things that you can use to make your printing prettier. All right, so this with, with base are, if you, if you make no effort, ah, head iris terrible. This is ugly, don't do it. Don't make anyone look at that, that, not in the dashboard. So the easiest way to make a print pretty is with print R. So all I did was load print R and then I ran the exact same line head iris and look how much better that looks. So much better and I don't have to do anything. I just load the library done. So that's obviously a pretty simple solution to print a lot prettier. Okay, so now descriptive stats. So before I was just showing a few rows and now I'm going to run a summary. Okay, so again, with base summary iris. You're so ugly. Don't do that. It's so easy to fix. Oh, look at that printer. Add it again. Summary, much better, much better. And you still, and you don't actually have to change the line of code, which is great. Okay, so you can achieve the same goal. Some of you may have heard of cable. You can achieve the same goal with cable, but it's just like a little bit more effort. You can take your code that you'd already written and it gave ugly output and just add the library at the top and then make it pretty output. This method you would have to actually go in and change your code. So that's like more work. All right, and then now a star gazer. So this, ew, this doesn't look good at all. Star gazer has a place, but for summary, just a basic summary stats, that is way too cramped. It's like painful to look at. So another one that's kind of cool that's getting better, but it doesn't have all its capacities in our markdown is summary tools. But keep tabs on that because it gives you a lot of cool information and soon when somebody fixes it, it actually has like distribution, like a column that has little plots of the distribution of that variable, which is super cool to have a plot inside of a table. But for now, I'm kind of just like, that's an option, but printer is also a pretty simple option and once they get the plots in there, they'll only start using that. All right, so then for models, again, terrible, don't do that. Base are, that's a terrible printout. And here, printer doesn't help us. So printer won't save you if you're trying to summarize model output. And this is where Stargazer wins, Stargazer model output looks just like Lautec model output, which you could use in, it works for Lautec also in other formats, but it works with HTML. So there's printing. Now I, oh yeah, there's some links here that you're going to need to see for using Stargazer on here. I specify which statistics I want to show and you have to know the little nickname for referencing them and I have a slide here that gives you the link because they can be hard to find. Okay, so now interactive data sets. So what am I, what does this mean? What is different about what I was just showing you and what I'm about to show you? Okay, look at this. Look at this data set. Can you see this hover like action and highlighting? So before we were just printing something, a summary, I just wanted a few lines. There's no need to sort things. I'm not going to like, I don't need to do anything with this information or the summary stats. It's short, it's simple. I don't have any intention of downloading it, whatever. It's fine. But if you have a whole data set, you would like to provide the user with a data set. This so easy with data table, with DT data table. The library's DT and all I did, data table, Iris. Like one liner, you can make it a lot cooler. But look what it already does. This is so much better. And you could change this, like 50 cool. I could sort things right on. I could search for stuff. S, whatever goes. That's awesome. And then a little bit more advanced, slightly a few more keystrokes. And now I can sort and filter at the individual columns and not just search. And also what else can I do? I don't know. 4.9. You've got search highlights. All right. This looks like an actual dashboard. This looks like a thing in a dashboard that somebody would pay for, but you can just build it yourself. Awesome. And that is, I mean, I made it into two lines, but really that's like a one-liner code. So pretty easy. And the code is right here. So you can copy and paste it and insert your data set, whatever it is. And it just works. And it's awesome. All right. So next. Interactive plots. Ha ha ha. OK. So now that we're not in PowerPoint anymore and we're talking about dashboards and we want everything to be interactive and fun, our plot should be interactive and fun too. So we're going to take a look at high charter, which is my favorite plotting tool. Once you get good at Ggplot, you will find high charter to be really quite similar pipes and all. I mean, it's actual, the actual pipes, not the pluses, but let's take a look. OK. So suppose somebody, you know, you're making some bar plot. So this is pretty simple. You can see, start with the data set. I can pipe it in, do some manipulation and then just pipe it into H chart. I have, I will reference, get, basically go to my, my GitHub for more details on this because I'm going to have to go kind of fast. I just want to show you what it can do and like that it's not very hard. So column, HK, a lot of this syntax looks pretty similar to Ggplot. And then look what you get. I have this thing, this bar plot and I can hover and I have information. So you know, when somebody says, oh, that bar plot, can you put the totals at the top of the bars? Oh my god. And with this, they're like fine, done. Like, oh, but also can you make it separately too? I want one with also just 2008. OK, fine. Oh, no, but I also want one with also just two, OK fine. And I also would just like one that's blank, fine. So pretty awesome. You can make a lot of plots in one plot. OK, so now I'll show you just kind of like skim through this bubble plot that I've made here. So it has hover action. Cool. It's looking at some star data and luminosity and distance. And you can scale the axis. So here I didn't actually log the x and the y. I just scaled. I did a logarithmic scale on the x and y, which is like, there's pros and cons to that. But in the end, if you're trying to show someone something and you're like, actually, it's log x or log y. And then they're like, oh god, I don't know how to think in terms of logs. Here you've just scaled the axis so that you're seeing the smaller. It's as if it's logged, but the data itself is not logged. And so people tend to understand it a little bit more if you're showing it to some product manager or a director or something. And you can very easily add colors in the same way that you're used to with colorize. So just pick the color scheme, assign with colorize, and then add the colors. Three equals color. Nice. So I colorize by distance of the stars. And then you can add axis titles, which also it's very easy. I showed you here. Essentially come back to this reference. Obviously I can't teach you a high charter in like two minutes. And then custom hypertext. So obviously this is like terrible. Wow. It should at least round those numbers. But also what are those numbers referring to? So this is probably a really, really great component. So you can create custom hover text. So you just assign the X with the X is going to be the name. The Y is the value. And then you use that tool tip and you can create custom tool tips. And it's really easy. Just like you copy paste that and then just swap out the names of columns and like it's amazing. This is like so this is the all it took this code to do this, make this whole thing. And everything up here is just colors and assigning a custom hover text. And then that's the all it takes to get you that plot. So pretty cool. All right. Last thing is, oh yeah, I also want to say of course there is also leaflet, which is great for making interactive maps, digraphs is for time series and plotly, which if you really can't go with GGG plots, you can use plotly as a wrapper. Okay. So exporting. Last thing to like really make your dashboard super useful so that when you send it over to someone, they don't have to come back to you. You go go to the dashboard. It's all there. So you can export the data and the plots from your dashboard. So let's go over the exporting tab here. Okay. So this one is a flex dashboard in a flex dashboard. Nice. So here for, oopsies for high charter, it actually, it's a little bit tricky, but I wrote, I found, I figured out how to do it. So there's a simple H C exporting, which is built in, but it doesn't work. I found that it breaks. And so there's this line of code that defines some things. I have a script, a separate script in there. You just source it and then you can export. So you don't have to go search the internet like I did. You can just source that script and then voila. In the corner, you can export the images or the data, whatever you want. And then with data table, same thing. I've supplied the code here for exporting and then you have a download button and you can download the data. Awesome. So that they have the data and the plots and whatever they want. And they don't have to come back to you and bother you while you're working on your next data manipulation or dashboard. So I'm out of time, but basically keep it simple, organized and explore things. So start with somebody else's CSS, keep things organized and don't make changes that you don't know how to undo. And every time you make a project, try something now. And that's it."}, {"Year": 2019, "Speaker": "Ami Gates", "Title": "Using NetworkD3 and R to Visualize and Explore Relationships in Data", "Abstract": "I'm Amy Gates, director of our soon-to-be-renamed Data Science and Analytics program. In this talk, I explore the use of the Network D3 package in R to create interactive network visualizations, specifically using Twitter data and association rule mining. Network D3 offers a more accessible way to work with D3.js, which is known for its steep learning curve. I demonstrate how to mine Twitter data, clean it, and use association rule mining to identify relationships between words. These relationships are then visualized as networks with nodes and edges, showcasing patterns such as frequently co-occurring words in tweets. This method can reveal insights into public discourse, such as prominent themes or topics in world news tweets. The talk covers the technical steps of setting up the environment, transforming data, and using Network D3 to create the visualizations, emphasizing the importance of data preparation in the process.", "VideoURL": "https://www.youtube.com/watch?v=AIK_FRzyUEE", "id0": "2019_09", "transcript": "I'm Amy Gates. I am the director of our almost to be data science and analytics program. We recently changed the name to something that makes sense because seriously, what does analytics mean? So data science and analytics, it's an absolute pleasure to be here. It's a pleasure to host this. It was a lot of fun. This entire group is awesome. Thank you to Jared. Thank you to Mark for inviting me the first time learning about this. And thank you to Heather Connor for working her butt off to make this happen. And thank you to our students and our volunteers. Thank you guys for coming. All right, good stuff. So today, we're going to talk about using Network D3. And we're going to use it in conjunction with Twitter mining, with association rule mining. But our goal is going to be to create an interactive network visualization. And so any of you have used D3 in the past. You know that D3 is, well, it is a steep learning curve. You could spend months and end up with a bar graph. So the good news about this is that Network D3 is a package that works in R. So relatively easy to use, which is nice. So let's take a look. And I'm going to go ahead and, by the way, I just did this last summer. It was awesome. I just had to put that out there. So why do we build networks? What are networks? They're graphs in a mathematical sense. And that makes them really robust because everything that's true about a network is true about a graph and vice versa, which means all the mathematics of graph theory can be integrated into analyzing, understanding, evaluating, and looking at networks. So we look at networks when we want to understand the relationship between things, any things. The challenge is, do we know if there's a relationship? Can we define that relationship? Do we understand what the relationship is that we're trying to visualize or analyze so that we can put that together? So in my talk last year at DCR, I went through the Twitter part and the Association Rule Mining part of this a little bit. I'm going to fly through that part this time. But if you want a lot more on that, you can also look a blast your talk. This part, I'm going to take a lot of that, but we're going to focus it in on how to then apply that and visualize it interactively using network D3. Okay, so why would you want to build a graph, though? What is the purpose of a network? And generally, so a network is considered an abstraction. Generally, you want to model things that are in some way relationships. What are the applications? They're limitless. Anytime you have a relationship that you're curious about. So the one that always comes to mind, and I feel a little guilty saying this, but it's true anyway, before the last election, there were two words that occurred together in tweets, possibly more than any other two words that are, you know, I'm not going to go there. But these two, does anyone know what these two words are? Does anyone remember? Two words. I'll give you a hint. The first one's email. Hillary Clinton. Hillary Clinton. And email. And just Hillary or Clinton, because I said two words, they occurred together in so many tweets, you could build a word cloud of the whole web, and you'd see these two words, the gigantic. That told a story, a story that we're not living in, right? It's told an interesting story. So you can use networks to do all kinds of things. And that's everything from the spread of a virus to analyzing terrorist networks to electrical networks that are not for terrorists. So what's a network? It's a graph. That's all network is. It's made up of nodes and edges. The edges represent relationships. The nodes represent anything you want them to. That's the beauty of building a network. The hard part is figuring out what the relationship is. And then formatting the data so that you can make it something that you can visualize, something you can analyze and understand. So basic graph theory, and let's go through this quickly so we can get to the good stuff. Graphs can be directed or undirected. There are many different ways to represent graphs. So humans look at graphs like this. Computers, they don't do this, right? They do things that are numeric. So one way to represent a graph is using an adjacency matrix. This matrix is that graph. They are one and the same. That's just the computer view. That's our view, which I think is kind of cool. Now for network J3, it actually prefers something a little bit easier on the eyes. Something that looks more like this, where you define the edges, you define the nodes, and then you can apply the network J3 object to create the network. So we are going to focus on network J3, but I don't want you guys to think that that's the only way to do this. There are many different ways to create graphs and networks, and these are several of the options. And here are some decent links as far as looking further in to network J3 as well. Now we're going to Tarantino it. Who's familiar with Tarantino? Thank you guys, seriously. And if you're not familiar with that, either you have two things now that you need to do when you leave here. Okay. So these three visualizations were created by using an API to get Twitter data. Any hashtag you want, I used world news, all the codes there for that. Then we took that data because that data is a mess. How many people have scraped Twitter with an API? Okay. So when you scrape that, you can put it in some different formats, but you've got to clean it up. We also want to clean it up, and in this case we want to put it in a transaction format, so that we can run it through association rule mining. Why do that? Because we're looking to create a relationship between words in tweets. The question is, are there words in tweets that occur more often together, more often in general, this type of thing? That's what association rule mining can do for us. It can tell us the words that occur more commonly. We can then take that and make that the relationship, and then we can visualize it. So that's where I headed with this. I did this with world news. So I just did hashtag world news, and I just put the Twitter out there to do its thing, grab the data, cleaned it up, did association rule mining on it, which we'll see, and visualized by three different options. So here's one. This is called the support. This is a network D3-vis. It's interactive. This is on the web live, so it's wrapped up in an HTML wrapper, which I'll show you code for as well. This one's interesting because it created actually two connected components, two disconnected components, which is kind of fun. And you can then see when I did the world news, there's obviously something was going on in Mexico. There's some intimations to drug, United States, cartels. That was one thing going on. World news was the hashtag, and so on and so forth. So this was one of them. The other one I did using confidence instead of support as a measure, got some really different results, which makes this fun and makes it interesting to use a social rule mining for this. Okay. So here's what's fun about network D3. People like to play with it. They can play with it. You can explore it, you can look at it, you can ask yourself questions about it, like are there clusters? Are there families? What's the diameter? What's the node degree? And you can even integrate that into the network D3 object as you wish. So in the spirit of Tarantino, that's, did you close my thing? It closed my thing. Why would it do that? Oh no. Now I'm going to have to search for it while showing you guys my screen. I didn't put that picture myself, I saw someone else did that. Okay. There we go. It's fine. Okay. Let's go back right where we were and pretend that didn't happen. Okay, dokey. Yeah. So that's where we're headed. So let's get there. Yeah, sorry about that, does it? Yeah. Let's get to the good stuff. So a couple of measures that you can utilize when you're evaluating networks, you can use a measure called between this. And between this will give you a higher measure if the node has more of a communicator effect to the other nodes in the network. So you can tell here that node four, if you were to cut that node, you would actually disrupt communication maximally in this network. So that's interesting. It's interesting if you're trying to hold, you know, put a stop to a viral outbreak. It's interesting if you're trying to put a stop to a terrorist network. Right? It's sometimes you care about between this in a graph or in a network. So that's one of the measures. One of the other measures that we see a lot is the notion of communities. So if you were to use Facebook data and create networks of Facebook data, you would discover that the relationship is who's friends with whom, and then you would see clusters of friends and then you'd see connectors. Those connectors have the highest betweenness measure. And so that's interesting as well. Moving on, if you wanted to communicator disseminate information. So network communities are also great interest when you're evaluating. And this was, there it is again, a terrorist network. And sometimes you'll ask the question okay, we have a terrorist network, we're going to try to take out one of the core elements, which one should we take out first? Because every time we do something, it costs money and has questionable results. So we wanna make sure that, you know, or at least picking the right one. Networking is everywhere and it can be applied to pretty much anything, you know, biological systems. And so finally, let's get to the good stuff. Okay. So how do we do this in R? What are the steps we're going to go through here? First of all, you need to get a Twitter developer account if you want to use my code. Then you can run this code and get these slides. Once you get a Twitter dev account, you will get a bunch of keys and tokens and the name of your firstborn child. And you will use that information to essentially access Twitter. So what's kind of cool is that when Mark gave his talk, he talked about APIs. This is an API for Twitter. And in this case, we're accessing Twitter using the oath, using all the tokens that we get, and so on. These are the libraries that I used. The ones in bold were directly used. The ones not in bold, I played around with, but left them. Because I think people appreciate that. Okay. So here's the build the environment part of this. This is where I set up the oath. So set up Twitter oath, sets up the API. And then the Twitter and then search Twitter. And you'll notice I searched for world news. I gave it a date. I gave it the number of tweets. I pulled that search object into a data frame. And then I want to take a look at it just to make sure that I'm grabbing that first tweet. So the search, DF text. I feel like this should function. Where's the beeper thing? It's okay, you guys know what I'm talking about. Okay. Here's the text from tweets. What you end up getting is a data frame. So this is one column of that data frame. There are other columns too, like geotags, like dates, like whether it was retweeted. There's a ton of information in every single tweet. This is part of it. Now the trick of association, well-lining in this case, is you don't want to lose the tweet. This is not the same as bag of words for people who are familiar with that. I'm not taking all the words in every single tweet and just simply creating a vocabulary of words with frequencies. That's certainly one way to do it in text mining. What I want to do here is I want to keep each tweet. I don't want to lose that context. And I want to ask the question, what are people tweeting about most often in each given tweet? So that's an interesting question. Now I'm going to do this part fast because again, this was done last year a little bit. So here's a single tweet. I can access it. Here's the part where I build the transaction data from the tweets. Here's what it starts to look like. So instead of now having tweets, I start to have individual words. It's still very dirty and I have to clean it a lot. So here's all the cleaning bits. I'm not going to read your code, but it's there for later. Now I'm looking a little better. I still have things one tweeted a ton, which is good. Once I have it in that state, I can read it in as a transaction data. So you'll notice that this is for my students too. This is not record data. Guys, I'm hoping you guys know this now. This is not record data. This is transaction data. Every row is a bunch of tokens in this case, which are words. So there's no columns per say, which represent variables and the rows can go on as long as you want them to. That's the difference between transaction versus record data. Once you have it, you can read it in. Do some fancy footwork with using a rules, the a priori method. This is all association, role-lining, and R. You can choose your support, your confidence, your lift if you wish, and things like this. And eventually you can sort the rules. Here's what we're trying to get to. So all that stuff, everything I just covered, that was a little bit of review last year. Here's what we're trying to get to. Because what we want to have is a relationship between words. We want to see that lives and world news are related to each other. We want to see that lives matter are related to each other, things like this. Because we've now created, determined a relationship, now we can build a network. So you can't just plop record data into a network. It won't work. You have to determine what is the relationship between words. And determine what is the relationship you want to show or visualize. From here, and this is a 17-hour error, just in case you run into this with a rules, this will fix it. Just FYI. In this case, what we want to do now is we want to build the things we need for the network D3 object. So a network D3 is going to expect things from us. It's going to want to know what are the nodes, what are their IDs, what are they called, what are the edges, which node goes to which node, how do we want to color them, how long do you want the edges to be, how fact do you want the edges to be, wants to know all this information from us. So right now at this moment, like all of data science, we're going to spend 90% of our time formatting it and getting it ready, right? And then five seconds enjoying it. But she reminds me of something else as well. Okay. Yeah. Thank you for that. Okay. Okay. So somebody just got it over there. Okay. Converting our rules to a data frame is our first step. Once we have our rules in the data frame format, we can start altering that data frame to represent the edges that we need for network D3 and the nodes that we're going to need for network D3. So it's basically a process of massaging our data into the format that we needed to get into. Okay. So here's how we are now. This is a much better spot. We actually have a from node, a to node, and some kind of options for weights. Again, I won't have time to go into what support confidence and lift means. I wish I did. I did that last year. So if you're wondering, just watch that. That's talk. And that's what I went. I did that a lot. That was the whole focus of the talk. So now I just have to take my word for it. These are three different measures I can use to wait the edges. I can wait the nodes. I don't have to wait anything. It's really up to you what you want to do with your network. Okay. Now I'm finally going to build the edges. Because I use association, I have sort of three different sets of rules that are of interest. When you do association rule mining and you sort by support, you get a different set of associations than you do when you sort by confidence or when you sort by lift. So just as a quick example, support is how often words occur together in a data set. Confidence is how often they occur together. So it's going to give you some different results. So in this case, what I'm doing here in this code is I'm taking different columns out of that data set because I want to see all three. I want to see what they look like. And I'm going to go one at a time. I don't have a loop here. So if you were to do this, you'd have to either unhash tag or put in a loop. And I'm going to then finally build the edge list. Now this is more understanding what an object expects. It's just, it's part of it. You have to do it. So in this case, network D3 needs something called a node ID. It needs a source node. It needs a target node. And the IDs have to start at zero. Whereas we know that R starts with one. So we have to make a couple of adjustments so that we're, again, shifting our data over to what network D3 is going to expect. And that's what all this does essentially. And here's the result. So now I have a source name. That's one node. I have a target name. That's the node that it's going to. So now I have an edge between Americas and countries. I'm going to have an edge between Americas and experts and so on and so forth. So this is essentially defining that edge list. I have a weight and I have now source IDs and target IDs, which network D3 requires. I need to build the nodes as well. I use iGraph. And then also used iGraph to update the IDs so that they were all subtracted by one so that network D3 is happy. In the end, you end up with a nice node list. So this is the name of all the nodes, the degree. What is a node degree? It's how many edges are incident to that node. So we want that information too. And you can use that information to affect the size of the node if you want as well, which is interesting to look at. And so on and so forth. So what we've created now is we have a list of edges or an edge list. And we have a list of nodes. So if you pause for a second, you think, where do we start? We started with the Twitter API. That's where we began. All we did was grab tweets and then cleaning and cleaning and formatting and data frame and then building an edge list, defining a relationship using association, we'll need to create that relationship all the way to where a point where we have something like this now. In the past, when I've seen people talk about building networks, this is where they start. I feel like it leaves out all the part of what you have to do to get here. That's 90% of what you have to do. The rest of it's just a fun bit. So here's the final part. This is the network D3 object. To me, this is very, very exciting. So the object is made up of all kinds of attributes, right? We need to define the links. And we did that. We have an edge list. Those are the links or the edges. We need to define the nodes. We did that. We built a node list, so we're going to use that. We have a source. That's our source ID. We have a target. That's our target ID. We went through and actually built all of these things so that we could create this object. We have value equals weight. And you can set the weight to be the confidence, the lift, the support, or some function of those three. We'll have options there. And then finally, we can affect some of the other things. One more thing I'll note here. Oh, my gosh. You guys in my lifetime? Oops. My bad. Anyway, that's the object. I will note one more thing before I stop talking. You guys are giving me that look like you need to stop talking. So network D3 has a JavaScript option that allows you to pull in functionality from value and affect the length and width and so on. This is how this works. So I'm done and let's take a final look at just network D3. Thank you. Thank you."}, {"Year": 2019, "Speaker": "Abhijit Dasgupta", "Title": "Coursedown: Managing Course Materials Using R Techniques", "Abstract": "Teaching can require the production of different kinds of materials, from presentation slides and homework assignments to course notes to a website to pull everything together. These materials need to be maintained and updated. Using RMarkdown-based technologies like xaringan, bookdown, blogdown and HTML rendering of markdown, we can create these resources. Maintenance requires updating just parts of the resources that have been modified or newly created. I will describe the coursedown package I\u2019m developing that includes several RMarkdown technologies for content creation, potentially using templates for consistency, and static website generation, and maintaining these resources using the drake package.", "VideoURL": "https://www.youtube.com/watch?v=7Opq6WZuTBs", "id0": "2019_10", "transcript": "So the story there is I've been teaching IKEA for 20 years. And so part of our testing procedure is a five person free stuff. Five people attack you. And so I've been a recipient of that and I've also been part of the five person mob. So that's what that referred to. Really other quick fact about that music, obviously Doctor Who. It's not, I know that there was another Doctor who theme song earlier today. The story behind this is about four years ago I had an opportunity to be on hardball sitting next to President Obama. And so he comes in, introductions, what's your name? I say, there's no way I'm saying that. And so for the next 30 minutes I was the Doctor. The only reference to the Doctor I know is Doctor Who. So that's what's behind that. So today I'm just talk about sort of some practical things that I needed to do. And so I get to present on it. So it's about dealing with teaching materials. And I thought it was relevant because I know plenty of people in this room teach in different contexts. See there is workshops, apparently Jared can't even stay at home anymore with them, how much he teaches. And Mark teaches here, there's plenty of people at Georgetown. So just thinking about this. So I teach among other things, the basic R course at NIH. And this was developed out of some necessities of teaching this over and over again. But before, let's see if this clicker actually works for me. It does astonishing. So who am I? Really big fat guy, regardless. I started this entire community with Mark. As he said nine years ago, it's grown and blossomed. I'm so happy. And we can have these conferences. So the story goes back the point before Jared came here last year, we were talking about trying to bring you as our here. And we're like, can we really do that? And then apparently Jared just announces a New York that he's coming to DC. And so we're much happier now. So I was on the board of data data community, which Janet ably leads now. So by training on a biostatistician, I work at NIH, which for those of you who are not local is about 20 miles north of here, the premier biomedical research institute in the country, and if not the world. And I also have a startup that I work with, which is across the river almost literally. And so it's so I issue the word data science or data scientists because I don't know what that means anymore. So I'm a biostatistician. I am a data analyst. What have you some people put that under the moniker. So that's fine. But the reason I'm going to give this talk today is I teach, as I said, are for the last three or four years, I teach the basic R class at the NIH grad school. And my population of students are. Ben scientists primarily. So it's a very non coding. Group of people, right? These are people who are barely comfortable with Excel in some cases, which means their data management skills are something to be desired. Some people are laughing because they've obviously dealt with this. So this is the course web page that they created about three years ago and has been evolved. This year I'm not co teaching it, which is nice, which means I get full control over it. But what got me going with this is our markdown and how you could create web pages with it and you could create blogs with and you create websites with it. And so this is actually. I'll talk about how sort of bits and pieces of this. How this is actually three web three sub websites with a cover shell. Why? And I'll get to why is three sub websites, but the needs are, but it's creating content that is easily available to my students can be updated easily and from my side can be maintained easily. So as a teacher, you have to produce new slides. You have to produce homework. You have to grade homework. You have to do all of that stuff that teaching involves. And you know, lazy me trying to get our to do things. I think I have the same virus Jared does where he tries to do everything in our. In graduate school way back when I had on I was at Washington and for those of you who have ever know people from University of Washington, they don't teach SaaS there. It's an R S plus school. Until one year in our second year, someone tried to teach a SaaS for a mixed models class and two of us decided we're going to do this in our regardless. So it's one of. So that's how far back I go in our. So this is the other class I teach with data is what very similar websites. And then also with workshops. The point here is that there's several commonalities here. You need to have a syllabus. You need to have some structure that's overt. You need to have lectures that are updated. You need to have homework. You need to have a way of sharing resources. And for the workshops, I actually have course notes as well. So literally making the book on the subject. So when I'm teaching, what I'm thinking about is two aspects. There's the in class piece that is when I am in class. So as the slides is potential demos. How do I express it? How do I show that? So sometimes it's in our studio. Sometimes is in a dashboard like Mallory described. Sometimes it's just interactive graphics so I can show people. So G.G. animate is another thing I use a lot. And on the other side is the research that people go home and get to consume. Those are things like course notes, other resources links to the RStudio cheat sheets, links to the wonderful community of our authors we have who are putting their books out on the web for free, which is wonderful. And I continue to encourage people to do that. But on the other side, I have course requirements, right? So I have to have a syllabus that I have to submit. I have to give out homework. I have to grade homework. I have to turn in grades. So how much of this can I actually automate so I don't have to wonder about how to do it every year? And so conceptually when I thought about this about three years ago, I said, OK, roughly speaking, I need a website. OK, I can do that in the arm mark down. Let's see if this. And then slides, I was doing IO slides. I got enamored with sharing and I wanted to learn it. So I all my slides are written in sharing and for those of you who don't know it, this is something that E.G. developed a few years ago. I personally like it. You don't have to like it, but it is what it is. And it has a lot of flexibility that I like. Homework, I make them submit in arm mark down, which makes it easier for me to see whether things work and for me to annotate. And my notes, course notes, end up in book down. Because I'm literally writing chapter by chapter what the topics are corresponding to each lecture. So that's the concept. And this is pretty straightforward. I need some glue to put this together into one package, which, as I say, will be an arm mark down website. But, you know, every year is a little different. I tweak things, which means I have to create new product. I forget new slides, new homeworks. I have to keep this updated, keep track of things, put it on the web so that I don't have so that people know where it is. And not rebuild every time. So I was thinking, make, maybe. So something that just updates what's new, right? And supposed to updating everything because it is at the end of 15 weeks, there's a lot of material on this website. So you don't want to have to rebuild everything. And it would be nice to have some direction, some people of contents where people can just go, oh, that's that lecture, that's that topic. And automated processes, can I not have to look at the calendar and go, OK, this date, I do this, this date, I do this, the by hand thing. And for my homework, how can I automate the process of capturing the homework? There's people who do this in GitHub. I'm not going to teach a bunch of Ben scientists GitHub. I'm not, I'm, I'm crazy, but I'm not mad. So there's not going to be GitHub. So the solution I came up with was Dropbox, actually. And then there's grading. How do I do the grades as I'm looking at the homework? How can I grab the grades and put them into a nice, tabular form that I can now submit at the end of the semester? So the syllabus ends up being, so I will say I don't know, attribute everyone, but I've stolen liberally from the internet to do some of this. So on this one, there's a bunch of code here. But regardless, the main point is that I can automate the dates, because I have a weekly class. If I have a start date, I can have R just go boom, boom, boom here in my weeks. I just have to give a vector that says, what am I doing each of those weeks? And then that just becomes a table, I put out on cable and it makes a nice format. I know Mallory's going to say, why did you do do do do do do do do do do do do do issue do, D2, right? Sure, I can do it. It's just, it's a small table, we're okay. So this turns out to look like this. So I don't have to worry about the dates in your agency to know the start date which is nice. This is very easy. This is actually part of have a hard done web page where I have the table of contents floating on the side. So this just doesn't just have the syllabus, it also has So if the requirements, the books and learning materials, academic policy, the usual pair for Nail Year around a course. Right? So the second piece I'm going to talk about quickly is the table of contents. This was interesting. So the table of contents from my lecture. So I'm going to do lectures weekly. I don't want to have to go back to my RMR dinner HTML go, oh, I made lecture eight today with this title, can I now put this in and put all the pieces in? The crucial step in this is learning that I can grab each RMR down slide. I'm the sharing gun, right? So it's RMR down. I can grab the title from the metadata and populate my table automatically. So I don't actually have to physically touch the table of contents. The other piece that's really useful for me was glue. This glue allowed me to create the web links very easily and it just populated it. You'll see I use a glue a lot. And in terms of formatting at the bottom, there's a patch of cable extra, which some of you may know, which puts a lot of niceties around cable. So I add some styling to it in terms of stripe and how wide it is and so on and so forth. Oops. So the nice thing about this, and I'll tell you how those links happen. So you're seeing I have the original RMR down. I have the HTML and they also have a PDF version because apparently I've got students who like to print them out and scribble on them. HTML does not print out well typically, right? And then it just automates it. So basically you can't see further down. I just finished lecture nine. This is my current website. So it actually published as I go along. Because I'm basically using something to process it every week when I do my lectures. Same thing with the homeworks. As long as I have a folder of homeworks that I'm giving out, I use this last few lines to automate using glue data. So I have a data frame and I'm using glue data to just go, you know, template it out. And so that ends up getting me this. Why do I give the RMR down out? Because that's the template for what they're going to submit back. They can download the RMR down, fill it in with the code and send it back to me. And so I've already given the template. So they don't have as much of an overhead to learn RMR down. You know, de novo. Oh, sorry, this way. So this was something that was really interesting. It actually ties into what Dan was talking about earlier today with Reticulate. And what Mark was talking about with about APIs. So the solution I have for getting homework is using a feature in Dropbox called, I think, Cloud File Receipts. So you can put a link out. And as long as someone has the link, they can click on it and upload their files to a designated folder that you design, that you state. Turns out, Dropbox has a fantastic Python API. Why? Because Gudev and Russell was there the last six years. The nice thing about that is that I can write a Python program that will automatically, given the parameters I want, the parameters I want are, what's the deadline, what's the name of the project, and what directory that gets sent to. It'll go out using the API and grab me the link. So I don't have to have to point and click at all. Right? And so here, I actually use Reticulate inside my RMR down for that page, for the homework page, and just grab that function and run it. I have my link. And that turns into that submission link. So entirely something I'm not touching, which I like. So how do we do homework evaluation? So you know how I get the homework? So I get the homework, I run it on my computer. Guess what's my friend here, the package here. If you've ever read Jenny Brian, you know how she loves that, it gives you a way of getting relative standardized relative path names. And so I use that, I run it, then I can comment on it, I just comment in Markdown. And at the bottom, at the last line, I structured that, I put the score. So nine out of 10, eight out of 10, what have you. If I have that structure, I can use R to just grab each file, grab the last line of the file. I just solve the manual entry problem. I just grab everything, now it's a vector. So the part I didn't say here, one of the things Dropbox does is, adds the name to the front of the submission, of who's submitting it. So now I parse the name from the name of the file. I extract the score from the bottom of the file. Guess what, I've got my linked spreadsheet for that homework. Now, it's no matter if it's just joining it across the homeworks to create the spreadsheet. Right, so I submit the homework and there's, I have a homework, one homework, two, so on, I can just do a map over that entire system at the end of the semester, grab everything, have a spreadsheet. It also allows me to say who hasn't been submitting, who has been submitting, because you get blanks, right, because I'm just doing left joins all the time. And so my names are constant. So that's the parsing. The last part, I don't know where I found this, but there's actually a package called gmailR. So you can get a Google API key and send gmails from R with whatever material. So when I do, I have a script now that uses gmailR, grabs their annotated homeworks, I have their names, I get, I have a database of their emails, right, because I get that because I'm the teacher. I parse that, put them together, I send emails out one button, it goes out to the entire class. I think Jared did that once several years ago and I think that's what got me, because you did that for some of your posts, right, originally. So that's part of my thing. That's the feedback loop that I created. When they gave it to me, it's an R process to get it back to them. I have not played with learner R and grader. I know Dan Chen's been working over the summer on that. It seems very cool. It just wasn't in my pipeline when I started this, but I think it's worth exploring as to how to deal with homeworks and grading that, making that more automated as well. So got the process done, I need to update this and need to some make-like thing. So I found Drake and Willanda was very nice and corresponding and helped me setting some of this up. Drake is one of many, many potential workflow management packages in R. There's a site, if you Google it, someone has a workflow R, it's a password workflow R, which has a list of like 25 workflow management packages in R. So people have been writing it, it's not prominent, but it's there. So I chose Drake and Drake is written in R, and the language basically ends up being, does this actually have a pointer? Yeah, it does. So this one, for all my slides, I'm creating the HTMLs, right? So I rendered the RMD file, tell it what the output directory is, and it happens, and essentially there's some R land code in there saying, I have a list of the file names, process it through as a list. The next one is actually fun. Slide 2 PDF is something I wrote as a wrapper, is running page down the Chrome Print function in page down. It actually does the best job I have found for the HTML slides as an automatic process. And so this automatic just takes the, my RMD files and converts them into PDF slides. And then the bottom, the last piece here creates the table of contents. So it creates the top level webpage. And so once again, it's a render site from R Markdown. And then there's some triggers that happen as to if any of the slides gets updated since the last time Drake saw it, run the top level again because you have to update the table of contents. And so it's very similar with the homework stuff. I'm not talking about the course notes today, simply because I haven't done, have not been doing much course notes for this class. But that's just a book down site. So you write up your stuff in R Markdown, run book down on it, you'll get a nice HTML page. You know, if you've ever seen, you know, R4 data science that Hadley has online or many of them, the R Cookbook, the JD, it is JD that has it, right? The version two. So a bunch of people have booked down sites. So if you go to bookdown.org, you'll see all of them that have been submitted. It's basically the same format, but it's internal. And I always keep an option once again, my audience. There's a PDF download version. So the point and so I said, so I was doing this piecemeal and I started working on packaging this is called course down. It still needs work. But the idea is to try and treat a template structure. So I have three websites, essentially, if there's a lecture site, which is a site unto itself, the homework site, which is tied into itself, and the notes, which is a book down site, and it's wrapped around an overall sort of front end web page, right, which has the syllabus and the resources and the sort of the top level, non dynamic stuff. And the assumption was that I'm deployed on GitHub pages. So there's a particular structure, so everything gets pushed the dock folder. And it's provided some templates for how slides should look, how the web pages should look and the like. So I created a course yaml file, which basically stores the course information. So when I run my templates through the yaml, all of them are branded with the course name and the course instructor and the dates and so on and so forth. So it's very much a work in progress. Happy to have people collaborate on this. Any move? Okay, so the talk today is about many opinionated choices. I made choices about drag. I made choices about what are mark down slide thing I'm using. But it tends to work. I'd love to have collaboration to fix the kinks. But it's possible to do a lot of this automated. So all I have to worry about really is content. Create the slice, create the homework. Everything else just happens. Thank you."}, {"Year": 2019, "Speaker": "Stephanie Kirmer", "Title": "Animation for Dataviz in R", "Abstract": "In this talk, I share my experience utilizing data science and machine learning to predict diesel locomotive failures, highlighting how these skills can transfer across various industries. Now at Gernera in the travel sector, I focus on creating effective data visualizations, particularly using animation to communicate complex data insights. I emphasize the importance of understanding your audience and choosing the right visualization technique. Not every dataset warrants animation, but when used appropriately, it can enhance comprehension, especially for time-series data with multiple dimensions. I demonstrate using GGAnimate in R to create animations, discussing when animations are appropriate and how to effectively design them, considering aspects like transitions, timing, and audience engagement. Finally, I stress testing visualizations with naive audiences for clarity and impact.", "VideoURL": "https://www.youtube.com/watch?v=tfpa_QYHebQ", "id0": "2019_11", "transcript": "Hi, everybody. Thank you so much for having me here. Let me find the clicker so I know what I'm doing. As Jared mentioned, I do know a lot about how diesel locomotives work because I used to work at a company that predicted failure of having machinery. So I use data science and machine learning to predict when diesel locomotives are going to break down. And I came into that role knowing nothing about locomotives of any sort. I had never been in the room with one. But it was one of the most informative and interesting opportunities in my life to get to work on that kind of stuff because it allowed me to see how data science can be applied to different content areas while using similar techniques to bring informative insights into different areas of business. And now I work at a company called Gernera, which is in the travel industry. Nothing to do with locomotives, but again, I'm using a lot of the things I learned working in data science and heavy machinery to make the travel industry work better and more smoothly. So it's actually an interesting career trajectory and path through data science. And I think that that's one of the things that has helped me to come to this topic, actually, because when I'm talking about animation. And I'm talking about building tools for communicating about your data. There are a lot of different audiences you might be facing, people you might be trying to speak to and really get through to in different areas of business. And you need to design the visualizations and the reporting and the things that you're bringing to that stakeholder to really hit their nerve centers. To really put yourself in the shoes of the audience you're trying to speak to. And in my case, I have learned to speak to students in health policy, or I used to teach at DePaul University, people who are mechanics on diesel locomotives, who are very different from students at DePaul. And people now who work in various areas of the hospitality and travel industry. So that's some of the stuff that I want to communicate about this here. And as you can see here, there's a GitHub link. Sorry, the blue is not especially easy to read, but you can also view it on my Twitter account, which was already listed here. So just hit me up on Twitter. I've tweeted the link just minutes ago. So the first thing I want to talk about, because of course, animation, super cool. It is incredibly fun. It looks cool. It makes you look like a rock star when you generate one, totally. But it is not necessarily always the right thing for every use case. I think we can agree. So I want to talk before we get into how to do it, whether to do it. You know, we want to have that conversation about what the right strategy is before you're halfway into designing something that may turn out to not be the thing that your audience is really going to find useful. So there are some reasons to generate an animation from a static plot, and there are some reasons not to. You just think it's cool. Do it on your own time. You know, use animation for maybe for a tidy Tuesday project or something like that. But don't put it in front of your boss just because it looks neat, because that is going to end badly for that project, most likely. Showing off, once again, on Twitter in your own time, that's great. But think when you're generating a plot like this about the audience you're trying to appeal to. And then thirdly, when you look at a data set, if you have a dimension that represents time, whether it's over years or days, if it's time series data of some sort, it can be easy to think, well, time moves in a linear fashion. So my plot should move in a linear fashion. I should use an animation that is sort of time oriented because I have time in my days. That's not necessarily true. There's no reason that your animation, your visualization should be animated just because time moves forward. However, having time as one of your dimensions is not a reason not to generate your animation if it's appropriate in other ways. Far more meaningful data dimensions is one of my sort of rules of thumb for generating animation. If there's something in this visualization that can't be represented in three dimensions or less, then it's going to be busy. It's going to be hard to view on the page. It's going to be too much for the eye to consume and the audience is going to get overwhelmed or confused. And that might be a time when animating is the perfect way to approach it. Oh, no. You went to sleep. Also, if you have a single dimension in your data that has many, many, many levels. So if you're thinking of a data set that in the case of the example I'm going to show you today has years represented as one of the dimensions, time where it's appropriate to use it is that I'm looking at over 20 years. So if I want to, as you'll see later, facet this data instead of using animation, I'm going to have 20 little playing plots all over my page. That's not going to work very well. Think about how busy and how crowded and how sort of overwhelming your page will be if you don't use the animation and balance the audience interpretation. And it's something I tell lots of people get a naive audience. You're, you know, grandma, the guy down the street, whoever to look at an animation that you're generating or static plot you're generating and give honest feedback before you're putting this visualization in front of business stakeholders, people that you work with, whose opinions are really going to make a big difference. One last detail. And this isn't always a hard and fast rule, but the animation is unlikely to be able to show all the data points on the screen at the same time for the entire data set. It's kind of by the nature of the process. That's one of the reasons you're using an animation is because you don't have to show everything all at once. One big chunk. But if to understand the data and understand the message you're trying to send with the data, you need to see all the data points at one big time. Animation may not be the right choice for you. So there are many great use cases online. The samples and and great examples of people using animation to extremely effective ends are endless. Thomas Lindpierson is the designer of the G.G. Anime package, which is what we're going to talk about a lot here. And he's got this plot on the G.G. Anime web page is one of the five, like the primary examples of how to use it. He is by my account, representing six dimensions in this little plot, because we've got country. We've got continent. We've got G.G.P. We've got life expectancy. We have the population of the countries and we have time. So there's a ton going on in this and it doesn't feel overwhelming. It doesn't feel like this data set is telling you too much, but it's not telling you everything that it that you could theoretically grasp from the state of set. And there are some things that are not particularly easy to do, like country against country comparisons. Each dot is a country. They move pretty quickly in this visualization and they're pretty far apart. So you're really only going to be able to look at the within continent country to country comparisons and not across time necessarily. So think about what you need to say to your audience and what you don't need to say and design your visualization accordingly. So there are multiple other amazing examples. I really encourage you to check these out. There are different use cases for visualization than just time going by. And I really encourage you to look into these, especially if you've got a data set, you think maybe animation might be the right choice, but you're really not sure how to sort of frame that. And then you can see the examples of other people's work, you know, steel liberally as we do in the Internet age, and it will, it will make your job a lot easier when you're trying to conceptualize this for yourself. So I've already talked a little bit about this, but let's revisit just a moment. Think about design when you're thinking about how to create this visualization. If you're pretty sure that animation is the right choice for you, you think about what you want the audience to take away. If you're a director of a class, you are only going to be able to communicate a finite amount of information to an audience in a single visualization. So don't get too carried away. Don't try to sell every in every insight that could possibly be gleaned from this data set in one plot. Think about each visualization as a as an opportunity to communicate maybe one or two insights and really make sure that it targets that effectively. Movement and speed are not innocuous. They're not just it looks cool when it comes to doing data visualization. When you make a plot move very fast, the audience will subconsciously or consciously think moving fast must mean something here. It must be a fast moving trend or it must be that these things move quickly. And you need to think about whether that's true. And so it really take a moment to consider the impact that every choice you make in your visualization process is going to have on the way the audience interprets. If one point transitions into another point, the audience is going to think those are the same point having some sort of transition or some sort of mutation occur. If they're different points and they really don't have a relationship, that transition should not happen between those two points. So those are some of the examples of things that you need to be sure to be conscious of and to take into consideration when you're designing. And be conscious every step of the way that you might decide animation was not the right choice for you and have to take a step back and that's okay. Animation is awesome and very cool, but it's not for everything. So now let's talk hands on actually how to do this. There are a couple of packages that you need for sure. You need GG Plutto, you need GG animate and you need Gifsky for this project because I'm generating a gift. And so you need to have your render for that. The other libraries that I've shown here, Transformer Patchwork and GG Force, are very nice to have because they give you a little bit of extra possibility, ways you can arrange plots, additional features that you can use, but they're not mandatory to generate a simple animation plot. First three packages really is all you need. And so we need to understand some concepts that are extensions of the visualization and grammar graphics beyond what we think about with static plots. A frame. If anyone here has heard of Walt Disney, we have the image in our head of animators with little paint brushes painting little animation cells one at a time and then you do sort of like a flip book and Donald Duck moves along the page, right? This is the same kind of concept. You're thinking of each frame as a cell of content that are going to be moved together rapidly enough that it looks like an animation and you are going to generate little frames that will be flowed together. A different issue, a concept that's related, but stinked and needs to be thought of that way, is state. For what we're doing, state is the grouping variable that identifies the changes in the data as it moves along. You can have a single state take up more than one frame. Different frames can be the same information a frame can hold for one to three seconds if you really want to emphasize something, or it can move really fast to be a quarter of a second. You just think that frame is just getting us to the next point, but the state is, for example, when we're looking at Thomas Lynn Peterson's plot, the year, the year is the state. And when the state changes, the data moves and the data moving is really the point that the state is for transition is, as you might guess, how it moves from state to state. And we'll talk a little bit about all the different ways that states can move back and forth. And there are many, many choices just in based G-G-Animate that are actually fascinating, some more businessy and some more fun than others, and then entering and exiting. So as I mentioned, a data point that transitions into another data point better be the same data point. It better be the same class or category dimension or something. But if you have a new class of data or a new level of a dimension coming into your plot, it needs to come in a different way. It needs to be totally unique so that the audience understands this is new. This is not the same data moving through time. And so that's what entries and exits are for when data comes into the plot for the very first time. It gets an enter and out. That was an exit. So now we're actually going to build a visualization. I promise we will get to some cool graphs. Okay. So I've got an example project here. If you look on the GitHub page, it will give you the data set and give you all of the details you might need to play with this yourself. But I'm looking at populations who have particularly refugees but displace populations moving in living in South Africa. Picked it just out of random. Data world has great data sets. I highly recommend you check it out if you're looking for data sets to play around with. And this came from them. So I want to look at where do refugees come from when they're moving to South Africa? Where are those places they're coming from in terms of the local region or someplace else? What are these groups? What are these countries of origin? What are the changes over time in the countries of origin that are most common for refugees in South Africa? And then is the number stable? Is the overall pattern that the top country is usually the same kind of rough estimate of number? Or is there sudden boom in refugees coming to South Africa from somewhere? We'll find out using our plot. So I've got a quick little like ping in the data set. We just got, you know, it's already grouped. It's already summarized. It came that way from UNHCR. So that was nice. And if you need to group your data in advance before you're basically starting with a grouped bar plot. So anything you would do to prepare your data for group bar plot, you will need to do here. I added rank to this and then also filter just for refugees because that was the population I'm interested in here. So this is the code for the group bar plot that I'm going to start with. We're going to start there and we're going to mutate it into a really cool animation. But this is really all you need to start with. It's very straightforward GG plot and I don't think it should be a real surprise to anyone in this room. So now here's what it looks like. It's not the most effective plot I've ever seen. It's a lot of years as I talked about more than 20 years of data. And there's a lot of countries involved because certain countries are in the top 10 of origin countries of origin for refugees in certain years and not other years. You know, there's a little bit of mutation in that space. So this is the group bar plot. It's not great. So I want to give it a give it some examples of how you might use fascinating to group these countries in a different way. And it's not going to be pretty. It is not good. There's too many countries to show off here. Too many years. It's not great. No, let's not. Let's not with the facets. Okay. Fasts are great in most cases and there's lots of uses, but this use case calls for an animation. So let's do this. These race plots have been very kind of popular in the Internet lately and I see a lot of people building them in D3 or other other frameworks, but you can really build this with GG animate in a very simple set of commands. So we have reform on our plot. We started with this. This is our original. And we're going to go to this. Now, yes, there is a lot changed here. I'll highlight the things. We're basically changing the grouping byte to rank. We're changing the theme styling stuff. We're moving and we're flipping the axis to be to be sideways. So this is what we get. So what you'll notice is we've got the same data that we had before, but now it's year over year over year flat placed on top of each other, just like animation cells, right? Every year is just stacked right on top of each other and in this case, completely unreadable. But once we animate it, it will make perfect sense. So how do we visualize it? How do we actually generate an animation out of this? That's it. One line. Ta-da. And it's moving. Here we go. It's working. It's not perfect. We have things to do yet and I'll show you how to do those things. But this is all you had to do to the animation. You start with a Ggplot. You would make no mutations to the Ggplot. You started with just add the animation stuff to the end, transition states, and off to the races we go. So we can solve some of the problems that were noted on the last slide. Easily descriptive titles that move with the data and label those bars so we know what they really mean because they're moving by pretty fast. We need to be able to see what those are without having to like fly on the line down to the x-axis. So here. What we've done here is just taken a couple of small code changes and as you can see, we've added this thing called closest state. And that is a variable that's unique to the Gg animate universe that allows us to tell the plot to just take whatever state, which we've talked about. We know what a state is, right? The grouping variable for the whole is. And render that above. So now we have the year, pinging right along as it moves. We can see all of that. We have our axes labeled beautifully. Things are moving very well. So there are things about how movement happens, though, that we can take into consideration here. So entering and exiting. What if we try grow and shrink when things enter and exit? Why not? Let's give it a shot. So here we go. It looks a little cartoony. It's a little whoo. Kind of neat. But I'm not sure that this is a, you know, sort of serving the objective of the audience learning, right? If you're doing this for a tidy Tuesday project where you're just going to experiment with techniques, give it a try. It's very fun. So we've got, well, I'm going to actually change that later for the final visualization. But let's talk about the easing, which is the in plot transitions. We have lots of choices for how the movement between positions on the page can really occur. One thing to note is that you can use functions for this. So the speed can be sort of changing as it moves, cubic, quartic, and so on. And this is what quartic looks like. It's a little fast, so you don't really get to see that it's going, sure, but that's what you can do. You can just do linear. You can just make it move at a constant speed. And that's fine, too. But the options are made available to you so that you can really design a visualization that's going to meet the needs of you in your audience. So bouncy easing is another one. I call it bouncy. It's called back in and out. But you can see it looks a little like, it's like, and there's another one that's similar to this. It's like spinging an elastic band. If this is the sort of thing that you think would make your audience understand the data better, go ahead, use it. It's there. It's available. It can be fun. So then, now we know how things are going to move. We have in, we have out, we have in between points. We need to think about the timing, because every state can have its own time. You can assign each state an exact amount of time that it's going to stay on screen. And in this case, what I've done is a quarter of a second for every frame, except the last one which gets 20 seconds. Because I want it to move smoothly through life. In the end, I want it to hold on the last one before we swing around to the beginning again. And by saying rap equals false, I'm making sure that it won't flow from the last year, 2017 back to 1990, whatever. Because that's not a linear relationship right there. The end of time does not flow back to the beginning, as Dr. Who might have had. Right? So here's what we get now. Slower pace, feels smoother, things are moving in and out with each other. And if we wait long enough, we'll be able to see the end where 2017 stops and moves all the way back to the beginning. But it's because it's all quarter seconds at a time. We have a nice movement there. So 2017 and then boom. Right? So a quick mention of transition states, because I need to make sure that I don't take too much time with you. This transition states is the core element that you need to add your GG plot in order to make it animated. So states tell it what state you want to use, what grouping variable needs to go state to state, the length of transitions. So how long should it take to go, you know, quartic or cubic or whatever, flowing back and forth, give a length to each state as it moves along, as I've talked about. And then that rap element, making sure that you decide whether transition smoothing should happen between the end and going back to the beginning. So this is our final animation. I have taken a few, added a few little elements to the animate function there so that it can be generated for visual, for app, sending out to a GIF. And then this is how you save it to a GIF. Pretty easy. And that's it. So to conclude, think about whether you need animation. Whether it's going to be the right thing for your audience, for communicating the insights you want to communicate, don't get too overwhelming with your audience. Think about where they're coming from and what knowledge they're bringing to the room. Then if you're going to make an animation, make it awesome. Make it really, really sharp. Make sure that it has the transitions that are appropriate for the information that are really going to communicate to the audience exactly what you want to say. And then get some feedback, test it on naive viewers to make sure that what you think is getting communicated is actually what it's saying. There are some more links, references, things you can find on GitHub for me. But other than that, thank you very much. Thank you very much."}, {"Year": 2019, "Speaker": "Alex Engler", "Title": "Better DataViz in ggplot2: Tips, Tricks & Examples", "Abstract": "This talk covers the use of GGplot, an R package for data visualization, and its numerous extensions that enhance the plotting experience. The speaker highlights several extensions, including the SF package for simple features, which simplifies spatial data manipulation and visualization, and GGtext for incorporating HTML-style text in plots. GGhighlight is discussed as a tool to simplify highlighting specific data points or lines in plots. The speaker also introduces the use of annotation custom for embedding supplementary plots within visualizations and facet geo for geographically structured faceting. Additionally, the speaker touches on the integration of tidy evaluation in GGplot 3.0, which allows more flexible function programming with plots. Finally, the talk concludes with a nod to the extensive ecosystem of GGplot extensions, such as GGJridges and Rachael, which offers possibilities like 3D visualizations. The speaker emphasizes the improved efficiency and creativity these tools bring to data visualization in R.", "VideoURL": "https://www.youtube.com/watch?v=h_QMd2d_o98", "id0": "2019_12", "transcript": "Okay, so a couple things about counting cards. One, anyone who uses it is like, oh, I'm a smart person, I learned to count cards, they're lying to you, it's for more. Like, it's all memorization, there's like almost no math, it's counting. Two, it takes an incredibly long period of time, and three, you can't make any money doing it. So I don't recommend, when I came back from my first trip to Reno, Nevada, I flew myself to Reno, Nevada. I came back and my brother asked, how much money did you make? He said so and so much. He said, how long are you there? He looks down, looks back and says, you made less than minimum wage. Anyway, I'm a moron, I'm here to talk to you about graphs. I really love DataViz. I've been teaching GGplot in various DataViz classes for a little while. Quick shout out to Georgetown, and thanks again, Amy Gates for hosting us here. Really excited to see this here. I worked to help build this graduate program, and this is the space that I personally am in, which is helping bring modern data science, computational methods, emerging technologies to public policy research and government analysis. I think it's an important space, I think a bunch of you are sort of in that field. If you're interested in this or the space at all, please come talk to me. Always excited to meet more people in this realm. That's my quick plug, and I'm going to talk about GGplot. This is from 2015, and I want to get the name of the author. This is Andrew Devise in a 2015 talk at the big DC, I'm sorry, the big R conference at UZAR, and the network graph of all the packages and package dependencies. If you see the blue area, the dark blue, that's a lot of GGplot and a little bit of the tidy verse as well. One of the takeaways you could have from this is that's a lot of packages. The pure number is very high, and the interconnectedness, this is dependencies, but people also use them together very frequently, right? It is really high. I actually want to make the argument that that interdependency with GGplot is going to remain, and it's going to be like to our benefit generally. And it's a little bit of fun at our Python-y neighbors. If you've never read this article, you should. It's hilarious. It's a dramatic tour of the Python data visualization landscape, and it walks through how you can make graphs in Python using like seven competing graphing systems. And some of them are quite good. Boca is quite good. Plot nine, which is a GGplot two, is quite good. But there's seven of them. And Python has not agreed upon sort of a common framework for working on visualizations. We are really lucky to have done that, and I think the result of this is a really high quality set of packages, even though there's many of them. They're commonly built around GGplot. They're easy to use. They give you a lot of extra functionality. And they're not going to get absorbed into GGplot. That is just not the model that the core developers and have these talked about this a couple of times. They're not going to go take all this cool extra packages and bring them to GGplot. It's going to be an ecosystem. So I'm going to bring up seven examples of packages. I think there's seven. Some of the packages that are low hanging fruit that will make your graphs better in GGplot. First, this was a big update. The SF, not spatial. Everyone thinks it's spatial features. It's actually simple features package. It's now really well integrated with GGplot two. Okay. This is really cool. And at first you might see something like this and be like, wow, finally we can make amazing maps and we couldn't before. This is from like three years ago without SF. And so you actually could make really great maps before the SF integration. This is still a really big deal for ease of use. And if you haven't done this yet, I really encourage you to get into it. Angela Lee, who's a rockstar and geospatial analysis. We'll be talking tomorrow about using SF and doing data manipulation with it. But the big difference is you don't have to do this part anymore. This is some of like, this is this core data-munging process of reading in before simple features. A shapefile. If you look, that's the read to OGR. That's pointing to a folder. It's looking for a light like the layer name, which is the name of that shapefile. It's using this at operator, which hopefully none of you will ever have to see again. And that's literally because there's like a data frame stored in an attribute of the spatial data that you just read in. And you just take the road names from that and use a fortify function and no one knows what that function does. So it makes it into a long data set with a giant column, essentially, that has all like latitude and longitude points. And then literally, you're doing all this work, so you can pass it to Gian Polygon. And this is beautiful, but that's what that's doing. It's using Gian Polygon to take this very complicated thing. And there's a raster file in there too, but the core plus part, the color by value area, so that's coming from Gian Polygon. And that's confusing. It also means that you have to use Gian path for the borders. And this was good. This was a solid, useful step on our way, but it was confusing. It was very hard to teach. I taught this for a while. And the new beauty of SF just completely hides this from you. There's an ST read function for reading in spatial data that comes right from shapefiles. And then you literally use Gian SF and it maps for you. And it's absolutely fantastic. It could not be more straightforward. This is the tutorial I'd point you towards if you want to follow up drawing beautiful maps for grammatically with r SF and g2 part one of the basics. And just to really quick look at what's happening that's great here, when you read in data, as in with ST read, it becomes an SF object, simple features object, which is also a data frame. The difference is it has a column of geometries. So rather than having this crazy series of points and making this very, very long data set that allows you to track all that's and all of that for each spatial thing could be a polygon, could be lines, single line, it could be points. That's encompassed in a geometry that like all of that ends up being in one cell essentially for unit. And SF is going to see this, understand that it's there and say, I know how to do this. There's no SF polygon versus SF points. It's just Giam SF for all of them. And you can see that here, I take the this is the world data path to GMSF, then for counties, then for points. So that GMSF is being used three times twice for polygons, once for points data, use the points of the cities, and then Giam text because this is all G-gibots, you just drop it on top. It would be ridiculous to ask or maybe people will make this easier. This is it'd be ridiculous to hope it gets better. So R has gone from geospatial analysis being a real weakness to being a huge strength in maybe five years or so. Nip pick about that, but that's where I see it. Okay, so that's SF, really encourage you to look at that. Here's two, you might think, wow, this beautiful map. This map is the thing that's appealing here. But when I saw this, I already knew the maps were great. I got really excited because of the bold text. Is this it yet? This is totally new. You couldn't do this before. The text right here is bolded. And I actually think it's a really, and like the Washington looks quite nice. This is a new cloths will package called G-gtext. And it lets you use Giam rich text. You highlighted there at the bottom. And it lets you pass an HTML style markdown onto your plots. And so this is silly example of why that's what he's doing, right? He's passing through a formula with superscripts, blue text, italics, large text, bold, change sizes, right? So this is nice. You might ask like, okay, what do I really want to use this for? And here's the first tip. For visualization, I actually really like the simple use of this, which is highlighting in your title and description what you want people to draw their attention to. So this is a graph from a student of mine about maternal mortality. And it's the rise of Poland. I'm sorry, I cut off some of these years, but this is over about a 40-year period of 85 to 2015. And the decline of the US, this is a rank chart. So we're going down to 95. And so it's simple, right? The coloring in the title, but I'm trying to pretty explicitly draw your attention for what the title text is to what I'm trying to show you in the graph. So a simple example. If you want to see something more complicated, this is also using geometric text, which I think is hilarious. It's taking labels and passing it in. And actually you see the pictures of the iris slowers appear. That's ggtext. Okay. Highlight. gghighlight. Also another package. This might look simple, but the way you would have had to do stuff like this before is really annoying. So if you wanted to plot a whole bunch of lines with no color, you could have said in this case group by state, this is opioid deaths over time as a percent of population. So what you're looking at is West Virginia having a staggering opioid fatality rate relative to other states. That's why they're all the end up. So you could have got no color just by doing g online in group by state. To get some colors, you would have had to do two g onlines. One maybe for all the states in another filtered, where instead of group used color, that would have done it. So two consecutive g onlines. Or you could have created a new variable for highlight in which most of the states got no highlight and some of the states got a highlight color. So gghighlight does, it sort of takes that away. You can highlight based on like filters, like where statements. So plus g online plus gghighlight. And this is saying max highlight equals four max fatalities per capita. So take the top five. So maxhighlight is highlight five things. And max, take them by fatalities per capita. And so you get the top five highlight. And if you wanted to change this, you didn't create a new variable. You didn't use two lines. You can quickly iterate through this. So the g online, you don't really want to be creating a new variable every time you want to change the colors on a chart. That's not super ideal. It works with fascinating too. So here's another example. In fact, this is actually changing behavior of facet. So this is a graph from a former student, Jonathan Tan, looking at the changing cost of solar panels over time. And it's showing three states, Connecticut, Massachusetts, and New Jersey. And what's happening here is it's gmstep. Then gmhighlight. So gmstep, working as you'd expect it. Then gmhighlight, that highlight is only highlighting if there's your sort of filter condition. So in the first one, it took the top five by a variable. You can also just pass it one or multiple sort of wear style statements, right? Think like the filter function in a deep layer. And then it specifies an unhighlighted color. And what's interesting is then he passes it to facet wrap state. And you're seeing it's only fascinating by those three filter states. So again, a highlight and a filter of the fastening at the same time. Would have been more annoying to do that before. That's gghighlight. Really easy to use. The other reason I'd mention this is because most of the time if you're using colors for all of your whatever you're showing, it's too much color, it's too much information. People dramatically overuse colors. Your max should be 6, 7, 5, maybe is a good number. Usually highlight, take out what you don't need. Focus people's attention, what's important. Shocking to be how easy this is now. This is with the annotation custom. If people feel like Grobes graphical objects, sort of kind of, grobes are cool. You can just basically make a chart, make it a grove and drop it onto other charts. And like three, five, so you make a chart. You say like, hey, be a grove, please, just one function. And then you just annotate custom. Plus, and then a grove. And that's how you get something like this. And I like this because you have some white space on the Spark chart. This is also from Andrea Koch, former student, looking at maternal mortality by births, which is a problem in the US. It's going up. It's going up a lot for African-American women. And looking at where this is happening and the colors of the quintiles. So you could argue there's some geospatial trends here, right? The South is not doing as well. The West United States is doing fairly well. But the geospatial side isn't like that strong that the map really becomes that necessary on its own. So maybe you want the combination of the map and the bar chart, but all of a sudden, that's a lot of space. Turns out you can do this really simply. Just as I said, the top is the map. There's G on SF again. You save that as US map. And then you have this G2 plot that you're actually plotting out, and annotation, custom, and dropping in that US map. And you're just converting it to a grove as you go. So if you want to combine plots, that become a lot easier. If you GG save this, both will come out. They're technically in one viewport, which I'm not going to talk about. But it makes it easy to take this and export it, put it wherever you want. Here's another great example. This is from Dan Snow looking at access to health care. So the blues, the darker blues are better access to health care, just in seductors, into the reds and yellows, which are worse. And what I like about this is you basically said, I have this nice histogram that shows the distribution of this, which is kind of hard to see from looking at the map. Remember you can't look at the map because these are counties and say, well, I see a lot of blue, that means there are more counties that are blue. They're counties, they're changing sizes, right? You just can't sort of infer that. So what he wanted to include the histogram and then essentially replace to the scale with the histogram. And admittedly, you don't see numbers on there because the metric is kind of a complicated Matthew metric, but high is high access, very low is low access. And again, same thing, made a histogram, made a map, saved as a grub, dropped it with the annotation custom. One note about when you may not want to do that, something like this. So this might be right, but I don't recommend it. What the student said here is they made a box plot in histogram. They extended the y-axis on the histogram to a negative space and then used annotation custom to drop it into the same space. This might be right, but I just want to remind everyone. And so that's what they did. I'll remind everyone that when you take a grub, and this is just for the sake of example, taking grubs and dropping them, they come with a lot of space around them. They come with the axis, they come with the margins. It's probably not a good idea to put different graphs on the same scale within a single plot. So if they're not on the same scale, probably fine. If they are, you probably want something like this, which is from G-table. So you want something, two separate charts on the same scale. It doesn't really replace that. It does allow you to put two things in the same place, though. And this is G-table-easy universe for that. But here's one more example of annotation custom doing something cool. You might think this is pretty hard. This is actually pretty straightforward code. You create a bounding box around New York. This is all the doctors in the US. You put a bounding box around New York. Literally you just define a set of latitude and longitudes. You put a bounding box in the Atlantic Ocean. You create two maps, one that's zoomed in, and one for the whole country, and drop the smaller one in. So these are the two bounding boxes. Again, there's some ST stuff in here. But if you're interested in that, Angela's talk tomorrow will, I think, cover this and hopefully not volunteering too much for her. But you're literally creating a box in a map. A box in the Atlantic Ocean. You're subsetting the points that are in the box in New York, making two maps, one with all the points, one with just the map, the points in the New York, and here you're dropping that small plot into the... As a crop. And if you're wondering those who just dotted lines with GM annotate segment or something like that, between the latitude and as. This is the kind of thing you might be able to do with just a little bit of annotate, custom annotate and groves. And again, it's pretty low hanging fruit. I like facets. I like facet geo. This also is really easy. This is from Jonathan Tan, former. So almost all this is... This talk should have been called like cribbing ideas for my students. I think I proposed that. And they rejected it. Like I think that happens. This is facet geo. It's showing you solar panel. I'll take so how many solar panels there are in the US. And then the colored bars are the existence of solar panel incentive programs in those states. And the students actually saying that they didn't seem to be that related. That was their takeaway point. The argument with something like facet geo is not necessarily that the geo spatial side is that important. It's that this is actually faster to find what you, an individual from a state, might care about than a grid that is in alphabetical order. So if you know you're looking for California or Maine, it's actually pretty easy to glance in the direction that's getting you very close to that. Actually much faster than like a grid alphabetically would. And facet geo is really easy to use. Geomstep, geomrec, facet by state. There are some built in, but you're like, how many could possibly be built in? This one is for Brazilian states, which I think is quite nice. And in fact, there are 62 of these built in by default, which I thought was quite a few. And you can build your own with the geo grid designer. So if you like facet geo and you want to build one for a set of provinces, but these are made up countries from various TV shows. Then Borgia is the one that only rich people can afford, I think, in 30 Rock. I love that joke. Anyway, so facet geo really easy. And again, the framework for you to build your own stuff and sort of do that faceting across the geographic areas quite nice. Teddy evaluation. So one I almost certainly should have, I shouldn't have included this. So it's really interesting, but I'm just unqualified to talk about it. So I don't really understand tidy evaluation, but I'm going to explain one use of it anyway. I read chapter 19 and 20 of advanced are at some point and it just didn't stick. But I do understand this. I understand that gender and mass as variables in the Star Wars data don't exist in the global names space. And thus, they like when I try to do this, they like nothing happens, right? That's where you get those errors. I get that, right? I feel like everyone sort of vaguely understands that the tidy versus doing something that makes the tidy functions recognize variable names. Cool. Okay. And I get this because of that, it's hard to pass like tidy versus functions that you're used to using into functions because it's interpreting things differently, right? So here, when you try to create data bar and buy in a function name and pass those to tidy versus functions, you get an error on this side. So this bottom part works and we like that, but it causes this slightly tricky problem. The idea of valuation lets you kind of have the best of both worlds. And in this case, what it lets you do is have things that you want interpreters the string, interpreters the string, and have things you want interpreters a variable name, interpreters the value. That it turns out is very useful in G-J-Pott. This is the other thing fully integrated in the G-J-Pott 3.0 along with SF. And this is an example from Aaron Williams and he's right to include it because he argued for this and he's right to because it's really interesting what it lets you do, which is I don't think anyone's going to be able to read this and understand it without an understanding of tidy evaluation, but this is what it acts, this is actually what it lets you do, which is in one function, I can make tab set function output into R Markdown. Like that's what you get out of that is I can treat this variable as a string when I want it to be a string and pass it into G-J-Pott as a function when I want to. And that's new, it's relatively important if you want G-J-Pott to work as part of a function or like in functional programming. And the last part, this is 7 of 7, this is just a small number of the extensions, this is some of the ones I really like, but there's a ton. And again, I think they're going to stay a sort of external to G-J-Pott core. This is G-J ridges and it's beautiful and simple. This is G-J-Mosec of Stop and Frisk. This is Horizon, G-J Horizon from G-J-Alt. This is a ridiculous one if you want half a violin plots, but the pure number of them is amazing and I encourage you to go and check out this whole really amazing ecosystem of sort of a round G-J-Pott. The G-J-Pott extensions is probably the best site for that. And so I'll stop there because I'm a little over. Rachael is cool too. Yeah, I'll show you Rachael. So Rachael does two things, it makes ridiculous maps that like just blows my mind. And then, and this hasn't done too much yet, but it also turns G-J-Pott's 3D. And that's all one function, it just took the G-J-Pott and made it 3D. And it's only doing that by taking the color variable and projecting it in a 3D. But I think this, if you want another thing I'm excited for in the G-J-Alt. It's Rachael. Okay, now I'll really stop. Thanks so much everybody. Feel free to shout."}, {"Year": 2019, "Speaker": "Selina Carter", "Title": "How Are Ideas Spread at Your Organization? Analyze Its Citation Network!", "Abstract": "Social network analysis (SNA) is a tool for measuring relationships among people and projects within an organization. You can map the spread of ideas by building a \u201ccitation network\u201d \u2013 when one project or person cites another in some way. To understand the breadth of innovations shared across the Inter-American Development Bank, Selina walks through how she built a citation network for loan projects, and then ran a statistical model (ERGM) to check for information silos across countries and departments. Selina also shows how to use regular expressions with the stringr package, calculate network statistics with igraph and statnet packages, and make cool network graphics!", "VideoURL": "https://www.youtube.com/watch?v=Y_ZGjE5rUwU", "id0": "2019_13", "transcript": "Social networks, baby. Who here does social network analysis? Raise your hand. Yeah, very nice. So did you know that, who's the lead guitarist of Queen, by the way? Brian May. Did you know that he has a PhD in astrophysics? So it turns out when he was in his last year of his PhD program, he was in the Queen band. And then Queen happened. And he was like, OK, I guess I'm doing the Queen thing now. Queen happens. And then many years later, in 2007, he went back and finished his dissertation. So he's been published in many academic journals. In addition, he does a lot of books on the Big Bang for the layman. And it's super fun to read. So the moral of the story is, in all of us, there's a creative person. And then there's the math nerd person. You should combine them both. It is your duty to combine both sides. And one reason why I love social network analysis is it's so clear why those two sides come together in data analytics. So there's the art side, because there's really fun visuals. And there's the analytics side, and the statistics, which I'll show you. So these are two of my passions. So today I'm going to talk about social network analysis, specifically in the case of the organization where I work, which is the Inter-American Development Bank. And I make a site, what's called the citation network. And I want you to think about how you could apply this at your company. So first of all, I'm really an art nerd. And recently we did this really fun pumpkin carving event in my house. And of course, I had to make art. I also may or may not have made our cupcakes at one point, which you can see in the corner here. And so I'm a super R geek. And I love this conference, by the way. So overview. First of all, I want to talk about what is a citation network? And then how do you actually build one yourself? We'll look at the example I used from the Inter-American Development Bank. We'll think about how do you analyze a graph, and then we'll do a wrap up. So first of all, what is a citation network? Basically, it's a type of social network analysis. So what is that? It basically means you have a set of actors, and you're mapping the interrelationships across all those different actors. So this is a classic example where you have people. And then you can see these little arrows across the different people that might represent friendship groups, or they could be communications. They could be some sort of management structure at an organization. But you can also replace those actors. Instead of people, you could do something like documents or papers. And a citation network often is doing just that. You basically take, instead of people, you can take academic papers and map the different citations across those. It also doesn't have to be academic papers. It could be something else. So here's an example. Let's look at the citation patterns across different academic disciplines. Think about what you think is the discipline that gets the most citations. It has the largest volumes of citations. Psychology, other ideas? Medicine, other ideas? Biology. Biology. OK. So here's the answer. This is a study done in 2008. And it turns out that the largest nodes, I don't know if you can see this, but medicine and molecular biology have the largest volume of citations. This is a study done on over 6,000 journals connected by over 6 million citations and then grouping them by discipline. So you can see that, for example, there are arrows between different disciplines, which might suggest that there's a lot of cross-pollination across those fields. So obviously, medicine and molecular and cell biology are very related, so they cite each other a lot. But economics doesn't look like it has very many citations with medicine. It also looks like economics has a small volume of citations relative to these fields, or physics, or neuroscience. So you can also think about, well, where are there opportunities for different types of research? So why are we not combining more, I don't know, probability and statistics with geosciences, for example? So you can kind of see both the interactions and the interrelationships, as well as the gaps in knowledge. Here's another example. So in this data set, in this study from 2017, you have political tweets on three different politically hot topics, gun control, same-sex marriage, and climate change. And then the nodes are colored by mean ideology, so liberal or conservative. So a node is a user, and a link represents a retweet to another user. So what can you get from this graph? Well, it looks like there's a lot of clustering within users of the same ideological community. So it looks like people are sharing information with those who have like views. And there aren't as many users who are sort of bridging the gap between these two different ideologies. So again, citation networks can apply to many different things. It doesn't have to be just academic papers. So why are citation networks useful? One, they map the influence of ideas. You can measure the impact and the spread of knowledge. Two, they identify key actors who is cited the most. This is often what people first think of when they think of who's important in a network. But you can also think about who does the citing. It's just as important that academics or anyone in a particular network actually is the connector between different nodes. Because if there's no one who's bridging the gap, then there really isn't a network at all. Three, they reveal clusters as well as gaps in knowledge. So how do you actually build a citation network? Network information can be written in two equivalent ways. One is called an adjacency matrix. So this is a very basic example of a graph. So basically you have these five nodes. And four of them are connected and the fifth is sort of floating by itself. It's called an undirected graph because there's no arrows. You just have a link or there's no link. And to record this information, you have a symmetric matrix here. It's always a square matrix. So you have five rows, five columns, because there are five nodes in the network. You put a one if there's a link between the two nodes and a zero if there's not. Notice, for example, that two is connected to one. So a two is a one with column a one. Likewise, a one is a one with column a two. So this is a symmetric matrix. If you have a directed graph, then that just means you have arrows. So in this case, two sites one, for example, but one is not citing two. And in this case, you no longer have symmetry in the matrix. So I've just highlighted in red the places where it was a one before and now it's a zero, because in this case, b one is no longer citing b two, so it's a zero. So that's one way to store your network data. What do you think is going to be a problem with this format? Sparsity. Yes. What if you have a really big network of, say, 1,000 nodes? You're going to have 1,000 columns and 1,000 rows, and a lot of those entries are going to be zeros. What's the point of holding in your memory all of those zeros? It's going to be eventually really hard to manipulate and it's going to slow your machine down. So there's another way, which is called an edge list. And basically, that just deletes all that zero information and condenses your data down into two different columns, from and to. So in practice, do this. Don't do the other way unless you want a really simple example and you just want to see what it looks like. Because once you enter your data like this, whatever our package you're working with can easily translate it into a JSON-C matrix if you want to see it. So let's say you're working with documents, like such as the case I'm going to show you. In my case, I was working with PDF documents. And say you're looking for citations of a particular document number. So in this case, the document number I'm looking for is this AR-thingy. Then for character pattern recognition, use regular expressions. Such an innocuous sounding word. Has anyone ever used regular expressions here? Yay! How many people love them? How many people hate them? How many people are like, it depends on the task? Yeah. OK, so this is usually hit or miss. In this case, it worked beautifully and brilliantly. And I'll show you an example. The R package to use is string R. Do not try to use base R for this. I did the whole thing in base R thinking I'm an R purist, and I'm going to use only base R for this and bad idea, bad. So string R makes this so much easier. Thank you to the tightyverse. So here's an example. Let's say you have text that is the project AR-L1281 and AR-L1302 were approved in 2018. Our task, we want to search for the project numbers. So picture, this is one big, huge, long 50-page PDF document. And I just want to pick out those particular citations. And so here's the regular expression. You can have a look. They're not that hard to pick up. The simpler, the better. If they start to get really complicated, then maybe you want to try something not a regular, something other than a regular expression. But there's lots of tutorials online, and I think they start to become kind of like a fun word puzzle. So you have the text like this, and then you just use the string R function, which is string extract all. And then voila, you get the lemurs out. So imagine having a 50-page document, and that's how easy it is to get these little citations. So review, why use string R package? Well, here's a fun crossword for you, and all of the answers are string R. So for example, it is built, it is part of the tidyverse universe. It has its own handy dandy cheat sheet from our studio, which you can just Google and find. It's much better than base R, all kinds of reasons. So remember to use string R for regular expressions. So here's my specific example from the Inter-American Development Bank. The mission is to reduce poverty and inequality in Latin American, the Caribbean through financial aid, poverty and inequality in Latin American, the Caribbean through financial and technical support. The bank has had over 2,000 loans to sovereign governments in the past 10 years, roughly $200 billion. Each of these loans is considered a project, and each of these has its own document. So it's basically a 50-page document that explains the why, what, how are we going to do this of the project? So what is this loan citation network? It is a directed network. It is built from the loan proposals over the past 10 years. Each node represents a loan proposal or a project, and the links represent citations to another project. So you just build it in the exact same way I said. You make a loop through all these different PDF documents, and you pull out the character patterns using regular expressions, and you format them in an edge list. It's that simple. So an example would be, like, this is a project approved recently, and it has this particular paragraph, and it has in blue, that's the citation of a project. And so what's the purpose of this text? Well, it's basically showing us past accomplishments of the bank in that particular sector. It shows us a related project, and it shows the spread of ideas. So this is why it could be useful to the organization. So you go through this code, which isn't that long. It's only like three or 400 lines, and you produce an edge list. And then you make a beautiful graph. I prefer Geffi for the graphing itself. Such as the package iGraph. They also make graphs, but you can do a lot of really cool stuff with Geffi, and it's also free. So this is an example of how the graph is actually made in Geffi. The graph format itself is a non-trivial thing. Basically, you throw your points out in the program, and they're just a random blob. But you can select a different type of visualization. In this case, it's a force-atlas algorithm. Basically, what that does is it clusters nodes together that have a lot of common citations, and it spreads nodes apart away from each other if they don't share a lot of common citations. So it's basically trying to give us a visual of what are all these clusters. And then, of course, you can color it by different things. Right now, it's colored by country, but you can also color it by, say, economic sector and voila, it's a different color. You can filter, you can change the node size. Here, the node size is relative to the size of a loan and dollars. So it's a really fun tool to use as a visual aid. So how do you analyze the graph? I would say there are three steps. One is visually. So you make a visual like in Geffi, if it's a really big graph, I recommend Geffi. You can look for clusters, you can look for isolates, you can filter by different traits. For example, in the IDB, you can filter by country or filter by department. And two, quantitatively. So one very common way of looking at a graph is thinking about what are the central nodes? And there are basically a bunch of formulas for calculating what's the importance of different nodes in the network. For example, in the case of a citation network, you might look at, well, who's doing the most citing, who's being cited the most, and then there's all these other types of centrality measures that once you get into social network analysis, you can learn more about. All of these are available in iGraph or the statnet packages. Tidy Graph was also developed in 2017, and I believe it's more of a wrapper around iGraph. Three, you can model the network statistically, and I'll give you an example of that as well. So visually, in our case, we can color by country, which is what I showed you before, or you can then see and compare the graph if it's colored by economic sector. So when we say economic sector, we mean education, health, transport, some sort of aspect of economic development. So the first thing you can kind of notice here is that it looks visually like the clusters are corresponding to the country more than they correspond to economic sector. So it kind of looks like there's a blue blob over here, which I believe is Brazil, and then there's green over here. So all the colors seem to kind of be overlapping with the clusters. Whereas in the case of economic sector, it doesn't look like the clusters are correlated very much with the colors. So that leads to one possible result, which is maybe the clusters, maybe the citations happen more at the country level, rather than at the economic sector level. So that's a hypothesis we can test with a model. Mathematically, as I mentioned, you can look at the most relevant loans in the network, for example, by country. It turns out at the IDB that Haiti has the most cited loans, although Haiti has a smaller portion of the total number of loans. So Haiti's loans, I think around four or five percent of total approvals, but in fact, they're getting a lot of traction. They're getting a lot of the ideas that are happening in Haiti as far as economic development are being spread across the organization. Brazil, on the other hand, being the biggest country does a lot of citing, but the loans aren't as relevant to a lot of the others in the network. As far as economic sector, climate change, trade and investment, energy and transport are getting a lot of attention. So you can think about this at your organization and think, well, is it, what are some of the different units in my company that I can sort of analyze? Is it department level? Is it a project? Is it a team composition? You can start to think about the different units you can analyze and see how relevant they are in the network. And finally, you can model the network. So in this case, we had a hypothesis, which was shared country increases the probability of a citation between projects. So if I have two arbitrary loans in Argentina, those loans have a higher probability of citing each other than a loan that's Argentina and Brazil. So you would model this statistically by asking, what is the probability of a link between two arbitrary notes? The outcome is one, there's a link or zero, there's no link. What kind of models is for mind you of? I'm sorry? Like a logistic regression, right? Except there's interdependence across the nodes. So that messes up the logistic regression. And then, you know, it's like a really nasty time series or geospatial model where you have to model all this interdependence across all the units. And to avoid that whole problem, you do Monte Carlo marker of chain. And this model does just that. It's called exponential random graph model. It was developed by the statnet team at the University of Washington, and they're always giving workshops about this all over the world, basically. So there's a lot of good literature on this model. And the package in ours called Ergum. So here are the results, sorry to show you numbers. Basically, you can't include too many covariates because the MCMC doesn't like too many. So it's very delicate and it takes a long time to run. But just focus for a second on the final two. So we can see that if two nodes have the same country, then there's a positive effect on the probability of a link and it's significant. In the same way, if two countries have the same division or the same economic sector, then again, there's a positive impact on the probability of a link. But you can see the shared country effect is stronger than the shared division effect. So you interpret it in the same way as a logistic regression. You can calculate the probabilities like so, and then you can compare the probabilities. And it turns out it's four point, around four times more likely that countries with the same, that projects with the same country share a link than projects with the same division. Why would this be interesting to an organization like the IDB? Because it's showing you that maybe there's country level silos of information where projects within the same country are really being, the ideas are being shared very frequently. But when there's a sector across countries, you're not really sharing that information. So as a wrap up, think about how you can create a citation network at your company. You could do so through emails, through documents. Maybe there's some sort of project level information you can use. There's also team composition level. If you need to use pattern, character pattern recognition, use regular expressions. What's the R package? String R. And finally, analyze the network with R using iGrap and statnet, visualize with GEPI. And then you can test hypotheses with statistical models such as an ERGEM. Thank you very much."}, {"Year": 2019, "Speaker": "Brian Wright", "Title": "How R Facilitates Diversity in the Field Data Science", "Abstract": "In this talk, I explore the role of the R community in promoting diversity within data science. I discuss the development of data science programs at universities like UVA and emphasize the importance of not restricting such programs to a single programming language. I argue that the R community, through its association with diverse fields like social sciences, naturally fosters a more inclusive environment. I present data from Kaggle surveys to illustrate the broader gender and racial diversity in R's user base compared to other programming languages like Python. Additionally, I highlight initiatives like the R Foundation Task Force, which aims to increase diversity within the R community. I conclude by encouraging educators and data science practitioners to support diversity efforts actively, emphasizing R's potential to drive positive changes in the data science field as it continues to evolve.", "VideoURL": "https://www.youtube.com/watch?v=0WI35rAUims", "id0": "2019_14", "transcript": "All right. Thanks. Do you guys hear that song? It's pretty funny, right? Yeah. Have you heard that before? All right. Well, this is it here, right? It's a love song about statistical significance. All right. Yeah. All right. So if you want to check that out, feel free to send that. The provost at UVA actually sent us to us. Pretty funny. Okay. All right. So let me get fired up here. All right. So, all right. I'll try to make this quick because clearly I'm between you and the bar, which is always a very dangerous place to be. So I had this philosophy. I, you know, I, so I'll talk about this for a second. So I've been, oh, good. It works. All right. So I've developed a few data science programs. I did one at GW and now I'm down at UVA helping them build programs. And there we have a school of data science at UVA. Do you guys know that? You guys know that? All right. Can I get a round of applause for that? Yeah. I feel like I just asked for a plus for myself. That would just happen. Anyway, so I think it's the first one in the country, but as a result, you know, we have to, I've been thinking about because we're designing a lot of programs, right? And we're saying to ourselves, you know, what do we want to teach? What do we want to be in terms of like the languages we're using? We have these conversations. Okay. And so there has been some discussion internally, like, hey, should we just be one language type of school? And I was just totally against that. I think the answer to this question, by the way, is yes, right? And so I've been having this thought and spirit in my head about how I can communicate that out. So I'm going to try and do that to do you guys because this is a user-friendly group, right? This is a target-friendly group. So I'm going to talk about this, uh, this question, right, that I had up here, right? So really does, does R, and really what I mean by R is not like R the language, but R the community, right? Does it encourage diversity? I think the answer is yes, so let's, let's talk about it. Um, all right. So just so you know, when I think about diversity, I think about our whole bunch of different ways, this might be the least technical talk, right? So the day, sorry about that, but it's the end of the day, so maybe that's okay. Uh, so I think about diversity. I think about it in all sorts of different ways, right? I don't think about it just as, as gender, which currently includes a race, but also like just multicultural background, and then also, um, the domain with which you come from. Okay, so data about this is hard to find. So we're going to try and use, and I have some, don't worry, I did do some analysis and I have like an R mark down, I'm going to show you. Uh, so when I think about it, I think about kind of that way, like how is domain specific that encourages diversity, all right? So that's part of the way that we're going to approach it. So this one talks about, there's all sorts of, there's all sorts of research that supports this idea that diversity is just good for all sorts of ways, including profit and efficiency, and all sorts of, all sorts of important things, right? Not just for the general use of it, all right? So this, so these are just examples of research. This is about how much gender backgrounds make a big difference, right? This one here too, I think. Okay, just so you know, I did my homework. All right, so as a community, we need to be cautious about how we move forward, all right? I think this is important to think about because in a lot of ways, it's a new, it's a new field, right? We feel like we're in it all the time, we feel like it's, you know, it's been around for a while, but it's new and it's still developing, right? We're building schools around it. And so this is kind of a bad graph, it talks about kind of the gender diversity in computer science. So it went up and then all of a sudden it went really down really quickly, all right? And so this is just a highlight of, a highlight of this is from data, and I, I still have these slides just for the record. From the Department of Education, all right? As well. And so these are also some slides here that talk about, again, women in STEM fields, it looks like it's doing, oh, no, this works. No, all right, it looks like we're doing pretty good up there in health related and life sciences, but pretty bad in computer science and engineering, all right? The same here, all right, this is a bit, this is just general diversity as we're talking about in STEM jobs in general. Again, not when we would tend to like to see, so we have to be cautious about data science. So the question is, where is data science? And then I'll get to the art part about how I think art is helping. All right, now this is from general assembly, so I said these aren't, you know, there's no perfect set of information here, all right? But this talks about kind of gender diversity in general, all right? And actually data science up here, this is bad, and that's not, we'd want that to be lower. Even though data analytics is pretty good, which is kind of interesting, right? Okay, and over here, and when we're looking about just kind of race general diversity, kind of data science is right in the middle. All right, okay, so how does our kind of encourage these types of diversity conversations, or is that, is it true that it could do that, all right? So I was looking to explore this, all right, and bring it to this community and talk to you guys about it, because I think it's important. All right, so where do we go to get data, right? We're just going to Kegel, okay. So that's what I did, so I don't know if you are familiar. Do you guys just want to listen to this again? I don't know. All right, you let me know. All right. Look, there we go. And I do think all the presentations today are basically a dictation of my inadequacy for this, so I'm sorry about that. They're all making me feel insecure. Okay. All right, if you, you know, those who can't do teach, right? So here I am. All right, so anyway, so there's these two really big Kegel data sets, all right, from 2017 to 2018, that is a survey. It's like a data science survey. Have you guys seen this survey? Have you seen it? Okay. All right, so, all right, first point of order, all right. It's definitely biased, so it's clearly not a perfect representation of the data science community. It's a representation of people go to the use Kegel, but which maybe is okay, all right. I did two of them. They were pretty big. One was 16,000 responses. Another was 23,000. And so I did a bunch of data myth validation and stacked them on top of each other. There's a really nice package called Stack. I don't know if you guys, do you guys know that package? Stack? Hey, there you go. So I gave you some stack actually just, it takes these two data frames and stacked them right on top of each other. It's just one way to do it if you want to do something outside of D-ply R, or the tidyverse, which is kind of taking over. So I ended up with like 40,000 answers, and I reduced it down to just these kind of these three or four variables here, gender and majority major. All right, and I had to read factor level major. Okay, so I put it into these big buckets. Also, there's a whole bunch of missing, and I did this at like, I think a lot of, I did this at like 12, 30 in the morning last night, so I'm sorry about that. So I didn't take out the missing values, but there's a whole bunch of missing, so I apologize for that. But anyway, so these are the big majors, and then as we kind of move down, we can see the percentages associated with the different languages, or these are counts associated with the languages. All right, so Python way up at the top, right? So Python is very high, we see R in terms of people responding is it's not 14,000, it's 4,600. Okay, but in general, we know that right, that there's a big larger base because it has more of a larger utility, not utility, but you know, for people that are using it more for different purposes. Okay, and so this is a graph across, so these are different language usages. And so again, what I'm thinking about here and just to clarify it is that, is that it's because are in large part, and I think this is true through my experience, it comes from, has a tendency to come from social sciences statistics, people are using it at that space. And those fields in general have a tendency to be more diverse across the board, especially gender wise, all right. So here, as we can see from computer stick here, we see that a large proportion of what's going on inside computer sciences Python related, right? And I think other people did this similar analysis online. And this is a bit hard to tell, but as we kind of move up into social sciences, right, and arts and humanities and in math and stats, the balance between Python and R has a tendency to be a little bit more even. Okay, that's compared to down here and with this is pretty much all Python and then ours represented slightly less. All right, and these are people that come from those domains. So this becomes a little bit more clear when we come down here and we just look at the percentages. Okay. So I was surprised by this one, but again, the data sample there is pretty small. But if we look across social sciences, all right, in terms of our Python uses itself, I mean, it's not perfectly even, but it's pretty close, right? And we see pretty big disparities when we look at computer science, which is above it, right? Okay, or just below in terms of usage here. Okay, now keep that in mind when we refer back to the gender diversity inside computer science. So it looks like here, you know, this is a good thing. All right. All right, does that make sense? All right. So. All right. Okay, so here's some more statistics, right, associated with this. All right. Is that this is commits to GitHub in terms of, and a lot of this information is about gender diversity in general, but I think about it broader than that. All right, so you can see R here, right, and this is information from this article here. You can see that the contributions for female contributes to ours is pretty high. I mean, it's in general. This is all female contributors, but are is by far the highest one here. All right. I mean, it's still small in general, but it is higher. All right. Okay. And so let's talk. And these are things, like I said, you guys are probably familiar with these. You probably know it, right? But I think that when we think about the R community and the reason that it is so good, we need to be sure to support it. And basically I'm making it. I know I'm talking to a, you know, everybody's on that same page here. But, you know, these are points. All right, there's infrastructure. All right. There's a foundation. This comes from that same article, right? And that the R communities are very connected in, particularly the strengths of the R ladies community, I think makes a big difference. And so when I think of all these things, even though I think in general, when we think about like how, what the overall usage of the different languages, okay, you can think of R as kind of like a market enforcer that has a tendency to create good behavior across the community. All right. So it's like these, have you guys seen that? Maybe this is a bad reference, but there's like a documentary on beer wars. Have you guys seen that? And they talk about how the microbrews make a difference. Like they're actually creating good behavior in the larger beer market, right? They have to provide more variety. That might be a terrible analogy. I'm sorry about that. But I do think that because of the power base for R, all right, the strong community, all right, the natural tendency towards diversity, and then the tendency of it to be more in social sciences where there's more diversity in general. It makes it a really big positive market driver for the data science field in general, right? I know you guys are like, you know, we're all there. You're on there with you too. R is definitely my favorite language, but I also think it's important for reasons just outside the usability. I think it has ability to kind of impact good behavior across data science as the field begins to mature. Okay. Does that make sense? Okay. All right. Okay. So there are other cool things to hear. If you don't know about this, the R Foundation Task Force, all right. So for under for women and underrepresented minorities, right? So that's a task force that they're looking to encourage and facilitate ways to encourage more women of color and gender diversity to be in the R community. All right. So that's a very cool thing that's going on. If you don't know. And then here's the last thing. All right. These are different ways that you can potentially continue, and I'm going to give you guys some time here to continue to contribute. You guys already know this already, but if it makes you, you know, be more passionate about the R community, then that's great. All right. So continue to support it. There's an article here. All right. That shows ways that you can get grants. If you're looking for that type of thing to actually encourage diversity in data science. So there's some statistical or some structured ways that you can actually seek funding to help build that. And then, you know, a lot of us here are educators, all right. And we're building programs. And so I'm doing this consciously too. So I think it is important. We're kind of laying the foundation for how we move forward. All right. So we'll be thinking about, and I'd like to get a thing I'm preaching to the choir here, but build our two programs that are at least think about different ways to give options. That's all I got. Eight minutes back. All right. Great. Thanks."}, {"Year": 2019, "Speaker": "BJ Bloom", "Title": "Topic Modeling Consumer Complaints at the Federal Reserve", "Abstract": "In this talk, I discuss using the R package text mining, specifically topic modeling, to analyze consumer complaints data at the Federal Reserve for identifying emerging risks in financial products. The Federal Reserve has access to extensive data from consumer complaints, which provide valuable insights but also present challenges due to their unstructured nature. I highlight the use of natural language processing, particularly latent Dirichlet allocation, to classify and better understand these complaints beyond existing taxonomies. R packages such as TidyText and text mining tools play a crucial role in this analysis, helping to organize and visualize data, run topic models, and assess model quality. Additionally, I emphasize the importance of focusing on the primary research question, engaging with subject matter experts, and adapting technical solutions to meet business needs. Lastly, I stress the significance of effective communication and visualization in applied data science, particularly within government settings.", "VideoURL": "https://www.youtube.com/watch?v=7Ls5YtJgkVE", "id0": "2019_15", "transcript": "All right. Hi, everyone. Good morning. Welcome. I am in the tough position of being the first to go today. I am talking, my talk is a little different I think than some of the other talks where people are kind of showing off a new package or telling about the technique. My mind is more about how you use our to solve a concrete problem, use something weird called topic modeling, advanced math to solve a concrete problem that we're looking into at the Federal Reserve where I work. So the goal here, my team is called Risk and Surveillance when the division of consumer and community affairs. The Federal Reserve looks at bank and their balance sheets, you know, all about the too big to fail. And then there's the monetary policy obviously is even the bigger part, but there is a huge part of the Fed that sort of looks into. In my division, we look at consumer products, so mortgages, home loans, student loans. And we want a way of identifying emerging risks. However, we define risk. Good to start the timer so I know what I am. So one, we have market data, we have data from our examination team, but we also have consumer complaints. And that is a good sort of real time way to understand what people are saying and thinking about their experience with consumer products. So the Federal Reserve has about 20,000 complaints, but the consumer financial protection bureau or the CFPB has over a million complaints. And through our data sharing agreement, we have access to the unredacted narrative text of these complaints. And so that's a good source of information. There are some challenges to that. So when the CFPB was created and stood up in 2011, I'm going to just want to put a few caveats in here about the complaints themselves as a data source. So one, we know they're not a representative sample. It takes some effort to actually go and make a complaint. So this isn't a representative sample of all the consumers, but it does indicate people who are having a problem. So complaints also vary in their salience. Having a problem with someone on customer service on the telephone when you have a late fee that you don't like on your credit card is very different than having your mortgage or closed on when you're in the process of working things out with your bank. Those things happen. There's a just counted as same complaints, and that's more of a qualitative measure that's a little hard to do. Also, and the banks always want us to caveat this in other financial services companies, just because someone complains about something doesn't necessarily mean the bank didn't anything wrong. Overdraft fees, everyone hates overdraft fees. It's always the biggest complaint. Unfortunately, the rules allow banks to charge overdraft fees as long as they follow the rules and regulations that we have around those. And so in our risk analysis, we assess meaningful trends. We're not trying to verify every single complaint as well. And these complaints are only one component of an overall risk assessment of the market in general. So when the CFPB first started, they had to start finding a process for in taking all of this data. And it's a little hard with unstructured data on how to try to come up with a way of classifying these complaints. And so they created a taxonomy, which is what you do. You allow people to identify what they are trying to complain about. It starts with a product. You're talking about your mortgage or your student loan. But within your mortgage, you may be complaining about the closing process, but there's just a lot of things. Within the CFPB data collection process, I'm not necessarily criticizing. I'm just saying there are some limitations to what we have here. So there are about 100,000 credit card complaints. There are 55 issues that you can choose from. That's a lot for people. So around 30 of those have less than 1,000 complaints. 15 have less than 500. So you get some categories that are way too broad, like managing your account for your checking account. Promotional offers are one aspect of managing your account, but you don't really see that in the way that they do this. There are some duplicate or ambiguous categories, like troublemaking payments can both mean I sent my company, my mortgage check-in, and they didn't clear it, and I got a late fee versus. I lost my job, and my wife got sick, and now I can't afford to make a payment. It's a little hard for both the person who created that category and the person trying to classify what their complaint is about to understand which one to distinct. And so fundamentally, the way consumers talk about financial products may differ from the way that regulators think about those products. And then the CFPB didn't realize some of these limitations in 2017 and recategorized all of this, which improved the process going forward, but it makes for some very awkward looking, time series graphs when you're looking at it from my perspective. So the solution here was to use the actual narrative text itself as the data and use a natural language processing approach called topic modeling, or latent deraclay allocation, which is a model, a Bayesian model that given a number of topics were iteratively assigned words to topics and topics to documents as it updates information contained in the documents. So the general process, the conceptual process, how many people are familiar with topic modeling? Yeah, I've given this talk before, and at the Fed, I have to really tone it down on the technical side. I gave it where I put in some math in the last one I gave. Now I'm going to put some archive it in. I'm going to try to add that to my audience here. So the idea is that you have a set of documents, we call your corpus. The LDA fits its best topics, and you get two distributions. You get a distribution of words into topics, and then for each document you have a distribution of probabilities for topic assignments. So this is a conceptual example. Imagine you had five years worth of newspaper articles from the Washington Post with no headlines. You create a topic model with three topics. The output could be election, poll, moderate healthcare rules, topic two, goal, score team, fans rules, topic three, theater, play, drama, fan. So it allows you to identify topics, review articles associated with specific topics, and look at trends in the topics over time. However, it doesn't actually replace the headline of an article. So there's a bit in the way I do this of the actual modeling process and then some manual review. And I told Tommy this is going to be a PR for his package text minor. I just want to give a quick overview of kind of delay of the land for NLP packages in R if you're interested. The biggest challenges in topic modeling are one, understanding the quality of the model, and two, selecting the optimal number of topics, which is K. Your main general NLP packages in R, TM, TidyText, Quantita. UD PIPES is a new one that I haven't really looked into, but it's got some very interesting things. Text reuse, which is very good for identifying similarities. It has a local sensitivity hashing that allows you to do it at scale a little better. TM is, I just highly advise not to use. It's like the okay boomer version of the NLP. It was good at the time. Glad you made in Rhodes. I think David Robinson's here and he and Julia Silgi wrote the TidyText package. That's the workhorse one that I use. It's a very good thing for all around understanding a set of documents, putting them into an organized form, visualizing them, and transforming them into things that go into models. A lot of these other ones like text-de-vec is sort of the word glove or word-de-vec, word embedding version, the Python has their version of this. Snowball C is for word stemming text rank as sort of Google's page rank in text. Anyway, within topic modeling, that's the more technical specialized thing. The two old ones that have been around for a while are LDA and topic models. I've used LDA before. It's fine. Topic models is a little special snowflake because you also have to re-index everything to zero because our index is at one. You have to understand how to work with lists rather than data frames. It's a little tricky to use. The two that I am still going a little bit back and forth are on our STM structural topic modeling and text minor, which is the one I use. STM is an interesting one that I'm happy that I think people should look into a little more. The reason they call it structural topic modeling is it allows you to incorporate metadata into the topic model. One thing you have to do when you're doing topic modeling is have some sort of assumption about how topics have distributed across your corpus. This allows you to say if I had issues, which is how the Steve B categorizes some of these complaints, I can say, skew it slightly more toward ones with more issues. The problem for me is they have some diagnostics. I've read this paper a few times. There are four diagnostics. I don't understand them that well. I should maybe go back and read it some more, but the text minor package has two that I think are pretty more intuitive. One is R squared, which is a measure of goodness of fit. It's similar to your traditional regression R squared value. Tommy has a paper that kind of explains this. I take it on faith a little bit with him on this. It's actually going to get published and I can cite it. The problem ballistic coherence is a measure of the quality of the topics. There's also semantic coherence, which is what STM and a lot of the other ones use. I find it a little hard to explain what semantic coherence is. Probabilistic coherence is basically saying if you have, it looks at the words within that topic and sees how well they work together as a group. How would I put this? If we go back to this one and you see topic two is the goal score team fans rules. The probability of goal and minus the probability of score given that goal is in that document, that's a measure of probabilistic coherence. If this were a topic of just the sports section, it wouldn't be good because goal and score would likely be found in other topics within that model. It's a little hard to explain. I'm not probably great at explaining it, but it is a measure that kind of tells you how well your topic model is doing and what you just measure should be used for. Here's the process I go through. I use tiny text for filtering and building the document term matrix filter out. This first part is to get the basic data frame that has a count of words per document and also a count of the entire. The next step is to get a word frequency count to edit that down. In this case for the specific model, I took words that appeared less than 200 times and removed those and joined it back to the original one and cast it as a data document term matrix. This is nice in that you can run multiple topic models with different values of K in parallel. Tommy will never be taken to school for not making his function names very obvious that they do, even if they're very long. TM parallel apply. You can specify how many CPUs you have on your machine to speed this up. This is very slow. If anyone's using this at home, depending on how big your corpus is, it can take a long time to run. It does. I use per and GG plot to do that. I'm not a computer scientist, so any time I get something in per to actually work, it's kind of a miracle, so I'm very proud of it. That allows me to then look at which value K would seem to be the best one. I can pick the topic model that then when you see the probabilistic coherence, our square just seems to keep going up on most of these models that I do. But then I just take a step back because there are some actual practical business concerns that I have to take into account. I build 10 separate models, one for each of these financial products. The K is limited at most to 40 topics. It's just hard to explain, summarize, and create trend lines for more than that, for the people that I'm working with. Also, topic models do output a distribution of multiple topics. So, obviously, if you're writing a document, you're not always just writing about one thing. But for the purposes of analysis, to avoid double counting and highlight actual trends, I just take the first most probable topic for each document. And then it does require some manner over you to get a better understanding of the topics. So, this is the result of, again, for mortgages. A similar trend line, things are going down, but you do get, like I see, these top terms are just words. Compared to the previous time series, it's a better measure. And then with some manual review, you can actually get phrases that fill in what those terms mean. So, in general, these results are incorporated into the broader risk framework. For our team, we have risk reports that we use to just understand emerging risks in these consumer financial products, even if the Fed itself does not oversee these specific products or these specific institutions, we want a just to know what's going on that our stakeholders are the division director and sometimes the board of governors. They want to be aware of what's going on in the marketplace. There are things that we can affect through rulemaking and policy like that. But this is more of just general how to understand what's going on. What issues consumers are facing, a lot of the risk metrics that the bank looks at are for bank balance sheets. We're looking at it from the consumer experience. What are areas where people are having trouble with their credit cards or their student loan or their mortgage? And then I built an R-Shiny dashboard that helps users explore this themselves. I'm not allowed to show that because anytime a company name shows up on screen, people freak out. Almost all of this is publicly available information. I do want to say that to start with the CFPB publishes most of this on the website. The only thing that I get that you don't get is the actual private narratives. So, you can opt in to sharing your complaint publicly. 30% of people do that. They redact out PII. They have a good interesting method for doing that. One of my former co-workers actually worked on that because I used to work in the CFPB. But I put it back in the dashboard. So, all of this information, you can go in there and make your own dashboards and look at what banks are getting complained about more often than not. But I just am not allowed to really show that. So, I do get a few parting thoughts instead. It's just my general advice and takeaways from doing applied data science in the government. I haven't worked as a data scientist in the private sector. But first of all, focus on the central problem and research question, not the technique. Everyone often are being hired for AI or some deep learning. You have a full toolkit. Use the best tool to fit the problem, not the fanciest one. Engage it with subject matter experts throughout the process, both from concept formation to validation. There's no use in building a cool model that works really well if no one's going to use it. A lot of what I do is sort of what I would characterize as operations type research. This is more policy analysis now, but at IRS, where I first worked, we would have examination teams. And you say, well, why don't we try to do a better job of picking which organization is to audit? And they say, well, I've been doing this 30 years, trust me, I know, like I'm in tune with whatever. And there's intuition and you have to respect that because they actually do know that and you have to work with them. And the validation isn't just technical validation. It's does this have a useful impact on what I'm doing and make my job easier? And one way to do this is to find low hanging fruit to demonstrate value. A lot of times there's data available and no one's ever looked at it. You do a quick visualization and say, hey, here's what you thought was going on. Here's what's actually going on. Maybe we should do something about that. My big one that I tell people who haven't worked in a lot in data science, applied data science, especially in the government, is that change management is far harder than any data science technique. You can build a great TensorFlow model, but getting people, again, to get on board with that evidence that you showed I both agree with. And that will now change the way I do my work. Almost never happens. The best model I saw was at the IRS I worked on a team that had brought in by the commissioner, some outside consultants. They were officially part of the government, but they had this mandate from the commissioner. And then that allowed them to get all of the stakeholders, the high level people that we can often not reach onto these calls every two weeks. It would just be, hey, here's the problem. Here's all the data that shows the problem. And we're going to, in two weeks, we're going to look into a little bit more. We've run some experiments. This was on identity theft problem, which was a big problem at the IRS. It's been improved. So you get these updates every two weeks. You allow everyone who has an actual stake in what's going on to have input into the process. And then if you six months down the line, you say, well, now we have to retrain all of our call service representatives. Everyone's like, well, of course we do. Whereas if you do all that analysis yourself and then drop that on someone, they're just going to roll their eyes and move on. And sometimes, like I was saying, sometimes business considerations override the optimal technical solution. And that's okay. Getting 80% of the way there is, and making allies and getting work in the future and proving that you have some value to add is better than the 100% solution that never gets implemented. So I don't know if this is just in the government. I think it's not. And finally, communication, visualization storytelling is crucial. This is my plug for liberal arts education still. I have a liberal arts education. I don't think everyone has to be like me. I just think between that and the social science background, like framing a question properly, but then also being able to communicate. Both with visualizations, but also writing clearly and effectively is as important as the technical skills, which can be learned a little easier than some of these softer skills. And the higher you go up, the more important the soft skills are. So that's my, the Twitter is personal account, GitHub. I have hardly anything on there because everything is enterprise stuff that can't be shared. So I do have an example of what I walk through here on a unknown data set. And this is kind of what Tommy and I are like when we run into each other at the conference. All right. Thank you."}, {"Year": 2019, "Speaker": "Tatyana Tsvetovat", "Title": "R is Not Just Basic Stats: Think Spatially!", "Abstract": "R is one of the most well-known open source programming languages for statistical analysis. But is it just for statistics? Is it really its only usage, to look at descriptive and inferential stats? In my talk, I will go over the pros and cons of using R when working with spatio-temporal data. I will go over some of my favorite spatial R packages and the add-ons similar to ArcGIS extensions. Finally, I will discuss some of the the best spatial tutorials and books for those interested to take R to the next level.", "VideoURL": "https://www.youtube.com/watch?v=y3rqCssxNwk", "id0": "2019_16", "transcript": "My name is Tatiana Sveldevat. Nobody can pronounce my last name. It's Curtis, so my husband. I went from Fisher to Sveldevat, so it's, yeah, trust me, it's true love. I am a data scientist in Busallen Hamilton. Yes, another capital, Belbury Bandit. And my journey today science actually was very unusual and in a million years I would never expect it to be a coder. I was a political scientist by training, working as a contractor, working with a lot of data, and actually taking that data to coders to make some sense out of it. And frankly, I got tired of that because they could never understand my data. They couldn't understand why cluster, one cluster was important than the other, what the outliers were, why something was important. So I said to myself, well, why am I talking to you guys? I'll just go out and code. In comes Mark Weisman with his data community in DC with his fantastic newsletter and a whole bunch of local meetups. So I started to go to PILADES, so Jackie K. Zoll, Katie Connie Haim. I checked out some groups that did R and I figured why don't I just start with both. As long as I'm asked the basics, I should be good. Well done, we all know what R is, but by now if you don't, sorry, I support several federal customers, let's leave it at that, and a lot of my customers work with spatial data. How did I start doing spatial R? So when I first started, I never worked with spatial data in R or Python in my life. I always stuck with something being more traditional, additional statistics, the works. Until one time, I had an urgent project where we had to use ArcPro. Are you guys familiar with ArcPro? Jias tools? Yeah, we all love them. And of course, the worst time ever, on a strict deadline, it failed. It failed to launch. It wouldn't start, it would crash. And I did not have a bit of data set. It did not have a database as a shape. I couldn't even open the data. So given two days, I had to find another way of how can I even look at the data what's inside. So Google is your help. You Google will say, okay, spatial analysis and income. A lot of these awesome tutorials from Ramen Lovelace, from ZFrost, where they talk everything about R. Oh my god, I can actually look at my data. So that's how I came about starting spatial with R. And created a panel, quite a niche with my team, who was like, oh my god, you can do all this stuff with something that we always thought was traditional statistical analysis. Let's look at correlation and recression. We can actually use geo databases and shape files with that. Awesome. There are so many fantastic packages in R. I love them. Every time a new package comes out or there is an update, it's literally the highlight of my day. Well, that and my child's performance is cool. True. True. I get daily phone calls, guys. Not his father, the Python, the R person gets the daily phone calls. Yeah. So what I love about R is, first of all, I love the interface. I'm an R studio devotee. I'm not saying this because they're a sponsor. I really do R studio. I have apparently turned a lot of people on my team to love R studio as much as I do and not do the, we'll just do the old school command line, leave us alone. And of course, Jupiter, if you, by the way, guys, if you have never used Jupiter, Jupiter does have the R kernel. It's pretty interactive. You can upload your code. The graphics are great. They're huge. So for someone who is, as me, as sighted as me, it's fantastic. I love the model environment. I love how you can do all kinds of spatial to put analysis. The only just some of my favorite packages, I honestly can do the whole presentation just talking about which packages I liked and don't like. But these are the ones I use all the time. RGTL, I would be lost without it. It's, I can't do my work without RGTL. If RGTL is not updated in my system, I am in trouble. If you're ever working with geometries, RGS is your next big thing. It's what I love about both these packages. If you run them together, it's literally one line of code. You have all your layers. You have everything. If you're working with statistics, because, hey, R is after all, this was great. It was founded as a statistical tool. SP and G stat and SPAT stat are fantastic. In lab, I actually recently discovered a package and it was awesome. I had to do spatial regression using some spatial temporal data. It did the work famously. It was so clean. Lots of great examples. Okay. Let's be honest. All programming languages, there are some good stuff about them. There are some not so good about them. These are what I call a Tatiana's list of pros and cons. You can use R or Python or whatever language you want for your analysis. Here's what I found it to be great with. I love the CLI. You can do the workflow. You can see actually what you're doing and how you can code it up and how you can work with algorithms. The graphics. I've heard some people before, especially the critics, saying, the graphics are boring. They're not. They get the job done. In the end, let's be honest, we all post this on PowerPoints to our customers, especially federal clients, because they are too afraid to look at our Jupyter Notebooks and they don't want to mess them up. The functions of spatial packages. That is one of my favorite things. If I were to code in Jupyenders, anybody knows what Jupyenders or PySAL? These are the top Jupyter packages in Python. They deal with data manipulation. They deal with spatial processing. They're great. They get the job done. If I want to do data manipulations in Python or spatial data, it will maybe do a lot of coding. I'm a lazy person. I don't like to write a lot of code. I don't like what I can just do with just a few lines of code and get the job done. You have a lot of functions. It doesn't just for spatial. I do a lot of time series. I love doing time series in R. I actually specifically request my customers, can I please do this in R? Will you mind? Yes, multiple functions writing packages. But it is still not perfect. There is no such thing as a perfect language. And the cons. You have a lot of libraries in R. I mean a lot. And sometimes in order to get the job done, you will need to upload. I take one time I had up to 30 libraries just to get the job done to do all of my analysis. And it's a lot. And it's not always updated with the most recent version of R. So if you try to run it, you will get, oh, yeah, that function has deprecated. But this doesn't work. So you got to fix this. Now, I just installed that to this. Oh, I don't have time. You guys all know about the data sets. A lot of the spatial packages will have preloaded data sets that you can practice with. What I found the hard way was you don't have preloaded maps of every single country. All of the European countries are there. African countries are there. If your customer wants you to look at drought in the Philippines, you kind of screwed. So what I found is you have to find a map only from someone else, either a digital boundary library in your agency or uploaded from somewhere else, which requires extra code. I know some people are going to kill me for this. I'm going to like it. How dare you criticize pre-decapabilities of R. Not always there. I love working with network analysis and network 3D and Neo4j. They're great. But if I were to do an interactive presentation for my customers, I would go with linking my R or Python to Tableau or to Plotly. Tableau, I know it's not the perfect way because you have to get a license, but Plotly is free. Especially if you have a server and you can get the job done. If you're trying to zoom in for into your maps, Leaflet, I found it to be sometimes kind of spotty. So it's my personal experience. I know people swear by Leaflet. Great, but that's what I had. Here's just some of the type of spatial analysis I have done. Anything from, hey, we want to look at where different cell towers are located within the country. And the insurgent area versus in the government controlled area. If you are trying to learn spatial R and if you're looking for really great open source data to play with, there are, I go to my four top sources whenever I'm learning your package. I go to Aclid, which is the armed conflict location events database. It's fantastic. They have a lot of notebooks you can play with. And there's also Gdelt, which is updated every 24 hours. And you can pull data by Google query. Gdelt is your social media. It's your media. And there's several tutorials, by the way, online. You can look at it, especially if you're trying to do network analysis or data manipulation in the data. Because the data can be a little bit complicated. I also like some of, thank God, some of our federal agencies still keep the open source data. So National Parks, great data to play with. And I recently found out that some of the DHS data has appeared on the I-Vables. So if you're looking for infrastructure, you can pull that too. So here are just some of the most basic visualizations that I have done. This is looking at cell towers locations within the US and around the world. Here's my favorite data set. I love working with Aclid. And I have also turned some of my customers to work with Aclid whenever they're learning and looking at conflicts and refugee movements. This is actually looking at fatalities in four countries around the Great Lakes region in Africa. So from 2000 all the way to 2016. And what I love about that data is always updated. So if you have additional analysis from other sources, this is a great supplement. They love seeing that. Time series. Oh my God. I'll highlight on my day. I cannot imagine my data science life without doing time series. In fact, that's one of the things they bring you for. Like, can you do a time series analysis? We don't know what it is, but can you just do it? So if there's spatial temporal data, I just go, yeah, let's see how these guys are moving. The bad guys, the good guys, where they are at. And can we most importantly predict the future? R has by far some of the best time series packages. I'm not just talking about zoo, but XTS, it's fantastic. And it works great with spatial data and already puts that function to clean the data. Here is a data set. This is, I tried to replicate my work for the customer using vehicle data where they're moving with a particular time period from 2012 all the way to 2016. Nice and clean. And spatial clustering. Before I became a data science, my clustering was always involved with people. So if you do, say if you're doing terrorist networks, for example, they have data available online, but this time I was allowed to add the spatial aspect and it was great. Where are these guys moving? Where are these insurgent groups moving? So this is also accurate data. And I'm looking at three different insurgent groups in support of who split the groups for Boko Haram. I wanted to see where the clusters are. Are they small? Are they big? Which was like, okay, they pretty much did all the same. There is so much you can do when it comes to our and spatial and you can take it to the whole next level. And if someone tells you, well, Python can do it better, maybe you can. For me, if I'm doing statistics of spatial data, I just ask what are. And that's my preference. They could leave it. And I'm done. I'm giving you guys back five minutes. Here is my, can I do my contact information? No, really. Yes. Seriously, you can type. It's just a shift to data scientists. But my email address is there. I would love for questions. Email me, call me. I'm DC based. If you're advertising this corner, I would love to come out. I need an excuse to stick to someone as I don't, you cannot go train for your Ironman. I'm going out to meet with some smart people. Thank you."}, {"Year": 2019, "Speaker": "David Robinson", "Title": "Ten Tremendous Tricks in the Tidyverse", "Abstract": "In this talk, I share my experience and insights gained from creating 36 Tidy Tuesday screencasts over the past year, where I recorded myself analyzing weekly data sets from the Tidy Tuesday project. Through these screencasts, I discovered that we all have unique, valuable R and Tidyverse tricks that can enhance our workflows. In this presentation, I introduce ten Tidyverse tricks, focusing on functions like `count`, `add_count`, `summarize` with list columns, `FCT` functions for factors, and visualization techniques such as using log scales and `geom_col`. Additionally, I highlight useful `tidyr` functions like `crossing`, `separate`, and `extract` for data manipulation. These tricks are designed to streamline data analysis, promote fluency in using the Tidyverse, and encourage the exploration of new methodologies.", "VideoURL": "https://www.youtube.com/watch?v=NDHSBUN_rVU", "id0": "2019_17", "transcript": "Hi. Thanks so much. I'm really excited to be here. This is the sixth time I've spoken for Jared at conferences and meetups and it gets more fun every time. It's my first time in DCRs. I'm really excited to be here. So my talk starts with the experience of who's heard of the Tidy Tuesday project. Isn't it fantastic? It's a project run by the R for Data Science online learning community and every week they release a data set to the data science community and say do anything you can. You can analyze it, make graphs, do analyses and what inspired me to do is start opening the data set each week and then recording myself analyzing it. So I've ended up doing 36 Tidy Tuesday screencasts in the last year. So I spent an hour with each data set, dive in, learn some things about it and post the recording on YouTube. So it's been really fun but here's an example where I'm sort of like how I'm exploring it. Trying out building a network graph I think. Yeah. And then I start working from there. And it's been really fun to learn about myself and learn about some of the how I feel like we're working with new data and really practicing that skill. But it's also been a blast to hear from people that say what they learned from the screencast because it's not always what I expect. One thing I found out is that all of us are walking around with tricks, things we know how to do with R or often in my case with the Tidyverse that we might not think of as an obscure trick but that is a critical part of our workflow and might help other people. So with that I decided I'd go kind of a clickbait route and say here are my 10 tricks that only a Tidyverse user will appreciate. Number six will blow your mind. And really just walk through rapid fire series of things that you may or may not know from that you can do in the Tidyverse. Each of the times I'm actually really interested is the first time I'm giving this talk. I'm really interested in learning what does everybody already know, what might be extremely obscure, what's somewhere in between. So I'm going to ask about each trick who has already heard about it. Sound good? So these are 10 tricks that are going to be combined into kind of themes. First, a couple tricks I found around counting and summarizing data. Second, one about G2P2 visualization and four cats for categorical data. And third, three tricks from TidyR that are some of the slightly less well-known functions from the package. So first I'm going to start with my favorite function, count. Whoever has used count in DPLYR before. Fantastic. So Hadley Wickham has said that most of data science is counting and sometimes dividing. And I find you can get a really long way just with counting data. So in fact, I did a little analysis of all of the screencasts that the aforementioned screencasts that have done and actually found that count was the fourth most used function in them after mutate, filter, and group by. So I really do love counting. What would I do with count? This was one of the, a lot of the examples I'm going to show come from my Tidy Tuesday screencast just because it's a lot of code I have lying around where I've used these tricks. This was one from the first screencast I did where I counted the number of times each, number of graduates of each major. So in this case, this was a table that, that divided college majors down into categories and showed here's the, there are 10 majors in agriculture and natural resources, eight in the arts. So some of you may have used count this way. Did you know it has three additional arguments, sort, weight, and name? Sort, let's just say, well, we're going to, we're going to count the number of times each category appears, but then we're going to sort the result in descending order to find the largest first. Weight means that instead of finding the number of observations, I'm going to sum up a column. In this case, I say the weight is total, which means I'll sum up the total number of graduates column and find the number of graduates in each. And third, the result doesn't have to be named N, it can be named graduates. So what you can give a name equals graduates, I think that's new as a D-py-R 0.8.0, fairly recent. So really in this trick, you get three tricks for the price of one. Who's used sort and count before? Who's used weight? Who's used name? It's awesome. So that was the first trick. The second trick is another way to use a group by or count. We've seen that you can count one of the existing columns. For example, here was a data set of bridges from Maryland, Maryland, and what year they were built. So I had this count point say how many times was each built. But I might not want to count the number of how many bridges were built in each year. But I might not want to count the by year, I might want to count it by decade. So in the same process of the count, I can create a new column, kind of like a mutate then account. So the, and then it ends up with a data set that's actually aggregated by decade, 1910, 1920, instead of being aggregated by year. So in fact, there's a trick 10 times year that's truncated division 10. It's a way you can aggregate years in a decade, which offers a bonus trick within this one. Who's already knew you could create some new variables in a group by or count? It's actually more obscure than I expected, but it's a really, that's a really fun trick. It saves you an extra step of creating a variable. So in a graph like this, I would, I did a set like this, I meant a pickiness, doing a geom line, and so the number of bridges built in each decade. Third is add count. Who's ever used the add count function in D-ply R? This one was added, I think about three years ago, it's a little bit more, it is a little bit more obscure. Who has ever had to do this? A group by mutate than ungroup, where all you're adding in is n equals n. It's really helpful, and we're going to see a couple of cases where it's helpful, this is a three step process where I'd say, where I want to add this n column and say here are the four, in these four observations, the total for that group is four, in the two for B, it's two, and this one for C, it's one, and that can be done in one step of add count columns, so taking three steps and putting them into one. Where I find this really useful is to combine it with a filter. So if I say add count and then filter, so this was a data set of space launches for particular US space vehicles, and I want this, and because I said add count to a particular variable, then do a filter on that n, I was able to visualize only the vehicles that had at least 20 launches. So add count and filter is a combination I like to play with here. A third way that I really like to add gate data is with summarize, is creating a list column. This is a relatively recent behavior of DPI, I'm not sure if it was introduced last year or the year before, but you can use summarize to create a list column. So I'm going to pull out a tidy Tuesday data set of New York City restaurants, and this was actually of inspections of them, and showing what, for this restaurant, how many times was it inspected and what was the average score, where a higher score means more violations. Now I could find the average inspection score with a group by and a summarize. So a group by, summarize mean average score, and I've added a column called avg score with that value. But did you know that if you put a list as the output in that summarize, you don't end up with a value, you end up with a list column. So this field in the result is actually a list, and every one of these objects is a t test. So we can put kind of anything into that summarize equals list, and anything we put in there is going to be turned into an object in this list column. So why would we want to create a table of many objects? That would cover a much larger scope than what I have time to talk about today. But here's one example is we could take those, use the broom package to tidy them, and then visualize multiple models, just here, it shows what types of restaurants have the most violations. Turns out Jew smoothies and cafes, the fewest Indian, Latin, Caribbean, Spanish, Thai have some of the most, and you get individual confidence intervals for each of them. So that comes from visualizing many models. There's a chapter of this in RFA, the data science, chapter 25, that if you're interested in visualizing multiple models, is really worth checking out. Who has created list columns with summarize before? Check number five is a story of a couple of tricks in one that combined to form something I do a lot, especially in these screencasts. It's actually my favorite plot. So when I ask someone what their favorite plot is, some people say something like, oh, I really like a really complicated plot. This is Menard's the polyonic war. And some people could praise a plot like this. But the truth is, I really like, I really like a plot like this, a bar plot that's sorted. So this actually, in fact, I showed this plot earlier where I said what functions in DPLIAR do I use the most? And it's a combination of four steps. Account an FCT reorder, a reorder who has used FCT reorder or built in reorder before. You have so amazing for visualization. It turns this column into an ordered factor, which allows you to visualize it in this kind of ascending way. I throw in a geom call, who has used geom call as opposed to geom bar? So if you've used geom bar before, you might have to say stat equals identity. More recently, you can say geom call instead of like geom column for that. And who has used co-word flip? Code flip is great because I think it's so much easier to read. You can fit some more text when you have a horizontal bar plot. So that combination of tricks forms a graph that I make a lot. In fact, these are just five examples from some of the screencasts that I've done of here's a bar plot. Here's the frequency of certain categories. You can stack them. You can do really a lot with this graph. Who has used FCT lump before? Continuing our tour of forecasts. So what's great about lump is that you might have a lot of levels and you want to combine some of them into other. So this is from one of the RStudio, this is one from one of RStudio's fantastic cheat sheets for factors with forecasts. And they show if you have a vector of like a couple of levels, you can FCT lump it and take all the less common levels and combine them into other. So I'll show an example from a screencast. I'm going to stand by this one. I have a little trouble with that clicker. So this was a data set of horror movies that included their reviews as well as their rating, like PG, TV14, or PG13. And if I just did a box plot, it would look like this. It would look like it would be kind of out of order. There are a few levels. There's only one PG rated horror movie in this data set. And there'd be these couple of levels and it wouldn't be really useful. So but with two steps, FCT lump, and then as we saw in the last trick, FCT reorder, we end up with a much more interpretable graph that combines all the rare levels into other. So I really like FCT lump for box plots, bar plots, counts, things like that. Who's used log scales before? So this is a tidy verse trick in the sense that it is a Ggplot2 function. But it's really also a data science trick that I think is really important and sometimes underappreciated. So I've actually suddenly said that if I had to give important advice to new data scientists, I might start with something like, well, you must start with a specific scientific question. Then I say, nope, it's actually the number one advice is try putting your axis on a log scale. Why is it important to put an axon on a log scale? I'm going to show a couple of data sets where it's really pivotal. One is this sum is this is a was a wine data set where we said what is the distribution of prices of wine? And if we looked at it on a regular scale, it would look, it would see everything interesting as crammed into one tiny part of the graph. Put it on a logarithmic scale, one on which every fixed amount of space on the x-axis represents a multiplication of the value from 10, 100, 1000, and suddenly becomes kind of more of a bell curve shape. This is a distribution statistician called log normal, meaning the log of the data is approximately normal. Why is it important for it to be normal? Well, what if we were trying to predict a wine rating from the price of wine? If we made a scatter plot like that, we said here's the price, here's how experts rated this wine, everything will get crammed together, and the prediction wouldn't be very effective. In fact, it would quickly go off past 100. Whereas if we do a prediction of the wine rating based on the price of wine, we have what amounts closer to a correlation to a linear correlation and a better prediction. And indeed, in the screencast I found it was considerably more accurate to predict wine rating from the log of price rather than from the price. I want you to start visualizing it with log scales, you can see that. So much real real world data is log normal. I sometimes suspect a lot more real world data is log normal than normally distributed. Examples are GDP per capita across countries. This was a visualization from one on plastic waste, and we saw that on the x-axis is log GDP per capita of a country, y-axis is log CO2 emissions, and if you had them on regular scales, everything would have been crammed into the lower left. Population across cities or countries, income across people is log normally distributed, revenue across companies. Usually these days if I open a data set and I almost expect a numeric column to be unless it's something artificial like a test score, I often expect it to be log normally distributed and throw some scale x log 10 onto the graph. Moving on to a couple of TIDR functions, who has used the crossing function from TIDR? Crossing is amazing and what it does is fairly simple. What it does is it takes vectors and finds every, creates a table with every combination of them. Here there's A, which is one to three, then a couple other vectors, and it turns it into all 24 combinations of A, B, and C, the inputs. Has anyone used expand.grid? Expand.grid is the basic, our equivalent. Crossing is a little more convenient for working with other TIDR functions because among other advantages it returns a table and doesn't convert strings to factors. It's really hard to explain in two minutes what crossing is so useful for. It's really worth a talk or a blog post on its own, but some of the things I would do with it are try every combination of two numbers so that you can visualize, Ggplot, a mathematical curve. When it's used alongside augment from broom, it's really good for showing how prediction comes from the input parameters. In this case, this shows that over time, the probability of getting a season two has become more dependent on audience opinion. And there's a really auspicious area of TIDI simulation where you can use cross and create all combinations of possible inputs and end up doing some TIDI simulation on top of those. All you can do for now is advertise the existence of the function. It's been really powerful. It ends with two more TIDR tricks for cleaning your data. Who has you separate from TIDR? This looks like about half the room. And this is from another one of our studio's cheat sheets. Separate takes a column that like something like a ratio and splits it out based on a regular expression or based on a string. So it's hard to appreciate how important this is until you've worked with some data where a lot of stuff is crammed into one column. This was a data set I had to work with during my graduate work. And that all everything to the right of name is all one column split up by pipes with lots of really interesting information. Back when I was in graduate school, I actually went back to my code and found this awful line for extracting one value out of there, the one that I really needed. Today what I would do is I would say separate, I would tell it all the columns I want to be divided into and give it a regular expression, a string pattern that would say here's how it's split it and get it in five separate columns. So once you've worked with data in the wild that has these combined together, separate is a really handy trick. The last trick I'll introduce is extract. Who has used extract from Tidyard before? Extract is fantastic when you have some data that's not necessarily separated by a delimiter, but kind of trapped within some pattern of a column. So here these were a data set of Bob Ross episodes and we had these codes, these season and episode. So what I would do here is extract from that episode column, the two, I'd say this is a regular expression for S something, E something and from that I get two columns of season and episode number. It's even really helpful that by saying convert equals true, I get those columns as integers, it actually figured out that those were integer columns and converted them so that I wouldn't have to do that myself. So steps like this can take two or three or four steps of data cleaning, turn them into one. So that concludes my 10 Tidyverse tricks. Now, why do I like to focus so much on trick? What tricks? What is so great about them is the Tidyverse nothing more than a bag of tricks. My take is that the Tidyverse is greater than the sum of its parts. I think that when you combine all of these ways of doing something, make every one of them a little bit easier and ensure that they all work together fluently, these benefits, they're not just a little bit of convenience here, a little bit of convenience here, they really accumulate and keep you in your data analysis flow. And that's one of the reasons that I've really liked learning tricks with people when I pair a program with them, when I talk to them at a conference like this, when I read other people's code and I've really loved sharing these 10 tricks with you today. Thank you."}, {"Year": 2019, "Speaker": "Angela Li", "Title": "The Care and Feeding of Spatial Data", "Abstract": "In this talk, I share insights from my experience leading an R-spatial workshop at the University of Chicago, where I guide researchers on the use of spatial data. I address common questions about acquiring and understanding spatial data, such as what spatial data is and how to work with shape files. My presentation breaks down the process of getting started with spatial analysis in R, covering where to obtain spatial data, what tools and packages like SF and T-map to use, and how to perform spatial analysis. I emphasize the importance of geocoding and selecting appropriate spatial data formats. Additionally, I discuss transitioning between static and interactive maps and explore advanced spatial analysis concepts, such as spatial autocorrelation and patterns detection. I conclude with thoughts on community engagement and the value of open-source collaboration in the spatial data science community.", "VideoURL": "https://www.youtube.com/watch?v=sgwVAuuyG6A", "id0": "2019_18", "transcript": "So, essentially, I work at the University of Chicago. I'm originally from Charlotte, to Virginia, so I'm always happy to become back to the East Coast. I lead a weekly R-spatial workshop there, so I teach researchers how to use spatial data. And this talk is essentially cribbed from that. Every time I teach that, every week I teach that, researchers come up to me and they're like, yeah, you showed me how to make these cool maps and do these analyses in R. How do I get the spatial data? What is spatial data? What's a shape file? Can you help me with that? And so, I decided to sort of take a step back. And instead of telling you about my cool spatial autocorrelation and regressions and things that I love, I wanted to take a step back and think about the step before that. So, how do you get spatial data? What is spatial data? How many people here have worked with spatial data? Okay, so a lot of people have. So maybe this is maybe too elementary for you. How many people feel like they have a good grasp of what spatial data is? Yeah, so hopefully today's talk will clarify a little bit of that. Talk a bit through some of the tips that I find helpful. Maybe walk through a few tools that I use as well. This is good, I just get like extra minutes to talk at you. Great, awesome. So, spatial data. A little bit about me, I wanted to talk a bit about the communities I'm a part of. The only reason I'm on this stage is because of the amazingness of our community. But if you want to be on this stage, you should be on it because you're part of our community and you're valued and you're awesome. So, let's see, about me, this is me, teaching my workshop at the Center for Spatial Data Science at the University of Chicago. I have to do research, have like help people with their own research. Last year I was just helping people with their own research and that drove me a little bit crazy. So I was like, put me on a project. So you can see up here, I'm teaching people how to make a core plus map. I'm helping people debug their spatial installations and I believe back here I'm presenting a little bit of my own research. I'm also part of the Our Ladies community. I run the Our Lady Chicago group. If you're in the area, you should go to the Our Lady's DC group which is run by Kelly and Gwyn back in the corner. Or if you're in Charlottesville or other parts, I think Sam is outside so she runs the Our Lady Charlottesville group. I have found this community to be super welcoming and just really helping me along in my learning journey. We do hackathons together, we have talks. We talk about cool things we've learned in our together. And for me it's been a really good way to force myself to learn things by getting up in front of a stage and practicing talking about shiny or maps or the tiniest. Okay. Carpentries instructor, I recently joined this community. So the Carpentries is a community that encourages researchers to become better at their computational ability. The idea being that when you're in grad school, people just sort of expect you to know R all of a sudden. Like I don't know when that happened. Like they sort of, they're like, yeah, you know Shell, you know R, you know Python, et cetera. Go to your analysis. And it's like, where do people learn that? So part of the Carpentries community is teaching people how to do that. So if you're interested, I have Carpentries stickers as well. I don't know if there are any Carpentries instructors in here. Is anyone in the Carpentries instructor? Okay. There are like a few in here, maybe one. You can also, by the way, if you are at an academic or nonprofit or I think probably even governmental institution, you can request Carpentries instructors to come do workshops for you at your institution. And we're volunteers. So anyway, sorry. And there are a few other things. When I'm not thinking about spatial data or communities in open source, I play Ultimate Frisbee, big fan of that. I don't know if anyone here plays Ultimate Frisbee. And I love biking as well. So this is me biking around. I think in Mexico City. So, okay. Previously, I gave a talk last year. I naively thought this would be the last time I would ever give a spatial talk at DCR. So I called it Bitly DC Spatial. But we're back for more. Bitly DC Spatial 2019. So I guess I'll just keep naming it the year or someone else can take on the spatial talk for next year. And I'll give you permission to use the 2021. Last year's talk was a bit more about a specific use case. I pulled some crime data from the DC Open Data Portal. You can go to that link and check it out if you're interested. This time, I want to talk a bit more about one step before that. How do I get started with spatial analysis in R? So this is, you know, where do I get that data? What tools do I use? Like, what can I even do? What does that mean? I'm going to break this down into three parts. First, where do I start? Second, what tools do you use? And three, the exciting part where there are lots of maps and things. What can I do? So first, where do I start? So this meme right here. You could replace this with a really ugly spreadsheet and a beautiful GG plot to visualization. And you get the same idea, right? So most people when they think of spatial data, they think of beautiful maps, amazing visualizations, cool stuff. What Alex told you yesterday. I'm here to tell you that unfortunately, there's a step before that. So this is right here. It's sort of what a GeoJSON looks like in real life. And then you see this and you're like, oh, no. How do I get from that to that map that I want to see? I actually don't believe the whole thing here. I believe everyone deserves a look at a beautiful map. Everyone deserves to have awesome visualizations. But I want to sort of talk about that data stuff before. So it's going to be really boring for like maybe five minutes. But I'm going to try to make it interesting. There's two main types of spatial data. There's vector data and raster data. So how are these two things different? Vector data is points, lines, and polygons. And raster data are grid pixels and cells. So I'm going to mainly be talking about vector data because in the social sciences, I have a background in economics. This is what we mainly work with. We look at where are firms located? Where are county borders? What are road networks looking like? Grids pixels and cells, you can imagine satellite imagery. So when satellites fly around, they take pictures of how land is used, like oceanography. Folks in environmental sciences use a ton of that. Not really spatial data. Controversial addresses, zip codes, counties. Why isn't this really spatial data? Well, it sort of is. It's spatial data to humans, right? So we know addresses. We can use zip codes to figure out, where is my favorite restaurant? Down the block. We can use this to navigate through the world. However, if you're going to be working with computers and if you're going to be using r to do spatial data, you need a way for addresses, zip codes, counties to become interpreted by computers. And that's going to be taking the form of coordinates, points, lines, polygons, and all of that. So this is sort of a hard thing to grasp. And a lot of people I work with have trouble grasping this, which is why I get a lot of Excel spreadsheets that say, hey, here's a list of emergency departments. Like make a map, right? Like analyze where things are. So right here, this is me reading in with read Excel. This Excel spreadsheet, I got a sign in to this variable called ERs. By the way, if you didn't know, if you put parentheses around variable assignment, you'll get a print as well. So that's pretty cool. So this is what the data looks like. I'm like, yes, emergency departments. And I'm like, no, it's not really a spatial data. So I have hospital names. I have the location, which is the city and the state in Illinois. And I have this mysterious region, whatever that means. Now, in order to make this into spatial data that I can work with, I can't just have the city and state and region. If I had outlines of the regions, maybe I could start doing a join, doing things like that. Let's say I just want to plot where these hospitals are in Illinois. So not good. We're going to do a thing called geocoding. And this is the step that a lot of people don't realize that they have to do. What is geocoding? By the way, thank you to Georgetown University for hosting us. This is an awesome auditorium. So shout out. I used you all in this example. So what you do is essentially take text input, say an address or something. And what you're going to do is throw it into a function, say geocode OSM, which means use the open street map API to do a geocode. And then I'm going to throw in this address and then get coordinates out. The coordinates are really what I need. So you can see that I get xy. And then the third thing right there, the bbox, is like the square around that feature. So imagine it's sort of like the edge of like the outline of your map, right? So you may also notice that we have xy. This is hard for people. It's not latitude and longitude. It's longitude and latitude. Every time I think about this, I have to be like latitude and longitude. Nope, the other way. Like longitude. So I've done this before and made maps that are like completely wrong. So I see some nods in the audience is you may have encountered this. Okay. So geocoded it. I have a appendix slide talking about some ways that you could geocode. I have access to the University of Chicago geocoder, which is awesome if you're at an institution. This is probably the best way to geocode things in bulk. Otherwise if you're sort of doing it for fun, you can use, there's a slide in the appendix sort of talking about how to do this. Let's say I go ahead and geocode. There's a CSV here. And this is much better. So you can see here when I geocode it, I get an ID column sort of just to do that on. I get the address and I split it up using separate and to city and state and I get latitude and latitude. I always get it wrong. I also get a match score, which tells me how close this is likely to actually be where it is. So talk to your GIS librarian or talk to your GIS expert or talk to someone who has a license of ArcGIS and they can do this for you or you can do it yourself. So all right, cool. Thoughts on a map? So this is essentially how you go from that Excel spreadsheet that someone just toss at you and you're like, make a map. And you are going to go ahead and do this process and convert it to this. There are a few intermediate steps here before you get into a map. But I'm here to talk to you about the part that no understands about spatial data. Spatial data formats are confusing. So the most common spatial data formats are the following. Shape files, GeoJSONs, Geo packages, CSVs, tips. There's so many more out there. There wasn't really a standard before and everyone sort of came up with their own spatial data format. These are the five that I use the most often. You'll see shape files a lot. Shape files are sort of the file format that was pioneered by Esri, which is the company that makes ArcGIS and it's sort of become prevalent. Unfortunately for you, shape files are actually four files because for some reason it was impossible to store, geospatial data can be really large because you have all these massive polygons and things. And so they decided to store it into four files. So you need all four in your directory in order for it to work. That's why I sort of prefer GeoJSON. This is more common if you're doing blood mapping, if you're working with devs and they want to make a map. GeoJSONs are sort of what they're probably going to use to create those maps and things online and apps. I really like, I've started moving to Geo packages. They're awesome. They don't cut off your variable names. Shape files cut your variable names off at seven characters, which is very annoying when you write them. And CSVs, as you can solve before, I usually get CSVs with latitude and latitude on them. So I just use them as geospatial data. And finally, TIFFs when you're dealing with Raster data. Many more. There are so many spatial data things. These are all read by GDAL, which is the geographic data abstraction library. Okay. So if you look at these all day. All right. So use these file types in R. I have started moving to a library called SF because it reads all of these file types in without me having to worry about it. I do library SF and then I use stread, you know, er's.shapefile. Alternatively, if you prefer tidy data, aka you don't want your files read in with strings as factors, I like read SF. So this is another way to sort of do that. Creates a tidy SF data frame. So it looks a little bit similar to your sort of normal Tibble. There's a little summary at the top with some spatial information. The most important thing is it is a normal Tibble, but it has this geometry column right here. So each geometry, aka each shape, is stored as a series of like coordinates and things. And it's in that last column called geometry. This makes it so that you can use the tidy verse and use those tools with spatial things like this. Because for me, it means going from normal analysis where I'm just creating, you know, with working with data tables that are not spatial, I can use the same tools when I'm working with my spatial workflow. And that is like huge because before this, like working with spatial data has been like can be a hassle. It can be like you have to change it into like this list column and then doing a lot of things that way. So this is what I prefer for that. Okay. So I think the most important slide on here and when I wrote it, I was like, why didn't I share this with anyone before? How do you Google search for spatial data? How do you find spatial data? These are the keywords that I would use to find spatial data. So park district boundaries, food vendor, shape file, administrative district polygons, road network geometries, abandoned car GIS files. Remember addresses, zip codes and regions are not spatial data and can't be interpreted by computers. So what you're going to look for are these like geo things, right? Polygons, boundaries, lines. And so for example, if I search DC boundary shape file, geo spatial, shape file, geo JSON, just all of it, just to be careful, I get this nice DC boundary. Remember. Remembering how to Google. Up to Google is how you get anything done. So if you want to take a photo of this, I think this is possibly, I want to just share it out because I think it's the most important slide I've ever made in my life. So, okay. So then you get the first three sort of options are from the open data DC portal. So I highly recommend, you know, sometimes just knowing what to search is like the door that unlocks everything, right? So, okay. So two, what tool should I use? And I'm definitely going to run out of time. So my tool kit is GORA, QGIS, ArcGIS, PostGS, Cardo and R. Given that this is an R conference, we're going to talk about the last one. But like most of us, I'm not necessarily a single, like a single for my geo spatial analysis. I don't use to use one thing. There are many R packages out there for spatial data. Does anyone know what this is? Anyone want to guess what this is? Does anyone gone to this page? Or like, no, what this is at all? Okay. This is what is known as the CRAN TaskView. So, if you go to like the actual CRAN website, which is where you get your R packages from, there are essentially like groups of packages just listed by topic. And in this, there's a spatial one. And when you go to the spatial task view, this is what that looks like. And you go all the way down, you're like, okay, cool. All these packages. Okay, there are a lot of packages. Okay, cool. I'm going down. I'm going down. There are a lot of packages there. And then you get to this list. Nice. So, I'm here to tell you that there are a lot of packages for everything in R. And you may not want to sort through this. So, I'm going to talk to you a bit about what my workflow looks like. This is what I mainly use right now. Disclaimer, I am sort of on the, like, I have the time, because I work at a university to sort of think through like what tools I want to use and sort of what I can choose, right? Like if you have workflows that use other things, it's okay. So this is sort of what I sort of see as the cutting edge of spatial right now. Similar are packages to these. I like my tidy verse and stuff, and I like working it with spatial data. And the reason that it's similar, that each of these are similar are the first one, I would say SF allows you to ingest spatial data, clean it, and manipulate it in a tidy way. The second, I would say T-map. I do use GGG plot two for some of my maps, but T-map, I'll show you later, has like a really cool functionality where I can go between interactive and static maps. SP-dep is because I work in like spatial kind of metrics and things like that. That's my modeling package. You can think of it sort of like the Glimnet. You don't use it every day. It's like not that, like, you know, sometimes you need it. And then SP, it's like data table, non-tidy, yet useful in certain socialized cases. And a lot of old packages sort of depend on SP, so the analogy is not quite there, but we'll go with it. Okay. What can I do? Here's the exciting part. I wanted to talk a little bit about the most recent, I think it's like actually the Tidy Tuesday two things ago. They had some data about the squirrel census in New York City. So the data set was essentially, they sent out these like volunteers to like go count squirrels in Central Park. And then they like recorded where they saw each of them. And I was like, yes, finally a spatial data set for Tidy Tuesday. So I was really excited about that, and I decided to like do some analysis. So I actually cheated a little bit. I pulled some like New York park boundaries areas. So I think they like figured out a way to classify different parts of the park, depending on something. And then I pulled into Team App. One nice thing about Team App is I went ahead and did like a population count of squirrels by area in the park. You can see that there's one area that's a little bit darker. I'm like, why are there so many squirrels in that area? Are there so many vendors? And so what happened was I decided to change the mode and go ahead and do an interactive view. So I could zoom in and out of that. So I ran one line of code, Team App Mode View. And then I ran the same code again. Let's see it again. So I'm running my code. I'm creating a static map. I get this count by area. And then I changed the mode, right? So I'm changing the mode. I run the same code again. And I get this interactive map. And so it can switch between static mapping and interactive like interactive exploration like that. That for me is super valuable because I want to know the context in which I'm creating the spatial data. So then I actually took a look and I think that red part right there, it's known as the ramble, which I don't know if there are people here. I guess it's like a woodsy part of the area. I thought maybe there were just like a lot of hot dog vendors. And I was like, what if I could pull the hot dog vendors and stuff? But they don't have, I don't know if they have data on that. So anyway. So this is one nice thing that you could now do in R, right? So you're changing, you're like running one line of code and then you're running the same visualization code again. By the way, T map, the idea, the syntax is similar to GD plot, it's just pluses and you're adding layers on top. So that's pretty cool. So I'm alternating between interactive exploration and static mapping. Check out map view also for a GIS like experience. So if you are familiar with GIS and GIS tools and you sort of want to replicate that in R, map view is awesome. It allows you to hover on things and sort of pop up ID tables. Also race shader for 3D visualization graphics. I had the fortune of going, being the last facial talk. So I was like, I'll just mention some of these packages that you could use. Okay. Last thing I want to sort of reemphasize, think beyond dots on a map. So this book, How to Live with Maps, I think they're really useful as sort of a visualization understanding basic things for me and for my center. That's sort of like the first level, exploratory spatial data analysis you're creating these visualizations. What else can you do beyond that? Why does wear matter? Why does wear something happens matter? Because for example, does the house price in this area of your city affect house prices around it? Does the location of pharmacies for certain populations, is it clustered? Are there certain places where there are gaps? So this is important because tolbers first law of geography says everything is relating to everything else, but near things are more related than distant things. And so when you start thinking in this way, when you start being a spatial thinker, you're going to consider, you know, what sort of thing, like, why are things close to each other? Why are they more similar? Is that a symptom of, you know, is that spatial out of correlation? Does that mean that there are patterns in the data? So this is using JIODA and are you going to detect spatial patterns? So I think this is Milwaukee right now. It was like Milwaukee a few years ago. We essentially had the real map on the right, which is the percentage of African American, I think, by Capita. And then we just sort of randomly scattered the values across the map. When you look at a histogram of this data, they look exactly the same, right? So what we've done is taken the attributes across areas and we just distributed those attributes across any area. So when you look at the histograms, you're like, oh, like, you know, the percent African American is like, you know, similar. But when you look at things like Moran's eye scatter plots, you'll see that you can detect spatial patterns. And that's really, really important. So something else that you can do is look at local spatial out of correlation. So our points, our values in an area, are they related to each other? Are values, like, are they more likely to be related to each other? Or are they likely to be distinct from each other? Is there some relationship there? And finally, you may have done clustering, normal clustering, spatially constrained clustering. What happens if you decide to limit your clusters only to things that are next to each other? Does that matter? Does that change the output of what you have? And so with that, I wanted to mention that we have a pre-alpha release of RGODA right up now. So a lot of this interesting spatial analysis you can do in our, and there's certain functionalities missing. So if you want to test this out, I think there's still a lot of work to be done, but you can sort of see what types of functionalities are in there. And I've definitely gone over time. So if you want to stay in touch, I will be, I'm always on Twitter and I will be TAing the Modern Geospatial Analysis tutorial at RStudioConf and leading the geospatial track at user. So please submit tutorials there and there's some in my context. So thank you so much. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you."}, {"Year": 2019, "Speaker": "Kelly O'Briant", "Title": "Reflections on a Year Spent Talking to Data Scientists About DevOps", "Abstract": "In this talk, I reflect on my experiences over the past year discussing DevOps with data scientists, and how these principles can be applied to the data science workflow, particularly in relation to the R and Shiny ecosystems. As a Solutions Engineer at RStudio, my role involves understanding the challenges data scientists face when integrating R into various industry environments. A key challenge is legitimizing R and Shiny within organizations, which often requires navigating resistance from IT departments. I discuss how DevOps, a philosophy promoting collaboration between development and operations, can help address these challenges. I highlight the importance of developing professional, performant Shiny applications, and aligning them with DevOps practices such as automated testing and sandbox environments. I also emphasize empathetic communication and making work visible as critical components of successfully bringing R and Shiny into production. My aim is to empower data scientists to build credibility and trust within their organizations using these approaches.", "VideoURL": "https://www.youtube.com/watch?v=IKMu-Q5Yms4", "id0": "2019_19", "transcript": "Thank you so much for having me, Farid. Always a pleasure to be here. My name is Kelly. I work at our studio. I'm a solutions engineer there and I'm not here to talk about the tidyverse because that's not what I do. But I am here to talk about something that I've been on the road doing for the past year which is talking to data scientists about DevOps. And I've been doing this sort of overtly and covertly sneaky and into my presentations and I wanted to spend this last conference of my year reflecting on what I've seen and heard and talked about and experienced along the way. So thank you for witnessing my reflections here. But first I want to talk about solutions engineering. A lot of folks don't know what solutions engineering is and I want to say right off the bat it's not dev and it's not really ops so why am I talking about DevOps? Well at our studio we as you probably know build a lot of open source, free and open source tools but in order to fund all of that awesome software development we also provide a professional tooling. So we sell professional versions of our software to folks who are interested in having that sort of license and support from us. And my team solutions engineering that I'm part of does a lot of work to try to understand what happens when we bring data scientist science the data science workflow and drop it into these various industries and enterprise environments and try to solve for those types of problems that we see there. So I get to be a part of these really cool conversations and be an advocate for our customers inside of our studio to discuss what are we seeing, what can we learn, what are the needs and what are the problems that folks have when they try to bring our into their organizations. So I want to talk about what the problems are. And number one still for our and now as we are embracing Python as well, Python is just getting legitimacy around the our programming language. So when folks try to bring our into their organizations, generally how that happens is they will download the free and open source tools because they've done that in grad school and now they're going to do that inside their organization. And we call that bringing our in through the back door. It's a perfectly fine way to get started using our inside of your organization. But it can come with challenges especially as you try to grow your adoption of our within your organization and help other folks learn the tool and get the resources from your organization in order to support that growth. So there are a lot of challenges here for the past year. Folks on my team have been doing a lot of writing about how to solve for some of these challenges when you're legitimizing are. We still see a lot of pushback, especially from higher level folks in management and people from it. This tweet, one of my friends from the community put out into the world sometime last January really summarized it well for me. But we see these these kinds of comments all the time like can anybody point me to some good resources for starting a conversation with the IT department about making are and shiny and accepted part of our software. So we still see a lot of struggle with this and that's the problem that I am really passionate about helping other people solve. Now there are frameworks that developers have used now for many years to start conversations with IT. And those frameworks have largely been lumped under this topic or this style of communication that is called DevOps. And so I wanted to give a really super quick introduction to what DevOps is by starting with what DevOps isn't. Now these, if you have heard of DevOps or have any exposure to it, many folks will try to tell you that DevOps is just like a collection of these crazy tools. You've heard things like CI CD and programmatic deployment and serverless computing and all of these tools, I've probably only heard of like maybe a quarter of them, half of them up there. There are tons of logos and companies who are interested in getting involved in this space. And it can be really fun to get into and tinker with. So many of my previous DCR and NYR talks have been about my experiences tinkering with these types of tooling and meshing them with R. That's not what I'm here to talk about today because I'm going to take a step back and talk about what DevOps really is to me. And that is a philosophy. DevOps is this philosophy or set of practices that create new processes for collaboration between development and operations or IT teams. There is nothing new in DevOps. It came from a framework around mean manufacturing that was largely written about in the 70s and 80s. And before that, it was just like a framework for making common sense decisions. So we are borrowing from these other industries, which is really great. It reminds me of a kind of stoic philosophy that's been around for ages now. But it is excellent to think about these things and how maybe as data scientists, we can adopt some of them into our own workflows. So here's the classic diagram that people will show you when they talk about what DevOps solves for. You have this silo that developers live in and a silo for IT and operations folks. And what developers will end up doing in theory is writing this code, they're developing their software. And then because they live in a silo, they're throwing it over a wall and letting it land in IT's lap to handle. So the fear from IT's perspective is that they are going to end up with something that is actually garbage that is now their problem. And the classic meme here is like the child who's grinning and smiling as a house burns down behind them. You're a problem now. So it ends with this vicious cycle of mutual resentment and distrust, which is what this framework for communication and making work more visible aims to solve for. And like I said, I'm really interested because I am a pattern-seeing person to see if we could take some of these same strategies and use them in communicating with IT as data scientists. Unfortunately, we tend to have an even worse problem. Because the data scientists, we bring our code to IT. And what they hear from us is, hey, I created this code using a bunch of open source packages, some random person from the internet created, and I built a web app out of it. Can you take this into production for me? Thanks. So not only are we asking them to do the same things with the same bad behavior as our developer counterpoints are, we're asking them to do it in using language frameworks that they are even less familiar working with. So I'm calling this our super vicious cycle of mutual resentment and distrust. And I'm making light of this, but it really does come down to the fact that even if we think that what we're hearing from IT is unfair and judgmental and not correct, it's important to take a step back and look at things from their perspective with a little bit of empathy. Because we want to make sure that they know that we're not trying to make their lives terrible. We're just trying to do our work and we need to take steps preemptively to help them understand that so that we can start building that trust. So here are the challenges as our users. Organizationally, we're struggling to legitimize our itself. And then we're trying to work with these IT teams to get them on board and they are resistant to that. On the other hand, technically, we also have challenges. We don't have the same software development background as our Dev counterparts do. We lack that kind of education and exposure to the types of processes and tools that software developers have become accustomed to so that they can communicate effectively with IT. And we're not sure how exactly those things map to what we're doing in data science. For the past year, I have been working closely with our shiny team and our solutions engineering team to create content and resources for bringing shiny into production. So the rest of the talk will be kind of shiny in production focused. But these same concepts will also translate to just our own production or our in general and organizations. So the shiny team at our studio has produced a large number of packages for the support of bringing shiny into production. And I want to talk through some of the shared goals that they translate to the map to DevOps principles. But first, before we get there, I want to talk about leveling up your own skill sets when it comes to developing shiny applications. So I won't call anyone out because I actually strictly not supposed to. But I have seen many shiny applications across my computer screen that do not look super great. And I think that's the number one thing to look at when you are trying to build that trust and legitimacy within your organization is you want to produce something that looks professional, that is professional, and that you've tested and that you understand the performance ramifications of. So I like to think of like, hey, it's okay, I do this type of development all the time where I'm prototyping something, iterating really fast, I'm creating a big mess. And I think of that as like creating the hour long talk of data products. If you've ever given a talk that's an hour long, that's a lot of space to fill. And it can be rambling and cluttered. And there are parts that really hit great and there are parts that don't hit it all. And what your goal is when you're moving something to show other people to put into production, is that you're looking to create the lightning talk of data products. So you want this thing, this five minute talk to be targeted and elegant and streamlined and optimized and performant. And that's the goal, but we don't give you a lot of information or structure about ways to iterate in order to get there until last our studio conference. Jo Chang, the CTO of our studio gave a keynote at our studio called Shiny of Production Principles Practices and Tools. And one of the best coolest gems that came out of that talk that I just haven't seen people embrace or get excited about Sensen is this performance workflow that we outlined. So within the performance workflow, he talks about using two key packages that Shiny team developed in the last year called Shiny Load Test and Profiz. I just so you know, Jo made a point in telling me when I was developing the Shiny and Production Workshop that the thing he thinks is most important for our development and Shiny development is code profiling. If you're not profiling your code, you're doing something wrong with Jo's eyes. I won't put words in his mouth, but Profiz, the tool for understanding what's making your code slow is the most important thing. He thinks trusting your instincts here are what really gets folks in trouble. So you're using Shiny Load Test to determine if your application is fast enough for production. And then if it's not, you're using Profiz to profile your code and understand how to make it get there. Then in order to make it get there, you're going through this optimization workflow. So he has different steps here. I won't go into them. This talk is available on the RStudio resources webinars page and always will be. So please, if there's one piece of homework or take away from my talk, it's to go watch Jo's talk because it's amazing. Once you've done this, you have this piece of code or a Shiny application that you do feel proud of, that you do think is performant. What's next? This is where we can kind of start digging into the DevOps goals here. One of the goals of DevOps is to reduce the risk of deploying a breaking change when you're putting code into production. So one of the ways that you can do this is to start writing tests. And we have a package called Shiny Test that will help you do this. So you've developed this nice application. You want to make sure that it continues to work into the future. And there are a number of things that you can do that will break the application. So developing these tests and a testing framework that you feel good about using around the app can help you talk to IT and say like, yeah, we have a plan for carrying this application and making changes and supporting it once we get it into the deployed state. But there's another piece to this. Automation. I don't want to remember to run this testing procedure every time. I definitely don't want to have to hand that job off to IT. And I don't want to need to communicate with them every time to make sure that they're okay with me having make the change. And they know that I've run the test. It's a big communication night there. So another one of these DevOps goals is in general just the improvement of daily work. This quote from Mike Orzen really captures this. He's like even more important than daily work is the improvement of daily work. You want to make sure that your IT folks know that you're not trying to make their jobs harder. And the way that you can do that with testing is to automate it. So here's where we do get a little bit into the gritty of these tools. And there are lots of different tools they can use to automate tests in R. One of the two frameworks that we have just available as examples with the shiny test vignettes are using Git and GitHub and Travis to automate testing. So you can go into the shiny test vignettes, pull that code off, make minimal changes and be up and running with automated testing very quickly. Here's my most important thing about putting shiny applications into production. If Joe's is proffiz, mine is getting yourself a sandbox. I think that maybe Joe takes this for granted that this already exists. But in my experience working with our customers, a lot of them do not have a place where they can simply test things, push them into a similar production environment and do things like user acceptance testing even before they get to the point of wanting to deploy things with an IT group. So the shared goal here from DevOps is to shorten the distance between development and production by having this environment where you can sort of iterate and test your deployments and make sure things work in like a twin environment to what your production environment looks like. The example I have here up is for our studio connect publishing platform. But you can do this with shiny server open source, shiny server pro or your Docker craziness that you put together. All of these things are really good and map across different tools. You don't have to to fall into step with with what our resources say. They work across many different tooling sets. Finally, you can get really deep into DevOps and pull out some nifty and cool techniques that also map to shiny development. And this is one that I played with in two talks at usar this year. I spoke about the importance of when you're taking applications to production to decouple the idea of a deployment from a release. So a deployment is anytime you push code into this environment. And that could be your testing environment or your production environment. But a release is when that code or those features are made available to the users. So there are different patterns you can follow to kind of hide a deployment from your users and tell you're ready to release it to them. And one of those is application based release patterns. So for that, I explored using feature toggles within shiny applications to control some of that. And the other is environmental based release patterns. And I'll cut to the chase environment release patterns are way easier to manage in my experience for using shiny applications. But both of these talks are available as recordings if you're interested in really digging into that. And there are many other things that you can pull from DevOps that translate and will help you communicate. This DevOps handbook has been really useful to me in coming up with ideas and creative ways to talk about DevOps and to practice putting things in different contexts for people. So I highly recommend checking that out. And I want to end by saying that empathetic communication is very challenging. There are a lot of accounts that we work with and folks who we try to help that are in really tough, really hard situations and struggling to make any progress at all with legitimization or bringing their absence production or creating these environments that are conducive to helping their data scientists get the analytic infrastructure that they need. So from DevOps, we say make work visible to find these shared goals, build a checklist and above all be prepared to iterate because this is a journey. And you're not going to get the perfect solution on your first try. These things take time to develop. And you should be prepared to have a little heartache. And as long as you keep a little bit of empathy along the way, hopefully you should have a good time. This slide is what we collected at our first Shiny of Production training about what folks ideas about what production is. And I liked this one that came out in the middle the most of all. Someone just responded to what is production with a single word credibility. And that means a lot to me. I think that really captures what my goal is in going out into the community and speaking with folks. I'm trying to help us all as a community develop that credibility so that we can work with other folks inside of our organizations. And you can judge whether I've done a good job of that. Here are my last 10 talks from the year about Shiny and Production and doing DevOps as a data scientist. They're all available on speaker death. And like Jared mentioned, I'm really trying to get that radman's hashtag going. I'm part of a big team at our studio that does Solutions Engineering. And we all write content that lives at solutions.rstudio.com. And if you're looking to get in touch with us, community.rstudio.com and the radman's channel there is a great way to do that. So thank you."}, {"Year": 2019, "Speaker": "Elizabeth Sweeney", "Title": "Visualizing the Environmental Impact of Beef Consumption using Plotly and Shiny", "Abstract": "Become a more environmentally conscious meat consumer! Did you know that there is a big difference in the environmental impact of consuming 100 pounds of hanger steak versus the same weight in ground beef?  In this talk, I will be exploring the environmental impact of eating different cuts of beef in terms of the number of animal lives, Co2 emissions, water usage, and land usage.  This project is a collaboration with Happy Valley Meat Company (https://www.happyvalleymeat.com/).  Along the way we will be using plotly to make interactive graphics and  R Shiny to make an interactive webpage to explore the data.", "VideoURL": "https://www.youtube.com/watch?v=3laYKACZi7c", "id0": "2019_20", "transcript": "Great. So today I'm going to be talking about visualizing the environmental impact of beef consumption using plotly and shining, hence the Shine Bright Like a Diamond Intro song. So I am an assistant professor of biostatistics at Whale Cornell Medicine. This is my Twitter handle. So if you want to tweet or follow me on Twitter, you can do it there. And great. So to start out, I wanted to give a little bit of background about myself, so what I've been up to and what I've been working on. So these are a couple of places that I've worked. Once I finished my graduate training, I wanted to see what it would be like to be an industry data scientist. So I went and worked at Flatiron Health, where I did cancer research using electronic medical records data. I also worked at a small startup called Kavera Health, where I did evaluations of different radiology interventions using claims data. And then I really missed academics. So I think that maybe not a story of people here that often of going to industry and like really missing academics and wanting to come back. So just recently, about six months ago, I started as an assistant professor at Whale Cornell Medicine. So if anybody's interested in the path back to academia from industry, I'd be happy to talk to you about it and give you some advice from my journey. And so what I normally work on and when I gave my New York Art Conference talk on, which you can see online, is brain imaging data. So I work a lot on structural MRI data and some of the problems that I work on, right here I have a structural MRI from a brain from a person who has multiple sclerosis. This is an axial slice of the brain. And you can see there are these bright areas on the image. And those are multiple sclerosis lesions. So a lot of my work is training different algorithms to find these lesions and to locate those automatically to help diagnose the disease and monitor disease progression. I also work a lot on something called super resolution. So right here on the right side, I have a low resolution image of the brain. And I use that to predict higher resolution images of the brain so that you can see more anatomical detail of what's going on. But today I'm not going to talk about any of that stuff. I'm actually going to talk about meat. So this is a side project that I've been working on. And it's a collaboration with the Happy Valley Meat Company. So this is a client that came to me and had a data problem that they wanted to work on. It's a little bit of a weird data problem for me to work on because I'm actually a vegetarian. So I don't eat meat at all. But actually the take home message kind of dies well with my vegetarianism. So what Happy Valley Meat Company does is they basically go and they work with different farms. And they go to farms that have very high welfare standards. So they treat animals really well. They give animals a great life. And then they take that meat from those animals who have been raised well and sell it to different restaurants in New York City. And their mission is to better the lives of the people and the animals that feed us. And so how many people are there some New Yorkers here? Okay, so we got a few New Yorkers. So some of the famous restaurants that Happy Valley Meat Company works for our mama fuku. You guys know mama fuku. So that one's like super famous. Dig in. They do like the meatballs that dig in. So those are like ethically made meatballs. So you can eat those and feel good about yourself. Fetsau, Frenchtte. There's like this huge list of restaurants I just picked like some of my favorite ones. And so the big problem that Happy Valley Meat Company faces is that they buy whole animals. So they will buy an entire beef animal. And they need to find homes for every single part of that animal. So there's very few of those steaks that you're going to eat at mama fuku on a single animal. And there's a ton of ground beef. So you just get tons and tons of ground beef and some other cuts. But they have to find a home for all of that. And so one of the things that they tried to do to do this was to quantify the environmental impact that each cut of meat has. So if it takes one cow to make 250 pounds of ground beef, we can quantify the environmental impact of one cow. If it takes 250 cows to get 250 pounds of a smaller cut of steak, we can quantify the environmental impact of that. And so that's where I came in to start quantifying that impact and to make visualizations of this that we could give to restaurants to help them make better informed decisions about which meat they were buying. And so the data that I got when I went to work on this project was this. So normally I'm used to getting like tons and tons of like brain imaging data. But I got, this is just the head of the data. But I just got a very small amount of data. So I got whatever cut on the animal and how much the total weight for that cut was on the animal. So right here I have ground beef, 250 pounds, bone and strip, 36 pounds. So I have just the amount of, or what cut of meat and the amount of weight on a cow that we have for that cut of meat. And so I started out by plotting this. And so I made a bar plot. So I think somebody was saying earlier this is their favorite kind of plot. D-Rob, it's my favorite kind of plot too. So, and you'll see I made like 10 of these. So like it's really my favorite plot. So this is the weight of each cut per animal. And so you can see you get tons and tons of ground beef on one animal. But things like hanger steak, tongue or ox tail, you don't get as much. And so that's like the first way that I conceptualize this. And then I just wanted to zoom in on a couple of like common cuts. So ground beef, bone and ribeye, tenderloin, skirt and tongue. I guess tongue is not that common. But there's like very little tongue on there. So I'm using it as an example. Okay. So the next part was to quantify the environmental impact that each one of these cuts had. So what I did was I looked up how much water a single cow used during its lifetime. So a single cow uses 6.36 million gallons of water throughout its lifetime. So that's quite a bit of water. The really bad thing about cows and eating them is they have extremely high CO2 emissions. So one single cow emits 102,000 pounds of carbon emissions during their lifetime. And then they use about 77 acres of land. And so the where I got this information was the shifting diets for a sustainable food future. So you can check that out where these constants came from. And so the next thing that I need to figure out was if I had a certain amount of weight or of cut of meat, how many cows that would use. So I have, I'm going to start with 100 pounds of whatever product. So this is 100 pounds of ground beef, 100 pounds of hanger steak and calculate how many cows that requires. So these are the calculations that I did. And so if I need a 100 pounds of ground beef, that would be one cow. 100 pounds of bone and strip, I would need 3 cows to come up with that. And then what I did was I took the number of cows and multiplied it by how much the environmental impact was for each one of, for each cut. So now what I've got, I've got the cut of meat, the number of cows required, the millions of gallons of water, the thousands of pounds of CO2 emissions and the acres of land. And so the next thing I did was I made another bar plot. So all this data I'm going to be making, tens and tens of bar plots. I like how many people use the West Anderson color library. Yeah, I'm a huge fan. I love West Anderson movies. And I think I'm like late to the game on this one because I just found it last year. And I think it's been out there for a long time. But so I use the West Anderson palette with the Darjillion color palette. But yeah, so I'm just making another bar plot. And so here we can see the environmental impact for eating 100 pounds of meat by cut. So you can see that if you ate 100 pounds, or if you bought as a restaurant, 100 pounds of tongue, that would be over 80 animals. So that would have a larger environmental impact than if you bought 100 pounds of ground beef, that would be only about one cow. I made a word chart. It didn't show up as well on here, but just the size of each one of the words in the word cloud represents how many animal lives is taken for each one of those cuts for 100 pounds of it. Okay, I found this function. So I wanted to share it with you guys. I don't, does anybody know the name of this? It's the percent sign plus percent sign. Okay, I don't know what the name of it is. So if anybody knows, please come up after like I've done and tell me what the name of this is. But it allows you to replace a data frame that you're using with Ggplot. So basically what I can do is if I have that bar plot where I'm plotting it for all the different cuts of meat, but I want to filter that down and only plot it for four cuts of meat, I don't have to redo all of my code. I can just say plot that out, whatever the name of this function is, filter plot data cuts in a particular cut list. And this is like my new favorite function. I found it like a couple months ago, so I wanted to share it with everybody. And this makes a smaller plot of these particular cuts. So my, my talk is on plotly. One of the really convenient things is if you are making a Ggplot, you can make it a plotly plot with just one command. So if I use the library plotly, and I use the function Ggplotly, take my plot and just put it in here, I get, oh, it got small. All right, imagine this a little bit bigger. Whenever I project things, they change. But now I have this thing that I can, that I can kind of click on and hover over the bars, and I can see the exact number of cows that each cut of meat takes. So if I look at round, that takes two cows to make 100 pounds of beef, or if I look at Oxtail, that takes 67 cows. So now I've made this into an interactive plot with just one command, Ggplotly. So it's very powerful. I use this all the time in my collaborative work. If I'm making like a scatter plot, and I want my investigator to be able to find outliers, I just take whatever Ggplot plot I made, wrap Ggplotly around it, and then people can investigate it automatically. So very, very powerful. So we can do this for those select things. And then we can do CO2 emissions, water usage, and land usage. So something that you'll notice is that all of these plots are like the same plot over and over again. Like they're linearly related. The CO2 usage, the land usage, because it's all based on the number of cows and a constant. So I made this plot, but nobody liked this plot when I was... So I think this plot is great, but it was beta tested with restaurants, and it was like very unfavorably, like no one liked it. But basically what I did is I took all the different cuts, and then I put four different axes on it. So I have thousands of pounds of carbon emissions, millions of gallons of water, acres of land, and number of lives. So I was like, who wants to look at the same four plots? And it has the same information. But that got like nixed, like right out the gate. It was beta tested with some restaurants, and nobody liked the four different axes. So I want to talk about and show some plotly code for some other projects that I've been working on. So somebody went to joke that I'm only passionate about two things, which are data science and hiking. So this I'm going to share something from my other passion with you guys. So how many people have ever gone hiking in the Catskills? A couple people. All right. So there's this notorious path in the Catskills called the Devil's Pass, and it's about a 20 mile hike with like 8,500 miles of elevation gain. So there are five different mountains on the Devil's Pass. And so I wanted to plot this thing, which is kind of a profile of the mountains in R, and I wanted to use plotly so I could see how far along the trail I'd be going and what elevation that I would be at. And so this is part of the Catskills 3500 Club. So if you hike all of the mountains in the Catskills that are above 3500 feet, there's 35 of them. You get like this little badge, and it's like a big deal. I just finished it this summer, so I hiked all of the mountains in the Catskills. Yeah, I'm very excited about that. It took me two years of like hiking like all the time to finish this. But yeah, so that's part of the Catskills 3500 Club. But so online there's this research called Caltopo, which will give you latitude, longitude, distance and miles, elevation and feet, and a couple of other things for almost any hike in the United States. It might be in other countries as well, but this is data I got from Caltopo. And what I'm going to be interested in for this is the distance and miles and the elevation and feet. And so this is one where I went and wrote the code in Plotly. So I didn't use Gg Plotly. I wrote this directly there. So you can see it's not like the syntax from Ggplot2. It's a little crazier. You need to kind of like learn it not crazier, but you need to learn like a different syntax to plot this. But I have a tutorial on my GitHub about all of like just using the plot underscore leave function to make plots. So you can check that out. It's all hiking themed. So I made like hundreds of like hiking themed plots using this. But you get something like this. So this is the devil's path. Along the x-axis, I have the distance and miles along the trail and the y-axis is the elevation and feet. And so you can follow the trail and you can go okay like one mile in. I am 2,260 miles, or excuse me, not miles, that would be crazy. And then you might be interested. This mountain right here is called Plotto Mountain because it has this like plateau top on it. And you might be curious like what's the elevation of Plotto Mountain. So you can scroll around and you can see oh it's about 3,800 feet. And so I think this is a really fun like interactive plot of like for hiking. Okay. So the last thing that I want to show you, I promised some shiny. Okay good. The internet's working. So this is the data product that I gave to Happy Valley Meat Company. They wanted to call it, I don't know if this name is very tasteful, but carcass calculator. I think like I was like oh. And if you want, I will do it. But let's say we want a thousand pounds of meat and we're going to get maybe ground beef. See I don't even eat meat, so I don't know what's popular. I think boneless New York strip and check eye. So you can click on whatever cuts of meat you want, select how many pounds of meat you want, and it produces first this table. And so this table is a way that you can compare how many animals. So think of yourself as a restaurant now. And you are deciding, all right I'm going to make my menu. And I want to have a low environmental impact for this menu. So you could say all right, if I buy ground beef that'll be four animals and like 411,000 pounds of carbon emissions. But if I did the menu with hanger steak that would be 667 animals. And that would be like a ton of CO2 emissions. So you can actually make choices based upon this. And then I've got the CO2 emissions for or these bar plots with the plotly thing so that they can visualize it this way. But this is where they didn't like the thing where I had the different axes, which I'm still stuck on. Because I think like click a god each one and it's the same plot. But yeah, so this is a tool that Happy Valley Meat Company actually gives to restaurants. And restaurants use this to plan their menu and try and pick the most environmentally friendly choices that they can. That's appropriate for the restaurant. Great. Well, that's what I've got to show to you guys today. So thank you so much."}, {"Year": 2019, "Speaker": "Kimberly Kreiss", "Title": "SHED-ing Light on Survey Data", "Abstract": "In this talk, I, Kim Kreese, discuss the use of R to automate data processes for the Federal Reserve Board of Governors' annual Survey of Household Economics and Decision Making (SHED). My role involves using R to streamline the production of tables and charts for the survey's reports, which focus on various aspects of U.S. households' financial lives. The talk covers the survey's scope, types of questions, data characteristics, and the challenges encountered in data processing. We transitioned from using Stata to R, which enhanced efficiency, flexibility, and reduced human error. Key tools include the Tidyverse, R Markdown, and version control with Git, along with custom R functions to automate data visualization and analysis. Our aim is to further refine these processes and eventually develop a public R package for broader use.", "VideoURL": "https://www.youtube.com/watch?v=P7jfxRlrAgo", "id0": "2019_21", "transcript": "Cool. So hi everybody. My name is Kim Kreese. I work in the Consumer and Community Research section at the Federal Reserve Board of Governors. You heard my colleague Vijay Bloom talk earlier this morning. I work in the same division as him. I'm going to talk a little bit more about kind of applied data science using R in a government setting. So my section does an annual survey every year. And I'm going to talk today about some work we've done to use R to automate some of our production processes for that survey. So just a brief kind of overview of the talk. I'll give a little bit of background on the survey context, more on the survey data and what we make the day look like. Then I'm going to spend the rest of the time just talking about how R fits into that, how it works, and then showing some code. So standard disclaimer, the views expressed here are mine alone and they don't reflect the position of the board of governors or any of its staff. So some background. So my section runs a large annual survey called the survey of household economics and decision making, more commonly known as shed. So we do this survey annually and we're a pretty large survey. So we're a nationally representative survey and we focus on the financial lives and experiences of US individuals and households. So I want to emphasize again, I'm in a consumer and community research group at the Fed. So I think a lot of people think banks are on target policy. So we're really kind of like a people focus group at the Fed. So our survey is really cool. We ask people a lot of different questions on a really, on a pretty big range of topics. So some kind of things we cover, economical being. So how are you firing financially, financial fragility? If you had some type of emergency expense, how would that affect your finances? How would that affect your wellbeing? And then lots of other stuff that we're really interested in in people's lives, the student loans education, how you finance education, how you feel about financing your education, income employment, your experience with accessing credit, the banking system, and then just a bunch of stuff about housing and retirement and also some financial literacy stuff. So you may have even heard of us. We get a lot of press coverage in particular for our economic wellbeing measures. We're a really unique survey in that we provide a lot of subjective measures of economic wellbeing and we also get a lot of coverage for the financial fragility piece I mentioned. So if you had emergency expenses, how you would cover them. So I want to just kind of give a snapshot of the data and highlight some of the nuances of our data and our use case. So the Public Use Data Set which I'll be using today has 11,316 observations and just under 400 variables. So each observation in our data set is a respondent so it corresponds to one person. So the kind of data we get in this data set, we get a bunch of metadata on the respondent. So how long you took to take our survey, how long you spent on certain questions, etc. We got a bunch of demographic information on the respondents, income, education and race, geography, things like that. And then of course we have our survey questions and we ask a lot of questions and they come in a lot of different formats but kind of the two main formats we have here is we ask exclusive multiple choice questions. So think like yes or no questions. We ask you a question, we give you a list of things, we only let you choose one of those things. And then we have another type of question that's pretty common. Is this select all that apply or like grid multiple choice questions? So we ask you a question, we give you a list of things and we tell you that you can select a bunch of options from that or you can tell us yes or no on those options. Okay. And just to give kind of a sense of what the data look like. So here we have the case ID. So that's a unique identifier for each person. And then some of the metadata and a bunch of the rest of the data we have. Okay. And then now I'm going to talk just a little bit more about survey, what we ask, how it shows up in the data because we have a lot of different types of questions. And then what we want to take that data and make it look like. And that's really where our comes in. So this is the question that was referenced in the Reuters article that I showed. So overall which one of the following best describes, how well you're managing financially these days. And this is one of those questions that we only let you pick one option here. Right. So you can't tell us that you're both living comfortably and finding it, finding it difficult to get by. That would really confuse us. So this is what that variable is going to show up in the data like. So again, we have our case ID and then just one column for that response. And that's pretty standard what you might expect. So we have this other question and a lot of questions look like this. We ask people, suppose you have an emergency expense that costs $100 based on your current financial situation, how would you pay for this expense? And this is one of those select all that apply questions. So we're really interested in how people are going to pay for this. If you're going to pay for this in a couple of different ways, maybe you'll pay some of it in cash, maybe you'll borrow some of it from a friend or family member. We really want to know that. That's going to look a little bit different in the data. So as we saw here, we have A through I. So each of these gets its own column for yes or no, whether or not you selected that box. And if you didn't select anything, it will come up as NA and we'll get a yes in that refused column over at the end. Okay. So I showed you kind of what the data look like. And this is really the kind of finished product that we take that data and make it into this nice pretty annual report. So we do this every year in 2018, which was published this past May. Our main report had 10 chapters. So it was 64 pages, 31 tables, 36 charts, which will be relevant later. And then our appendix, which was broken into two separate parts. So the first part is just our survey instrument. So the questions we ask, who they get fielded to, et cetera. And then the second part is tabbing out each of those questions. So we want to see what the questions look like among our whole sample. And just to kind of give a little more about what that actually looks like in the report. So we get this text, this analysis, and all throughout the report are scattered. Just a bunch of different types of tables and charts. So at the Fed, we're really interested in looking at a lot of different things. So we look at how within groups, and we compare cross groups. So we have some different types of charts to kind of do that. Yeah. So kind of a nice classic is the bar chart showing just family income distribution. This is just a cross tab. So we want to see where you're getting your income from and how that varies by different age categories. And then again, you can see down here, we'll kind of note where respondents might be in different categories here. And then this is one where we're comparing different groups. So we take people who tell us they have a stable income or they have a varying income. And then we just want to look at how willing they are to take financial risks. So different types of charts and visuals here. Okay. So now I'm going to talk about how, well, what our production process for this report looks like and how our fits into that. So just kind of a pretty high level explanation for our production process. So one of our big, our first step really is we want to generate all the numbers that are going to go into the report. So we're talking about all the numbers that are in the text that I showed you, all the numbers in the tables and charts. And once we kind of have that settled, our next step is to generate the tables and charts and all the visuals for the report. And then our final step in this process is a quality control. So we really want to make sure that all our numbers are right, that we're doing everything correctly before it goes out the door. So today I'm just going to talk about all these steps are very, very data intensive and take a lot of work. But I'm just going to talk about the second step, which is generating tables and charts for the report. So it takes a lot of work to turn the raw data into analysis for the report, especially the generation of tables and charts. Like I showed you before, we have data that's not always standard across different questions and variable types. And we have a lot of data and a lot of questions. So it can be a little bit challenging. So then talk about some kind of constraints we have here. So like I said, a lot of different tables and charts. We really want to be flexible when we're setting up our analysis. And we really want to be flexible in case we decide we want to do something else last minute. Maybe we want to change something. It should be easy to do that. And finally, while we do all the data work and the analysis for this report, we don't actually go through the process of making the book and publishing it. We have a publishing department that does that. And from us, they need Excel workbooks of underlying data for each table and chart. And they also need a rendering of each table and chart. So they're going to make it into that nice format that I showed you in the report. But they need to know what it's going to look like. And they need to know beyond just an Excel workbook of that. Okay. So we use R for a lot of things. Our previous process had a lot of manual data entry, such as copy and pasting. So is anyone familiar with a software called Stata, any economists in the room? Okay. Right. So prior to using R, we use Stata, which is a great tool for using economic data, doing economic analysis. But it's not as flexible as R. It's what we used to do is do some tabulations, have that go into a log file. And then we would transfer that output into an Excel workbook. And you can write directly from Stata to Excel, but we still needed to do some reshaping of our data that we just couldn't do or Stata couldn't do for us. So we needed to kind of eliminate that process or eliminate any manual kind of entry of data or copy and pasting. So we use R to do that. And we also use R to generate tables and charts for the report. So all editing needs from us is just what the charts are going to look like. We can give that to them in just a PDF. So we can use R to do that. And finally, this year we implemented Git for version control, which was incredibly helpful for us. We had a lot of chapter files, my initials, my coworkers' initials, V1 and V2. This really eliminated that and just made things much better. Okay, so if I have to convince you, our process with R is efficient. No one is spending time doing manual data entry. Our process is flexible. If we decide last minute that we want to change some type of graphic in our report before it goes out, we don't have to regenerate data in a log file, copy and paste it, do all this entry. We just change one line of code in our programs. And our process minimizes human error. Data entry is tedious. People don't like doing it. And it's prone to errors. And finally, we have Git and Git neatly tracks our workflow and manages our file structures. So we're doing really well here. Okay, so just going to give a brief overview about how it works. So we use a couple really great packages. Tidyverse, markdown, we use cable extra to do some rendering of Lateck tables. We use a package called question R, which is a nice package for working with survey data. And then just a bunch of other kind of great classic packages. So how it works is we go through a survey vendor and they give us a nice pretty clean data set. We do some of our own basic data, basic cleaning on the data. And that's kind of just to generate our own variables that we use a lot. So we ask a question about what income, where you fall on the income distribution, as I showed you earlier, we might want to do something like group people into high income, middle income, low income, instead of using just these, those preset intervals. So we write custom functions that are going to take in some basic information, such as what variables we're looking at, the chart type we want to produce. So maybe we want to do just a basic bar chart. Maybe we want to do group bar chart, something like that. And then just some parameters like titles, things like that. And then we use that to produce the figures. So for each chapter, we can create an R markdown file that just produces a PDF of all the tables and charts in that chapter. And it's also going to produce that Excel file, which we have to send over to our editing department. Okay. And now I'm going to just walk through a quick example of what that actually looks like in practice and what some of the code looks like. So I'm going to use some tables and charts from the income chapter. And I'll show just some code for some different functions. So I'm going to just start with this really basic type graph. So this is just bar chart, family income distribution. Pretty simple. Okay. So this is what our question looks like. This is one of those multiple choice questions, where you can only select one of these. So we ask you which of the following categories best describes the total income that you and your partner receive from all sources, etc. So you pick one of those. And that will come up in our data kind of the way I showed you earlier. And this is where we've we've written some of our own programs. So we have two programs here. And the first is a read data program. So this is going to read in a clean data set as I mentioned. So it will do all the internal cleaning that we want to have. And then it'll define some paths that we that everyone's going to use. So it's going to make sure that people are using the same data set. So you're not going to load in this file and you're not going to make any edits on that data set. It's going to be the exact same data set. And you're going to be using the same paths. So it's going to go to where our output needs to go. And yeah, no flexibility there. And then finally, we just source our basic functions file that we wrote that has some custom made functions to do this. So I want to just show like what we're doing here. And this is from the question R package that I showed you earlier. The presentations earlier this morning have given me a lot of ideas for improving this. So we're just using this weighted table function. We give it the variable, the weights. And then we can type it into prop table and data frame. And we just get this frequency distribution. Pretty simple. And that's what we want to show in a graph. Okay. So what we do next is we just specify where it's going to be written to. So for each chapter, we're just going to have like income, if we're writing for the income chapter, it's going to be an income Excel file. We'll open a workbook. And then we have this next step, which is one of the custom functions that we wrote. So we're going to prep this data to go into the visual we showed. So the function we have for this is called prep one-barre multi-cat. So it's just for when you have one variable that has multiple categories. So this is what the function looks like below. So it takes in a data frame. And I should specify that this data input is going to be what's loaded into your environment when you source this read data function or read data file. Yeah. So then we have, so in the function up there, the prep one-barre, our input is the data input and then the variable we're looking at. And down here, we can see that it's just using this other function we wrote called shed table, which produces the kind of output we want for our report. And I'll show that on the next slide. And it just does some basic formatting of stuff to kind of make it look nice for what we want. So yeah, this is what it looks like. So we just wanted kind of one function that people can put some variable names into and just get like a basic tabulation out for what our format is going to look like. So it takes in a data frame, either one or two variables depending on if it has two variables, it'll do across tab or not. And then our data actually comes clean with labels. So we use the attributes and labels to add labels to that. And then we will just return this. And finally, we have a do all function that we wrote, which combines all of these different steps in the process. As you can see, it's not as nice and pretty yet as it is in the final chapter. But this is good enough for editing. They get the idea. And we send that off to them. And oh, I should, so in here we specify a chart type. So this is just one bar chart. And then we have just a function there that looks like this. And that's just going to have some basic Ggplot making a pretty simple graph. Yeah. And then some other specifications for that. So we had the format of our data in particular can make it a little bit challenging to easily produce tables in charge for this report. Unfortunately, we can use R to kind of account for that and automate a lot of the production processes for our annual survey. Still needs a little bit of work. We produce this kind of on the fly last year. I produce this mostly with the help of a PhD statistician that we had on detail in our section last year, who was amazing. And it still needs some work, but it's really, really improved our process. And eventually, the goal is to make this into a package that will be able to be used by the public so people can pull our data, work with it, reproduce what we have, see how we did things, all that. And I want to just kind of, I guess, flag what a lot of people have been emphasizing here before I presented, especially my colleague, my colleague, BJ about communicating. You really need to be able to, my boss is a PhD economist. She's been at the Fed for almost 15 years. She's used data. We would be able to communicate kind of the points where the data is not working as well, and maybe we can use something else to kind of fix those bottlenecks. Yeah. So if you want to learn more about Shedd, this is where we have all our information. If you want to learn more about me, you can find my personal website, my GitHub account, I'm on Twitter. If you have any questions, feedback, suggestions, or if you just want to email me and brainstorm some ideas about automating your job too, feel free to email me right there. And that's it. Thank you."}, {"Year": 2019, "Speaker": "Jon Harmon", "Title": "RBERT: Cutting Edge NLP in R", "Abstract": "Last year, researchers made several leaps in the state of natural language processing, particularly with regards to transfer learning (the ability to apply learning from one problem to a new problem). One of the most influential of these leaps was BERT, developed by Google Research. In this talk, I will demonstrate how the RBERT package can bring the power of cutting edge NLP research to R users.", "VideoURL": "https://www.youtube.com/watch?v=1kV0fPDefBM", "id0": "2019_22", "transcript": "All right, so I'm going to be talking about a package that we made, actually a couple packages we made for using BERT in our BERT. I'm going to explain what that is, but that's the Google Natural Language Processing Framework, and it's really cool. All right. First I want to acknowledge, oops, sorry. I want to acknowledge Jonathan Bratt, who did really most of the work on the packages. He couldn't be here today, unfortunately, but his GitHub will be featured very prominently throughout this. He's a colleague of mine at Macmillan Learning, where we are both data scientists. All right, and so if you want to follow along, you can go to this randomly generated URL that bit lead put together. That will take you to a tweet, which you can like and retweet and reply to, and then you can click through to go to my slide day. So yeah, I'm going to, today, be doing a really brief overview of transfer learning, because that's what BERT brings to natural language processing. Then I'm going to talk a little bit about BERT. I'm going to talk about the two packages that we made to use BERT in R, and then we're going to see how you can kind of see how BERT thinks, sort of, using, looking at attention and at the layer outputs, and all of this hopefully will make more sense by the end of the talk. All right, so first transfer learning. A key example of this was about five years ago. We're making models to classify the image net, image set. It's like 14, 15 million images tagged to like 20,000 categories. And it got to a point, or it is very much to a point, where we can see that on the early layers of these neural nets, you get just simple feature extractors. So here we can see that it's an edge detector. It can see light to dark, or dark to light edges. And then those add things like that add up together, where you start getting things like a face detector, or a text detector in later layers. I've got a link to this in the slide deck. If you go to the GitHub and the video that they made about this is just amazing. It's really mind blowing how they were able to really kind of explain how this neural net is doing image classification. And an important thing here is no one trained it to detect faces, or to detect text. For example, one of the categories was bookcases. And so in order to find bookcases, it figured out, so to speak, that it needs to be able to find text, to find a book, to find a bookcase. And so it just learned these deeper layers. We've had kind of transfer learning in natural language processing for a while now, in the form of word embeddings. Word to VEC and Glove are two examples of that. And the famous case that really people should find other ones, because if only this works, that's not that impressive. But if you take the vector for king, you subtract the vector for man, you add the vector for woman, the closest word is queen, which is pretty cool. You can start doing math with language. The problem with that is each word has one embedding. So if you have these two sentences, I saw the branch on the bank versus I saw the branch of the bank. Those two sentences, like every word in those, is going to have almost exactly the same, well, every word except on and of will have exactly the same embedding. But branch in the first sentence means a stick, and bank on the first sentence means the edge of a river, whereas branch in the second sentence is a subset of a bank, which is a building. So last year, there were a whole bunch of advancements in NLP doing more kind of sentence level embedding, and it culminated in October of last year with BERT. That stands for bidirectional encoder representations from transformers. We're going to look at what transformers are a little bit, and hopefully it'll kind of make sense. The bidirectional part is just, they're looking at both ends of the sentence. They'll mask a word in the middle, and then use the words before it and after it to figure out what that word was, which obviously that's how language actually works is the whole sentence matters, so that was a good advancement. This was trained with a very large corpus. They used Wikipedia and Google books, like all of the books that they've scanned. And the cool thing about this is it is transferable. You can put in a sentence and extract some features and use them for your own model. You just take off their classification task at the end, put on your own, and you can do some cool things. And to do that in R, we have this package, Arbert. It's available on GitHub. You can use dev tools to install it right now. It's an implementation of BERT in R. Right now, you can use it for that feature extraction where you just pull out, you give it a sentence or tons of sentences, and for each sentence, it gives you a vector to represent that sentence. So you can do that right now. We're working right now on making it work for fine tuning, where you could put your own neural net piece on the end of it and let it kind of relax and find new features. It'll probably be a future talk in January. And then we also made this Arbert vis. This has basically two functions, visualize attention and display PCA. We're going to be looking at that because those two functions, the reason we made this package is it lets you see how BERT is thinking, which, again, will make more sense in a second. So all right, attention. The code on the left here shows just, it's actually, it's a little aspirational. It'll work this way next week. If you really want to, if you want to use it today, go look at the code for my slides, and you can see the extra arguments that are in there. But this is how it'll work next week. But it's almost this simple. It's just simpler. Or it's a little bit more complicated inside. But you tell it what model you're going to use. Give it a sentence. Tell it what layers you care about in the BERT base. There are 12 layers. And you tell it you want attention. And it will pull off the attention, send that through Arbert vis. And it creates this HTML widget, which I am going to hopefully, yes. All right. So what we're looking at here is, again, this particular pre-trained BERT model has 12 layers. So we can go through and look at different layers. Distrap down is an important for most tasks. But that's, if you feed it two sentences at a time, it can compare across the sentences. But we're just going to look at one sentence versus itself. And then within each layer, what the transformer is, is it has these heads. We're going to just focus on one head right now. In this particular head, it does some math, and it decides what other words in the sentence should each token pay attention to. The CLS token is just a class is what it stands for. And it's just like an overall token for the whole sentence. But each word in the sentence is given a weight of how much it should pay attention to each other word. And then it uses that weight to update its own vector. And as you go through the layers, the vectors change. We're going to see how that comes out. Oh, an important thing here is surprisingly, the vocabulary BERT uses isn't that good, because it can't be a good vocabulary if it doesn't know the word taco. So it breaks it down into TA and COAS. As it goes through the layers, it's able to figure out that those go together and that they mean something important clearly. All right, so something interesting, we'll look at it here and then we're going to look at it for the other sentence, is just a few layers in. You get something that's kind of like the edge detectors where it doesn't look like much here, but it's just, hey, pay attention to the next word. The next word might matter. Use the weight from that. And so each of these words, other than the weird tokens at the beginning and end, they're just looking at the next token. Let's go back. So if we look at some more complicated sentences, we've got the chicken didn't cross the road because it was too tired. The chicken didn't cross the road because it was too wide. The dog fetched the ball. It was excited. And the dog fetched the ball. It was blue. Obviously the last word in each of those sentences changes what it means within that sentence. As a human, we can read this and say, oh, yeah, it's the chicken. It's the road. It's the dog. It's the ball. And the cool thing is, well, I'm sorry, I'm not getting into the cool thing yet. So, cross all those sentences that layer three, head one is doing the same thing. It's saying, hey, look at the next word. Most of the point of this is just to see that a head has a function. A head is doing something. If we look at layer nine, head five, it's a pronoun resolution head. This is kind of equivalent to that face detector. No one told this thing to learn to find pronouns just in order to understand language that was something that had to train. And so we've got it chicken. If we look, it's it road. It dog and it ball. Now, it's a little, some of them are a little distributed. It's not 100% sure. But that's really cool. It really is kind of understanding the language. So the other thing, all those heads add up on each of those layers to give us an output for that layer. To demonstrate this, I did a poll online where I just asked people to submit sentences. One sentence about learning using the word train as in teach. And then another sentence about travel using the word train as in the vehicle. I've got some code there. Again, so it's just, you know, we can extract the features. We're saying we want the output. Here we're going to start at layer zero, which is just the input embeddings that it gives before it does any travel through the neural network. And then I tie those labels back to the labels back to that output. And so this is just looking at one example of each of the words that people used. So they use train down here, trains, trained and training. And at layer zero, that's all it knows. It just has one thing for each word, one point. When we add all the other ones, there's a little bit of wiggle on them because it also takes the position within the sentence in the context. But for the most part, you know, these are all train, these are all trains, these are all trained and these are all training. And here the green means that it's teach and the black means that it's a locomotive. And so as we step through the layers, we can see that it actually does start to separate those by meaning. Except for this one. Wait, what's going on with that one? And when I first made this, it's like, oh crap, I'm going to have to find something else to use for my demo because I want to show it actually working. But then I went and looked up the sentence and again, this was gathered from random people on the internet. So that wasn't a sentence about a locomotive. It's not really even a sentence. But it was classified into teach because it's more teach than it is vehicle. All right. So we have a lot to do. Arbor is usable now. But it can be a lot better. Like I said, some of this is aspirational already that I wanted to show how easy it's going to be in about a week. It's already pretty easy. I've used it to build a couple of models. It works really insanely well. That just pulling out those features and then using those within XG Boost and it can do classification really well. Our goal is to have it up on CRAN by the end of this year, which is about a month away. So we're pushing hard on that. And towards that, right now it works with TensorFlow 1.13 in that range. We're going to upgrade it to TensorFlow 2.0, which just came out. That's in testing right now. And actually, Jonathan is back in Austin working on that right now. The other thing we want to do is we basically copied this code from the Google Python code. It's under the hood. It's got a lot of weird things that it's doing. And on the front end, it's not friendly. So we're working on that, making it more friendly. And then if you've worked with tidy models, we're going to put a recipe step in there to make extracting those features super easy. So you can just pull out the features of your sentences and use them within a model. We will have a poster at our studio count in January. And I encourage everyone to go to bit.ly slash arbertposter and give us ideas of what you would like to see on that poster. We might just use the submissions of ideas in our poster because that's text that we could parse. But whatever you might have, we don't know yet. We're not sure what we're going to show. There's lots of cool things that we could do. You can contact me. You might be shocked to hear that I'm on Twitter a lot. I'm at John the geek. That's J-O-N. You can go there, maybe like and reply and retweet some things. You can find our packages on GitHub at Jonathan Brett's GitHub slash arbert and slash arbert is. Again, those are usable right now. And you can just watch them every day. They get a little better. You can find my GitHub. I've got my forks of these. But I've also got some other stuff there. You can also find me on the R4DS online learning community at R4DS online. Most Saturdays I host an office hour there for just any random R questions you might have, not this week and actually not next week, but most Saturdays. And I also have a podcast, tidytuesday.com. Hopefully if he's still in the area, I'm going to be interviewing David Robinson and that will be up on Tuesday. But normally it's a podcast about visualization, which is hard to do sometimes, but I think I do okay. And I managed to talk very quickly and so I'm going to give you back a few minutes."}, {"Year": 2019, "Speaker": "Ronald Cappellini", "Title": "Defense Analysis \u2013 Organize for the Mission", "Abstract": "In this talk, I shared my experiences transitioning from a military background to a more relaxed setting and the insights I've gained working in defense analysis. I discussed the importance of assembling diverse teams with specialized skills in defense operations, emphasizing the need for expertise in operations research, war gaming, and data science to conduct effective campaign analysis. I highlighted the role of campaign analysis in the Department of Defense, which involves using complex models and simulations to inform decisions on resource allocation and strategic planning. The process requires integrating subject matter expertise and operational insights to build credible and efficient analysis models. I concluded by outlining the importance of having a strong leadership figure to guide these efforts and the necessity of maintaining a collaborative approach across diverse skill sets to achieve meaningful and actionable outcomes in defense strategy.", "VideoURL": "https://www.youtube.com/watch?v=JdwfzYU0t0w", "id0": "2019_23", "transcript": "Thank you very much for having me today. So this is my first time doing a DCR function and I'll tell you what, it's been absolutely fabulous. Coming from the military side of the house, it's definitely a little bit more stuffier button down shirts, suits, ties. Feeling like I should be in board shorts and flip flops versus a full military uniform is actually an outstanding experience. But that being said, I'm going to talk a little bit today, kind of a little bit more squish year and not as many technical as the speaking opportunities prior to me. I'm going to focus on a certain type of defense analysis and the teams that I have put together to try and get after it. How I'm going to portray what I think is the right way to do it isn't necessarily a standardized way, nor is it really the views of everybody in the Pentagon or in department of defense. I feel I've had great success doing it this way and I'm just going to talk about the complexities of it and then some of the issues that we see within the department of defense of trying to actually implement it to do really good analysis. First thing I'm going to ask you, I'm going to pose a question to the group. Say you have a heart attack and you get rushed to the hospital. Do you want that doctor immediately to start operating on you? Well I see everybody saying how do I know that that's the right kind of doctor? Is it a pediatrician? Is it an eye doctor? Is it a foot doctor? Everybody has a specialty and everybody has a role in being able to do the best work they can for the common good. Analysis should be looked at the same way. Not everybody brings the same set of skills to provide the same part type of end product and we need to know how to organize and operate to be able to effectively do that mission. In the Navy, data science is kind of a black box. It's just starting to emerge but you're hearing it touted widely and loudly from the most senior executives within all of the services and in the department. What they don't realize is the military has done operations research which incorporates parts of all of the data science fields, mathematics, statistics, but then merges that with really the war fighter and in his subject matter expertise on weapon systems to try and better basically use how we fight and how we win and succeed in combat. The trick is though that makes us a little bit of a generalist. We know a little bit about a lot but not a lot about everything. What we really need to do is make sure we maximize who we have participating in our analytic efforts to really get the deep subject matter expertise and provide the best product that's got the right rigor, the right level of detail and that just plainly our assumptions and the way of doing business is the most efficient, elegant. We put the right weapons on the right target at the right time by the right platform against an enemy that's trying to stop us from doing all of those things non-stop. The work that I've been doing probably the last seven years or so is a type of analysis called campaign analysis. People have a tendency that when they say that word they think it is one singular tool and one singular modeling program called Storm which is basically the synthetic theater operations research model. It is a very large clunky stochastic model that runs thousands of times with about a million lines of code that what it does is it aggregates every single dimension of warfare from cyberspace, air power, naval surface power, ground, undersea and it incorporates both the blue forces which is US, the red forces which is whoever your adversary is and then potentially if the sponsor or the studies designed to explore with the contributions from allies and partners would be. All of a sudden you've got like a three large multi-dimensional problem and this thing gets ugly very, very fast. What we're trying to do though is create a way that is repeatable, tractable and I think we saw the word earlier credible so that way we can inform the war fighter to figure out how to best fight his force and then two to help inform the budgeteers and kind of more importantly the folks on this side of the river over here on the hill when they say to what buy more submarines or do I buy more strike fighters you need a way that you can post to them why you're deciding what to do and how much of it to buy. You're talking about billion dollar decisions and there needs to be a way to understand and analyze how those things all interact together so you can actually procure what you need to procure in the most efficient means possible. Now with that being said I'll caveat right once it kind of goes into the bureaucratic gankulator all fair right so who's got what going on or who has what you know rice bull that they want to protect or what not this is where you can kind of really go sideways fast. Our job is to provide that decision maker though with the most unbiased decision making tools and knowledge to help him inform that decision. So I mentioned a little bit about the complexity. So you kind of have you know blue red green each one of those areas has a large trade space there are hundreds of variables within each one of those those domains everything from every single aircraft every single ship every single ground unit you can think of as an entity every one of those entities has a host of attributes that are assigned to it everything from how it sees the environment how it interacts with the environment how fast it is how slow it is how long does it take to burn its fuel how does it see the enemy then it go you can take it a step further what weapons are on board that entity that are associated with it how do they see the enemy how do they shoot at the enemy how many of them do you have and all of that has to happen in a geography at a space and point in time. So now you start overlaying all those variables all of the distributions of variation on every one of those variables and try and build a black box to be able to tell you we will beat you on day five with this force that's never going to happen that is not the intent of this analysis the end of the day this analysis is merely designed not to be predictive but to be quantitative in a way that allows me to show and create relationships about orders of magnitude I want to be able to show that if it takes ten things of me to do something to you it takes you five things to counter it it builds relationships to understand how we are interacting with the adversary and then it also creates conditions about which how our force operates so if I want to do submarine warfare very close to an enemy's operation area I may need to do things in that operating environment to create the conditions for those submarines to operate so it starts to build out quantitatively and understand where I first order second order third and so on conditions and those relationships defined by the math right of what needs to happen when and where in time to help us kind of shape and organize by the time you get done in the gonculator here you have a huge problem one of the things that we'll see and I'll talk about in a minute though is how do you figure out where to start from there's so many variables so many combinations and you've got a limited amount of resources there's no way combinatorially I can actually do all the variations and all that space right if I ran a design of experiments I could take everybody's computer here give you about a thousand course and give you to the end of all of our lifetimes and that thing would still be churning away so Naval Postgraduate School has done a lot of work and they have come up with nearly or thought in a Latin hypercubes to try and create space filling design of experiments but even with that you really can't get after the amount of trade space we're talking about here the other part is a lot of that trace trade space is operationally infeasible so now you've got to think about from the operational perspective but not only the combinatorics combinations so just because the math says we can put these things together space and time doesn't necessarily mean operationally we would want to do that so there's this now you're starting to integrate this subject matter expertise of your operators into this mix so the process I made it as cartoonish as it can be because I'm usually talking to general officer and flag officers I'll leave it at that so with that being said on the right hand side here or your left hand side you will see 14 or so different topical areas those are kind of big high-level warfare areas and you can see it's everything from air for fair down to logistics so how do you get you know ripits and diacoke and you know all the necessities to the warfighter oh by the way you might want gas to and weapons right to be able to do all this in space and time so we need to take subject matter expertise folks that know how to do this and have operated in this world to kind of enter into the equation so there's there's ground rule one right we need people that have subject matter expertise we then feed into this middle area here and this is what I call concepts of operation development this is where we build the tactical level understanding of how we organize and operate the force to do a particular mission set and I say mission set it can range in complexity from literally me picking up a rifle and shooting something like at the backside of this room that's the most tactical instance you can get very singular dimensions not a lot going on one-on-one to much more complicated scenarios where now I have literally thousands of things competing at the same time for space all firing all seeing and doing a lot of things so there's a whole lot going on there and I need to build those organizational structures in rules of how do I want to employ my aircraft carrier do I want it to sit at a thousand miles away from the enemy well when do I move it from a thousand miles to 500 miles and when I move with the 500 miles do I want it to run 24 hours a day or not 24 hours a day should I put am around any 7 am 17 is on it or should I put more surface to air missiles or surface to ground missiles on it so there's a lot of different complexities that need to be happened and these operational guys start to to start to kind of mold and build that plan out at a very high level we then take that plan and we throw it into to a war game and so this is where now we can iterate a little bit faster we don't have to deal with the coding but you can put them in literally in front of a a paper map and say if you have this and you have a red adversary and your red adversaries is doing it too like how would this play out it's just it's a way for us to kind of walk through at a high level very quickly these concepts of operation so this is a highly iterative process and you can go as as deep as you want or as shallow as you want you can really tailor it to to the work you're doing from there we've also got to do a whole bunch of modeling and simulation work we've got work that happens at the the phenomenology level right so you've got radar systems you've got sonar systems where you need to understand the literally the engineering level phenomenology of the environment you're operating in how far can I see a low observable stealth aircraft you know with a particular type of radar you need to understand all of that work and you have to model that in a whole different range of scenarios to be able to develop that base level of knowledge after that you've got to start doing engagement level knowledge this is where I've got say a joint strike fighter versus a an adversary stealth aircraft and they're literally going mono-e mono in the air and you need to understand how the entire weapon system of one at adversary person works against the the other pilot and so there's a lot of that goes on there from there we go into the mission modeling this is where you start to get a little bit more complicated if it wasn't complicated enough already right this is where now I've got multiple weapon systems maybe from multiple domains and when I say domains I mean maybe you have an aircraft a ship in ground forces all fighting an adversary that have ground you know surface and and air force as well but it's in a very constrained environment so maybe it's like literally over Washington DC right you've got this little bit of engagement going on and you can control for the number of variables and the number of a number of entities but once again it can grow combinatorially very complex very quickly from there we move into combat modeling and simulation at the campaign level this is where it gets really complicated this is where you try to look at aggregating everything from the layers above and all of the stuff from the left and incorporate that into one modeling and simulation environment now you're compromising some fidelity when you do it but what you're doing is creating an environment that you can create in a stochastic way thousands of replications of this interaction of these large force on large force engagements and from that I can infer a whole litany of things which is one of the areas where I think we're actually starting to fall short a little bit and I'll talk you to you in a minute so when we're done there we come down into this campaign analysis and really the way this should be represented it is not a discrete block at the end campaign analysis is actually operations and analysis that's happening throughout this entire cycle you're continually learning you're continually refining and that all goes into your final set of analyses so how do you do this right so like on the left hand side here you get navy seals they're very organized they're regimented it's the team providing the real horsepower to be able to carry out the mission on the right hand side you got a navy strike group with air force aircraft and navy aircraft all together to be able to conduct the mission so it's this organized structured format but with diversity right so we don't want a singular type of person a singular type of skill set but we need to figure out how to bring organized structure and diversity together to be able to find this product and end result that's that's useful so who do we need first we definitely need Marlon Brando the godfather is key right and so that i was going to put Jared's picture up there um and especially with the new york datum office shirts i thought it would have been priceless but you need somebody that who kind of sits at the top and helps you as you're building out the war games as you're building out the con ops as you're doing the model and simulation that helps you understand should i go left or should i go right and then what are the bounds i can't underestimate how important this position is and i can't tell you how often i see analysis when in the department of defense that it's not realized that this is needed i see it all the time where i think i see people it said well he's an oar or a data scientist he can do that study director thing too when that is a very unique skill set you need to be a project manager right you need to be able to schedule you need to understand the operational side of the house there's a lot of different things that kind of go into that you know that chicken soup and so if you think that anybody can do it i think that's a fallacy and so this is you know requirement one in my my seal team here is i need a marlin brando i need a godfather somebody can go kiss the ring and say boss i want to do this does this make sense and this does this align with all the other disparate pieces of analysis that are probably going on simultaneously you need that second thing you need the warfare operators without these guys it's utterly impossible to do this an example i have was i had a an absolutely incredible incredible data scientist working for me she was extremely proficient in sass so that was kind of her her medium right and it was like if you're comfortable in it and you could produce the results i need knock yourself out the problem was she came from a background where she had zero operational exposure when you said aircraft carrier she didn't know if that was a b52 airplane right it was to her it was it was numbers it was it was data in the uh in the data warehouse so that was all it was to her it wasn't until i linked her up with the warfighter when she was actually doing the post-processing analysis was she able to actually kind of start to tease out that the the real critical insights that was in the data the data she was showing us in the analysis she was showing us prior to that was was almost useless and insignificant she was making connections with in ways that were irrelevant right um and so it was only through the the instance of incorporating the operator into her kind of cycle of learning that that really made that difference the next one which people take this for granted all time it's it's the war gaming side of the house this is a real skill set i mean if anybody who's tried to put a war game together and facilitate it and execute it it's it's a skill set kind of unlike anything else and i can't tell you how valuable it is to be able to create a a strategic dilemma or an operational dilemma for a set of individuals force them and their behavior to answer the questions that you need and in a format of data that you can then use to then go create a statistical model or a simulation environment is absolutely key so this was the next part of the puzzle that you need then you need you know Matthew Broderick right you need the guy who can sit down a computer terminal and pound away on the keys and you give him Doritos and some Mountain Dew and he just let him rock out right but with that being said there's a danger in that because even though you can hand him a a road map of how you think you want the fight to be incorporated in the computer i guarantee you within every 30 lines of code he's going to come up to a place where he's like hmm is i can't instantiate this explicitly i need to create an implicit way for the program or the model to understand and operate and depending on his level of understanding of that operational environment and of the intent of the warfighter that can go sideways real fast so this is another place where that warfighter needs to be literally left seat right seat over his shoulder explaining here's kind of what i want to do on the battle space how do we figure out how to make that happen inside that computer screen right it's like Zoolander that you know the files are in the computer kind of thing you know we kind of have to make that happen here the next part the next person i'm going to talk about is the data scientist if you wanted a different headshot you should have gave it to me so with that being said this is another person that needs to be involved from the beginning because at every step of the way you're trying to learn something and data is part of that understanding the operational context and the key critical questions at the beginning are paramount so this is i can't underestimate how important this this part of the puzzle is and i'm speeding up here because i'm running a little bit long the final part is mr rogers right this is that that person that's been involved from day one of the study understands the intent that the study director has had and then what all of the pieces along the way were and really understands those key conclusions back to the strategic insights that you're trying to communicate because every senior leader needs to be able to be understand comprehend what you did why you did it and how is it important to him this is the guy that wraps that all together 30 more seconds so when it all fits together you kind of form this team of the director the operators all the way down to the storyteller and it's really a constant dialogue between all of them with that said i want to say thank you extremely i'm extremely proud to have been here today Jared mark the whole team thank you so much"}, {"Year": 2019, "Speaker": "Danya Murali", "Title": "Renewables, Reproducibility, and RMarkdown", "Abstract": "A dive into how the data team at Arcadia Power combines databases with RMarkdown to conduct reproducible research about renewable energy and conduct business analysis. The talk will discuss how to organize your RMarkdown chunks for ease of readability and flow, how to begin incorporating RMarkdown into an existing team, and why we should all care about reproducibility.", "VideoURL": "https://www.youtube.com/watch?v=KHL6YiagHGc", "id0": "2019_24", "transcript": "So, hi everybody. My name is Danja Morale and I work at Arcadia Power and we are a sponsor of this conference and you probably have been hearing us a whole bunch of wondering who we are so I'm excited to tell you guys who we are. But before I start I want to tell you another story of how I got this job. So last year I was sitting at this conference in the audience like all of you all. And this guy, Max Richmond, who is actually sitting right over there came up and gave a talk. And at the time I was sitting in the audience and I was in a place of transition. I had just finished graduate school, I had been working at the Department of Energy for three years and at the Department of Energy I was forecasting U.S. crude oil production using SaaS. And so at this time I kind of wanted to have a different job and there were two things that I really wanted in my next job. One, in grad school I had learned that R is amazing so I really wanted to be able to do analysis in R. And two, I wanted to not work in oil and work in renewables. And so Max comes up, starts giving this talk about this cool company called Arcadia Power and starts telling us all about how he actually uses SQL. He didn't really talk about R. But he threw R into it a little bit and I just assumed that they use R. And I was like wow, I want that job. So just like Jared's been telling all of you guys, if you go and say hi to someone, I don't know, you might end up having a job. So here I am a year later. I work at Arcadia Power, I love my job and I'm really excited to be here to talk to you guys about renewables, reproducibility and R work down. Let's jump in. So that warm and fuzzy introduction, let's talk about climate change. So this is how I feel about climate change. It's fine, it's great. So I think one of the issues that I personally have with climate change is that it's this huge existential crisis. It's like doom and destruction is coming to get you and all of us as individuals, it feels like there's not very much that we can do to resolve it. How Elizabeth earlier talked about you could reduce your meat consumption. So that's one thing. But when it comes to your electricity, we are a society that depends on electricity. We have to use electricity and unfortunately most of our electricity currently is sourced from fossil fuels, which are putting carbon and methane and all these wonderful things at the atmosphere. So we're eventually going to have us all die and that sucks. And so it sucks. It feels like you don't have a lot of agency over this huge issue. So that's something that Arcadia Power is trying to resolve. So we are kind of in the business of giving people agency over their electricity power decisions. So we do this by giving you access to clean energy. And we do this through matching your usage to renewable energy credits, also known as REX. And we believe that choosing clean energy shouldn't be something that causes you to have to pay more money. It shouldn't be a premium thing. It should be something that everyone should have access to. So with that ethos, we match 50% of your usage to renewable energy credits. Completely for free. And then to get to 100%, we have a small premium. And that's one thing. Another way that we help give access of clean energy to people is through a community solar program. And we do that. The community solar program is important because a lot of people, like I'm sure we would all look to have solar panels on our roof, but the reality is they're very expensive and a lot of times we are renters or we live in places where the sun doesn't exactly shine in the right part of our house to be able to put a solar panel there. And so instead, to get access to solar energy, you can get access to community solar projects through virtual net metering. And also by doing that, you can lower your energy bill by getting credits back on your bill for what those solar farms produce. So we give access to customers through that, completely for free. And then the third thing that we do, which is not strictly about clean energy, but it falls under the idea of energy shouldn't be, clean energy shouldn't be something that's more expensive is we have a smart rate program. And with the smart work program in the regulated states, so most of the northeast, where you have the option to choose your energy supplier, we kind of work as a broker to help you find the lowest rates to your energy. So you can lower your power bill and also try and get clean energy. So it's a cool business, it's a cool software, we're sort of like in this weird in between place where we're not a utility company or like a tech company, but we're like trying to solve this big energy problem. And it's super easy. You can sign up in two minutes, that's my plug. Moving on. So I work on the analytics and data science team at Arcadia Power. And our team has four main roles. We do a lot of the operation support, so that's like supporting the product team and the marketing team and any of the operational teams and making sure they have all the data access and data needs met to it from the business. We do a lot of business analytics for keeping track of things like attrition and how many wrecks people up by and just big, big picture business things. And then we do a lot of market research. And so that's something that I personally work in quite a bit. The energy industry is this huge thing. When people talk about the grid, it's like, what is the grid? It's this big thing. And so we spend a lot of time trying to understand the energy industry and understand where we fit and where the need is and how we can meet that need. And so along that, we also do a lot of prototyping of different products and new products before they're put into production to make sure that we're meeting the right customers and getting them the things they need. So we do this using a variety of tools. Some of them are right here. So we do a lot of our data warehousing is done in Amazon Redshift and PostgreSQL. We used Tableau for a lot of business analytics. I would love that to be like R. Maybe one day we'll get there. Unfortunately, you can't ever escape Excel. So we do use a little Excel. And then we also use, especially for a lot of market research stuff, we use a lot of R and a lot of R studio. And so the reason I bring this up is on as a data scientist and someone that works on a data science team, having a workflow or a set of analysis that's reproducible is something that we really care about. So what is reproducibility? When I think about reproducibility, I think about it in three different perspectives. So the first one is the philosophical perspective. And this is something that I think we all talk about. It's like the idea of finding the ground truth in whatever your respective fields and whatever you're trying to do. And the scientific method is based on this philosophical idea of reproducibility. So that's great. We should aim for that. But something I think we don't talk about enough is operational reproducibility. And so by that I mean the idea of having a workflow that is optimized and scalable and easy to iterate on. And having that really reduces your need for a lot of institutional knowledge of how processes work and it makes it very easy to pick up your analysis and give it to different people, which is really great for a business or for research or whatever you're trying to do. And so related, I would say reproducibility on the business perspective is kind of the idea of being able to make smart decisions using the power of hindsight. So if you are a data driven business like we call ourselves, sort of buzzwordy, we are making decisions based off of our data and based off of our analysis of the data. And if it's really hard to reproduce that analysis and you're making big like monetary business decisions based on that, you're kind of leaving a lot of money on the table if you can't reproduce that. So it's really important to have reproducibility from a business perspective as well. So I'm going to speak a little more to the operational reproducibility part. And so one of the things that I think we all struggle with a little bit is operational reproducibility. One of the reasons behind it is because of the traditional workflow for analysis. So traditionally, you kind of have different pieces of software to do different parts of your large data science lifecycle. So you might do, if you want to get some data into your organization has databases, you might use Redshift to do all your data querying. And then you might pull that into Excel to do some data cleaning and maybe some exploratory graphs and then like, oh, I want to build a model. So then you're going to go throw that to R, build a model, and then eventually your boss is like, I want a deliverable. So you're like, OK, I'm going to take all of that stuff from Redshift and Excel and R and put into this PowerPoint. And somewhere along that process, you make a valiant attempt to document what you're doing. So you do that in Word. And then you get to the very end, you deliver your results and your boss is like, can you change that one thing? So now you're going to go all the way back in this long linear process and figure out exactly where you did the thing and then how does that affect all the other pieces in your workflow? And while you're doing that, you document things just becomes like a big pain. So you kind of stop doing that and then you give the answer. And then a month later, your boss is like, hey, can you do that thing again? And you're like, oh, I don't remember how I did that. So that is a traditional workflow. Now, an alternative workflow is the arm markdown workflow. So when I think about the arm markdown workflow, I think of it more of like spokes on a wheel. So instead of it being a linear process where each bit kind of depends on the other, it's sort of more like everything happens in one place. And when you change one thing, it can kind of naturally change all the other parts of your full data science lifecycle. So why are markdown? Sort of the sunrise. All stages of the data science lifecycle. I keep saying that. All right, one place. Documentation happens along the way. Super easy to automate. You guys probably have also heard a lot of this like Mallory and Elbegett yesterday talked greatly to this. So I hope I don't sound like a broken record. But still, it's great. It simplifies collaboration. It reduces dependency on institutional knowledge. And you can have multiple different outputs with little pain. And so when you have all of those things together, you get reproducibility. So this might be a little elementary, but I would like to just kind of go through. How to create a very easy arm markdown to show you that it is so easy to do. And it takes very little kind of like skill and knowledge. You can kind of, if you know a few commands here and there, you can build your own arm markdown. And maybe you can go back and start building arm markdowns for your analysis if that's not something that you're doing already. So this is an example of an arm markdown that I built for an analysis that I'm still in the middle of doing, where I'm trying to identify predictors for residential electricity usage. And so sort of like are these demographic things? Like what makes a person's electricity bill or electricity usage the way it is? So I'm doing this using the Energy Information Administration's residential energy consumption survey data. And I've gone through mostly the entire process. And so I don't really care about what I'm trying to show you. I kind of want to show you how I did it. So in this arm markdown, you can see that it's really pretty. I like that. You can have the data that I'm using. You can show it. And Mallory showed this yesterday, where it's interactive and you click buttons. It's all there. You can embed graphs, which is really cool. All of the text where you're just describing things is all in that document. My personal favorite thing is the floating table of contents. When I discovered how to do this a month ago, I was like, oh my gosh. This is life changing. And the other thing that I think is really cool is just the numbering of sections. There's ways to do this automatically. You might already know a lot of this. And then Ggplot is my favorite. You can standardize that very easily. I don't know. You can do things like tabs. And you can see that all parts of my analysis are all in one place. It's really nice. It's easy. And when I want to make a change, I can just make the change, click knit, and then the whole thing populates again. So it's great. So going into some specific features, how I did that, so first formatting. So this, when you're creating an arm mark down, this is what kind of the beginning looks like. You're going to give your title, your subtitle, if you have one, the author, that's me in this case. And you can define your document format. So there's the HTML document. I like using HTML documents a lot because it's really easy to do interactive stuff and it's like it's cool. But say if you want a PDF output or if you want a word output, you can do all of those things just by changing that one little thing that says HTML document. The floating table of contents is so easy, all you do is use the to command in your header and you make that true. And then you get table contents. And then you can make a float on the side just by putting to the underscore float equals true. So super, super easy, super cool. This number of sections is in the table of contents on the side, you can see how those sections are numbered. And the cool thing about that adding that thing is that when you unadd sections to take away sections, the numbers will automatically update, which is just nice. Dynamic table displays are really easy. If you just put this dfprint equals page command at the top, then every table that you print, you don't have to even use the nitter cable, any of that stuff, it will just come out in this nice format. So that's cool. And then finally, the document theme, you can put all these different themes. I like using the boot swatch themes. There's so many of them. I don't know if you've ever gone here, but there's a lot of really cool themes here. And I like to go in and you just put the name of the theme inside of your markdown and then you have your theme. And that's cool. And I learned a lot of this from our markdown, the definitive guide, which was by Yiwishi and JJ Allaire and Garrett from Alamond. And it was so easy to learn just by reading this. So I highly encourage you to go look at it if it's something that you, if armworkdown is something that's scary and you haven't tried yet. Great resource. And then also boot swatch themes are cool to play with. So when you're creating your armworkdown, so there's all the formatting. You can make that nice. You can do all of the writing inside of it. And then you can also create your code chunks. So that's where this is really powerful. Creating a code chunk is super easy. You define them by these three back ticks. And if you are going to do it at R, you can put R. If you're going to do Python, which I learned yesterday, apparently you can make that Python and then do Python inside of it, which is really cool. I, when I'm creating markdowns, I like to put all my libraries that I'm going to use the analysis all inside of the setup chunk. And I do this so I can kind of keep track of all the libraries that I'm using at the end of my analysis. And oftentimes when I am doing analysis, I'm like, oh, I need to use this other library. I'll go all the way up to the setup chunk and stick that in there. You can set global chunk settings. And so doing that, settings are like echo equals false. And so if you set echo to false, then when you knit your markdown, you won't have your code output, but it will execute. So that's nice. And if you want to show your code, then you can make that true. So that's cool. And then another thing I like to do is standardize my ggplot theme, which you could use this as a standard theme. I like to mess with themes. I like to make things pretty. So if you use the theme set command at the very beginning and you set it to the theme, then all of your ggplot's moving forward will be of that theme. So you won't have to like theme set in each plot. So that's great. And lastly, in my setup chunk, I like to standardize my numerical output. So I like to make sure that there are no scientific figures and that my significant digits are limited, so I don't have big crazy numerical outputs. Text, I just want to show you how easy this is. Like all what's on the right is what is on that side. The left is the code and what's on the right is the output. And so when you put text, it goes straight into, it comes out exactly the way you write it. It goes up in your table of contents. Exactly the way you write it. So it's really easy. It's really dynamic. Embedding images is super easy. You just create a chunk and use the include graphics function from the neural package and it will output as the exact image that you hope for. And then also doing the simple things like making bolded lists, like that's really easy and it's very intuitive. So that's one of the reasons I really like the text part of Markdown. And then the other thing I want to talk about a little bit is the main code chunks. And so with the idea of having a really reproducible analysis in an organization, I would say it's a good idea to sort of like talk with your team about how you want to structure your code chunks. And so the nice thing about Markdown is that you can have both your back end and your front end kind of in the same place. So the main back end chunks that you would use is your database connection if you're doing that or your data import query if you're doing a database, any sort of data cleaning. And then you also have all of the front end chunks with your exploratory data analysis if you're doing any sort of model and then your results and session info. And I think it's good to just talk about this. This seems really, really obvious. This is how anyone really do analysis. And I think saying this is the way we're going to do this and kind of setting that in an organization is a really good way to make sure that you have the most optimal workflow. So if I do an analysis and I take my analysis and I go hand it to Max, for example, he can look at my Markdown and know where things are going to be and know how to expect my analysis to flow. And I think that's a good thing to do. One thing I would really like to encourage you if this is something that you use. If you use databases in your organization, connecting your database directly into your Markdown is really powerful and it truly makes your Markdown the most reproducible it could possibly be. So you eliminate, in addition to eliminating things like moving, like, handing graphs and analysis in different places, you eliminate the need for handing over your initial data, which is really cool. You can throw in your credentials the library, the RODBC library allows you to do this. You can very easily use the driver connect function to connect your database. You can use the SQL tables function to list the tables in your database. And then you can query directly into inside of a chunk and output that into a data frame and then just use that data for the rest of your Markdown, which I think is awesome. And finally, on the front end, you guys already know this after listening to this whole conference. Like you can do lots of, you can make these plots with Ggplot or whatever you'd like and put it directly inside your Markdown. And I love this so much more than I then excel or other point and click software because when you want to make a quick change, it's so easy to do. And then lastly, the session info. This is a command that I like to put at the end of all of my Markdown. It kind of just tells you all of the operating system and all the packages and all the things that happen in the background that you used to make sure you did your analysis or that helped you do your analysis. And I think this is a good thing to do just to kind of keep track of what you're using. And since, I mean with R, since so many people are making different types of packages, sometimes you lose track of what packages you're using. And so this is just like a good way to document that. So takeaways, climate change sucks. Renewables are great. I don't know, sign up for our KDA power. Prioritizing operational reproducibility will improve your organization. And I think if this isn't something that you're doing already or this is something that you want to do, or Markdown, is a really great tool and it really empowers you to be a better analyst. Thank you. Yesterday I decided that the time was ripe to finally make a professional Twitter. So now I'm data Danja. My first tweet had a typo in it, so I'm doing really great. But yeah, feel free to read that to me. Thank you."}, {"Year": 2019, "Speaker": "Kaelen Medeiros", "Title": "Metric Design and Dashboarding with tidymetrics and shinybones", "Abstract": "Business teams are kept accountable to a success metric, but designing an appropriate one is hard. After deciding on a main metric, how do you make sure that you\u2019re not accidentally optimizing for an unintended outcome? As a data scientist, how do you code the metric and quickly share its results across your company? In this talk I\u2019ll outline the principles of good metric design with a practical demonstration of how to create a North Star Metric, which measures the value your product delivers to users, including discussion of how to balance your North Star with safety metrics to guard against side effects. I\u2019ll show how to use two new open source R packages, tidymetrics and shinybones, that make coding metrics and tracking them in a dashboard easier than ever before. After this talk, you\u2019ll have a process you can use to design success metrics, balance them with safety metrics to keep you and your team honest, and easily code and share them.", "VideoURL": "https://www.youtube.com/watch?v=BL5NBRxnl3E", "id0": "2019_25", "transcript": "So I'm Kaylin and this is Scully. You're probably unsurprised to find that out. I am the moulder of the family. That was very clear from the beginning. So that's us. You can find me on Twitter. I know my name is kind of hard to spell. It isn't the program though. GitHub and I'm a data scientist. I have a master's in bio stats, but I've worked in health companies, worked in a map company, worked in EdTech, I'm all over the place. But I love data and I love aliens. Both things. It seemed like the right place for it. So here's what I'm going to walk you through. I'm going to talk about North Star metrics. If you're not familiar with that product thing, we're going to talk through it. Talk about metric design principles because I have some ideas about metrics that I want to share with you. Then we're going to talk about how to create a metric in R with tidy metrics. Specifically, we've got some cool functions and some documentation. It's pretty neat. Then we're going to turn our metric into a dashboard with shiny bones. It's very fast. It's very quick. It's going to be a lot of fun. So a North Star metric is, if you Google this, you're going to get growth hackers, stuff like that. The metric that defines is that one metric that defines how your product delivers value to its users. So I put some examples here. For Airbnb, it's night's booked because that's the thing people are coming to Airbnb for. You've got Facebook. They originally used to use a daily active user's metric, but now they found that a better metric was number of users who add seven friends in the first 10 days. That's because your North Star metric should be something that helps drive retention and it should be at the very least some kind of proxy for profitability, for your product being a success. So again, North Star metric, one metric. I'm going to abbreviate it as NSM the rest of the way through this. I'm big on a breeze. Here's the problem though. The North Star metric, for example, if it were a number of questions asked, dramatic foreshadowing, this can easily be gamed by driving users to your website. So if I just put a bunch of paid ads out being like, I have this website where you can come ask questions and I do that forever, I'm just going to drive traffic in and I'm always going to have this perfect North Star metric and what a successful product I have except for all the money on ad spend I've spent. You see the problem? So sure, fine, whatever. I'm just going to gain my North Star metric. No friends. I'm here to tell you that we can do it better. We can account for long-term retention and we can keep ourselves honest about those spikes in our North Star metric with metric design. This is Maryland's Strathorn summarizing good heart slow. You might have heard this. When a metric becomes a target, it ceases to become a good measure. So if you think about it, we set a metric and then we say, this is the thing that says that this is success of our product. And then we start doing things to make our product successful. So we're making the metric better and better. That's why I want to argue we need safety metrics. This is what I call them. Some people don't like this name. Some people want me to call it other things. It's what I'm calling it for today. And they balance out the North Star metric. They're going to guard against you gaming it. Artificial inflation, the decisions you make that influence the metric. Not entirely. Like, nothing is perfect. But at least it'll give us a system to really think harder about what does this success really mean. And while this really comes from product, I hope these design principles could be carried out into whatever industry you're working in and hopefully you can use it in what you're doing too. So I'm going to use the example. I'm actually also going to use the Stack Overflow R questions data available on Kaggle. I'm specifically going to use the questions data set. I understand that perhaps the real value of Stack Overflow is getting your question answered, but I wanted something that users were actively doing. So asking a question. So that's how they're deriving value from Stack Overflow. They're asking questions, getting answers, and gaining knowledge. So this number of questions asked, that's our North Star metric. So let's break this down mathematically. We could break it down where the number of questions asked is the average number of questions asked times the number of active users. Where the average number of questions asked is the number of questions asked divided by active users. I can also break number of active users down. That's the number of users times the percent of users who are active. They've asked a question. So if I break down my full North Star metric, I actually end up with five different metrics that I can calculate that'll help keep me honest. So if number of questions asked is our goal, that's great. We do want people to be asking questions on Stack Overflow, but wouldn't it be great if the average number of questions asked per active user was also increasing over time? People are coming back and they're doing more and they're getting more and more value out of your product. I think so. So these are the metrics we have to calculate. We'll calculate the number of Stack Overflow, our users. Kind of. You'll see in a second. The number of Stack Overflow, our questions, then the number of active Stack Overflow, our users active, like a kind of alluded to the definition here. They asked at least one question in the time period. The percent of active users and the average number of questions asked by the active users. But now you may be, you may be, Scully, you may be, which I assure you the Scully in my life does all the time. You're like, Caitlin, this is a lot of work. How do I do this? Is it fast? Is it easy? It is. Tidy metrics. Let's bring this in. This was developed by Ron Noth fighting with on and Dave Robinson, who unfortunately had to bolt, which sucks because I had a great joke about how I actually came here just to teach DROB how to do something he wrote himself. And he's gone. So you guys got it without the benefit of him getting it. You can get this on GitHub for right now. And it allows you to create metrics in a tidy way. We're big on that in the top by DROB household. So the big thing this introduces, which I really love, which I'm going to walk you through, are these cross by functions. Specifically, cross by periods and cross by dimensions. And this is going to help you as we're moving toward dashboards. So cross by periods, the stuff in quotes comes directly from the documentation because they said it way better than I could. It allows you to take, you're going to need a variable called date. So a lot of times, whatever data you're using, you're going to have to mutate, turn it into date. You're also going to want to strip the time stamps off. You want it just to be year, month, day. And it's going to accept periods, date windows, intervals of time. And it's going to create all of the combinations. It'll create your summarized output across all of the combinations of those. I am going to go through an example. So don't worry if you feel lost. Then if you compare this with cross by dimensions. So this is an extended group by, just like cross by periods. When you take your dimensions, you need some kind of categorical variable. It doesn't necessarily have to be a factor, but you do need some sort of leveled variable with strings, that situation. So if you're familiar with using the tidyverse, you'll often be like, okay, I got a data, I'm going to mutate, I'm going to group by, I'm going to summarize. These cross by's can replace those group by's. They're extended group by's. So let's look at an example. Let's say we have the stack overflow, our questions example. Where we've got the owner user ID, we've got a creation date, and we've got a tag. And then we've got these five things I said we're going to calculate to build our North Star metric and to turn it into a dashboard. The number of users in theory, this is how I would calculate it. Of course in this data, because it's actually already active users, it's okay, here's the people who asked our questions. This is what the code would look like. I cross by the periods, I have a variable, I realized I forgot to show you guys. The uppercase is just a list of different periods. Here I think I cross it by day, week, month and quarter, just to really demonstrate the power of this. I'm only going to cross by the dimension tag. I took the questions data and I pulled like six random tags that appealed to me. So I think in here we've got like Deep Liar, Ggplot2, package, data.table. I just pulled some tags. I was like, there's a lot of them, let's do a couple. And then I want to summarize the number of users. It would be the number of distinct owner user IDs. But I know all these people asked the question, we're going to fake this in a second. Bear with me. So separately, I want to calculate all my engagement metrics. So this is, you know, people actually engaging with Stack Overflow. One thing that's really helpful is to turn that questions data I showed you two slides ago into what I'm going to call by day questions. This is a standard that Dave taught me at DataCamp. We often use by day. I've aggregated the data by day and then questions tell me what it is. It's just a really cool naming convention that I've carried over into my own work. Helps me keep things clear and straight because naming as we all know is hard. So this gives us every question asked per tag per day. So you can see some of the tags I actually pulled. And here's how I would calculate the engagement metrics. I have the by day questions, my cross by my periods, my tags. I'll calculate number of active users. And then my number of questions asked, just get a sum. Ungroup at the end of everything. Sometimes I don't ungroup and I literally will sit up in a cold sweat at home and be like, I think I did an ungroup something. I'm sure it's ruining my life somewhere. So just do it. Just ungroup. So that'll turn it into this. This is a metric. This is what a metric looks like. We have our period, day, the actual date, which in this case is the actual days. Here's our tags. And you start to see now we have this all tag that didn't exist before. This is what cross by dimensions does. It takes every combination of those tags and it calculates individually across. So you can see on 2015, January 1st, we had eight active users, nine question asked. We had two in the data.table tag. We had two in the ggplot2 tag. So it creates that all for you, which is going to be really helpful for dashboarding. We'll see in a second. Like I said, I faked the number of users. So I just randomly inserted, okay, here's the number of people who could have asked a question about R on Stack Overflow. DROB, of course, told me if I'd gone to Stack Overflow's website, I could have gotten used data. I should have asked him first. That's another lesson. One thing that's going to be critical here, active users any. I want to pull the number of active users on the any tag. This is important for my denominators. You'll see on the next slide. So here I have everything. I'm going to put it all together. We really got a stew go in here. So I'm going to take my question engagement raw. I'm going to pull in my active users any and calculate my average number of questions asked. That's line three. I'm going to pull in the fake number of users, get my percent users active. And then the code from the group by period and below just trims off the current period because on a dashboard, if you leave that in, it'll often show a drop. And then you'll think, oh, why is this metric dropping? It's not. It's an incomplete week. It's an incomplete month. It's an incomplete time period. So it's really helpful to do that. And now I have a metric. So this is what it looks like. We're excited about this. We've got a metric in the sense like this is a metric, the data format. Documentation. Super important. I learned a lot from Foxmolder. And we're in the right place for this. So if you document your metric, the nice thing about shiny bones and shiny metrics is all of that documentation will turn into the titles and the tooltips on the dashboard. You can get this is made a lot easier with this use metric scaffold function. So if I use use metric scaffold on this question engagement metric object I made, it gives me this full scaffold full of to-dos telling me, hey, Kaitlyn, you get a lot of stuff to do. And you take that and you turn it into this. This is going to be probably recognize this just from all of the armarkedown, fantastic armarkedown talks we've seen. This is your yaml header that's going to go on. This is in particular a .RMD armarkedown metric. You can save your metric. You're going to want to do create metrics. Save metrics with type equals local. This was a fun thing. I was doing the default data from S3. So you're going to want to set type equals local. You can also preview your metric. But let's get to the fun stuff. I'm going to rush through so that we can get to shiny bones. You'll also need shiny metrics. But shiny bones is an opinionated framework for creating and organizing shiny dashboards. And you get visualizations for free. Right now line and bar graphs. As long as you make a metric, it's going to detect, okay, I have this one. This metric and I'm going to turn it into a line graph or a bar graph depending on what it is. You can also create your dashboard with a yaml file and page modules. And Ramanath built in once you've got it installed. You can just pull down in the new project. Scaffle the shiny dashboard app using shiny bones. You'll need shiny metrics installed. The preview metrics and a couple other things are there. It's a dependency. So you write your site.yaml. So here's my actual one for my dashboard. You can see there's all these rules. These are in the documentation. My dashboard is going to be called Stack Overflow. I'm going to have a sidebar link called North Star. And then I'm going to have these tabs. Questions asked. Average questions asked. Then I create an app.r. There's some rules here too. But it's going to turn it into a dashboard. And it's not really fun static. And the GIF was way too big. So I'm going to show it to you. I'm not going to live code, but I do actually have this all 100% already coded for you. It's a hot tip. If you're in an app.r file and you want to start an app, I am on a Mac. Sorry, Jared. Command Shift, Enter. I guess it's called Return, whatever. And I have a dashboard. So let's put some of the stuff we talked about together. So here's our North Star metric that we created. I have it set by default to be aggregated by week, but I can come down here and change it to month. Change it to quarter. I can also, right now it's set to all time, but I can also set it to a custom period of time. I'm going to go back to month. Tag. Where did tag come in? I kept calling this a dimension. The reason we call it a dimension is because up here, now I can flip over to tag. And I can see by tag how many questions were asked. I can double click and keep only one. This is just plotly graph. So the way you normally work with a plotly graph, you work with this. I can hover over and see the exact numbers. And now I can tab through and see my other metrics. So I have average number of questions asked. Same thing by tag. And if I were curious, okay, what does that mean? I hit the tool tip. There's the documentation I wrote. So it really brings everything together. If you walk through this process end to end, you create the metric, you pull shiny bones and shiny metrics in. You have a dashboard, a lot of things for free. And you can still write your own custom shiny page modules and embed them in here. We have some really cool custom ones that are kind of a mixture of the shiny bones, yaml format and custom page modules that Ramnath has built. I've done it too. I've embedded like value boxes and stuff on top of the graphs. It's pretty great. Percent of user is active right now because I fake this data. Looks like nothing. But on a real product, you'd probably have some sort of seasonality, some kind of trend here. And you can stack two graphs on top of each other if you want. I have a number of active users stacked on top of number of users because probably somewhere else at Stack Overflow, we know where a number of users is. It's just here for completeness. And then this is what my metric looks like. It's just an RMD file. There's my documentation. My setup. And then all the code I showed you is in this. So it's really, really cool. And I really like it. And I want to talk to you guys about it all night. So please come find me after. So thank you to DCR and Jared Lander for having me. Thank you for putting up with, I also want to talk about aliens. So if you have theories about that, come find me or catstogs. It's, anyway, thanks to Dave and Ramnath for their work on this and for jing me how to use it and so that I could come to show you guys how to use it. There's really only a small group of us who've been doing this, but they are all open sourced right now and on Ramnath's GitHub so you could start adopting this if you want open issues, open pull requests. And then come talk to me later. So this is me with the Robinson family dog, Abby. This is like my requisite final slide. So come find me later. You can email me, Twitter me. And I'm actually glad that Donia talked about jobs because incidentally I'm a couple weeks away from finishing my current place of employment. I've got about four years experience as a data scientist. So if that's something you're looking for, let me know."}, {"Year": 2018, "Speaker": "Refael Lav", "Title": "Practical R - R as LEGO to Solve Real Problems", "Abstract": "We all know some part of the R ecosystem well, mostly dependent on how we got started on this journey. In this talk, I would like to discuss one aspect that fascinates me, how different capabilities link to solve real problems. I will demonstrate how we go about connecting different components into a pipeline which allowed us to move from images to NLP, to time series to ML to API calls. I will present a few examples of how, with R, we connected the dots. I hope this will open a path for anyone to use what they have learned at this conference to demonstrate a clear data problem to their clients.", "VideoURL": "https://www.youtube.com/watch?v=I5w4GimIzK4", "id0": "2018_01", "transcript": "If you want to contact us, contact me, please feel free. I am available 24-7 usually. So if there's anything that will come up from today that I really want to point out over here is these two points. Again because it's not a technical aspect which is kind of weird for me, I really want to focus on again a concept that I have to deal with in the past two years with an individual across the data community practice which is how do you take all those individual knowledge bytes that you have? Any one of us over here is really knowledgeable in something very specific and we know some other stuff and we all know how to Google which is again, I don't know what we will do with that stack overflow but that's a whole other story. But regardless of that, the person next to you knows something else but that is very nice, that's very great but then comes the problem when you actually have to apply this. When you actually have to take this information and apply this to an actual problem in the real world. And that's essentially what I'm going to touch on which is how do you take all those knowledge bytes and how do you think about this to actually delivering a solution to a real problem that is end to end. So two points that I really want to make over here. So the first one is when you're thinking about your problem, again, and the problem I'm not defining as I want to create a graph but rather I want to create a decision tool for a decision maker. I want to make a decision about where to go in an expectation, whatever the range might be. So the first thing is your objective function. You really need to consider all the parts and we'll touch this in a second, really need to touch about all the objectives that within your, when you're trying to end up as a value. So that's the first thing. The second piece is think about all those little bytes of information that you know about how to do NLP and how do you do, for example, word embedding which is very, very cool and in practice we'll see an example in a second. But how do you take that and actually apply what you take out of the word embedding in order to deliver a solution. So these are the two elements that I really want to touch on. Then, I really want to start with a demo. So like I said, we're not going to show this live over here but okay, can I have the next slide please. All right, and the next slide. So what I really want to go over here is let's say you have an application over here and this is just a representation, this is a shiny dashboard. And you really want to take a end-to-end solution. So what that looks like, can I have the next slide please. So essentially what this tool does if I, later on if you want to go in the back in the table I can show you this live, is to take a call, customer service call and really make a decision about the probability of whatever you want to apply to that particular call. Think call center, think any end to end, end to end of an audio file. So you have essentially multiple components over here. The end result is how do I make a decision. And in order to do this you're really breaking it down backwards to your elements over here. So what you have over here is essentially the ability to upload the file and audio file. How do you process audio file? By the way, last week, audio processing book with a RK mount and an excellent book really highly recommending that. But you have an API call. So we have an API call to Google over here. We have transcription, we have deep learning network and we have the audio representation. And as you can see at the bottom left we have a probability, we have topic, all of this really putting together. So this is essentially a representation of an elements that kind of, this kind of a solution, this kind of thinking will deliver. So you know how to do an API call for example. But how do you connect that to an actual decision? And now I have a text. I can take the text. I can work with an audio. Next slide please. And you have the sentiment over here and I'm not going to dive into this so much. But you can see that now you can make a decision about a particular call. So this taking into account, let's move to the next slide and really going to talk about how this is worked. So in practice, your model and I'm sure you all know elements of this one is really just any machine learning, any code base is really a small element in your entire architecture of your solution. It's not really going to be the main thing per se. I read someplace it's roughly about 5% of actual solution is actually your machine learning code. It's really a, how do you consider the different elements of the operation? How do you consider the different elements of the graphical representation? The user interface you need to take into account. So you have machine learning, how do you take the machine learning and connect that to your user interface so the user can actually do something about that. So that's where your objective function comes into play and this is really the main point over here. And I really want you to think about when you approach a solution, your machine learning piece or your code base over here is really it's a portion of it and you need to address with a capability, preferably an R, that will address the different elements over here. So you're going to have to consider the input. You have to think about what data you have available, how do you generate data, how do you access that, how user will access that information. And that is through a variety of packages, a variety of capabilities that you might have in your tool. You have to think about the cost elements of this one. So I'm talking about computational power, I'm talking about architecture, how much it's that's going to cost you across the user and is it worth it. So think about the return on your investment over here. And like I said, the cost also comes as a human cost, like how much time and effort this will take. Your process really takes into account how much interaction you need. So in the slide that I just showed, your interaction is just one button. Again, in the back, you will actually be able to see this live. You just upload the audio file. You upload the audio file. We will do this tool itself. That's the audio processing. It does the API call. It does the NLP capability over there to make decision. The machine learning around on top of that. All that is happening in the back end, the user doesn't care. So that's important to us for you to know when you're developing this tool. And then, again, it's really about what the business problem is. And that's really where I spend a lot of my time and a lot of the teams that we running really need to take the problem and convert it to a business problem, which is what is really the objective that you're trying to make a decision or you're trying to impact the operation. And every piece over here will impact what you're actually going to do with this particular tool. I want to end to point over here the value. Really where's the value of your tool? So if you have a nice graph, you have a nice analysis, you did your statistics. That is important. That is important to the decision, but it's a one-time thing. Is it solving the problem on a continuous basis? Is it an end-to-end solution? Or the next month when you have to do it again, or somebody else has to do it again, how do you actually approach that? How do you actually implement that? So each element over here in this objective function really needs to into account, every element needs to take the capability by itself. And that's essentially where you apply your different knowledge and work as a team. So let's move on to the next slide real second. And this is how we actually think about this. So I'm really starting with the problem, really looking at the business question, and really how do you decompose the business question. And it's not only a data science exercise, it's really around the different elements that goes into the solution. Do they really care about seeing the data? If they don't care, then you don't show it. But if they do, you do. Where's the data coming from? Do I need to apply any sort of web scraping, or the data is labeled and nicely organized? So that's something. And the main point in this slide is the point in the middle over here. You solve a problem, not all at once. You can assume, and this is something that I talk a lot to different leaders within organization, which is I can make the assumption, because of all of you over here, and because of other developers out there. I can make an assumption that when I look at it as a kind of a lego pieces. So I have a problem on the right, and I want to get to a solution. I can assume that, OK, I need to do x, I need to do y, I need to do z, and each one of those, somebody has a deep knowledge of that one, and solves that, or to a degree. And then I can apply that capability. And then comes the problem. How do I actually connect those capability? But I assume those capabilities exist. So that is a, and then we do a step approach over here. So if we go to the next slide, and is really where I want to take this, and really show this to a practical example, and really, how did we take this approach of really solving one problem at a time, and assuming that the capability exists, and assuming that those capabilities can interact with each other, and really talk to pass information from a capability to a capability. So if we go to the next slide, I'm going to go through the architecture over here, and you can see that each element over here, I'm using different capabilities, different packages, different information that will feed into that. So the problem statement over here is, how do I find information? How do I find a particular piece of text in a really large pile of text that, again, I'm not, there's an NLP discussion later on, I'm not going to go into that, but how do I go and find information in this pile of text that I know that exists out there, in order by my query. So it's essentially information retrieval problem. So how do we go this, is I break it into the component of capability. So the first thing is you take the, you take where's the data. So if you remember from the objective function, I took the, we took online data, we really scraped that, so there are different packages and different capability to do that. Okay, I have all this information, a lot of PDFs, I need to OCR this information and really connect this information into a one location, do the data cleanup, again, not NLP discussion, but I know now I have a bunch of clean text, I need to detect elements of this, of an information. So for example, entity. So there are capability like cornLP or an API call to other services, for example, Google that will provide that, and so I need to build an API connection to Google. Once I have all of that, I can start building Word embedding and that's an other classification capability. And again, for this one, there's a, a Word vector is one of the ones that we use. And the idea is, okay, now I have the connection of the, I know, and enough about those words, I can then link that to a particular data, data bank that we need to create. Because the user doesn't want, doesn't care for lack of a better term, where is this data? They're assuming it's just out there. And then from there, I need to create the user interface, a connection point. So again, we use shiny and other capabilities as you link that. And what you get is essentially we think about in a data science like the picture that you see at the bottom, which is I take every token and I have information about this token. So for example, the term what, will trend is a request for information, it's related to a will. I know that the term post doesn't mean a post, it means a location, et cetera. And so we're thinking about this in this kind of term, okay, nice little table, a part of speech, I know what they related to. I know what the connection is, post is related to a, to a special agent, et cetera. But that's really cool, I say, okay, I solved the problem of tabulating my information and now I have all this, but the user doesn't care. This is very, can I have the next slide please? The user doesn't care. So what we develop, what I want to present, again, taking this concept into account is you're taking this query and this information that you receive and you can OCR it. Okay, you did OCR. Now what? Now the OCR takes into account the fact that our question, can we identify question, et cetera. And all this needs to happen seamlessly because you took into account in the objective function that the user, the value to the users that it's going to be simple and they don't really care what's happening behind the scenes. So we care about the table from the last slide, but in reality, okay, we have the question, we identify the question, we submit the question, can I have the next slide please? And what happens in the back end is that we created the semantic link. So the point that I'm trying to make is that, and again, I wish I could show this live, go in the back, I'll show it later on in the break, this will be live. But the idea over here is that taking that word embedding for example and identifying what the relationship of a word to a word is or taking your wordnet synonyms capability and taking all of that, this is really cool by itself, but putting it together to say, okay, this is like this. That's what the objective function gave me and that's what I'm really presenting out to the user. And that's essentially what happens over there, it's when it found out the user see a one piece of information and they can outvote down votes, that's really where it's the user decision. So again, as you think through your problem, the data elements over here are very interesting, but it's really matter how you really delivering value at the end. So let's move to the next slide. So what I really want individuals to keep in mind and really what I found, and this is again a concept that I have to deal with on a daily basis, which is how do you deliver a solution and not just an interesting capability? And the answer is connecting different capabilities together to deliver an end to end and we have those capabilities running through this data science community. So I encourage everybody to consider the value of the different option that you add into your tool, into your capability. And it really should be easy to explain what you're doing. It really should be very easy to deliver that. User should be able to enter new information into your system, into your tool. If it's not easy and it's just a data science, then it's a data science exercise. And really the main point over here is that you need to, like I said before, develop one thing at a time, not all at once, different pilot sections, one thing at a time and assume you can assume that different components exist to make that Lego visual ready for you when you're done."}, {"Year": 2018, "Speaker": "Soumya Kalra", "Title": "My Open Source Journey in R\\Finance", "Abstract": "Open source contributions in the R community are an incredible way to learn and become part of a community. Often making these types of contributions are quite scary and one doesn't know quite where to begin. In this presentation, I will share the approaches I used to start contributing specifically focused in the finance space. I will also demonstrate some of the tools/analysis I have built along the way with lessons learned (both good and bad). In addition, I will lay out a path for all R community members to foster more collaboration and contribution in the R Finance community.", "VideoURL": "https://www.youtube.com/watch?v=s5WeEVCi8VI", "id0": "2018_02", "transcript": "Okay, so my talk is about open source contributions in our and Finance. They're working on the slides. I can at least talk to you before they get started. Before they put everything up. So really it was just to introduce people to open open Source work because it's my introduction to open source. So basically i was going to first start with introducing Myself. Hi, i'm samya. I'm on a couple of different initiatives related to r. I'm an analyst by day and a lot of this talk will not, Actually all of this talk will not represent any of my Employers opinions, only my own. Always have to caveat that. And i'm on the, i help organize the new yorkly r ladies Chapter as well as i'm also on the committee for r and Finance. So i wanted to talk about open source contributions in r Finance and basically about about a year ago when i started Working with r ladies i was introduced to the world of Open source contributions and so it really kind of opened my Eyes and i saw people like e-rob and all these other r ladies Doing these amazing things and i was like wow i do this, i do Some of this stuff but i don't know. So it was a little scary and i hope i just put my contact In for up there so please reach out even if you wanted to Criticize my talk. Okay so contribution to open source. Initially when i thought about it it was terrifying. I really thought that everyone that's out there that's Contributing is probably an expert and i was like oh everyone Has to be like haddlies level to contribute so i was kind of a Little lost and where to begin. So of course i thought of the one thing i wanted to do which Was do real estate investing i live in new york we have very Small spaces where we live and so basically i always am on the Lookout for a bigger and better closet. Okay so i'll tell you how this started a little bit so two of My colleagues at work will show pictures of at the end actually Are really into this one of them invest a lot and so i'm So he kind of got me started and showed me the data and got me Very excited. So the first data source that i'm going to present is on the Survey data that's done the american community survey which is Actually done every year but it's but for this analysis i'm Using the five-year data from 2012 to 2016 so that's basically From december 1st or january 1st 2012 to december 31st 2016 and It's only because every year they do it only for populations Greater than 3.5 million the survey is done for that no it's Done for population greater than 65,000 but 3.5 million people are Served so generally just to get more thorough data i chose to Look at the five-year set for now it includes pretty much a lot Of cool stuff like social data housing data economic data and Demographic statistics. Okay so how did i get this in arm i learned that there are 22 Packages that really deal with census data talk about like Over contribution in the open source space and the one cool one That i started working with because i saw julia silky working With it was the tidy census package even has a tutorial on it Which like just made my life thousand times easier i have the Link in the slides okay and the other set of data that you Really want to get your hands on if you can if you want to Really play with real estate would be mls data so what is mls data So it's actually list individual listings but they're all done At the local level and there's no like aggregated database that Collect all this information allows you to access it so there No i think there are some aggregated services i think like Zillow etcetera like they have access to some of the zip codes But not everything there's other people in the audience that are Far more knowledgeable about this please come up and explain More to me and i just couldn't find a whole lot of any Public or free apis we actually eventually ended up going down A subscription and that's how we built a lot of the stuff that i Built today and basically there's no standardization as in the data is Incredibly dirty so when i first started mapping things it was Mapping into the wrong areas and i'll give you examples when i show you stuff And so and a lot of the analysis that i'm going to be doing today is All based on zip code analysis so the census data if you're familiar with It at all is all in the state and county level so there's some like There are areas a lot of people when they do census data is like it's on The state and county level but there isn't a way to pull in some zip code Level information so it's interesting to see how that can match up and really show you some trends Okay so how did i get this in our i had to purchase it okay so the Packages i used today are leaflet for visualization Veritas i can i'm not going to or good al can i can't pronounce Package names so you're going to have to forgive me and read them so for Finance i use the fin cow package if any of you have like the calculated they Give you for like the cfa exams really put in the mortgage bond something like that okay And the data science obviously the tidy verse and Deflier so yeah since i live in new york and i told you about my constant Need for space in new york i decided to look at new york so let me tell you What i first did so as i said the ml s data and the census data is the ml s data is Incredibly incredibly dirty so i had to do a lot of cleanup and then i Moved all of it so i wanted to show you at a snapshot for how like this is this is not at all Incredibly detailed i would do a lot more now that i know how to do this so but Even this information is quite a lot so you have like days on market you have you have it at the Postal code level you have square feet you have the year build i wish i had done this before moving Into my current place you have like the price and then i calculated the annualized income the price to rent ratio The annualized expenditure etc which i will show you graphs of but so it has a lot of information that we can use okay so the first thing that you do right when you get this data say you're like okay i just want to get a flavor for what's happening right so you look at Medium prices because if you think about it or and essentially any area right if you're looking at the zip code level There are obviously some houses that are far more expensive and there are some houses that are far cheaper right just because of many different factors so i figured i thought in this case since i was just doing it Initial glance i would start looking at medians so i used the leaflet package to develop the first round of median prices for every neighborhood in new york city so this is what this came out so i also discovered an interesting problem with this so in my Our studio window this actually rendered and showed everything in the right place but i think here it does not so that's very upsetting but but the point of this graph actually that are the map actually is quite interesting so i'm going to show you how to do this So if you look these are the five boroughs of new york and this is broken out by percentiles so if you look here all the yellow is essentially where the most expensive places are right which is which is not rendering very pretty up there okay but it's it's basically all of this So it's so that's like all of the you know like so hoe trabeca i think it's even the uppercid side yeah it's a pretty side so these are sort of the more expensive places you can see there's some yellow out here this must be willingsburg or brooklyn yeah some more carol garden um so just to get an idea right so now we know this so we go to the next next slide okay so the next thing you're looking at and you're saying okay well that's great i know what how much something costs but absolutes really do nothing for me so let me let me understand this in a different way i get this rent amount so let me see if i can do a median price to rent ratio and what that really you can interpret that is if i to bought a house and put it on the market today for rent how long would that take me to pay it off okay so but i'm going to do that so basically i would expect that the payoff would be faster for less expensive homes presumably so i look at this again but the median the median rate here again is is pretty high so it's even here it's still it's still pretty consistent in terms of results there are a couple of neighborhoods that are changing in direction where the median rent to a medium price to rent ratio is a little different so i say okay well let me see some more information so the second the other thing that the MLS data also gives you is the days to market so that's really interesting so how long does it take to sell a place and i would expect that again and a more expensive place or a bigger place would be harder to sell right so again if you look at this which is crazy which is what tells you how much demand there is in new york is that it's actually harder to sell things in the outer boroughs and it's much easier to sell things inside right so in Manhattan itself this whole area that was yellow before that was because it was so expensive apparently does not sit on the market very long so that's very interesting okay so next up we have the median price to rent ratio over the days on market so i don't know if any of you are familiar with like doing cash flow analysis and doing asset turnovers and balance sheets so essentially you're trying to measure how fast you can turn over and how efficient your company is turning over assets so this is like a similar ratio to say okay well how fast can can someone actually move this you know based on how fast can they pay back and how many days they take to sell on it so kind of efficiency so again so oh okay oh yeah perfect so this kind of shows you again so on the on the left hand side you have the the median days to market and you have the median price to rent ratio and it kind of again oh man these are like this has been very difficult i don't know if at all we have tried using plotly at all in markdown documents and zerrigan i've been having a lot of trouble with it so anyway but a lot of these labels match up to what you would expect to be sort of the expensive neighborhoods i think out here was like trabeca um another so just as an idea this was an interesting measure and it was interesting that it all clustered on one side um to say that you know most things are actually probably pretty shit so and then the other thing that comes up is when you look at a neighborhood as we talked about earlier was to say okay well there must be a spread right like every every apartment in that neighborhood can't be the same price can't be just expensive the other thing i didn't do in this was i didn't break this down by bedroom type so that was something i was trying to do but it um i think i have to do some more merging to get that information um and there's oh yeah in terms of the dirty data there was quite a bit of dirty data in in actually the MLS because they have provided locations but none of the neighborhoods actually match so i had to lay down a map of new york city in order to make sure it mapped to the right neighborhood because the latitude launch is all off um so so going back to this the spread of the prices per neighborhood so again it's it's interesting because again a lot of them are concentrated in like the lower millions but you see like something like trabeca is like oh there's like a 10 million dollar apartment there i was really hoping that it would go the other direction that there'd be like things in the lower end um and i'm hoping to obviously expand this analysis outside new york city where things are a little bit more affordable so um but it was good to try and have fun with it okay so as as i promised since this is slightly about finance as well um and it has to include finance um i figured we could make a cash flow statement because that's really the goal of this analysis is to say well how can you actually project this information out once you have built these uh built these statistics how can you uh tell somebody what how where to invest so we just so here i'm just building a very simple model um i'm taking the annualized income which they gave us uh the MLS gives us monthly estimates so i'm just you know um annualizing that that i'm doing um expenses which the again the MLS data gives me the the hoa and i'm using the fin calc package to actually calculate the mortgage payments um so that would be my expense portion and i'll show you i think at the bottom i've laid out the formula on how to calculate mortgage payments so you know you take it i assume it's a 5% rate 30 year regular standard mortgage um And then so that gives you just a basic idea of what profit looks like which is kind of cool because most of the time when you're looking at houses I don't know if any of you've gone shopping prizes It's not that easy to figure out what something's worth um and there are not that many data sources that aggregate things in a way that's actually coherent or cohesive um so that's so this is i wish i had really used this before um so i decided you know um since we have a lot of our ladies that have recently moved to brooklyn I think we were talking about this a couple days ago So I thought I would just show you the top like you know the highest profit you can make in bling'sburg these days It doesn't seem to be a whole lot as you can see um but there is some positive happening there um so there are so it was interesting that there are some neighborhoods where You kind of expect that positive cash flow versus others so um i i imagine that eventually i would probably turn this into an into an app once i like clean everything up and Um so I hope that all of you can try it out and give feedback on it Okay, so as I said next steps would be to include much more uh much more information in terms of uh Extended areas so even outside new york city outside the boroughs obviously um And probably I would want to extend my analysis to other areas like pennsylvania and new jersey etc Um and so the other and the other things i'd like to do is to maybe build some more signals So I was really interested in building uh incorporating more retail sets like um I think I was reading about how they were incorporating like douc and donuts and starbucks Um into that so I wonder how much of a predictor that is of areas and you know their prices Um so if anyone's on that work, I'd love to talk about it Um and again extend this all over the us Okay, so why am I doing this right other than wanting to buy an apartment? These are my two co-workers that love love love real estate data um So uh they got me hooked um and it gave me a chance to really learn something different from my day job um, and I got them got to learn super cool packages like uh like we flip Which you'll actually hear about angela she's going to talk about it later on today. I think And that's pretty much it. Thank you"}, {"Year": 2018, "Speaker": "Michael Powell", "Title": "Democratizing Data Science: Using R for End-to-End Intelligence Production", "Abstract": "Organizations seeking to build data science capacity face many challenges; debates over hardware, software, data science team member roles, and even which problems to tackle first can make it hard to get started. In this talk, MAJ Mike Powell of the US Army Intelligence and Security Command will explore how the wide range of offerings in the R/RStudio community have allowed small data science efforts to gain traction without having to wait for enterprise decisions and resources. Even analysts without extensive formal training have access to the necessary tools to create websites, web apps, reproducible reports, and script-driven analysis and visualizations - all within the R/RStudio ecosystem. These tools have effectively democratized data science by giving analysts a single platform suitable for every phase of intelligence production.", "VideoURL": "https://www.youtube.com/watch?v=NTb1w61_Bwc", "id0": "2018_03", "transcript": "So, good morning everyone. My name is Mike Powell. I'm an active duty U.S. Army officer. I work at NSCOM, the U.S. Army Intelligence and Security Command on Fort Belvoir. And while my material is coming up, you probably think there might not be that many things that we share in common. I disagree. You probably use R for work. So do I. You probably use it for a side project or hobby. So have I. A good day for you might be several hours of uninterrupted coding and no meetings. Same here. But where we differ is that I do all my coding and combat boots and camouflage. In a cubicle, in an office building. But hey, I don't really mind because if people can't see you, they can't bother you. So, when Jared talked to me about presenting here, another thing about the difference between you and me is I can't put my hands in my pockets because you don't do that in military uniforms. Crazy stuff, right? So, when Jared talked to me about presenting here, I thought, yeah, sure, it sounds great. I asked my boss, he's like, yeah, absolutely. And then I asked the people that control what you say in public and realized it was going to be pretty hard to give an R talk without talking about my work. About the projects I'm working on, the data I use, the methods I use. Because as I said, I work in an intelligence organization and they're kind of protective of those things. There we go. The other thing I noticed, and I might take this back to my organization, is in our ability to look cool and camouflage, we've made it impossible to read from a distance anything on the title slide. So, what I want to talk to you about is how we're using R&R Studio really to democratize data science, to bring it down to our analysts and get them doing all sorts of things that they weren't doing before. All right, next slide, please. So, the other thing I did was I'm thinking I'm going to be all sneaky and I'm going to save my PowerPoint slides as a PDF where they're uneditable and they're locked so that nothing could be manipulated and misrepresented me later on. And of course, I did it in such a fashion that the computer that doesn't have Adobe Acrobat on it can't pull up my PDF. So, you're going to see it through a web browser. I think it'll work. Okay. So, the things I want to talk about, a little bit how we're involving R&R Studio, the whole ecosystem there to incorporate data analysis in every aspect of what we do. And like any other organization, the Army and my organization NSCOM wants to leverage data in new and powerful ways. We need people to do that, we need skills to do that. I'm going to try to explain a few of those today. So, we'll kind of walk through a little bit about how we're using data, a little bit how we're trying to increase the data literacy of our analysts, a little bit how we're trying to turn some tedious manual processes into automated, repeatable, reproducible workflows. Some of the questions we're trying to get after that really just weren't possible before. And then finally, how we're trying to leverage Shiny to put app development in the hands of the analyst who's got a little bit of know-how and a good idea. All right. Next slide, please. Everybody, every organization has some diagram like this. This is how we do things. So, this is the US Army intelligence process and I would say it mirrors many of the data science pipelines or workflows that maybe you've seen. I'll start in the top right corner. Assuming we've got a well-formed question, and that's a big assumption, right? Most of the time, I think the people that come to me with it with data science dreams are thinking that data science is going to give them the question that we're then going to answer for them. We really like you to come with a good question and if not, we'll help you refine it, scope it to something we can do in our lifetimes. It may involve some collection, a little bit of cleaning, some processing. We'll throw a few analytic techniques at it. And at the end of the day, hopefully, we've got something that is worth packaging up, the results of methodology, something that we want to share and maybe use again. And I'll point out that I would caution you from adopting this model for your own organization because as you see, it goes on forever. Which I think you can interpret it two ways. Either, well, that's good. They're always trying to improve. That's noble. Or, you know, who cares if we get it right this time? We're just going to do it again. All right, next slide, please. There we go. Okay. So I'd like to introduce the first analogy that you may or may not keep up with as we go through. And that is, I walked out my front door this morning and my yard's full of leaves. It wasn't yesterday, but it is today. So how do I deal with that? I think I've got a few options. The first is, I grab a rake and I pick them up. The second is, I get with my neighbors, and I use that time I could have been raking to come up with some grand neighborhood plan. And we're going to come up with something that may be by next fall will be effective at taking care of our leaf problem. Or, maybe I bank on Fairfax County saying, all our neighborhoods have leaf problems. We should try to solve this problem for everybody. That's going to take 10 years and a ton of money and never happen. So none of those are necessarily like bad ideas, but only one of them really takes care of my leaf problem today. And that's me doing something about it. So in our venture or a endeavor here to incorporate data science into what we do in our intelligence community, we need the long term plans. We need the short term plans, but somebody's got to do stuff today. And that's what, that's what I'm going to talk to you a little bit about today. I think it's really important that we take the analysts we have, with the resources we have to solve the problems we have today. One of the things we did to get after this in the open source intelligence community was to create a data science course. So we call it a data science course, but really it's more like data science counseling. People come in. They tell us about their problems. We give them some coping techniques like D-ply R, G-G plot. And hopefully everybody leaves a little bit happier and better able to tackle their job. Alright, next slide please. So the Army is a pretty old organization. 243 years old in fact. And as I've made the rounds in my organization, I've encountered a few Excel spreadsheets that I think are almost that old. And what's tough about the Excel spreadsheets is, you know, you've got some analysts who's sitting there for hours every day or hours every week and they're, you know, copying and pasting and filtering and joining and splitting and merging whatever they're doing. Man, you just want to automate that stuff for them because it's so painful to watch a guy do that. And then you run into the mystery VBA script that like it's critical. Our section depends on this, but nobody knows how it works. And we'll never know because the guy who wrote it password protected that spreadsheet like 10 years ago, and then he retired. So you're out of line. So when you find this guy who's got the spreadsheet job and you know, you say, hey, let me let me take the eight hours and turn it into eight seconds for you. It's really interesting to see the transition that happens both in the scope of what they're willing to consider as a potential question to answer. Like maybe we don't just have to look at a week's worth of data because it takes a day to do that. Maybe we can look at months or maybe we can look at years. But then they start getting 80, right? Because it takes maybe it takes 30 minutes to code that has a five year analysis. And I'm like, you know, you're doing 100 times more than an hour than you used to do in a week. Like lay off me. And sometimes it takes a minute for complicated things to happen. But I like to see how when you give somebody their time back, that's how we always try to start is like, let me give you your time back so that you can think of something better to do instead of juggling spreadsheets on Monday. Maybe you just get to think for a little bit and maybe you get to do something a little more worthy of human thought. Next slide, please. Okay, so in a zero diversity focus group of the three people in my office, one of them did not recognize the left hand side of this as my attempt to build a Plinko board. You all know what Plinko is? Okay, so if you don't, Google Plinko, the price is right. And you'll learn what Plinko is all about. And you'll see why I put that up here in a minute. So manual processes are prone to air or at least variation. I think back to some of my grad school work. And I pity anybody that followed me and was supposed to like follow, you know, fall in on that project and take it to the next level because some of that work, I mean, I knew it was good at the time. I knew how the scripts worked. I knew what order to run them in. I knew what assumptions I was making. But the critical shortcoming and all that is that it required me. And if I'm not there, then the whole thing falls apart. Well, I think anybody, if you tried to recreate some of my old figures or something, you might as well try playing Plinko to rebuild what I had built. You just go through this random sequence of maybe you did cross validation, maybe remove some outliers. I don't know. And I think that's garbage, right? So life with our markdown is a different thing all together. I can capture everything from how I acquired the data, what I did to clean it. Any steps I took along the way, I'm capturing it and explaining it as I go. And so this is a skill that I think has been critical to teach our analysts. Like I should be able to run a back trace on any conclusion you draw all the way to the start. What data did you use? What did you do to that data? Where did it come from? What assumptions are you making? Things that seem reasonable to ask. But all that is lost when you've got somebody personally handling a bunch of Excel spreadsheets. And there's no quality control to that. I mean, how do you confirm results with that model? You run it again. You do the work again. And what if the answer you get the second time is different? Do it again. And then it's like a best two out of three? Or like at what point do you stop and say, okay, this was the right answer. I'm a huge fan. We tried to instill this in our analysts through training in any project we do that we're going to document this thing so that somebody else with the same question in a different area can repurpose it. Next slide, please. So a lot of our work has been focused on turning some of the wishful thinking ideas of our analysts into our code. And so something as simple as like the map in front of you. If a there's nothing complex going on under the hood here, you've got a few data sources coming in representing events of a certain type and certain places. And I want to look at them all together. A good luck doing that in a spreadsheet. But it's not hard to connect an analyst to the leaflet code that does it, to a little bit of de-plier, to bring stuff together, to merge stuff from the same source under the same location and put it all in a pop-up. It's a real easy way to let somebody be able to drill into events in a way that was just unthinkable before. So as we introduce them to these ideas in training or just in sort of the consulting model that we use, it's pretty interesting to see where their mind goes and the questions that they'll be willing to start asking. So think about the next question. The next question would be, where have protests in whatever country you're interested in? Where have they occurred in the last two days? Like, how do you answer that? Well, it's pretty easy. You go to Google or you go to your favorite news site, maybe you got some go-to sites for, you know, protests in the last two days and Uganda.com. And you'll get exactly what you want until, I say, I don't really care about the last two days anymore. I want to know about the last two months. Okay, well, maybe there's a protest in Uganda in the last two months.com. And if there's not, it gets a little harder because those protests aren't in the headlines anymore. It gets terrible if I say, what about the last two years or the last 10 years or the last 20 years? To find that stuff, now you're going to start working with a database. Not a problem because there's publicly available databases that categorize events and make them available to you for free. It's pretty easy stuff except that the website that you go to to interact with that data may not answer all the questions you have. But they'll give you the data. In this room, like, that's the dream. No problem. Give me the data. I'll take it from there. Outside this room, that's a nightmare because if you didn't put it in my web interface tool, then I don't know what to do. So we're trying to bridge that gap with our analysts. I think the analyst response saying, like, if I'm going to use this data and I have to build my own analysis and visualization pipeline, that data is worthless. How am I supposed to do that? I don't think it's necessary. You have to build your own pipeline. It's you get to build your own pipeline. You get to bring all that stuff in. You get to do whatever you want with it. You get to merge it with whatever other data provider data you can get a hold of. You can do whatever analysis that data will support. I think it's a really powerful paradigm shift for people to start thinking along those lines instead of just accepting whatever you've been given. Okay. Next slide, please. Okay. This is the closest I'll get to telling you what we really do. But so at some point, we've got to share the work we're doing. You can do that in our markdown document. You can send it out as an HTML document. If you don't work for an organization that strips all HTML documents from your emails or maybe you want to host it as a shiny app. A shiny app is fantastic for us because a lot of the questions our analysts have that are geographically focused easily transferable workflow to another analyst in another location with a different country of interest. I bring up protests in Uganda and you say, well, I care about Belarus. Well, there's a pretty good chance that whatever workflow I use for Uganda, you could probably apply. That's one of the big things that we use shiny for. But the real reason I love shiny is not so much just that capability. It's what I appreciate about it that maybe is unique to me. And that is, I've been in the army 14 years. So I'm not a full-stack developer. I'm not a front end back end. Whatever other ends are required to build a nice sophisticated production quality, cloud-based, scalable app. I don't know how to do that stuff. But I can make a pretty sweet shiny app. If I know the question you want to ask and you can connect me to the data we need to use, that's a pretty powerful thing. I didn't learn that in the army. Flying helicopters or teaching statistics or anything else I've done, it's not that hard to connect an analyst to a really powerful capability through the form of shiny. So if the right solution for a problem is to create an interactive app, I think we have a couple options. The first is we try to go by the closest thing on the market. The closest thing on the market probably isn't exactly what we want and it's probably really expensive. So I liken this to buying really overpriced shoes that don't quite fit. Okay, maybe they look good, but they don't really do what you want. The other thing we can do is if there's nothing on the market, we can basically solicit vendors to create something. That's exactly what we want. Well, we're going to spend a year coming up with our requirements, then we're going to solicit bids, then we're going to run a pilot study, then we're going to review contracts. And by the time it's all done, it's not probably exactly what we want, or maybe we don't even have that problem anymore. So I liken this to ordering custom back-ordered shoes for your kids. Like by the time they get there, the kids' feet don't, they're not the right size anymore. It's a terrible plan. Or maybe I can just build it myself. If I build it myself, the shoe analogy kind of breaks down here because I don't know anything about building shoes, but I take that back. If you gave me the materials and gave me the tools and stack over for them, I could probably build you some shoes. They wouldn't be that pretty, but they might be effective just like my shiny apps. All right, so to go back to my leaf-covered yard problem, only one of these solutions puts a solution in the hands of the analysts with any speed. And that's if we do it ourselves. Now, this isn't necessarily the solution for enterprise applications, you know, to run our entire HR system. Maybe I shouldn't build that in my office. But for a small-ish problem that going through these massive procurement processes is just too painful. Those problems just go unsolved unless we give ourselves the tools to do it in-house with an analyst or two, which is actually really fun. So I spend pretty much all day with our studio open because all the things I just told you about are the tasks that I do to bring value to the organization. I'm automating things with scripts. I'm creating workflows, a document, common questions, and how we approach them in our markdown. I'm building shiny apps to make a tailorable question answerable to an analyst anywhere. They don't need to have extreme data skills to be able to use it, a little bit of training, and we can get them on their way. All right, so I'll kind of tell you what's going on in this slide then. We're interested, of course, in what's going on around the world and instability in particular. So I made this a couple weeks ago after as kind of a repurposed slide from a presentation I gave a month ago when I was highlighting this big dip in article tone in the top right corner. So the big dip there happens to be corresponding to when In Uganda the president was really being criticized for arresting and allegedly torturing one of the members of their government who, his name is Bobby Wine, you might know him as a Ugandan pop star. He got pulled over as a weapons in the car, at least that was the charge. He gets arrested, and so the whole world starts piling on about how this is, you know, abuse of power and all that stuff. Well, it creates this huge in article volume related to the president of Uganda. Everybody's getting upset about it. The whole world's calling for him to behave better and such, but it's kind of short-lived within a couple of weeks. It returns to normal. Maybe this is something that an app could bring to an analyst's attention, but I'll also point out this is the trouble with new. You see this other big spike on the right hand side and a lot of positive tone about it. That's when Kanye West visited and gave him some shoes. So our interest was higher, but short-lived for the latter. Okay, and with that, since we're not taking questions, I won't just have to say that's classified if you ask me a tough one. And I am completely, thank you very much for your time and Jared for the premium talking spot."}, {"Year": 2018, "Speaker": "Emily Robinson", "Title": "Building an A/B Testing Analytics System with R and Shiny", "Abstract": "Online experimentation, or A/B Testing, is the gold standard for measuring the effectiveness of changes to a website. While A/B testing is used at thousands of companies, results can seem difficult to parse without resorting to expensive end-to-end commercial options. But using DataCamp's system as an example, I'll illustrate how R is a great language for building powerful analytical and visualization experiment tools. We'll first see how Shiny dashboards can help people monitor and quickly analyze multiple A/B tests each week. We'll then dive into the open-source funneljoin package we've created using variations on dplyr's joining function, allowing you to analyze sequential actions using behavioral funnels.", "VideoURL": "https://www.youtube.com/watch?v=zDONDfXzKdA", "id0": "2018_04", "transcript": "Hey, everyone. So, my name is Emily Robinson. Should have the slides up here in a second. I'm talking about building an a-b testing analytics system with R and shiny. So, a little more about me. And first, most importantly, that's abi. She's my part-time dog, as i like to say. I borrow her from my parents. So, i'm currently working as a Data Scientist at DataCamp. I'll go into briefly what DataCamp Is. We'll make the talk make more sense. But it's an online learning platform for data science, which has courses, practice sessions. It's very interactive. I've been an R user for about seven years, so i was lucky enough To go to Rice University. And when i was studying, Hadley Wickham was there as a statistics professor. And so, he had designed some of the R courses. That's where i first learned R. I also, as jared said, socializing is a big part of this Conference. So, when you see me around the Next couple days, some things i really enjoy talking about Are building and finding data science community and part of The R. Ladies New York City chapter. One of the reasons i really enjoy programming is R is that i Find it a very friendly and welcoming community. And so, i hope you find that here too. I also like talking about diversity in stem. So, i said, i really appreciate jared cares a lot about this Too. And you can see that reflected in The speaker lineup. And of course, R. So, briefly, this is what you might see if you go to a Data camp course. So, as i said, it's an online Interactive learning platform. So, with python, R, over 180 courses, projects as well. And we're based on a subscription Base model. So, i'll talk about that a little Bit later. But basically, people can register. And then we have both b to c. So, like, just one of you, Just signing yourself up. And business to business subscriptions. So, first, i want to make sure we're all on the same page And define what is ab testing. So, if we have a user joe, and Joe logs on and says i want to learn deep learning. And so, joe comes to data camp. And they might see something like This. Right? So, this is a course page. And it says deep learning and python. Great. But if we're running an ab test, They might also see something like this instead. So, there are a couple differences here. But the big one we'll see is that we have the registration Page. So, basically, ab testing is about When you have two, you have randomly assigning people when They come and visit your website to one of two or more different experiences. So, the idea, basically, here is that instead of just launching Something and try to measuring what change, you're looking at and Saying, okay, because we're randomly assigning people, the only Difference between those who saw the control and the treatment Should just be, not sure what, i think it's catching up to me now. The only difference between the control and treatment should be what We changed on the web page. So, if there is a difference Between those groups and things like a registration rate or conversion Rate, then that is a sign that what we changed is the reason That they had this. Okay. Not sure what's happening here. So, i want to talk a little bit about me before data camp. So, we saw previously, it just has like a mind of its own. So, before data camp, i was working at atc. And atc, if you're not familiar, it's a big online e-commerce platform. And atc has, you know, over, it would have been running Experiments for more than six years by the time i joined, it had, You can see here, over five data engineers working on the Experimentation platform, over a thousand metrics computed for each experiment. So, it was a very sophisticated online learning, sorry, a very Sophisticated ab testing system. And so, i came into that. And that's where i first started using ab testing. And, you know, i also really benefited with these five data engineers. There was a very nice ui for experiments. So, when i came in every morning, i would Sort of get my t, sit down, and i would see catapults, our Internal tool. And it listed all experiments that were currently running And the metrics for each of them. So, what we see here is that we have Two metrics. This is from a blog post, how atc handles The And we see that, you know, okay, there are two metrics. One of them, there was no change from the control of the treatment. And one of them, it says not enough data. So, what does that look like? And the nice thing is, when you have these great engineers who know PHP, we get fancy little pop-ups. And so, we see here that, okay, there Was no detectable change, we get what was the observed rate, what's the Change, what's the confidence interval p value power. And then, if we see not enough data, we would see that, okay, your Metric is not powered yet for 1% change. So, this is how we help solve the peaking Problem. So, checking the data too soon. So, we can see, yes, the p value is Significant here. But we're saying be a little cautious because you might be Checking the results too early. So, this is the system i was working with. And then i come into data camp. And data camp, i was brought in to help build Their ab testing system. And this is basically how i felt. So, i came in here. And to be clear, since it's a new record, i had not Lied on my resume. They knew, sort of my experience at Etsy and ab testing. But, you know, what the difference here was that i wasn't Coming into a fully formed system. So, i wasn't just doing, you know, working in A system that was already functioning. There was no system for planning, analyzing The present experiment results. We hadn't really started doing that yet at data camp. And there was also no data engineer to help build it. So, this was all on me. So, my first lesson i want to give in this talk is to build tools that empower Yourself. So, when i started, ab testing is Really about one thing. And that is conversions. So, it's all about who did this and Then did that. So, it may be if you're an e-commerce website, maybe that's buying for data camp. Might be subscriptions. But we can even count a conversion as something like a Course star. It doesn't necessarily have to be paying money. And so, this is really, you know, a first this then that. So, who here has had questions like, you know, first, who tried x then did y? Who are the people, what percent of people who did x then did y? What are all the things people did after doing x? So, these types of questions, these are time-based questions. And, you know, with an ab testing, i'd have exactly these types of questions. So, i might do something in talking about, all right, what about subscription rate? So, if i had a pricing page, what course did people start after seeing the course Search page for the first time? Okay, what were the ad clicks that had a Second start within two days? So, these are, again, are like time-based Questions, right? And this sounds like maybe it's kind of simple. But i want to walk through three users of actually some of the challenges that Are here. So, if we look at three example Users of data camp, and this is their path. So, this is over time. We see, for example, josh and Lin, and they have their page views and conversions. And, again, depending on the context, a conversion might be a Course start, and the page view might be an ad clicker or a course search page. So, and then we have steve. So, i want to go through and, like, how, okay, If i were looking at this, what would be the connections i would draw depending On my question? So, if i had the course started after First course search page, so i'm looking at josh here, and then i want to see, okay, So, what's the first course search page, and where's the first course start? Okay, so it's those two. Great. So, josh, that was kind of easy. All right, let's go on to linn. So, with linn, we see that linn has the Course start before she had a course search page view. And so, that is, then we're connecting the second course start, right, because we Need to have the course started after the course search page. We don't care about what linn has done before. And then the final one steve, we can see that here we have, right, we do a slightly Longer time period. All right, so that's the first course. So, there's his course start, his course search page view, and we connect those Two. Seems simple. Okay, now what if we change it up and We're looking at all ad clicks followed by a course start within two days, and this is A very arbitrary time period, but now we see, right, josh has multiple conversions. So, we see, all right, he had the ad click at first, and then a course start. And then he had two more ad clicks, and each of them was followed by the same Course start. So, we draw both those connecting lines. And then we see linn as well, she just has one, and then steve, we see actually is Not at all. So, steve's ad click is too far away from the course start. So, when i was doing all of this, i had this kind of very lengthy repetitive code, and i Was saying, all right, so if i was doing a left join, you guys summarize, get the first Registration, you know, then kind of doing some ad counts, percentage, etc. So, this, you know, it is a little bit hard to switch back and forth. So, if i was switching from, okay, i want to take their first followed by the next One, if i want to do it within a time period, you know, i'd sort of be redoing a lot of Code, and it wasn't that easy to switch back and forth between them. And then you click through, yep, so there's lots of copying and Pasting, which, you know, as we know, is coders, we don't want to do too much of That, it can lead to mistakes. And i had to, it was hard to switch between different Types of funnels. So, the solution here is, you know, more possible questions we might have, right? So, even more than the two i started, who subscribed for the first time after Seeing the pricing page? So then i care, i want to make sure they've never subscribed Before they saw the pricing page. i'm just looking at first time subscribers. And, you know, what was the last page people saw before starting the course? What are all the courses people started after visiting the home page? So a bunch of different if this, then that, you know, interjoints, just people who did both, left Joins, percentage, etc. So we have all these types of questions. And, you know, the next step for doing that is, click next slide. You know, and when you have repetitive tasks, we have had the Wicom here telling us we got a time to write a function, right? If you're doing lots of copy and pasting. And what i'll actually add to that is that It's also time, sometimes, to write a package. So when you have a function can be great, you know, but a package can help bundle all of that together And make it a little bit easier to use across different scripts. So we only had one issue here. Okay, great. Now it's time to write a package. And that was that i had never Written a package before. So i was coming in and i said, okay, i have this task and how am i going to do this? Can you click the next slide? Keep going. So i was sitting there, like this poor, very confused Pug saying i don't really know. All right, i sort of, i'm going to listen to the great Hadley wicom, you know, my teacher of old, but i don't really know how to write this package. Now, fortunately, i did really luck out in my work place. And that's because i had david robinson. Oh, wait a second. Not that david robinson, if only. No, i had this david robinson, who actually is my brother as well. So if you're familiar with him, he worked at sack overflow. He's a data scientist at datacamp. And he's written some great packages like Tidy text and broom that you might have used. So dave helped me out in starting this package funnel join. What it does is it takes different types of funnels. So whether you're saying i want the first time someone did this, and then the first Time they did x, and then the first y after that, i want all x's and y's from the same Person that happened within two hours. I want any combination of the pair. So you can really easily switch in these Time-based joins. And i'm just going to give a brief Demonstration of how the code might look. So if we go to the next slide, this is a after-joined function. And that's funnel joins main function. And there are a couple of points here. So we see first, if you keep clicking through, that we have the first table. So a pricing page view. And then we have a second table, whether people Subscribed. We tell, okay, what are the user Column names? What are the time columns? So this is telling it, okay, how do i tell, you know, which happened first and Second? Then i have a type of after-joined, which you can read about in the Documentation. So in this case, i want to take the First time someone viewed a page and their first subscription afterwards. And finally, the type of dplyr join. So i can enter left, etc. So on the next slide, i'll show how it changes really easily if you want to do a Different type of time-based join. So now we're looking at within gap. So if you go back one slide, we see that this is saying now all we change was the Type to within gap. And then we added this argument dt, Which is saying a diff time. So now it's only joining them when there's a two Days or smaller gap. And so what i've done here is i've tried to make it Really easy to switch to look at different types of time-based joins. And on the next slide i'll show how, you know, funnel join. There's many different funnel types as i mentioned. So there's last before, first after, smallest gap, any, any, etc. It supports all type of dplyr joins. So whether you want to do a left join, An anti-assema, etc. Bug fixes, pull request, feature request, All welcome. As i said, it's up on github. And i really encourage you to try it yourself. I will say a bit of a note of caution in that it is so under construction. We're very actively using it at data camp. But just be a little careful. Oh, and also it does work on remote tables. So if any of you have used dbplyr, right, so you can basically, it's Transling, sequel code and r. Funnel join, because it uses dplyr On the back end, will work with remote tables. So now i want to go into my next lesson, which is on ab testing. Everything that can go wrong will go wrong. So basically we've had a couple things happen at data camp. When i was first starting out, it even happens at old shops like Etsy too. So some things that has happened, If we keep clicking through, is that people are put in both the Controller the treatment group. And this should never happen. The idea is you're randomly assigned based on your cookie and you Should stay in your controller treatment group. So some of the other things that happen is that people in the Experiment have no page views. So we see this experiment start Event, but we haven't recorded basically any other activity on Data camp. People have multiple experiment Starts in the same group. So they stay in the control, but They should only enter the experiment one. Why are we seeing them having five experiment starts? There aren't the same number of people in the control and the Treatment. So here that's called the big issue Called bucketing skew. So it should be that if you set It's going to be 50-50, that's what it should end up being. And if it's not, something has gone wrong in your setup. And experiment starts didn't have cookies. So we couldn't track the user. So if we didn't have recorded With the experiment start a cookie, we couldn't see what else They did. I'm also not sure how i ended up with That red dot. Yeah, it's just hanging out there. So one thing we did was that ab testing sanity checks. So this was saying, okay, let's try to figure out early on and Make sure that these things aren't happening once we are Finding these data integrity issues. So you always need to Check your assumptions when you're making. So we never thought that we'd have people without page views or That we'd have people coming in who didn't have a cookie. But we were starting to find this so we realized, okay, in Every experiment we need to check and make sure this isn't Happening. You click the next slide. So the initial solution if we go through here is that, all Right, i had all these rscode. I was like, okay, let me check This thing. Is anyone i had to experiment more than One's? So bucketing skew, if we keep going there with Some things, like, okay, does everyone have a cookie? So i was writing these lines of r code and copying that into Every script i was running for an experiment. So is anyone switch variant, et cetera? What proportion of domain use or ad's are null? So just all these different problems. If we go to the next slide, we can see that this is a similar Problem to before. So you've seen this advice when You've written the code three times, write a function. I want to change that a little bit. So if we click through, we'll See when you've run the same process three times, i suggest Making a dashboard. Sometimes it's very handy to Write the same function. But that still is a little bit Repetitive. So i want to say build tools that Also empower others. So on this next slide we'll see a Tool that i've built for data camp, which is a health checks Dashboard. So this is all fake numbers. But what we see is now for each experiment we can monitor what Percent of duplicate cookies. How many cookies are multiple Variations? How many have no page views? That variation size which will warn us if there are an unequal Split between the control and the treatment. So this is something that is even easier than me running the Same function for each experiment. It's just have it run Automatically in the dashboard. And on the next slide we'll see That i've also built some dashboards for monitoring experiments. So as we click through here we see first we see all the, we can Choose to see the running experiments, the recent experiments, etc. Then we look at experiment metrics. Okay. Do we want to look at registrations? Course starts, exercise Starts. We can see now for each experiment, All right, what's the rate in the control, in the treatment, Percentage change and the p-value. And it will change colors based on That p-value. And if we click to the next one, We'll see that we also add confidence intervals. So it's not so even i'm very lucky at data camp that a lot of People have some technical and statistical skills. But even if you don't i think you can sort of grasp an Intuition from this. All right, so if the confidence Interval is very far from zero and small, maybe it's real change. If it's just barely not overlapping, maybe we're a little More skeptical. So if we look at the next slide, All right, how can we keep leveling up? So i had a common request of what percent increase can we detect in a Two-week test. And so the next part, it looks at, All right, so i'd be answering this question. And i was getting to the point where i could answer it pretty Easily. But i, you know, it would be better if Other people could answer it for themselves. So if we click to the next part, can i make a tool that people Can answer this and without code? So even with having some technical colleagues, sometimes you don't Want them to have to switch to code. So it's going from delivering Information to discovering information. And i said, all right, Challenge accepted, how can i make a dashboard that people will be Able to do this? And on the next slide we'll see the impact Calculator that i created. So this was here, you could choose What's the population of your test? what's the urls? Do we only want signed in, signed out people, et cetera? And then we can see, all right, this is how many people are in The courses, how many, what percent start chapter, et cetera? And on the next slide what i've paired this with is a power Calculator. So it will say, okay, based on that, You will be able to detect this size increase. So now people could look for themselves and say, oh, if i want to do Something on this page, well, it only has 10% of people starting A course that will, i need 20% increase, that's probably not Going to happen. So just in conclusion, go in the next Slide, we see these are the main three lessons, right, which is First, build tools that empower yourself. So this was the funnel Join package. And the second lesson was to build tools that Also empower other people, which are often dashboards. And finally, everything that can go wrong will go wrong, so Definitely check your assumptions. And many thanks to the growth And data science teams at datacamp, to my co-authors, Anthony baker and dave robinson of the funnel join package and to The analytics and data engineering teams at etsy. With that, thank you all so much. You can find my blog hooked on Data.org if you want to try out funnel join yourself. Again, you can find it at github.com slash datacamp slash funnel Join. And thank you. Thank you."}, {"Year": 2018, "Speaker": "Jared Lander", "Title": "Many Ways to Lasso", "Abstract": "The elastic net is one of my favorite algorithms, implementing both the lasso and ridge, and a combination of the two. The main way to fit the elastic net is with glmnet, written by Hastie, Tibshirani and Friedman. But there are many other ways, including xgboost, Stan and TensorFlow. We fit the elastic net a few ways and see how they work differently.", "VideoURL": "https://www.youtube.com/watch?v=R-lVeYjJtw0", "id0": "2018_05", "transcript": "So, today we're going to be talking about the lasso. Yes, this is, I'm pretty sure this is John Wayne, but wrong lasso, we're going to talk about the mathematical lasso. So the lasso is this equation, essentially. Your goal is that you have an objective function you're trying to minimize. The old machine learning is just minimizing something, and depends how you're going about minimizing it. So who here has seen the lasso? All right, so a lot of you hopefully like this. Hopefully a lot of you see this. Basically, the left hand side is ordinary least squares. You minimize that, you have linear regression. We've been doing that for a few hundred years. The right hand side is a penalty term. It's the L1 norm of the betas that is the sum of the absolute values of the betas times this lambda at your penalty term. We could also do the ridge, but today we're just going to focus on the lasso. Similar solutions for both, but just the lasso today. Geometrically, we're talking about the blue diamond on the inside. So that has sharp edges, and that's what gives you the singularities, which gives you variable selection. Similar to, it lets you do variable selection that's actually, you know, kosher to do. So we're going to start with the data, because about data, what are we showing here? And for our example, we're going to use just a portion of the Ames housing data. There's more of it, but it's not on the screen right now. Normally there'd be like a whole bunch of little red dots clustered around the town of Ames, as you can't see it unfortunately here. And we're using this because it's available. But also because it's not a lot of rows, but it does have a lot of columns for the number of rows, which is where the lasso excels. That's what it was invented for. It was actually invented for genomics when you had 5,000 columns, but only 1,000 rows. So this is really good for high dimensional data. Getting the data is pretty easy. You use Ames housing package, make Ames, and then you split the data using the R sample package, and you get into a training and testing split. The data has a lot of information about the sale price of the house. You have the latitude and longitude, square feet, quality, a bunch of different qualities, quality of the house, quality of the kitchen, quality of the basement, all this good stuff. So let's do some data prep. There's an old joke. I'm sure everyone's heard of here, so I'm going to give you a groaner. 80% of the time is spent doing data preparation. The other 20% of the time is complaining about data preparation. So we're going to start off like you do anything else in R for modeling with a formula. You can see we have many, many variables, lots of them. All the price will be our outcome variable, our response, and we're going to regress that on all these other input variables. We're going to be using this a lot of times, so instead of re-typing it, we just save it as a variable. That's new to a lot of people. They don't realize you could save a formula as a variable, but it's a really handy thing to do. We're going to take this formula and put it into a recipe. So we're going to take this recipe, which is like the new cutting edge, cool hip way of defining your design matrix. Instead of relying on the old semantics of a formula, which I know we are using a formula to get started, we didn't have to, but this sort of makes it more explicit what you're going to do. Like for instance, step other, we're taking any categorical variable as many levels and the least frequent of them, grouping them into other. We are getting rid of any variables of zero variance, which means they don't provide really any information. Centering and scaling, all the numeric variables and creating dummy variables. In this case, we are not dropping the baseline. If there are K levels, we are getting K dummy variables because with the last so, you don't need to worry about co-linearity. And we're adding an intercept explicitly because if you don't, the model might not give it to you. After you define our recipe, you then go and prep the recipe. You get it ready. And this is going to do things like calculate the mean and standard deviation of each of the numeric columns. It's going to figure out what levels of the categoricals are available. This way, it can be applied the same way from the training data to the testing data. And I'd like to see model.matrix do that. This is much better. And by saying retain, that means when we want to use it for training data, it's much faster. So there are many ways we can fit the last so. And we're going to look at a bunch of them today because we can. People know me that I like to give my talks like let's do it in an hour because we can just do everything. So first up, Glimnet. Yes, it's pronounced Glimnet. I used to pronounce a GLMnet. Then I got a lunch with the author of the package and he said no, it's pronounced Glimnet. And I've been crushed ever since. So came up with hasty chibsjani and Friedman, like the godfathers of machine learning. And Glimnet was written in 73 lines of Fortran code. It's insane how little code that took, right? It's amazing. So we start off with our matrices. We get our x input matrix, our y output matrix. And for the input matrix, we're going to use a DGC matrix. That's a sparse matrix. The benefits are it takes up less space in memory, but it also computes faster. Sparse matrices have faster computation. So we take our prepped recipe object and we juice it. And this is how we get the training data out of there. Yes, you're going to be juicing and baking. Thank you, Max. So we're going to do this. We don't have to do the calculations again. That's the benefit of juicing. We get our x and y. We call Glimnet. One line of code to call Glimnet to give us penalize regression. Give it the x. Give it the y. We're going to explicitly say it's a Gaussian model. And alpha equals one, which means it's explicitly saying it's a lambda. Don't include the intercept because our design matrix has that already. And don't include, don't standardize the data because we've already standardized everything in recipes. One line of code to fit an entire path of regressions. So when you do Glimnet, it doesn't fit one model. It fits roughly 100 models, one for each lambda. And ends up looking like this. Each line is a coefficient. And for a given value of lambda, it tells you the value of that coefficient. Further away from the zero line, the bigger the impact. As you increase lambda, you're increasing the penalty, the variables get shrunk down. And eventually, some of them go straight to zero and they've not been selected. That's your variable selection. And we've all seen this plot if you've done Glimnet. And you're like, how do I read this? Which one is which? There is a label argument, but it's nearly useless. So fortunately, there's an interactive version in the co-op plot package. You can call co-op path and it will give you an interactive visualization. If I wasn't giving a presentation, I could hover over this and it will pop up a legend showing you the values. And you can even zoom in and pan around and see the different values of your graph here. I think it's a big improvement over the base R plotting. And let's say out of your 100 lambdas, you've decided on one of them and you want one. You can call co-op plot and see the point estimates for each of the coefficients. This is showing the point estimates for lambda of 0.004. I chose that because I did cross validation. I did it a bunch of times. It seemed to work the best. Notice there are no confidence intervals because confidence intervals be damned. Now, I actually like confidence intervals. I think they're really, really important. But so much machine learning doesn't include them. So we're stuck about them. You can't bootstrap this because it's a biased estimator. So the bootstrap doesn't work. You need an adjusted bootstrap. Though, Tipzurani did put out a package recently that lets you get confidence intervals. But it takes more computational power than fitting the last one does. So that was Glimnet. Let's move on to XGBoost, the darling of Kaggle competitions. So similar to Glimnet, it takes sparse matrices. You give it an X, a sparse X, and a dense Y. But you had to compute this XGB dot dean matrix object. And this keeps the X and Y together. And you can tell when they built this, they were thinking of classification because they call the input variable data. And the output label is if the output isn't part of your data. And they call it a label as if you're not going to do regression. But you can totally do regression. So after you get this special object created, you go ahead and call XGB dot train. You give it the special object holding the X and Y. And the key line here is the booster. By default, XGBoost fits a boosted tree. But by setting to GB linear, it's boosting a linear model. Now when Hasty and Tibjani came up with gradient boosted machines, it was a machine. Any model could be boosted. So we're going to boost the linear model. But what we're doing is we're setting alpha. It's a different alpha now. They had to change all the names. So while Glimnet has a lambda and then alpha is a mixing parameter, and XGBoost alpha is the last penalty, lambda is the ridge penalty. So we're going to use alpha of 0.004 to set up a lacer regression. And we use a objective of linear regression. And we boost it 20 times. When you do that, you get a co-op plot like this. Seems like a similar model to before. However, this shows you all the zero variables. So there's a little trick of co-op plot where you can say, don't plot it, filter out everything, and then plot it again using co-op plot. So co-op plot can take data frames in addition to models. It's a little bit of effort. So co-op plot's going to be updated so that it's one step you don't need to use filter in there. So next up is Lars, least angle regression. This is one of the early functions we're doing the last. So written by Hasty, Tipturani, and Ephron. Bradley Ephron, the inventor of the bootstrap. So for this, we need dense matrices. We can no longer use sparse matrices. So again, we use juice from the recipes package, and we say composition is matrix. With our dense matrices, we call Lars. This is from the Lars package. Notice it looks very similar to Glimnet. You have your x, your y, the type is lasso, no intercept, don't normalize. And like lasso, unlike Xgboost, this will fit the entire path of lambdas, not just one of them. And Glimnet actually gets faster when you fit 100 than when you fit one. It's pretty cool. So there is no co-opplot function built into this. So we extract the coefficients, we store it as a data frame. We add in our non-existing confidence intervals because we don't get them. And you call a co-opplot on this constructed data frame. And once again, we are getting similar results as we would hope because we keep fitting the lasso. So then we come to Stan. Gg and MC MC. Now we could write this in pure Stan code. We could write all the Stan code of all the blocks. But we're just going to use our Stan arm because it's all built for us. The reason we can use Stan is because the lasso can be interpreted as a Laplace prior. It's not exactly the same that you have to talk about the mode versus the mean, but it's close enough for our purposes that if you put a Laplace prior on your coefficients, you're going to get the lasso. So thanks to this little trick, we can do this. Stan is written for statisticians, so it uses data frames. So we need to go back to recipes. It's almost identical except we're not creating dummy variables. We are leaving them as categorical because Stan will take care of that for us. So we create a recipe and then we juice it. And this time we say the composition is a tibble. So we've seen sparse matrices, dense matrices, and tibbles, all available thanks to the recipes package. So given this data frame and the formula we created a while ago, we call Stan GLM. And yes, this is GLM, not Glim. We give it the formula and we give it the data frame, teletic alceon. And we say the prior is lasso. Now Stan has a Laplace prior which we could have used. But then you need to specify the parameter. You need to specify the penalization parameter. And I don't want to do that. By using the lasso prior, it will learn the hyperparameter from the data. So yes, it takes longer because you have to do MCMC on this. But I don't have to do any variable tuning. I don't have to do hyperparameter tuning. It does it by itself, sort of like cross-validation in Glimnet. So this is really, really awesome. Let's it all happen automatically. So Stan has its own version of co-op plot called MCMC intervals, which gives you confidence intervals. We finally got them back for the lasso thanks to Bayesian statistics. So Bayes to the rescue here. I don't know how to sort it yet. I'm going to have to ask them how to do that. But a nice feature of Bayes is that you don't need to just get the intervals. You can get the density of your betas. This shows you for each beta, each coefficient, where the mass of it is and where it's most likely and where it's less likely. And I really love the fact that you get an entire distribution, not just the interval. Next up, quadratic programming, because we can. Why not do it? So back to our loss function that we saw before, the sum of squares plus a penalty. We'll get rid of the 1 over 2n because it's a constant. And we rewrite it in matrix notation. The L2 norm of the objective plus the L1 norm of the penalty. This is now an unconstrained quadratic optimization problem. And the lambda is the Lagrange multiplier. We could rewrite this as a constrained optimization, where now it's just the objective function, the sum of squares, subject to the sum of the betas being less than t. And t is inversely proportional to our old lambda. So let's take our objective function and do some matrix arithmetic. All we did was took the transpose properly. We did foil. Yes, we're foiling from middle school. It's matrix foil. We expand that. We rearrange some terms. And we get that bottom line there. You get to that bottom line. And you can rewrite this in quadratic optimization notation, which looks like this. And the optimizers have slightly different notation. For us, x is our data. And the betas are our unknowns that we're trying to solve for. Well, optimization people in general, x is the unknown variable that they're solving for. So in our case, we have betas. They have x's. So you just need to rewrite it that way. In order to rewrite it that way, we need to do a little mathematical trick. And we say the positive side of beta minus the negative side of beta. And that's a trick. If you do the beta plus minus beta minus, it gives you beta. And by doing it this way, I know I'm going through this really quickly, because it's a 20-minute talk. By doing it this way, if you didn't do it this way, you would have to have 2 to the p constraint formulas. And that's too many to do. So by splitting up into the negative portion and the positive portion, we dramatically reduce the number of constraint equations we need. And it will allow us to optimize this quicker. So there's no program to do this automatically. So we need to write our own custom function like this. Now, I took this from this guy, Eric Drysdale, has a wonderful write-up of doing quadratic optimization for the lasso. Essentially, you're computing all those V and X matrices and all those vectors. And then you're fitting a regular linear model to get the upper bounds for the coefficients. And that's sort of what Glimnet does. It computes a dot product between each predictor and the response to figure out the top values. But the crux of it here is the bottom. We do low rank Qp, low rank quadratic programming. It's quadratic programming, but since we have sparsity, it needs to be low rank. So it's a special function to do that. We give it all of our matrices and vectors we created with all of our constraints, which I'm not going to walk through right now. And that's our function we can use. We then build our matrices again thanks to the recipes package. And then we call our function like this, one line of code after writing 20 lines of code. You give it your X and Y, and you give it a T. And the T you would need to tune. I chose five, and I chose it because it looked good in the pictures. And it converged in 25 iterations. And yes, there's a guarantee how quickly this will converge. I forget the exact number, but there's a guarantee it will converge in a short amount of time. We get the coefficients, and it looks like this. Nice sparsity. It didn't choose all the variables that actually did its job. All right, so that's quadratic programming. Back to easier code. TensorFlow. TensorFlow is essentially just a generalized matrix algebra library. So it can do linear regression. It's essentially a wrapper around eigen. That's really what it is underneath the hood. So for TensorFlow, we need dense matrices once again, because it can't do sparse. But also, it can iterate data into memory, so it's not a big deal. We then design our network. And our network consists of one layer. It's the output layer, and it uses a linear activation function. If it's a linear activation function, it's a linear regression, especially since it's one layer. So we give our input, and I chose the kernel regularizer to be L1, lasso. And I chose a, they call it L, our lambda of 0.02, again, because it looked good. You would need to tune these things. And now the weights in a deep learning model are just coefficients from a linear regression. And instead of solving for them using Newton-Raphson, we're going to solve with stochastic gradient descent. So our model looks like this. 64 parameters. All the numeric variables, plus all the dummy variables, plus the intercept. We then compile the model. And if you haven't seen Keras and TensorFlow in R, you don't actually save it back to variable because you modified in place. We're going to use mean squared error as our loss function. And we're going to use RMS prop as our optimizer. We could have used atom, but RMS prop was fine. And we have a metric of mean absolute error. And then we train the model. I think Keras took more work even than quadratic optimization. So we give the x and y matrix. We do 30 epochs. We choose our batch size of 64, the smaller, the better. We use a 10% validation split. We do a early stopping. We do learning rate adjustment. And we call it TensorBoard. You do all that. And you get your metrics, how you're doing it. As the epochs increase, you get better and better. And it appears we have not yet started to overfit. So that's a good thing. So now that we have a one layer model, it's no longer a black box, we get coefficients. You can get that. You get the weights. And if it's one layer, it's a linear model. So you can understand it. It's an interpretable neural net. Who would have ever thought you can get that? So we did all the models. Let's very quickly look at performance. Because a lot of people think machine learning is just like a game where you try to get the highest score. So we take our test data, and we can no longer juice. Now we bake the test data. And so we bake our data, test data, and we make predictions for each of the models. With the exception of quadratic programming, which is just matrix multiplication, everything even Keras uses the predict function. Each one might be a little different, new x versus new y. Apparently, baking might change the order of the variable columns. So that's something that we need to get fixed. So I had to do it there. And you have little things here, but it's all the same. For each of these, we go through, add in the actual values, calculate the residuals, and put them all together as a data frame, allowing us to make this plot. This shows us our actual values versus our predicted values. And ideally, you want them to be as close to the diagonal line as possible. Let's just get the mean square to error for each of these. When we do that, stand one. Stand was the best. I'm so proud of the Bayesians. And XGBoose came in like almost tied with stand, followed by Glimnet, Lars, my quadratic programming model, followed by Keras. Now I'm not saying any of these are better. I hypertuned more or some more than other, because Glimnet, easy to hypertuned. XGBoose fairly easy. Keras took a little more time. In truth, that Keras model took about 10 seconds to run on this laptop on battery mode. It's not terrible, but I didn't spend as much time doing it. So today, we learned Glimnet, XGBoose, Lars, stand, quadratic programming, and Keras. And we did it all with all of these packages. So these slides will be available. You can check out all the packages we use today. So thank you very much. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you."}, {"Year": 2018, "Speaker": "Daniel Chen", "Title": "Structuring Your Data Science Projects", "Abstract": "When we start programming we are happy when our code just runs. As we get more experienced, we begin to add more structure to our projects. We start off by dumping all our files into a folder, then we might create subfolders to help organize our work. We might even structure our projects so we can share our work easily with potential collaborators. As our projects grow, we might need to reuse bits and pieces in other projects, and finally consolidate our work into some written report. R has given us the tools to make your projects more structured and organized. And as we start to become more familiar with these tools, you realize that many people converge on very similar project templates. They all aim to make projects clearer, shareable, and simply \"work\". It doesn't matter where you are in your learning path, you can always benefit from adding a little more structure to your data science projects.", "VideoURL": "https://www.youtube.com/watch?v=UQHz38s3DyA", "id0": "2018_06", "transcript": "Hello everyone. I work pretty well. I'm a Ph.D. student at Virginia Tech. I'm a data engineer at the University of Virginia. I've done some work teaching for data camp and the carpentry. Jared has literally doubled my work trying to start this conference and made me do this talk. I'm involved with the DC meet-up system here. You may have seen me around. I'm also today's event photographer. I do some scuba diving stuff, snowboarding stuff. I do some Python stuff. Like Jared mentioned, I've talked about a bunch of this conference at R in New York. Originally in 2015, I talked about using shiny. In 2016, I talked about testing your code. Then in 2017, I talked about all the random tools or skills you need to do data science. Last year, we talked about just the infrastructure, like all the various servers you need to run a data science shop. What does this all have in common? It's really how I do my work. It's what I teach my students. Usually when I go talk at meet-ups. It all revolves around just how do you work together with multiple people and how do you know the things that you run are correct. Today's talk is about structuring your data science projects because I really just want stuff to run. R has a pretty good ecosystem about getting that all organized. If you really think about it, many people have the same problems. The more you think about it, you realize everyone converges to roughly the same solution. For today, it doesn't matter where you are in your learning path. Hopefully you learn at least one thing. That's usually the goal for everything I talk about. At the end of the day, I just want stuff to run at the end the first time around. I don't want to mess around with stuff. The dataset we'll be talking working with comes from Hadley-Wickham's tidy data paper, specifically the billboard data set. If you go to that repository, he has the raw data, the raw billboard data, and the R code itself that he used to tidy it. We'll be working with that. This begins the tale of two dialects. That tidy data paper was originally written before the tidy verse came around. We'll take the original code and translate it into modern-esque tidy verse code. The first thing for me, if I really wanted to run Hadley's code, you might end up writing something like this. Clear your environment, set your working directory, and then below is the code that Hadley had. You can also see what I have highlighted. There's a lot of columns involved in this dataset. And, whoops, Hadley actually has it all written down. At the end of the day, down here, he converts everything into ASCII text, do some string replacement, rename the column, so it's just as wk, and then the weak number. Reorganize the data frame, and then some of the songs, so these are songs in year 2000, they're just really long, and we just make them shorter so they fit on the screen. So, you end up with a dataset like this. You can see for a given song, it's ranking, billboard ranking from week one to week 72, I think. And you can also just take that thing I said and just condense it and rewrite it in tidy verse world. And doing something like this, it really took me an hour and a half for hearing this out in the back of the car. So, it's not just because I show it to you, it doesn't mean that it's really that simple. But then the nice thing about tidy verse is you no longer have to specify like all of those columns, right? You can just say like I want all the columns from here to here, and then it saves you a lot of typing. And then there's this like one bit that I just like gave up trying to figure out how to do an entire diverse world, so I just went back to baseart. And then you end up with roughly like the same dataset, right? So, let us do a demo. Does that work? All right, cool. The first bit that we'll do is we'll tidy our dataset. So, you can imagine you saw that we had a different column for each week. And then so what we really want is a dataset where each row just has a song, the week number, and then it's column, right? We didn't want like week one to 72 in our dataset. So, we end up cleaning that part up and then saving that out. What we also want to do is normalize our data. So, song information is repeated. And so, me as a data engineer, this all goes into a database. How do we get it to take up less space? You know, we break up our dataset into two pieces, just the song information, just the rank information. And then we can do some exploratory data analysis. So, here for example, we can look at the average ranking across songs, and then the red line just shows what the rank is a year later. So, that kind of makes sense. Songs just drop off after you go. You can also do it by month. So, average rank by month. So, you know, songs tend to just rank higher in November, but that could just be maybe more songs are being released at the end of the year. I don't know, but that's what exploring your data is for. And then, and then you can do the same thing, but what's, and then my question was, hey, maybe it really matters at like what month something starts. And then so, we can draw a plot like that. And then you wonder like, what is this random little peak here? And so, you know, that really, that peak was roughly like 27 weeks after a song was released in December. So, that's what that was. Maybe that has some meaning, I don't know. And then at the end, maybe you want to fit a model. So, let's look at how does, does week, just the number, just the week, can not predict like it's rank. And, you know, you can fit the model, draw out the plot for that model. And, you know, maybe we don't care about the intercept because we're not trying to interpret the intercept. So, roughly you can see, at least the point estimate for every given week, the rank is dropping by one school, one point. All right. So, what you can do is then, you know, put in all the models for all the artists and sort it out. And just because it's zoomed out, but you can kind of look like Missy Elliott down here isn't really doing too well in the year 2000. All right. So, you saw what I did. And, you know, Jenny Bryan's talked about this, setting the working directory, removing, clearing your environment. And then, so what's really wrong with setting your working directory is really because you're assuming a folder structure, your hard coding something in, collaborators can't collaborate, computer might not have the same structure, et cetera, et cetera. And then version control system, don't like that. So, the way you fix that, make a project, you can do this in our studio. It really only adds these two files down here into your environment. And then, you know, you might say, maybe our studio project is doing everyone's using our studio. Yes, that's true. But if you use other things, EMACs allows you to set a working directory when you load it up. And then, when you're just running code, the CD command and Linux, that's really setting your working directory. So, if you could just find that file, you know, where to run everything. What's wrong with the RM command? Well, mainly because it doesn't detach packages. So, when you run your scripts, you no longer, you can actually end up running a function from a library that's not actually written in your script. So, one way to go around that, instead of running that command, just hit control shift F10. And it will just reset your studio environment, which is probably what you wanted to do anyway, but it will actually clear everything. And then, when you run things from the command line, that's exactly what's going on. So, I'm going to skip this demo for time. But are you really done yet? Well, not really even if I set the RStudio project, I end up with a folder like this, right? And this is just that one little simple piece of analysis. And you can already see, like, there's a lot of files that got generated. And so, I'll write some little Python code just as a title. And, you know, how do we fix this? So, there's this paper by William Noble. And he says, hey, maybe it's just a structure folder, something like this, right? Like, here's an idea. And then what we do in our lab is we do something like this. It's, but basically what you really want is you have a data folder, a place to store code, place to store output. In this case, also a place to store your functions, having a read-me file is really nice to have. And then, you know, you can just make stuff folders that's needed. The one thing I noticed is sometimes when you follow this and you start using Git, you might end up with conflicts with images, which is something that I don't know how to totally minimize yet. But we'll skip that. But after you structure all of your project, can you do better? Well, yes, of course you can do better. Let's look at the analysis script that we just wrote. It's about 171 lines. And what does it do? It loads, it cleans, it tighties, normalize, does some EDA, and then fits a model. So then how do we improve on that? So let's just take those six things and then let's write six different scripts. This way, if all you care about is the figure, you don't have to run your entire data analysis pipeline. You can just run the little bit of code that just generates your figure. You don't have to say, hey, run this, take this script and run lines one to 20 and then skip down to line 300 and then run that line. You don't need to do that anymore. And it should be pretty obvious, just like what Major Mike and Iraba talked about. It's pretty obvious what order you're supposed to run this stuff in. And you can give it to someone else and they probably will have an idea of like, hey, where do the plots come from? Well, maybe a DDA script, right? And so we'll skip that. So what else? So Rachel Tappmann, a couple of months, weeks ago, gave a talk here in DCR ladies community. So she runs the Seattle R ladies group and she gave a talk here in DC about putting together a data science portfolio. And so this is totally from her slides. So what you want to do is you'll have a project and you just want to be able to show that you know what you're doing, right? Break your stuff up into pieces. That's all great. But the one thing that she was able to put into words and I couldn't really convey was, you know, avoid like when you go into a meeting, no one really cares about all of the data processing code that you have. So when you have like a giant R markdown file and you're like, okay, that's great. Just go to page 20 because everything else was just me cleaning data. You really just want to come into a meeting and be like, all right, here are my plots. And then just people can talk about that. And if you really need the code, which R markdown gives you, it's just the code for the plot. And then someone is like, what is this X, what is the actual variable being used? Well, at least you have some documentation about like, what's the actual variable being used. And so that's where Knitter comes in. So you can see like we still have everything in multiple pieces. But only the things that, you know, you might want to take into a meeting will make those the R markdown files, those instead of like the entire thing. But sometimes working in Knitter and RStudio projects get weird because of working directories. If you haven't noticed that, if you try to knit a document, it by default thinks that the working directory is the actual Knitter file and kind of the whole RStudio project then kind of falls apart. So how do you fix this? So Jenny Bryan wrote this package called here. It's based off of this package called RProjectRoot. So when you wrap a file path around the here function within here, it has a set of heuristics that tries to find like what the root directory is. Right, so we have this RProject folder and it actually knows to look for that RProject folder. So now you can actually say something like, you know, you don't really have to change your code. The file path is the same and then you can put it in your Knitter document and you can click Knit and things work and you don't have to have, you know, this weird commenting out of like, this only works if I'm clicking the Knit button or like this only works if I'm running the Knit command in the command line. Only downside is when you use the here package in a Knitter document, you kind of lose the nice ability of tab completing your way through the file system, but maybe someone can figure that out. So I think I might actually have saved enough time to show the actual demo. So what we have here is, you know, this is a little nonsensical. At least the first one is like loading a dataset just to write it out. But the trick to this is each one of these scripts, the first thing that you do is really just loading in some data that the previous script dumped out, right? And, you know, you don't do this like at the first time. You might actually write this all out in one giant RScript like I showed you in the first time. But then when you go back and you start cleaning up your code, you'll end up breaking things up this way just because at the end of the day, you might just want to rerun the figure and not the other thing. And you might just rerun the figure and not the entire pipeline. All right, so this stuff is a lot. It's really flexible. Just, you know, tweak it as you need. So if we end up down here to the R Markdown files, you know, it's the same amount of code. It's the same thing. Now we just put everything into these R chunks, right? And if you, all of this code is all on the GitHub, so you can actually click through and look at it in more detail. What I ended up doing is just running the entire analysis through the tidyverse dialect in R. And so now you have this nice situation with using here and here. So you can say like now, if I really clicked knit before, it's going to assume that the working directory is this knitter analysis billboard EDA folder. But now I can say like, hey, I can still specify from the root directory, right? After I built out my folder structure, you can see I had an analysis folder, a data folder, an output folder. So now because it knows that this R studio project exists, I can just specify like, hey, the file exists under analysis billboard EDA, 0, 4 models at RMD, right? So that's what all you have to do to make here the here function work. And so now you end up in this nice world where you can actually click knit and, you know, this runs like the script. If you were to run this in a command line, but then knit button actually does the right thing and generates a document that you wanted, right? So you don't have to like fudge around with, you know, commenting out something just to render a document. So one thing that this example doesn't really show is like, what do you do with functions, right? This is a pretty simple analysis. When things get more complicated, put things into functions. So my suggestion would be, you know, create a capital R folder. And if you ever have a function, just dump your functions in there. And then what R has, you know, the ability, the source file. So one, it saves you a lot of typing. You can just source the file under the root directory R. R also has like the source directory function. So you can just say like source everything in here for my analysis. But what this really is, is really setting up, setting up the analysis to be turned into a package, right? All R needs is a capital R folder in like two other files. And then this thing can be installed as a package, right? So if you end up running this analysis and another analysis is kind of related, you can now just import all of your functions, right? You can turn this thing into an R package and your analysis project that we have broken up is already set up for that. So you don't have to like change what you already have. So then the next question was that I, like what about like scholarship or formal documents? I prefer to, if it gets at that level, just do everything in tech. It just gives you a lot more control. And so there's two things, there's two ways you can approach that as well. You can have a totally separate repository that's just your tech, just your scholarship and your writing. And then you can have it as a sibling project. So the analysis project folder is on the same level as your tech project folder. And then if you wanted to, because you have like this output folder, because everything is, you know, everything is all nice, in your tech document, you can, you know, go one directory up, go into your analysis folder, and then under output, which is where all your figures are, you can just insert figures that way, right? Another way you can do it is like a child project. So you'll have your report and then you somehow link in your analysis folder inside that tech project, right? One way if you're using Git is you can use Git sub-modules for this. But even something as simple as like a symbolic link or a shortcut, this works as well. And so, oops, uh-oh. That works as well. And so if you're on like a Unix system, you can create a shortcut that says, I'm going to create a symbolic link. Here's the folder that I want the link to. And then this dot just says, I'm assuming that I'm in my tech folder. And then your code doesn't really change. You now don't have to say one directory up. You can just say, hey, just go down this file path and insert my figure there. So then, you know, the near button puts by default, if you're generating these reports, the output or the HTML or the PDF, it's in the source file location. That's pretty annoying because I wanted stuff in the output folder. Why stuff in the analysis folder? And so the way the solution to do that is under the R Markdown library, there's a function called render. And then there's this parameter called output directory. And then you can just say, hey, when I click knit or when I run the render command, like, take the output and put it in this folder. But then you might end up in the situation where like, okay, I'm running like the render command from the command line. Like there's all these commands to run. If I have like six scripts, like I have to type all of this stuff out. So you could make a shell script and just runs everything in order. You can use make. And then you can take all those scripts and you can take your make file and RStudio has this ability to create a build file. So you can actually use RStudio to be like, hey, rebuild my analysis. And so what does a make file look like? So I can do something like, hey, I don't want to type the analysis billboard EDA thing over and over again. So I'll give it a shortcut. By default make, we'll look for a command called all. I'm not going to do anything with all other than if you type in make. I'll just spit back the possible commands that you could do. But then here you can see, I'm going to say like give it essentially like a variable called billboard EDA. And when I say make this, these are all of the commands that need to get run. And you can see like this make file becomes self-documenting. Like how did you do that? Well, like look at the make file and everything is like listed in the order of supposed to be run. And then at the end, you know, it's kind of nice to have like a clean thing. So like, you know, delete the intermediate data sets or your figures. So you can just rerun everything in a clean setting. And so my last demo. What does this look like? So in make, you can say make and you can see like I just said it will just spit out my commands. I can then say make billboard EDA and it will just run through all of those six scripts. And then in here you can see that all of my plots and figures would have been dumped out into the output folder that way. All right, so in sum, use R. Like I said, it doesn't matter where you are in your R learning career. You can always, you know, hopefully you've learned something. And then slowly as you can always add a little bit more structure to your projects and then hopefully you'll end up in data science, data cleaning nirvana. So since these sides are posted, I've just linked to a bunch of resources. If you care about like, hey, that slide deck looks cool. How did Dan make it? Use something called Mark JS. Like I said, a lot of people who think about this all converge into the same roughly the same ideas. So Jenny Brian, John Miles White, John Bliss trick. They've all, you know, tackled this problem in many different ways and have also all written about it. And so for that, I would say thanks. This is my dog. I've had him for two years. He just turned four and that's it."}, {"Year": 2018, "Speaker": "Marck Vaisman", "Title": "gg-what?: A Look at the ggplot Ecosystem", "Abstract": "Most R users are probably aware of the ggplot2 package for creating elegant visualizations. ggplot2 is either imported by or is a dependency for approximately 1,400 other packages. In this talk, we explore the connections and influence that ggplot2 has with so many other packages. We explore some of these packages and the extension mechanisms for ggplot2 by visualizing LEGO data obtained from Rebrickable.com.", "VideoURL": "https://www.youtube.com/watch?v=tP-2D6sFc88", "id0": "2018_07", "transcript": "So, I'm really excited. I am going to talk about GG what? Oh, by the way, for those that are kind of strictly on design, yes, I do have a red background and I know it's sort of a no-no, but it's after lunch, so I had to figure out a way to keep you guys awake. Okay? So, we're going to talk about GG what today. It's actually about GG plot and a little bit about sort of the ecosystem, right? Because GG plot has been around for a long, long time. All right, before I do that, kind of the caveat, you know, my opinions are mine alone. I am affiliated with a lot of organizations here locally. My day job, I work for Microsoft. I am a technical solutions specialist for Azure Data and AI. I work with the federal government, so I help the federal clients sort of transition their workloads into the cloud. Anything that's related to data science, analytics, machine learning, etc. As Jared said, you know, I am one of the founders of Data Community DC. For those that don't know, Data Community DC was started in 2012 with two meetups, Data Science DC, Statistical Performing DC, and now it's sort of grown to a community of over 20,000 people in the local area. So, we always have events. If you go to datacommunitydc.org, you can look at the calendar, see what's going on. I still run kind of the statistical program in DC meetup. Not as often as I used to, but I still try to kind of have a few events a year. I also teach. I am a dad junk professor at Georgetown in a GW. I teach a big data class, and this semester I'm actually teaching a data-vis class. So, kind of the talk today sort of spawned from that. So you might ask yourself, well, what do I do when I'm not teaching, working, or running a meetup? That's my family. My daughter Samantha, my son Joseph, and Karen, who's in the back. Hi, Karen. And she's really embarrassed. So a little anecdote about my daughter Samantha. I went to the Our Ladies meetup last week with Kelly, and I got home. She knows I run all these things. And when I got home, she asked me, what are you doing? I said, I was with this meetup, and her eyes just went like this big. She's 13. She's like, and she's not about our, but when I told her about Our Ladies, she just went like that, and she's like, I want to go to the next event. So I will bring my 13-year-old daughter to the next Our Ladies event whenever that happens. So I'm excited we might have a potential, our user here. I'm really excited. Okay. GG plot. So raise your hand if you know and or use GG plot. Okay. Yeah, that's what I figured. So how many people got started in R because of GG plot? Okay. So a lesser, but that's actually kind of my, I mean, a little bit of my story. The first few things I did when I started learning R, which by the way, I started running the meetup group without knowing R. I was really interested in the community. I had gone to New York a couple of times before Jared's time. Just clarify that. And I'd seen what they were doing over there. And I'm like, you know, what is it? Imitations is the best form of flattery. So I figured, let's just do the same thing. And I volunteered to kind of start running the group a few years ago. And I didn't know. And first few meetings, we just sort of sat a small group of people, 10 people, and our speakers would just talk and we would just listen for three hours. Sort of everyone was in the room odd about what was going on. So that was the start. But anyway, going back to GG plot. The first things that I started doing a lot with R was of course visualizations. And this was probably about 2010. So this is before GG plot even 1.0. It was probably still 0.9. So the GG plot you know today is very, very, very different than the GG plot of yester year. Right? It is 10. There was an article a few months ago or last year about the 10th birthday of GG plot. And it's a version 3.0. So you know, before, if you wanted, and you know, this is actually, I found this today. This is a little gist that I put on my GitHub account years ago. I needed to do a hex bin plot. GG plot didn't have a hex bin plot. So I used the hex bin function, the hex bin package, and the hmys package to do all the cutting and basically extracted the data from the subject and threw it into GG plot for the actual plotting part because GG plot didn't do hex bin. But now it actually does. But the point is how many packages? So there are about, I don't know, 13,000 packages on crayon today roughly. How many packages do you think that GG plot either, like, packages that depend on or import? These are two different words, but essentially that, for lack of a better explanation, packages that actually depend on GG plot. Throw a number out there. How many? 5,000. 5,000? No, that's too big. Book good guess. Anybody care to guess? 40,000. You're not the next contestant at the prices, right? I'm sorry. All right, so the number is 1400. I mean, think about it, right? And these are things that you probably don't even know about. Neither do I because it's so big. But this is kind of the point, that's sort of what I wanted to show you a little bit about the things that are out there now, you know, focusing more on the more current packages or things that are really ingrained into GG plot, right? Because the GG plot ecosystem has evolved so much. It's a great tool. I keep using it not on a day by day basis, but, you know, I still love building a graph, kind of thinking from data, building a graph, and sort of coding it and making it look beautiful. So, all right. So I know, sorry, the text is a little small, but here's a plot. These are, so this is a facet of plot, you know that. So first of all, this is a plot that shows the, essentially, the number, on the X axis, you have the number of CRAN downloads for a given package over the last five years. I figured there's no real way to know, understand kind of the extent of a package, but as a proxy for that, I used a package called CRAN, I forget, to pull the statistics from CRAN. And you know, which tells you like all the different packages, what depends on what, so you can actually build a graph, like, what's the dependencies, not from a DevOps perspective, and you know, Max can talk about that because Max builds GGPlet every time there's a release, and I hear it takes like 12 hours. So, but more for like understanding. Now, here's the thing. I actually, the color blue, so the color red means just a package. The blue color means a current GGPlet extension. GGPlet has a whole slew of extensions going for it today, about 40 if I'm not mistaken. And these are really, think of them as add-ons, things that allow you to do other things that you can't really do with GGPlet unless you, you know, you could code it. But these are just mechanisms of extending GGPlet with adding new statistics, new geomes, new geometric objects, new, new layer, sorry, new layering capabilities, et cetera. But what's really interesting, right? So, we see, you know, what's interesting is like the top, the top packages over here. So, there's depends and imports. I'm not really sure what the difference means. So, anyone that's more versed in the package ecosystem to talk about that. But it's just the way, you know, some packages depend on a package, some package import. Import usually means it actually loads another package into your namespace as you, as you load in your package. So, but we see at the top, this is the depends. So, we have HMISK, that's Frank Carroll's kind of historical, our package, like utility package. It depends on it's a GGPlet, I didn't know that. Carrot. Now, some of the blue ones are current extensions, so we've got, I'll just read them from my screen if I can. But they're like, GGP something. It's just, sorry, I can't read it. But the point is, you know, what's interesting is you start seeing kind of the, now branded. This is over, this is just a total over a five year period. Some of these extensions are newer, so the numbers are definitely, you know, aren't supposed to be that high. But they're actually pretty high if you look about it. Now what does that mean? That means that people actually know about these. I thought people would not necessarily know about these extensions or other packages. But there's a really long tail of all these packages that depend on GGPlet. That again, I have no idea what they are. And probably neither do you, unless you're doing something really, really specific, or you wrote a package and you uploaded it to CRAN a few years ago because you were doing something really, really specific. There's just no, I guess there's no other than like some of the, you know, like the R studio site and the website that I'm going to show you in a second. There's really no good mechanism of understanding the extent of it. But I wanted to show you that there is a huge impact, right? There is a huge extent of what, like how GGPlet is being used so much. So actually, this is the code. So actually to build this plot, I use one of the extensions. It's called GG stance. And this is just the code. And essentially what GG stance does is it converts, it takes your standard geomes and makes them like flips them horizontally. Rather than like building a plot normally and doing coordinate flip, you build it from kind of, you just sort of switch your variables around and it builds your geomes in a horizontal fashion. If we go back to the plot, that's, you know, it's a horizontal bar chart, but I define the X as X and the Y as Y and I didn't have to flip anything. I wanted to try a new one, another of the current extensions called GGPel, which allows you to take text. It actually, I'll show you a picture about it, but it doesn't, the, it doesn't, the geome, sorry, the geome bar H, geome doesn't take a label as an aesthetic. So I couldn't use, it wasn't working that well. So what are these extensions? These extensions are called G-plot extensions. There's a great website called GGPel, 2.XTS.org, a beautiful website. And if you go to it, it's over here. And again, there's all of the ones that are published. I don't know if there are more. There may be more, I'm not sure, but at least these are published on the website. You can actually add your extension if you build one. So this is, this is a good resource, right? If you've never heard of these extensions, like at least you know where to go find them. And a lot of these incorporate, you know, some of the newer dynamic plotting stuff. You know, it's not just static, it's for rendering stuff on shiny or for web applications or whatever. More of what I'm focusing on today is more on the static stuff. So now you know not. So I'm going to just, you know, I'm going to talk a little bit about kind of the ones that I kind of like the most. So there's 40 of them. I'm just going to go through the screenshots. The first one is G.G.I.R.F. And what this does is just allows you to make G.G. plots interactive. You know, you don't sometimes you don't really have to build a shiny app or do something really complex. You just need to make just a simple graph interactive. So that allows you to do that. This is G.G. Repel. So actually I have the website open here. So the whole idea of G.G. Repel is like, oh, are you? Oh, sorry. Let me exit presentation mode. So the whole idea of G.G.I.R.P. Repel is that if you pass in a G.O.M. with labels or something, it will actually relay out the text part of it so that it actually looks legible because otherwise it's like your labels on top of your data point. So pretty good. I mean, sometimes you need to plot stuff and your text is just. And again, I really love G.G. Plot. And a lot of times the first call, like the G.G. plot and the G.O.M. gets you probably 80% of the way there. But then to get it to really, really look nice and to look picture perfect, publishing ready. There's all these options you can tweak. It's so extensible. But learning all the possibilities is just overwhelming some time. But anyway, I still love it though. Let's see what else. The next one is this one is called G.G.F.hops. Who here makes graphs for a living? Okay. So if you haven't tried G.G.F., I really recommend. If you use I.G.F., G.G.F. is actually a wrapper on top of I.G.F. And it uses the G.G. plot syntax. It uses sort of kind of the tidy verse mechanisms and the G.G. plot syntax to really build a graph. So I'm going to show you quickly what that might look like. That's it. Okay. So this one here. I guess I'm jumping the gun here. So this is actually a plot made with G.G.F. and what you do is you build, you use that combination with a package called tidy graph which basically allows you to make a graph object from tables and data frames. And then you pass that into G.G.F. and you tell it to plot your nodes, plot your edges. And it uses the same type of syntax, the aesthetic syntax and this and that. So you know, I graph is great but the plotting part of I graph is, so raise your hand if you agree with that. Okay. There you go. So try G.G.F.F. Let's see what else. There's another one called G.M.Net. I haven't used that one but it's also another plotting package. Let's see here. This one is called G.G.Themes. It sort of packages together a bunch of pre-made themes. I think there's like an Airbnb theme and there's a whole bunch of themes that people have created for making like the economist theme or things like that if you want to use make your plot look like you're not in standard G.G. plot thing. Again, there's 40 of these. So I'm not going to talk about them. I did want to make you aware and that's really the purpose of today. I just wanted to share all this information with you in case you didn't know about it and show you like a couple of things how easy this can make it. Another one, these are more. So there's G.G.M.O.S.A.K. and these other ones. The last one I'm going to show today is this one called G.G.Ridges which I really like. To do that I was going to go and use, well I guess first of all, who likes Lego? So we see a Lego theme, right, Rafi? I didn't know you were talking about Lego either or Lego as a theme. Again, in order to demo some of these things I wanted to build a couple of Visas and I got the data from a website called Rebrickable. If you don't know about this it's really cool. It's actually a website, kind of a user, kind of a Lego enthusiast maintained website. There's another one called Lugnet I believe. But what they do, they actually have all this database of all of the different Lego sets that have been created over time with an inventory of parts, what colors they have, the themes, what not and they actually provide all the data in CSV formats and this is sort of the quas, this is kind of, I guess, not in a database format but this is the link, the data diagram or the data dictionary between all the sets. So you can see what theme set is, what year it is and so there's actually like a lot of really creative stuff that you can do and I was hoping, I mean, I started playing around with some of these extensions to do some visualizations. I was successful in some and not so successful in others. So the first one is, at least I wanted to show you right, for the purposes of GGBrigger. So this is just a plot, somebody else made this, this is a plot of the average number of Lego parts over time, kind of just taking all the sets per year, taking the average and counting the number of parts. So you see that on average it looks like the sets have been getting bigger, there seems to be somewhat of a dip, I'm not looking to explain that. Anyway, so it's a nice looking plot, probably could be better. So that's kind of the data that we're working with, the table of the years and things like that and so I kind of wanted to extend that a little bit and. So I always tell people not to live code and then I always end up live coding. So here's my data frame. So again, the next thing is I just wanted to look at a distribution, like I didn't want to look at the average, I wanted to look at a distribution. So what do I do, I just do a box plot. That's kind of ugly, it is sort of, I mean, you do have some sort of a power distribution, you have a lot of sets with a small number of parts and a very little number with a big number of parts, so that's where you have all this stuff and all these outliers. So again, this is just using basic GGPOT, but I wanted to show you the GG ridges because I thought that that might lend itself to do something that looked a little bit interesting because I really like this, at least the way they show it over here in the website. I like, let me go back here to the extensions. I really like this. What I was hoping to do, it's, oh, sorry, it's all the way down here. Something like this, basically, it gives you plot the difference, maybe the Y-axis being time, so just kind of see how that distribution changes over time with some color. With GG ridges, the nice thing about GG ridges is that, this is a personal opinion, but I like these because unlike box plots or violin plots, so box plots just gives you a square, and violin plot takes the distribution but makes it, it opens it up and you have two sides to it. A lot of times that makes the data look inflated, I believe. Just visually, I think of violin plot, it probably has its use cases, but personally, I believe that I think a violin plot should spare you sparingly. I kind of like this because it allows you to plot all these different distributions over whatever, however you're going to partition your data. So I attempted to do that. It wasn't very successful, but I'll show you at least, so I'm loading in the library, I'm using GG ridges, and I'm just loading this in. It looks like one big ugly mess. So yeah, I still don't know how to get it to overlay, but you kind of sort of see, you see a bit different, I don't know if I'm plotting the right thing either. Again, it's kind of a nice thing, it's true because I'm calculating, I think I'm calculating a different kind of distribution. Again, read the docs. Yeah, read the docs. So to wrap up, I will go back to this, and again, this was a little plot that I made with the G graph. And here's the plot, here's the code for the plot. What I was hoping to do is to do something like this. With the colors of the Lego, the evolution of colors over time, of Lego, so does anybody recognize this? So I'm sorry? Yes. So this is the evolution of Crayola colors over whatever, 70 years. This was, I don't remember who did it originally, but actually someone, this is not, I don't believe this is a GG plot version, but then someone went and took and made it like a GG plot version of this. So I wanted to see if I could use the extensions, some of the extensions to kind of make, you know, take, because the colors of Lego have changed. And the nice thing about the data set is actually rich enough that it tells you the colors of the pieces with RGB values and all this sort of stuff. So, again, I'm kind of a Lego nut. I'm kind of was playing around with this, and I thought that was an interesting data set to use. So that's what I wanted to do, haven't achieved it, but I'll get there. No packages were harmed in the making of this talk. These are some of the other packages that I use that are not tidyverse or GG plot, so I use reprex. Thanks, Mar for telling me about that last night. So this code is actually cut and paste from our, in our session, and I could just paste it into PowerPoint and it looks great. So thinking about that, cran logs and deal stats are where I got the data for the packages, like the package metadata. So thank you all for being here. It's really a pleasure to have you. Come to our meetup events. Join us. Be a part of the community. Enjoy the rest of the conference. Hang out. Have fun. Talk to people. And thanks once again. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you."}, {"Year": 2018, "Speaker": "Catherine Zhou", "Title": "Anomaly Detection With Time Series Data: How to Know if Something is Terribly Wrong", "Abstract": "With the rise of streaming data and cloud computing, data scientists are often asked to analyze terabytes of data. The sheer amount of data available leads to a lag time in identifying irregularities, resulting in lost time and revenue. We can pinpoint these outliers through anomaly detection algorithms, which can be repurposed to monitor key metrics, website breakage, and fraudulent activity. I will demonstrate how we can build a system for anomaly detection to uncover blind spots in large datasets and reduce fire drills at work.", "VideoURL": "https://www.youtube.com/watch?v=5nfe835TVcY", "id0": "2018_08", "transcript": "I really like Bikes Bloss. It was actually for a DC bike conference about five years ago. But now I am a senior data scientist and manager at Code Academy. And today I'll be talking about anomaly detection with time series or my alternative title, how to know when something is terribly wrong. So a little bit about me first. I'm a proud New York R. I've been a member of the New York R community since about 2015 or 2016. But I'm born and raised in Brooklyn, so I've been there my whole life. And I'm currently at Code Academy, but before this I was at JetBlue in New York or DC Sports Clubs. And here's just some social media stuff if you want to tweet along. I know Emily will be. Okay, so my app shot is kind of one thing and you can read the full thing online. But basically my guarantee for this conference is that I will help you uncover blind spots in large data sets and hopefully reduce fire drills at work, hence the burning fire emojis. So we have a lot to cover today. So today we're going to go over time series data and then we're going to learn how to plot and visualize data using Google Trends data the tidy way. And we're also just going to explore different algorithms for anomaly detection and talk about case studies, how this can be used at work or for personal for fun. So this whole talk started because Jared pitched an idea to me to do a talk on forecasting. But what Jared didn't know is that I have a really complicated relationship with forecasting. And so why is this? So he knows that I've been working with time series data for a while. But in doing so I realized that as data scientists when we work with time series data we want the data to be clean and ideally if we want it to be able to forecast it they're like clear seasonal trends either daily, weekly, this is like Google Trends data for Halloween. So annually we see it up ticked, this is like forecastable data. Or maybe something like this where people look up movers throughout the year, around the same time of the month and there are a bunch of people who move in the summers. So you see like annually a year over year you see clear trends. The thing that I realize is that for a lot of companies or public policy or just a lot of data sets that we handle aren't forecast grade. So there's this clash right like we want to work with data that's forecastable, that we can predict stuff with and model off of. But we often work with data that's highly inconsistent, terabytes in size and instead of there being like several key metrics for you to direct your attention towards your boss is like can you look at this and like these 10 other things. So it becomes really difficult to monitor and a lot of I feel like my job is diagnosing things and it's like finding a needle in the haystack. So I think for data scientists and like the tech industry growth is a double edged sword. So you see a lot of these hockey stick growth charts. And obviously working for any company you want it to grow and do well, but it makes it really hard to do anything cool or fun with the data because it's so unpredictable and the volatility just makes it really hard to do a new modeling. So here's an example of some data that I work with and you can see that hockey stick growth right here. And when I try to project out to 2019 using this is a forecasting package profit, which is open source by Facebook, look at when you the degree of uncertainty for 2019 is so why that this forecast is like not actionable or useful. And it's just not not really great. So I just want to switch gears and talk about dealing with fire drills at work. So how many times, so a lot of the time I feel like my job is like this. So I joke around my product manager that we're like firefighters and constantly putting out fires. So these are real emails and slack messages from work. You can see it's like April, August, September, like every month there's a new fire drill. And something is terribly wrong. And oftentimes by the time we realize it, it's kind of too late. So I think by the time we saw this, it was already down here and we would have wanted to realize this like way earlier, right? And the reality is something like this can result in like if you're working for business, it's a lot of like time and money that's going down the drain. If you're looking at like natural disaster data or something, like you could be missing like some sort of earthquake or something like that. So I feel like it also, the problem with fire drills is it really did capture your workflow. I'm working on something really fun, exciting, like long term analysis. And then suddenly like I hear from my VP and it's like drop everything like code red, start working on this thing. And then I just, you know, lust after the thing that I actually wanted to work on. So I've hit this idea to Jerry to talk about anomaly detection in time series data, which I found really useful at various companies I worked at and even more actionable than any type of like week forecasting model. So yeah, so anomaly detection leads to earlier detection and hopefully reduces the number of fire drills. And I think a lot of people get the feeling that a lot of the time our work is more like reactive and proactive. So this kind of helps us get ahead of the curve. So here are some applications of anomaly detection. If you remember my DCR promise at the beginning of this talk, it's to help you uncover blind spots and large data sets. So an example of that would be the chart I showed you before that was breakage at the last company I worked at an airline like flights have gone out empty because of human error and we would have been able to get ahead of that with anomaly detection. So this is going into the end of the talk where I will simulate live voting. But hopefully by the end of the talk we'll be able to go from this to this which plots your outliers and anomalies really nicely and you can explore and like you can do like cool things like send emails to your team when like something drops. So let's get started. I put up my talk on the top and I'm going to add the slides after the talk. But one of the really cool parts about going from a fortune 500 like non tech company to tech company means I can like share my code now which I haven't actually been doing very much but this is like the first big thing so hopefully it keeps me accountable. So the first thing we're going to do is create a data frame. So I showed you some proprietary data before but I thought it would be cool and fun to use like Google trends data. So here I pulled the Google trends data for the word vote for 2004 to 2018 and produced the table here to see that everything worked okay. And then next the two packages that we use are tidy verse and anomalies. So anomalies actually sits within the tidy verse and it was just released this year in April so it's really new. It like draws from anomaly detection which is open source by Twitter but it plays really nicely with the tidy verse and it's Matt Danso I think at business science does a lot of cool package work so this is all thanks to his package. So first we prepare the data and get it ready for anomalies and then this is all the code that you have to run which looks kind of lengthy but I promise it's not that bad and you can mostly like plug in place afterwards for fun. And you come up with this. So for the word vote you see some like cool trends here like these are the presidential elections in 2004, Obama, Obama re-election, that giant uptick of fear is the 2016 election and the really cool part is the midterm elections actually outperformed the last few presidential elections because of all the momentum. So that was really cool to see. And then so I'll talk more about the methods used up there Twitter and GESD. But I wanted to replicate this with Florida to see if this was like a proxy metric. So it's kind of cool. You see the seasonal data here. So around the beginning of every year in the winter all the DC and New York like snowbirds jump down to Florida because we're too weak for the winter. And then it knows that those are outliers but these over here this is the 2016 election and I believe this is the midterms and I'm not sure what these are maybe like hurricanes. But yeah you can see how like Florida being the swing state correlates with election timing. Yeah so I'm not going to go to this slide too much but anomalies also has features where you can like look at the inner workings of how the algorithm detects the anomalies. So there's just like a lot of really cool functionality you can use. So I'm going to go over briefly like what method you should use. So I think this is like my cheat sheet for anomalies. So Twitter and GESD does really well for highly seasonal data. So the voting data that I showed before there are a lot of like annual trends or four year trends to year trends in voting data. For everything else standard, Lewis and IQ are tend to do okay. And trend period I think that's like the fun part where you just apply your domain knowledge. So if you know a lot about like voting or like if you want to analyze like your favorite sports player or something like that you can do all of that. So going back to one of the examples I used earlier the moving data here you can see the difference between. So this is a default. So standard, Lewis and IQ are you can see how like it doesn't really detect the trends and moving data from 2005 to today. And it points out some outliers. But when we use the Twitter method it controls our seasonality. So you see these like clean trends and removes the outliers. And then once you pull in GESD which is also from Twitter's anomaly detection package you can see how the gray area tightens. So that controls your stream value. Oh you can try this on different keywords. I mentioned like sports or like pop culture. So I thought something that would be funny if we're not looking at longer term data would be Pete Davidson. So, so this is so for those of you who don't know Pete Davidson is a comedian on SNL who kind of went viral and like broke the internet this year with by getting engaged to Ariana Grande who's a pop singer. So they met around this time and they got engaged shortly after. So internet went crazy. He instantly shot up in fame. And since this is 2018 data we can use STL instead of with the Twitter method. And then the second outlier was a few weeks ago when they sadly broke off their engagement. So yeah so you can do a lot of cool stuff. I would recommend playing around with it. Try this out. You can use whatever data you like. You can replicate this if you want. You can do you can look up your favorite politicians. Yeah it's a really cool package. And I think a good addition to the tidy verse. I'm trying to see. Yeah so there's a lot more of this package should do. You can see the documentation. I'm looking to it here and I'll add the PDF slides to my GitHub. But yeah there's a lot more I can do that I didn't get to go over today. But hopefully just pick it up and go. Google Trends is also really fun to work with. So the last thing I'll leave you with is just good luck and have fun. The New York art community I think has been a real gem in my life. It's just professionally and personally the bike rides and pizza which Jared will hopefully pull up a picture of later. And yeah I've just met some of the smartest and funniest and interesting people with weird niche knowledge about things. So that's always fun. So yeah if you're niche you don't give up have fun with it. Like analyze weird things do whatever you want. But yeah have fun with it."}, {"Year": 2018, "Speaker": "Anna Sofia Kircher", "Title": "Proximity Matching with Random Forests", "Abstract": "Estimation of treatment effects needs data on a control group. If there is no randomization in the assignment of data points to the treatment group or the control group the estimation strategy - no matter how sophisticated or good it is - will always suffer from selection bias. In the realm of financial investment this bias can be massive. But there are many ways to create a synthetic control group. This talk will focus on finding a control group using proximity scores from the proximity matrix of a random forest to monitor investments. We will discuss how a random forest can find similarities between observations either in an unsupervised or supervised setting and use that to create a control group for comparing the performance of a purchased portfolio of loan receivables against one that has not been purchased.", "VideoURL": "https://www.youtube.com/watch?v=ULm9BiTJDjI", "id0": "2018_09", "transcript": "So hello everyone. I'm very excited to talk to you about proximity matching with random forests today. I'll quickly introduce myself. My name is Anna. I work at Lendable, a little fintech startup company. And we represent a link between alternative lenders in Eastern South Africa and investors in the US or Europe. And we help to kind of our alternative lenders, our customers to grow in a reliable and fast way. And we do that by purchasing part of their portfolio that could be microloans or asset backed loans with like motorbikes or solar panels and stuff like that. And we use our heavily for portfolio managing and portfolio monitoring as well as to predict default rates or cash flows with our risk engine. So after we purchased our portfolio, we have like, we maintain our ongoing relationship with our clients and we have like monthly meetings where we talk about the current portfolio performance and things like that. And the big thing that we heavily use our in is to monitor performances. And so one major thing is to look at do they still care about our portfolio that we have purchased versus the other kind of part of the portfolio that we have not purchased. The problem, what this is like a randomly chosen graph from this kind of monitoring thing. And so in this case, the green one is not purchase portfolio and the yellow one is the purchase portfolio. And if that would be repayment rates, we would be a little bit concerned by this swap, kind of the normal portfolio. So to say, it's doing kind of averagely fine. And the other one kind of drops. Sorry, the other way around. Nevertheless, the problem with this comparison is when we look into a deal into purchasing a portfolio, we have a like a very extensive catalog of criteria, due to legal or investment rules and things that I'm not that able to talk about because I don't know them all. But so when we purchased the portfolio, there is a lot kind of selection going on. So this comparison is by the selection filters. And there are multiple ways to solve for this bias. And one of them is approximately imagine with random forest, spoiler alert. And I will walk you for an example, we have Lending Club data. This is online available. They do like a very similar thing. It's an online platform. And you can get a personal loan for yourself or your business. And they connect you with investors. And they have a huge data set online available. And I use that to kind of walk you for an example. So first of all, we create our chosen purchase portfolio by using variable filters. So for example, we look, we only want to kind of collect, choose something between certain interest rates and zip codes and amounts and grades and what was in there. So in this case, I know exactly what those filters are. In some cases, I don't know exactly what those filters are. And or at the data is not available. Or it's not collected in an ongoing way. So if we compare now our purchase portfolio, our chosen portfolio to the rest of the portfolio, we can see significant differences in characteristics. In this case, it's the loan amount. I mean, we felt that on the loan amount. So we kind of expected that. But just to kind of keep the example going. As well as with the zip code distribution, there is like a huge difference here indicating that a raw comparison is like unfair to say the least. So if we would look at like default rates or number of loans that are still active or heavily paid, we should know now that it's biased. There are selection filters going on here that we should think about. What to do? So the question is, how can we eliminate those selection filters? To put that in another question, the thing that we want to have at the end of the day is we want us a similar group, right? We want to kind of find all those inherent filters, those selection filters that lead to kind of our chosen portfolio. And the keyword here is similarity, which kind of pops up more or less obvious solution to use proximity scores from random forests. And quickly, what is a random forest? A random forest is an ensemble learner. It uses bootstrap samples and random feature selection. And it consists of multiple decision trees. A decision tree here, represented in R, is a classification algorithm. It also can do linear regression. And it starts by looking at all the available feature variables and kind of tries to split each variable into two groups such that it can label the resulting observations. So in this case, we want to kind of label our chosen and not chosen portfolio. And you may not be surprised if we use interest rate and loan amount, our former used filters. It can split those observations up into the other portfolio and our chosen portfolio, he labeled by other and folio. And a tree really is, if you want to kind of visualize that in a two-dimensional space, that's a really easy to do. It's a feature space partitioning, right? So first we had the split between the in the lower amount. So we had one group to this other. And the other one needed a little bit more work to kind of split it in distinguishable groups. You may notice that there is, there are a few green dots in the big blue square. Some, so kind of, this splitting in decision trees happens either until you have a kind of sufficient number of observations in your end notes. There are other stepping rules. Sometimes you want to kind of grow it fully. So to say that you have like very, very clean end result, like end leaves, we call it. And this is, this kind of those green dots within the blue is one major key point. From a tree to a forest, you put multiple trees together, you have a forest. The randomness in the forest comes from two different parts. First of all, each tree is based on a bootstrap sample. So usually a sample that is as big as the original data set, but randomly chosen. And you can show that then like around two-third of the entire data set are in each bootstrap sample, if you set replacement to true. So you kind of drawing and kind of putting back again. And the other part of the randomness comes from randomly choosing feature variables. So, oh cool. So in each split, when you decide whether or not to kind of what variable would be best to kind of distinguish between group A and group B, you not have all the variables available, but only a random subset of the variables, which kind of brings another randomness into the whole tree. The reason to do that, I forgot to mention that earlier is that trees very much suffer from overfitting. They're very, very sensitive to like really, really small changes of data. And kind of putting all these trees together with random feature variable selection and bootstrap samples kind of helps with that overfitting. It's kind of the whole part of why we use own sample learners. So what does that mean now for our observations? So imagine each tree has its own bootstrap sample. Each observation might be in multiple trees, not in all of them. And at each kind of selection kind of crossing, we have other features available to perform the split. And so observation A might end up labeled as orange in one tree and as apple in the other. And kind of going through multiple trees, how do we combine all those kind of labels at the end? We usually take the majority vote. So each tree labels each observation and the label kind of in the end of each observation is then the majority vote of all the trees. So what that means is based on certain features and certain characteristics, observations can be considered similar or very, very different. If I look into this room, there's a person we probably all have seen or used are. So we are very similar in that kind of field. Other may have used a lot of Python in the past, others may not. So there's a difference in there. And if we look at a feature characteristic of have you used or seen are, you would end up in the same kind of labeled group. If we look at a characteristic of have you used or seen Python, it may look a little bit different for various kind of examples in that room. And so kind of dissimilarity measure is built in into random forest. And Leo Briman, who kind of created the random forest algorithm, mentioned that the proximity scores that I will introduce quickly are one of the most useful tools within a random forest. And proximity scores basically count if observation A and observation B end up in the same end node, so in the same end group, they can be considered similar for that specific tree. And this proximity score is then raised by one each time that happens. And we can normalize that measure by the number of trees to get like a normalized between zero and one measure. And this measure of distance has three main advantages. It's a metric free. You can run the random forest in a supervised way to kind of backward engineering your selection rules. And it does not suffer from the curves of dimension. So whatever kind of number of variables you have, put them in there, random forest will be fine to some extent. Take a look at the proximity matrix I have here observations A, B, C, D and E. And the other one is the normalized one and the matrix at the bottom is the one where we actually count how many times two observations ended up in the same end node. Please note that diagonal has always has to be one or like the number of trees. And so for example, observation B and observation E did not end up in the same end node ever while observation, let's take D and observation E ended up in the same end node five times. Observation A and observation D ended up in the same end not 165 times. So out of A, B, C, D and E, well actually B, C, D and E, A can be considered most similar to observation D if we kind of use that similarity measurement. So how does it work? I can now use that measurement, those proximity scores from a random forest to create my so-called control group, right? The treatment group here would be my purchase portfolio, my chosen portfolio. And I want to find all those observation in the rest of it that can be considered most similar to have a fair comparison. So this is the code you can run the random forest supervised or unsupervised. Then you have all those indices that mark whatever is in your portfolio already. Then you take the proximity matrix and delete all the columns for the in-group indices, RIDs and select rows only for those such that you can find for each in-group observation the one from the other outside group which has the highest proximity score. And maybe you remember the main reason that we kind of wanted to do that is first of all to find another group that can be considered similar based on the characteristics that we're looking at, as well as to kind of backward engineer our selection rules which we may not know all the time. So we had the zip code and the loan amount and after proximity matching it's a lot better which was kind of the whole point of it. And when we look at the zip code I split them to kind of see a little bit better. We had those huge spikes, right? Because we actually selected like various specific zip codes but the kind of overall distribution from what's left of it is like way more similar than before. Another thing that I wanted to mention is why do we even kind of do that? Why don't we just use the same filters and kind of apply it to the kind of outside group? One problem with that is they might not be available anymore. And so we really have to look at what can be considered most similar. So if we look now at a comparison like whatever performance measure we want to use we know now that the difference are not based on the selection filters anymore. And when we compare that for example to a randomly assembled portfolio to kind of as a control group we can see that the characteristics are much more similar in the proximity matching. So what about landable? What about our original problem? So we had this comparison, this is our comparison before and we actually know it's unfair because we know that our criteria are very specific sometimes and also from a quality perspective which kind of lays behind that. After proximity matching this looks much different first of all and we can now be sure that the difference in those performance metrics are not due to selection filters anymore which was the whole point of it. So now we have like a very much better basis to compare different performance metrics and to kind of actually talk about. For example this was the original empirical distribution from a feature variable and after proximity matching it looks much better. Yeah and that's it. Thank you very much."}, {"Year": 2018, "Speaker": "Roger Peng", "Title": "Saving Lives, Thousands at a Time, Using R", "Abstract": "Air pollution levels in the United States have fallen substantially over the past 50 years due to increasingly stringent regulation, resulting in dramatic improvements in air quality and health. In parallel, there has been a revolution in air pollution epidemiology through the use of big data coupled with sophisticated statistical methods. At the center of all this is R, which has played a key role in allowing methodological development to happen in an open source manner, and has significantly accelerated the sharing of data and tools across the world. In this talk I will present some key examples of how R has enabled the creation of high-quality scientific evidence in support of air pollution policy and has contributed to protecting public health around the globe.", "VideoURL": "https://www.youtube.com/watch?v=y9KJTWAJUnE", "id0": "2018_10", "transcript": "All right. I think one of the reasons I'm here today is because I've been using R for a really long time since version 0.65. And so anyway, that's my only real credential. So there's that too, I guess. But my actual employer doesn't like me saying that. They like me saying that I work at the Department of Biosatistics at Johns Hopkins Bloomberg School to the public health. And this is the picture of it right here. So I'm a professor here in Biosatistics. As Jared mentioned, I also have a podcast with Hillary Parker, who's a data scientist at Stitch Fix. It's called Not So Standard Deviations. And I teach a few courses online. And so I struggled kind of a little bit about what to talk about here. And I thought, I would just talk about kind of what it is I do. And so the motto of our school, the School of Public Health, because you know the school of medicine, they save lives one at a time, right? But we protect health, save lives millions at a time. This is our motto. And I just add a little insertion there. So I do a lot of work in environmental health. I want to talk a little bit about how we do that and how it's done and how it's different from what it used to be and how it is now. So if I were to just show kind of one slide about kind of the improvement in kind of air pollution and the environment over the course of the last 20 years, I could show you this slide here. So this is the average particulate matter levels in all the 50 states, well 51, I can include Washington, DC, over the past roughly 18 years. And you can see there's about a 40% drop on average in air pollution levels across the country. And this is not by accident. It's not because we all felt good about it. It's because of an increasingly stringent regulation of kind of air pollution in our environment. Similar picture, this is the level of sulfate that's kind of over the last 20 or so years part of this decreases due to the acid rain program, which is just part of the kind of east coast management of sulfates. So there's been dramatic improvement in pollution over time. And that's how to major benefit from public health. And so I want to show you this picture. This picture is arguably the basis of almost all air pollution regulation in the United States. It is from the Six Cities study. And you can see there's a correlation there. On the X axis you have the average kind of particle level in each of these six cities, which names in which I don't remember because they're all kind of cities in the Midwest where I don't live. The Y axis is the mortality rate. So its pollution goes up, mortality rate goes up. And if you compare the most polluted to the least polluted city, there's a 26% increase in mortality, which is unheard of really in any kind of environmental application. Another picture that is kind of a summary of how air pollution research got started is this one. This is a London fog in 1952. So on the left hand side, that black line is the level of total particles, or it's called black smoke. And it's about 1.5 milligrams per meter cubed. So just in case you're wondering, we don't measure pollution in milligrams per meter cubed anymore. It's too big of a unit. We measure it in micrograms per meter cubed. So back then they measured in milligrams. And that's like 1,500 micrograms per meter cubed. The naturally ambient air quality standard in the United States is 35. So things were much worse back in the 50s. And you can see that the number of deaths spiked up almost immediately after the pollution went up and their estimates that there were about 10,000 excess deaths just in this two week period. So this has spurred a lot of the research and policy making in air pollution health. So what does an air pollution study look like? Basically, here's a lovely picture of Beijing, China. I don't know if anyone's been there, but it's a great city. It's a great city when it looks like this. It's not so great when it looks like that. And the basis of all air pollution, or basically every air pollution study is we get a bunch of people and we say, can we follow you on days that look like this and on days that look like that and see what happens to your health on both of those days. So these are both in the same city. We can also say get a bunch of people who live in Beijing, recruit thousands of people in this city and compare them to people who live in lovely Chicago, where it's quite a bit cleaner. So those are roughly the two kinds of studies, there's temporal studies and there's cross sectional studies where we look across space. And so the problem with this kind of old way of doing things, so the six-city study that I mentioned earlier, had 8,000 people enrolled. They enrolled in, it took 20 years to do that study, they mostly enrolled in the early mid-70s and the paper was published in 1993. So it's very expensive, it's very time consuming, it's very resource intensive to do that. And so it's harder and harder to do that these days because there's not as much money running around. So what do most air pollution studies look like today? They basically look like that with a little bit of R thrown in. So there's a lot of stuff that you can do now, just sitting in my office with a computer and a lot of R. So I want to talk about a little bit of how that's done and what's involved. So the premise here is that the old way was hard and expensive and this new way with puppets and computers is easy and cheap. So especially if the puppet's doing all the work. So if there were a licensing exam for doing air pollution and health research, there would only be one question on it and it would be, can you navigate the EPA website? Because basically what every pollution study starts here at the air quality system website and you just start clicking randomly until you get to this site. It's not going to be the same link every time. So just keep clicking. Now so this, each one of these are different gases. You got ozone, you got your sulfur dioxide, carbon monoxide, NO2. And each row of this table is a year's worth of data which each file has all of the data from all of the monitors for that gas across the entire country for that year. I don't care about gases though, I care about particles. And so these are all the particle data. The file that I'm interested in is this one here. So this single file has all of the 2018, it's not finished yet, but all of the 2018 data for every monitor in the country on a daily basis. So daily is a 24 hour average. Measurements, now I don't want, it's just a single year's worth of data, I want all the years. And so I got to get all of these files. And so that's where my for loop comes into play. And I just want you to know that this for loop only works if you indent eight spaces. So just so you know. Those are you with your. I see, I see your four and two space indents out there. And I don't even know how that code runs. So I'm going through and downloading all these files, these zip files. And now I got to read these zip files so I can just read directly from the zip file but it's kind of convenient. And of course as soon as I run this code it doesn't work. And I'm screaming because of course I forgot to load the package. And so now I can load the package and it will work. And this is the typical file. This is the typical air pollution data. I think this is for 2017. And there's a lot of variables here. This longitude minus 87. I'm guessing that's like Alabama. Latin 230. And so the variables that I'm particularly interested in here are the arithmetic mean. So that's the kind of like the sample mean, not the sample. It's the mean of pollution for a given day. It's a 24 hour mean. I want to know where this monitor is. So there's a state code, county code and a site number. And so state code one, they're in alphabetical order. And so I think it's Alabama. So that's where we start. And this is for only a single year. And I want to kind of put all this data together. And so a number of, oh, I need to know the date of course too. Number of group by summarizes later. We can produce something that looks like this. This is particulate matter data from the fine city or the Cook County, which includes Chicago, this is for 2008 and through 2010. So three years of data, you can see I put a dashed line where the National Ambient Air Quality Standard is since 2008. And you can see Chicago is not the worst offender, but not exactly the best staying below the standard. This is just Los Angeles County. Same exact data, particulate matter data since 2008. And you can see that they often greatly exceed the standard. And so this is kind of the data that we can produce from downloading all those files in the EPA website, assuming we finally figure out which link to click on. So OK, so we're one third of the way there. We've got our air pollution data. We've kind of aggregated it all together. And we produce a really nice time series. So we're doing air pollution and health studies. So we've got the first part. We need the second part. We need the health data. So we get our health data from two large national insurance programs called Medicaid and Medicare. These are all insurance claims. So we have information about people's hospitalizations, emergency department visits, doctors visits, prescription drugs. And these data are surprisingly clean. Part of the reason is because we pay about $50,000 to ResDAC for them to give us the data. And they do a lot of work to clean it up. So the problem is not so much that the data is messy. The problem is actually that the programs are messy. So Medicaid is a joint federal, state, administered health insurance program where every state has a slightly different way of defining who's eligible. And it's just a very difficult to understand. Medicare is somewhat simpler because it's a uniform national system. But either way, there's about 45 million people in Medicare. As of 2010, there's 50 million people in Medicaid. But as of today, there's more like 75 million people in Medicaid. So that's kind of the amount of data. And we like to look at a national level. So we look, tend to grab all the data at once. So first of all, the first file we usually deal with is the personnel file. So this is the file that has all the individual level information that we have for everyone enrolled in Medicare or Medicare. Oh, this is for Medicaid. But they're very similar. So this file has 668 variables. And this is the metadata for it. And we need to choose which variables we want. We don't want all of them and the file's too big to kind of grab all the data. So I don't even run this through R. I just use the cut command in Unix to just grab the columns that I need and to spit out a much smaller file that I just read into R. So once I've cut all these personnel data files, I get a master list of IDs. So these are the encrypted IDs for all the subjects in Medicaid. And in this particular study that I was doing, we're only looking at children. So there are about 25 million children in Medicaid that were eligible for this study. With this information, we have some demographic information. They're aged, self-reported gender, race, county and zip code of residence, and their eligibility status for Medicaid. So that's kind of on the one hand. On the other hand, we need to know what their healthcare utilization is like. So when do they go to the hospital? When do they go to the doctor? And we know that because all the hospitals and doctors bill Medicaid for reimbursement. So when we get that bill that comes through, we have that information. So we can gather all that. So for example, in this case, I'm looking, I'm interested in asthma cases. So that's ICD-9 code 493. So I'm grabbing all of the instances of 493 in this case is the other therapy file, which is basically the outpatient file. I'm using Pearl equals true otherwise it takes forever. And I'm grabbing it. So now I have all of my cases here. This is just for one state. And how many times they went to the doctor? How many times they went to the ER? I also have hospital admissions too. So I've got my demographic information on the one hand. I've got my case information. On the other hand, you know what's coming. It's going to be a join, right? So I can join that together. And now I've got a file that has everyone's demographic information and everyone's hospitalization, healthcare utilization information. And so once I've got that file, I know where they live. I've got to have their county and zip code of residence. And so given my air pollution data, which also has county and zip code information, I can start linking things together. All right. So we are two thirds of the way there. There's a final third, which is all of the other stuff that's trying to make your life difficult. So we need weather data. So weather data is a huge confounder. It influences air pollution. It has a huge effect on health itself. And so we need to get weather data. We get the NLDAS data from NASA, which is a computer model system, as gridded to an eighth a degree across the entire country. And so we can grab these, essentially image files and overlay them on the areas where we're looking at air pollution and health. They have to have census data for other kind of confounding information, things like poverty and other socioeconomic status type variables. And someone mentioned already there's 20-some packages for looking at census data. So we use a lot of those there. We have air quality modeling data. So this is again from the EPA. This is the community multi-scale air quality modeling system. They have a computer model that simulates air pollution all across the country. And it's pretty good for some cases and not so others, but we do incorporate them into some of our modeling. So C-Mac is another input that we use. And finally, we have the Natural Emissions Inventory, which has all the sources of air pollution. And we have two sources of air pollution across the entire country. And so we can, we don't, the problem with air pollution that tends to move around in the air, so just because you know where the source is doesn't mean you know where the pollution is. But knowing where the source is does help quite a bit. So we have, we use that information too. So having all this information together, anyone who's worked with spatial data knows that the change of support problem is a huge problem. Okay, so this is Baltimore City. The boundaries of the bulk was where I live in the upper left corner of this county. So first of all, we have air monitoring data from the EPA. They have three monitors in Baltimore City. These are point locations. So, and remember our health data is aggregated to the county level or the zip code level. All right, so we imagine we have like a number of hospitalizations for this for Baltimore City on a given day. But we have point level information about air pollution monitoring. We've got our weather data, which is gridded, right, because it's computer model output, so it's gridded to eighth a degree. And so we've got that. And then we've got C-Mac model, air pollution monitoring data, which is at a different resolution. And so it's on a different grid altogether. And so we have two separate grids. We have point location of the monitors. And then we have this weird outline that's called the county. And so all of that has to be kind of assimilated together and aligned so that we can see if things are correlated with each other. And in order to do that, there's no right way to do that. You have to make assumptions or you have to build models to do that. And every time you align two things together, you introduce a new source of uncertainty. And so that's one of the problems that we deal with all the time. All right, so what do we get when we do all that? We can make maps. So this is a national map of a fine particulate matter that we built for the purposes of our health studies. This came from a paper that just came out this year. So this is every county in the United States. And the black dots are where the monitors actually are. So that's where we have like observed information. But you can see that leaves out a lot of counties in the United States if you only use the areas that have monitors. And so here's our kind of prediction model for particulate matter. Here's a close up of the state of Maryland. You can see Baltimore has a nice hotspot there for pollution. Also in Western Maryland where there's a lot of coal burning stoves. And also they get a lot of pollution from our friendly neighbors in Pennsylvania and Ohio and West Virginia. And so we can put all this information together and build these prediction models. Which they sound fancy but it's actually just a really large linear regression model. So we can also have these maps of the other hand, other side of the equation. So these are the hospitalizations for asthma in Medicaid. This has just come straight from Medicaid. There's no modeling here. And the gray areas, those states have not seceded from the union. Their data is very different. And so we couldn't really include it in the study. All right. So what's an air pollution study after all this work? It's just, is the left hand map correlative of the right hand map is all it is? And we can look at correlation at different spatial scales. So we can look for the whole country or we can look at small regions and things like that. So there's all kinds of variations that we can do. But ultimately we want to know if the left hand side is correlative of the right hand side. And this is on a spatial comparison. We can do temporal comparisons too. So we can see on the left hand side we have time series of pollutants. And on the right hand side we have time series of things like hospitalizations. Again, we want to know if the left hand side is correlative of the right hand side. It's not super complicated. Of course, we're adjusting for all kinds of other things in our models. But this is the basic idea. All right. So like any other analysis you might do, I spent most of the talk talking about wrangling the data. And that's exactly what we're going to do. So I'm going to spend one slide on statistical modeling. And that's it. That's where all the time goes. I guess I should be spending all the rest of the time complaining about the first couple of slides. But in the two kinds of studies we do, there's time series studies where we look at acute effects, short term effects on the order of a couple of days. Because the data says tend to be too big to just do all at once, we always do them in multiple stages. And at the first stage we fit a generalized linear model with a count outcome. It has a pollution. And there's a lot of stuff in there that couldn't fit on the screen. So I just call this stuff. But a lot of information goes into there. And we do basically a generalized linear model to look at the short term association between pollution and whatever outcome we happen to be looking at. And we do this for every county that we have information on. And then once we get risk estimates for all those counties, we combine them together in a hierarchical model using a package called TLNIS, which is a two level normal model. And then we get a final kind of national level estimate for the whole country. National level estimates aren't always the best thing to do, but they are nice from a policy perspective because EPA sets things like federal policy. The other kind of study we do is a cross sectional study where we're comparing different areas to each other as opposed to different times to each other. And there, this first stage we have to predict all the pollution. So I showed you those maps where we do all the prediction. So we make all the predictions first. And then we incorporate those into semi-parametric regression models to look at what the health effects are of that pollution. Of course, we have to incorporate the uncertainty from the prediction itself using a kind of non-parametric bootstrap. So these are the basically the two kinds of studies that we do with this kind of data. And these are the kind of results we can produce. So this is from a paper in 2006. We're looking at fine particles. And on the right-hand column, it's probably hard to see, we estimate how many hospital emissions could be prevented if we lower it air pollution by 10 micrograms per meter cubed, which is a lot. It's asking a lot. It's not like a trivial amount. But here are the very kind of averted hospitalizations for each of these different cardiac and respiratory health outcomes. And if you total it all up, it's about 11,000 hospitalizations per year, which is not a trivial amount. It's not millions at a time, but it is in the order of thousands. And the fact of the matter is that we could save 11,000 hospitalizations per year in two ways. One is we can cure all those diseases, right? Or we can do something that we've been doing for the last 20 years, which is just this, right? So we know how to do this. We don't know how to cure all those other diseases, right? At least not yet. It would be great if we could cure those diseases. We'd save a lot more than 11,000, right? But we already know how to do this. And so I think the environmental, air pollution control policies have a huge effect on health outcomes across the country. So very quickly, just lessons learned. This kind of data will fight you every step of the way. There's no question about that. One of the things that we started doing very early on in this research is investing in hierarchical modeling because it's a very easy and a very cheap way to get around the fact that you can't fit all the data into R. So it's kind of staging things out into separate stages, allows you to kind of take advantage of independence between the data and split things into kind of manageable parts. Of course, there's a price you pay, which is an increase in uncertainty, but it's probably worth it. The fact of the matter is these data only get you so far. So it's nice to be able to work in your office with the computer. It is cheaper and it's a lot simpler, but you can only answer certain kinds of questions. You can't answer very subtle questions. These are very blunt questions we're asking, are you going to the hospital? We're asking, are you dying? These are very blunt questions. And you can't ask something more subtle with these kinds of data. It's just not possible. So as it turns out, the new way, it's also difficult. It's difficult in totally different ways than the old way. And it can be expensive too. So it's not a panacea for the old ways. All right. Thank you."}, {"Year": 2018, "Speaker": "Jim Klucar", "Title": "Hacking Gmail to See if Anyone Actually Cares About Data Privacy", "Abstract": "Two years ago I spoke about data privacy techniques at the NYR Conference and I had a secret: I hadn't written a line of R since that one course in grad school made me do it. Attending that conference opened my eyes to what R had become since the tidy revolution and I began learning and using it more for my daily work. This talk is about what I learned in my journey over the last two years of adopting R, using a recent project where I analyzed my Google Alerts emails for data privacy article trends.", "VideoURL": "https://www.youtube.com/watch?v=YakqtIkZm5U", "id0": "2018_11", "transcript": "Yeah, about me. I guess first, I'm gonna go back. Yeah, interesting title. There's no actual hacking involved. It was all legal and I gave myself permission to read my own Gmail. But hacking sounds interesting like I'm gonna crack into your Gmail. But I'm not, you can do it yourself. So I don't know if Jared planned this, but Roger before me and Stephanie after me are Hopkins people and I'm a Hopkins grad so you get a nice Hopkins afternoon. I guess they breed our people there. And like Catherine earlier, I got here by talking to Jared mistakenly and he saw something I did and said, you should present that. And I was like, okay. And that was months ago and he remembered. So I'm here. But yeah, so about me. Yeah, I raised chickens. That's a view from my chicken can. When I get chicks in the spring, y'all can get in on the live feed. It's great chicken cuteness action. I hate pie charts, which I assume the rest of you do. Raise your hand if you hate pie charts. All right. Those of you without your hands up, talk to me after. And my real job now is I find people doing DNS tunneling kind of things, cyber stuff. So when you all signed into the Wi-Fi today, you got that. So did you all instead hire up your DNS VPN network to your home DNS machine? Probably not. Just me, but you could have and got free internet. So next time I'll bring my burner laptop. We'll have a different presentation. But yeah, so now a confession. I spoke at the New York R conference in 2017. That's me there looking angry because I'm an angry person. And I talked about data privacy and how the lawyers are going to come and ruin your analytics by things like the GDPR. And you can't have data, so you can't analyze data. And I was probably a little ahead of my time there because people were like, what? And then afterwards, during these great sessions, a lot of people came up to me and was like, so I totally released data and I probably shouldn't have. Am I going to jail? And I'm like, not yet. I'm glad you're not in Europe. But my big secret there was that I wasn't an R person, right? I was just like a math nerd and I was a Python guy and I was asked to speak at the ZAR conference and I was like, to my CEO, what are you doing? I'm not that guy. He's like, you don't have to talk about R. Just talk about data privacy and our people like data. So that's what I did. And it was nerve-wracking and I was like, I don't like R. And in fact, I was kind of anti-R. In fact, I was like this guy. I was like, R is a programming language written by a statistician. Like who thought that was a good idea? Nobody. It's a big joke on the world. I sat at that conference and it started to sink in, which was good. So I kind of made an effort over the last two years or so to really learn and embrace R. And I found out at that conference really that R had changed, right? So I had that one course in grad school at Hopkins that we had to use R. And I was like, okay, I wrote a bunch of arrows. And I was like, what's the point of all this? I don't get it. Give me my grade and I'm done and I'll never touch it again until a couple years ago. So, but it changed, right? That's what I learned. And I hope you all have learned this. Like go read the tidy paper, which is, I guess, pre-tidy verse, but whenever it kind of put words to how I now think about data and probably how I should have been thinking about data and even change like how I think about big data, like, you know, big unstructured sequel became the big thing. And just like, well, it's actually tidy data, but it just has a bunch of NAs all throughout it. You don't have to worry about them, right? And then they came up with this funky pipe thing and the tidy verse. And Ken and I was hooked. And the other thing that happened at that conference that has happened here, I hope, for most people is this format. Like, we got 20 minutes to talk. I can talk for 20 minutes about anything. In fact, I'm fluent in piglet and I could talk for 20 minutes in piglet. And you can, at the bar later, we'll do piglet. But yeah, but this community is great, right? We've got 20 minutes and we've got these big breaks. And Jerry's good at getting nerds to force themselves to talk to each other. And then it turns out we're all kind of have little quirk things, like speaking piglet and we get along. So yeah, this community is great, right? So anyone, this is like, get on Twitter and start following everyone who you've seen speak and around and talked about, right? You'll get great. This was great conversation in this whole thread. We could do a whole speech on it. I still don't know what the real difference is between equal and that arrow-y thing. But I am not a cop, so I don't use equal. But seriously, like joining is great stuff. Everyone's helpful. Jared has a slack. Have you even mentioned that today? No. Get on his slack and ask questions. And don't really post a cool plot you made because you'll be up here like me. That's what you want. So yeah, back to the real talk. So before I, my other talk was about data privacy techniques and differential privacy is kind of like the main thing in that now. And the new 2020 census, lots of census talk, they're actually releasing that data in 2020 using differential privacy. So what does that mean? This is the formula. It's all self-evident, I'm sure I can go on. But I'll go over it. So differential privacy is really, if you have two data sets and you perform some kind of aggregation or operation on those two data sets, the result of that operation shouldn't differ by more than this e to the epsilon. And epsilon is what differential privacy people call their privacy budget. And so the census, and the technique to do this generally is you submit a query to a data source and then you get the result of that query and then you add noise, proportional to the data or what specifically the query was about. So in the census, it'll be a lot of counting. Their sensitivity is what they say drives that. And when you count things, your sensitivity is one. So it's an off-by-one area. You'll never be able to zero in on someone specifically with the new census data if they did differential privacy, right? Hi. I hope they did. I've seen them talk. And yeah, so the new data will have noise in it. Add another noise term to your models. I don't think it's going away. So you can learn more about privacy and data privacy. This talk isn't really about data privacy. There's this Harvard Privacy Tools project, which has kind of started to be this, despite it not being Hopkins. It seems to be a good spot for data privacy professionals to collaborate. They have MIT and a bunch of other people there. They claim they're putting an R package out. I haven't seen it yet, but I know they are a bunch of R people and I'm sure they're busy like the rest of us and haven't done a good vignette quite yet. But look for it in the future. So yeah, then when I'm in the midst of all this data privacy stuff like this happened, right? The Facebook and Cambridge Analytica. So everyone knows what happened. You had to know that if your first girlfriend was a Sagittarius, you were going to have a happy marriage later in life, took a quiz, they stole your data. And whatever happened happened after that with your data. So with that, I was like, all right, cool. Google alerts. I have Google Alert set up for differential privacy. This is my Google Alert for multiple sclerosis, another favorite topic. But basically you set up keywords and you get articles about that keyword semi-periodically when they find it. I mean, if you put a too broad of a keyword, you get an email every day with all kinds of stuff, but something like differential privacy that you kind of get them periodically. So when that Cambridge thing went down, I was like, oh, cool. I think now differential privacy is coming into my Google Alerts more and more. Let me see if I can figure that out. So in my new R hat, because I went to the New York R conference, I did just that. So I blindly dug in and I said, how do I read email and R? And I knew Jared like sends emails and R's. You guys get our email from Jared for this whole conference. I'm sure if you look at the bottom. And so I was like, all right, I can do this. And if you go to takeout.google.com, you can just download your whole mailbox or parts of your mailbox or actually anything in the Google verse that is your data. They will tar it up and send it to you. So cool. Go do that and download it. And so this TM is text mining. There you go. And you have a mail plug-in and you kind of plug in your inbox file, Google send you and you do a v-corpus. And I was like, this is feeling like old school R. And then your thing looks like old school junk. Like a raw email is just text all over the place. The bunch of like all your attachments are just encoded into text. And it's a nightmare system that it actually still works is amazing. But I was like, this, I started to parse through this, all the blank lines and figuring out what they use as carriage returns. And it depends who sent you the email, what they use as carriage returns, spig nightmare. So yeah, in typical R fashion, I found another package. So Gmail-er. Gmail-er is awesome. It interfaces with the Gmail REST API. So I hope you use Gmail because otherwise the rest of this will kind of be useless. So there's a bunch of setup kind of required to authorize and go through the OAuth and create a project and stuff. And it's not that hard. The Gmail-er site goes through that pretty good. Go to the GitHub, explain it. But then you just get a bunch of functions that are really easy and then really work well with, with a deep plier and the tidy verse and all that. Like just message gives you a bunch of message IDs with a query string. So and then you just pass in those message IDs to other functions like subject and from and all that stuff. And it just starts ripping apart your email for you in a nice kind of R fashion. There's still some quirks to it. But in general, it's pretty good. So here is my little simple program. But yeah, basically you just get the secret file from Google that they give you when you go through the whole setup and load it in and then you just get messages. And this is a simple query string like you can type in Gmail search. And what I'm looking for here is this line of text in the email with I'm actually looking for this number. So I wrote a little, you know, G sub thing to grab. Sometimes it says news. Sometimes it says web, whatever, grab it, grab the number. And oh, hydrate message list. This is not that it is grabbing the body of the message after you get the IDs. But once you do that, you can grab that number out of these emails and start saying if people care about privacy. So this is the cumulative number of Google alert articles in my email. And then Cambridge Analytica hit and we got a spike. Later on, the GDPR hit and nobody cared. So I guess Europeans don't write articles or whatever. But I was hoping for another spike. But no. So that's about it. There's still some quirks in our, you know, what package is the function tidy in? It's in the broom package. It's not in tidy versus not in the tidy package. It's in broom. Whatever, it's fine. You get used to it. How do you find the right packages? Crayon searcher I like. If you guys see crayon searcher, like probably can't pull it up without screwing this whole system up. But it's built into our studio and you can just kind of search around crayon for the right package and sort by when it was published, which is the most useful thing probably lately. I feel like there should be a crayon new or something, like some kind of flag. It works with dplyr or it kind of was before that. But people were writing packages to wrap those packages all the time. So, yeah. So the other thing I like to do is play the R game if I can guess the package. So paste is kind of clunky and you're kind of looking for something else. Well, it's glue, right? Because R people are clever. So the package you are looking for to replace paste is glue. And yeah, so if you're looking for something, you know, this is really how I kind of learned R was by analyzing myself. Like, I just started downloading all my own data. Here's all the links to go download all your own data from all the major platforms. And you'll get a nice zip file and stuff like that of everything. And you can usually select what you want, like if you want every image or not. And just go do it and start ripping through it with R. And I find that to be a more interesting way than trying to understand MT cars or why people care about that so much. But I mean, you really have to understand that to learn R. So yeah, reach out to me. That's me. And I think that's all I have."}, {"Year": 2018, "Speaker": "Stephanie Hicks", "Title": "Analyzing Genomics Data in R with Bioconductor", "Abstract": "Advances in biotechnology are leading to the generation new types of biological data with decreased costs in concert with increases in volume, resolution and diversity of data. However, effectively deriving knowledge from this data to understand biological systems and disease requires continuous improvements in computational methods, analysis tools and associated software engineering. Bioconductor is an open-source, open-development software for the analysis and comprehension of genomics data using the R programming language with over 1560 software packages and an active user and contributor community. In this talk, I will 1. give an overview of the R/Bioconductor community, 2. discuss the relationship between Bioconductor and CRAN, and 3. give examples how Bioconductor can enable the rapid analysis of genomics data at all stages of a project, from data generation to publication.", "VideoURL": "https://www.youtube.com/watch?v=l1MQ7x8cn7Y", "id0": "2018_12", "transcript": "My name is Stephanie Hicks. I am a faculty member at Johns Hopkins, actually, a little bit about me. I've loved that everybody started with the about me slides. It's very helpful. So I am a faculty member at Johns Hopkins in the Biosatz department, similar to Roger Pang. I have a couple of interest. One is data science education. So if you have any interest in talking about that, I would love to talk to you about that. That's not what I'm going to talk about today, but I have a lot of opinions about that. What I am going to talk about today is specifically a little bit about what I do for my research. So my research is in the analysis of genomic data. Who knows what genomic data is? Okay, that's a great number of you. Awesome. So if you do work with genomic data, you may or may not have heard of the software project called Bio-Conductor. I'm going to explain a little bit about what that is. It's kind of like the cousin or sister project to crayon specifically for the analysis of genomic data. And a few other fun facts about myself. I recently founded the R.L.A.D.E.'s Baltimore chapter in May 2018. So we are on our third event next week, but I've loved getting to meet so many R.L.A.D.s here in the local D.C. area. And then I'm also creating a children's book for feeding women's statisticians and data scientists. I have, as Jared mentioned, they have young kids. And I went on a mission to try and find children's books featuring statisticians and data science. And that's a little weren't really any. There were some specifically for STEM, but I'm on a mission to make that happen for statisticians and data scientists. So I'm going to give you a little bit of a taste of that at the end of the talk. All right. Another thing about myself is Jared mentioned. I have two young kids. Theo is my youngest. Happy birthday Theo. He was born a year ago yesterday. And Alex is my oldest. So he's two and a half. And if you look at this picture long enough, you'll quickly realize two things. One, this was taken very recently. It was the day we voted earlier this week. So he's got his future voter sticker on. And then two, you'll see that he's got minions on his shirt or as he likes to call them aliens. And so when I was preparing my talk at the last minute, which is all of us did, I decided to go with the minion theme. So cran. So as most of us are here for an R sets conference, we all know what cran is. It's a great software project that's got over 10,000 R packages to the state and it does basic things such as data wrangling or creating plots. But then it also can do more advanced things such as web scraping or analysis of financial time series data or analysis of clinical trial data and so forth. We all love cran. So if we look at the subset of people that are in this room, for example, half of us are on our phone, either tweeting or looking up cool links or learning about new packages. You might also be on Twitter. And so you might be tweeting about these hashtags R sets CC and R sets and our ladies. And you can go on your phone or computer to the R project dot org website. And if you go there, there's a link called other projects. Is this where, yeah, there's a link called other projects. And if you go there, you'll see a few things. There's this thing called community services area, which is wonderful. And then right below it is something called special areas of application. So under the first item under the special areas of application is something called bio conductor colon bioinformatics with R. So it has a list of broad goals. The broad goals are to provide access to a wide range of powerful and statistical graphical methods for the analysis of genomic data, facilitating integration of biological data, allowing the rapid development that's scalable and interoperable software, promoting high quality reproducible research, and creating training in computational and statistical methods for the analysis of genomic data. So that gives you a broad overview of what is bio conductor. So for those of you that have not heard of bio conductor, formal introductions, crayon, meet your cool cousin bio conductor. Okay, more seriously. Bio conductor is an open source and open development software project, meaning open source. Anybody can go look at the code. Anybody can go use the code. Open development, meaning anybody can contribute code. Bien in 2001 under Robert Gentlemen from Genentech, now at 23andMe, I believe. And now it's today, I think it's got over 12 full time employees that are funded by the bio connector project that work on developing the core infrastructure and the maintenance of the packages. And bio connector has some big priorities. I would argue that those are reproducible research and high quality documentation. So for example, every software package and bio connector comes with a vignette. And that is really helpful when you don't necessarily want to scroll through just the reference manual. The vignette is meant to walk you through an example analysis or to demonstrate actually how to use a package, which is not the same thing of just looking up a function inside the reference manual. It has a pretty diverse community support group, so we have an online forum that you can go ask questions and you can get experts to respond to your questions. We also have a Slack team that you can join and you can ask those questions in the Slack environment. Workflows. So bio connector is really nice because they've developed, these users have developed and contributed workflows specific for different types of genomic analysis. So if you've ever heard of the analysis of gene expression data or DNA methylation data or mutations in your genome, there are workflows. If you are new to the field of genomics and you want to figure out how do I analyze my fancy genomic data, you can just go download a workflow and follow the steps that they have demonstrated for you to analyze your own data. And then teaching resources. So bio conductor has spent a lot of time and effort developing teaching resources that you can use in the classroom to teach your students. So when you think about bio connector, the image that bio connector aims for is that when you are analyzing a lot of genomic data, there are a lot of great and unique packages out there and tools that you can use to analyze the data. But they are very unique, similar to a musician. A musician is very unique and bio conductor serves as the conductor of the set of musicians and an orchestra to keep things on pace and make sure everything works together in a flexible way. Okay, so in the next part of my talk, I'm going to give you a quick demonstration of a little bit about bio conductor, like what's in bio conductor, and then a package that I think you'll find really useful. So first is a package called bio C package tools. And this contains functions to that assess metadata around bio conductor packages. And it's in a tidy data format. So if you load in the library package tools, there's one function bio C download stats. And in one line of code, you can have a table that's loaded into R with a row containing information with the name of the package, the number of distinct downloads IPs for a given month and year. And there's this last column called repo. So this is the type of package it is. So you could ask, well, how many types of packages are there? So for example, because this is a table, then the world of tidy verse is open up to you. So you can apply anything you want in the tidy verse here. So for example, you can use the filter function to select the rows from 2018, select just the package and repo column, ask for the number of distinct rows, group by repo, whatever that is, and then summarize by the total number of packages. And you'll quickly see that bio conductor has three main types of packages. One is software similar to the software that's available in crayon. And then two, we have these things called annotation packages and experiment, experiment data packages. And if you've ever done any type of analysis and genomic data, you'll realize there's a lot of bookkeeping that has to be done. So for example, if you want to look at genes that are expressed in the human genome, you have to ask what reference human genome it is. For example, every six months to a year, the reference genome changes. So like the position where you would expect a gene to be in one reference genome may be slightly different than the position you expected in a different, a different reference genome. So there's a lot of bookkeeping and the annotation packages basically streamlines that process and makes your life much easier. The experimental data packages, they contain essentially processed data. That is great for teaching because the data is just kind of ready there for you to go. Yeah. You could also ask, so crayon, as I mentioned, has over 10,000 packages currently available. Bio conductor has around 1,700. But it's been steadily increasing. So inside of the bioc package tools function, it only loads in data from, I think, 2000 to 2009. I'm not sure why there's no data before that. I know there's data somewhere, but it's not in that function. But starting in 2009, it looked like there are around 400 to 500 packages and we've been steadily increasing through 2018. And this is just software packages. So we have a really big, diverse community of packages. Okay. So what is the standard object? When I talk about genomic data, how does R think about that? So there's this thing called genomic ranges. And I would argue that that's one of the most standard objects that bioconductor uses. Genomics ranges is essentially a data frame, a constrained data frame. So you can, up here, there's a chromosome. So this is not a piece of data. This is like, and you can imagine a chromosome in our body, chromosome one. And then inside of this region right here, this is a genomic location in that chromosome. And so when you translate that to R, or when you translate that to something on the computer, we want to record, for example, in that one specific genomic region, we want to record a few things. One, we want to know what's the chromosome number. So in this case, it's chromosome one. Two, what's the start of that genomic location? So how many bases in starting from one, two, three, four, five all the way down? Does my genomic region start? And does my genomic region end? In the world of genomics, there's positive strands and negative strands. I'm not going to go into that. But it's again, bookkeeping, essentially. And then there's this thing, there's a dotted line here. And everything to the dotted line is considered metadata. That can be whatever you want. So on the left side of the table, that is a constrained data frame. A G-range's object expects those four columns every single time you create a G-range's object. On the right side, you can put whatever you want there. So if you think about this in the world of the tidyverse, this is actually already a tidy data frame because each row is one observation, namely a specific genomic region. And then every column is a specific variable. So I'm going to explain why that's useful in a minute. So how do you create a genomic ranges object? It's really easy. There's a package called genomic ranges. And there's a function called gr ranges. So you can create a genomic ranges object by providing those four column names that it was looking for. So seek names contains information about the chromosome location. Strand contains information about the positive or negative direction of the chromosome. And then there's this thing called ranges. Ranges are essentially a simplified version of G-ranges. It doesn't have information about the chromosome name. It just contains information about the start and the end of the genomic region. So thinking back to this figure that I was showing you before, it was starting at this position for these two genomic regions and ending at this base position. And then we had these two pieces of metadata, G-N-I-D or the G-name. You can think about it like that and score. Score is super generic. But you can just think about it as something that you've measured about that genomic region. So in a really quick way, we've now loaded up a standard infrastructure. I'd argue these standard infrastructure that bio-conductor uses to build almost all of its packages when you're talking about genomic data. Okay, so what can you do with that? I mean, that's great, but what can you do with that? So for example, it's common to ask what's the width of that genomic region? How long or why does that genomic region? So for example, you are literally subtracting the end value from the beginning value and you can get the width of that for those two genomic regions. You can also do things to select or filter for a set of rows. This is base R notation, so GR, which is the genomic ranges object. Open bracket, this is the classic base R way to filter a set of rows looking for all the genomic ranges that have a score greater than 15. And so similar to the reasons that the tidy verse and D-ply R was invented, that's not super human readable. And so there has been recent efforts to try and make the analysis of genomic data more human readable, specifically with what I have discovered recently called the ply ranges package. So if you're interested in getting into the analysis of genomic data, I would suggest checking it out because I've incorporated into my workflow and it's been wonderful. And the whole goal was to just make the genomic data analysis more human readable. So this is not my work. This is the brainchild student, Stuart Lee from Monash University, Guy Cook, and Michael Lawrence. He's one of the, he works at Genentech, but he's one of the main core bioconductor developers. And so the idea for this is to define an API, meaning literally extend the D-ply R package that maps relational genomic algebra to verbs, similar to the way D-ply R does, that act on this tidy genomic data. Another great idea to just straight up borrow D-ply R syntax and design principles. And another great idea, compose verbs together with a pipe operator from the gritter. That would make this process of analysis of genomic data much more human readable. So for example, you can load in the ply ranges object and instead of doing the base R way to selecting a set of rows, you can take the G ranges object now, pipe it into the filter function. So for those of you familiar with the D-ply R package, you'll recognize the filter function. So you're filtering for the genomic regions that have a score of greater than 15. So we had one genomic region that met that criteria. You can pipe that genomic ranges object that's now filtered into the function called width and ask for the width of that genomic region and so forth. So we're really taking the analysis of genomic data or the packages that have been developed on the G ranges object and making them much more human readable. There is a whole set of verbs that have been developed for the ply ranges package. For example, some of these you'll recognize the ones in bold were literally the origin of them came from the D-ply R package. So you'll recognize, summarize, mutate, select, range, group by and filter. And then the ones that are not in bold are very similar. They're just some unique things to genomic data that make the analysis more fun as I would like to say. And so they are doing very similar things. Like for example, there's a whole set of joins and unions. There is a set of filter by overlaps, filtered by non-overlaps and so forth. So for example, if I were interested in joining two genomic ranges objects, for example, in the bigger A, I have a genomic ranges object X and a genomic ranges object Y depicted by the pink bars. So if I wanted to join those two genomic ranges objects, there are a couple of ways I could think about doing that. I could join them by taking the overlap of the intersection or of the inner part of the genomic ranges object or the genomic regions or the intersect or the left. There are just a lot of different things that are unique to genomic data. And then because everything is in a tidy framework, again, the world of the tidy versus open to you. So for example, you can make fantastic duty plots and so forth. And that's it. So this is my hat tip to Gabby. So she's the founder of the global organization called Our Ladies. And I mentioned that I'm creating a children's books. I'm working with a sketch artist who works at Hopkins and she's been doing a fantastic job. So if anybody's interested in getting involved in this project, I would love your help. I can tell you all about it. And thank you for your time."}, {"Year": 2018, "Speaker": "Angela Li", "Title": "Data Science? Make It Spatial", "Abstract": "Many data scientists are familiar with techniques to handle traditional tabular data. But what happens when the data is location-based, or spatial, in nature? In this talk, we'll cover techniques for spatial data that may be outside of your traditional toolbox. We'll look at how spatial analysis and methods are being used in current social science research and discuss how you can bring some of these methods to your own work. Topics may include exploratory spatial analysis, spatial autocorrelation, clustering, and mapping.", "VideoURL": "https://www.youtube.com/watch?v=uegTPr4PfC8", "id0": "2018_13", "transcript": "Hi everyone, that was Maps by Roon 5, which is indicative of this talk. We just want to show you some maps. So my talk is called Data Science Make It Spatial. And I work at the Center for Spatial Data Science. So I thought it would look stupid if it was like, Data Science Make It Spatial, and then it was Angela Lee, Center for Spatial Data Science. So it says University of Chicago, which is where I work. So without further ado, a little bit about me. I am the R Spatial Advocate at the Center for Spatial Data Science at UChicago, which is a mouthful. Basically I'm a wannabe Mar-Averic, and I look at a lot of maps. So that's what that job is. I also help researchers do their research, whatever. I just want to be Mar-Averic. Okay. I'm the founder of Our Lady Chicago. And we started last year of July 2017. This is actually a photo from the launch. I'm speaking, so I've done this at least once before. I wrote a thesis using spatial conometrics about property values in Detroit under Luke Enslin, who literally wrote the book on spatial conometrics. It's called spatial conometrics. I was going to talk about spatial conometrics, but I learned from econometrics V that you should not talk about econometrics after 3 p.m. in the afternoon. So I'm just going to show you some nice maps. So I want to talk a bit about geography. Geography. And most people hear this term. This is what they think of. This is a map from Lord of the Rings. But I'm here to tell you that geography can and does involve data science. Woo-hoo! Look at this cool app. Geography currently is used in all sorts of decision support tools. We work with public health officials. We do cutting-edge social science research. We look at air pollution. I think Rogers talked about air pollution, actually, dovetails with some of the work that we've done at our center with Chicago air pollution. I believe right here this is an app about medical services. Before we get to the really cool, interesting part of making shiny apps and things with spatial data, before we get to that, we have to do exploratory spatial data analysis. What is that? It basically is making a ton of maps. This is not how I make maps, by the way. When I searched map making, all there was was this person putting pins in a map and then linking string to it, which is not part of my day-to-day routine, but it looks very meditative. So I might start doing this if I get really frustrated with my code one day. Okay. So the way I thought I would demonstrate what ESDA or exploratory spatial data analysis was, was by looking at data. So what I did was pull the crimes in DC in the last 30 days from the DC open data portal. And I read it into R. The wonder of R now is that you can actually use it as a GIS. So you can do a lot of the GIS work that you used to do, for example, ArcGIS or QGIS in R. And you can work with spatial data in R in ways that you could never have previously. SF was released last year in 2017. It's really sort of revolutionized how I work with spatial data. So what I'm doing here is essentially reading in a GeoJSON, which is like a JSON, but has Geo in it. And reading it into R. I'm looking at the observations. If you're familiar with SF, it is essentially a data frame or a table. And on the end of it is tacked one column that's special. It's a geometry column. And these can be points, polygons and other things. Of course, the first thing I wanted to do was make a map. So this is all the crimes that happened in DC in the last 30 days. I just threw it into leaflet. You know, the first thing I want to do is I want to see my data. I want to look at it. This is the equivalent to making a scatterplot and GDplot. You just want to look at your data, just see what's there. And I'm able to zoom in and out because it's in leaflet. Ah, maybe I can't zoom in. OK. Ooh. Very exciting. So I think this is where we are right now. No crimes, nice. All right. Just we wait. After, maybe after this conference, they'll be so I don't know. I think, well, maybe people just haven't reported in this area. Who knows? OK, so I've made a map. Now I'm like, ooh, I don't like dots. I like surfaces. So I'm going to change it into a heat map. So I'm using the leaflet extras package. And I'm like, ooh, look, I've made this cool heat map of things. And I'm like, there's hot spots everywhere. I feel like I'm in some sort of TV show where we're all looking at hot spots and pin pointing crimes. Cool. Very fun. The problem with heat maps is this. And I don't know if you can see this or if you've seen this XKCD comic before. What it essentially says is if you plot counts by area, it often just reflects population. So in this comic, we see that our site's users has this map. The second one is subscribers to Martha Stewart Living. And the third one is consumers of furry pornography. And this guy is saying the business implications are clear. So the caption, of course, is geographic profile maps, which are just population maps. So counts are not that great. They can give you a sense of where things are clustered. You can get an idea of where things are. But if you want to do actual analysis, you want something that's a little bit more spatially intensive. I want to adjust for population. The thing about population is population data is collected in very specific units. And those units are census tract polygons. So the US Census happens every 10 years, counts all the people, and counts them in these little, oh, sorry, in these strange little buckets called census tracts. Census tracts are the smallest units that you can get population data at. You can get these in larger chunks. You can have blocks and block groups, and they're slightly larger. But tracts is generally the most granular form of data you can work with. So for example, if we zoom into where we are, you can see that the mall is just one giant census tract, which makes sense because no one really lives on the mall. And there are some other tracts. So I could do that. Maybe I want to add crimes as a layer. And so you can see in my code that I've just put another line that says add circles. The crime data with colored red. I can zoom in a bit more and take a look at where the dots are in relation to tracts. I noticed that there might be some more dots in this little area right here. For some reason, in Georgetown, there are a lot of crimes reported. So I'm like, cool, this is interesting. But there's no way that I can sit there and count each one of these dots by hand. And that's where spatial joint comes in. So the question I'm really asking is how many crimes happened in each tract? This is a question of spatial aggregation. So if you have points and you're going to a polygon, you're aggregating up by the spatial unit. Also called point in polygon analysis. That's essentially what geographers are saying when they say point in polygon. There are points which represent crime, houses, or any sort of data you might want. And then there are polygons which are census tracts, block groups, counties, zip codes. So I really want to count points in the polygons. Normally you might do this with a GIS software. You can now use R as a GIS software. I'm going to use this thing called spatial join. This is from the SF package again. And so I'm going to spatial join essentially the crime and then the tracts. And I want to pull out the census tract from the census tract geographic data. What this does is instead of joining on a common column, it joins on a common geography. So for all the points on the left, if they fall within one of the geometries of the polygons on the right, so DC tracts, they will be joined to that. So a point, I should have shown an example of this. I'll show you what the data frame looks like in a bit. If a point falls within a polygon, it will have the associated census tract attached to it. So I get an error. STCRSX equals STCRSY is not true. What does this mean? I just wanted to join things together and have it work. I don't understand. Morning. The worst part of working with spatial data is ahead. Projections. Julia Selgy said this well. This is a tweet by her saying me making maps but not really knowing anything about map projections. I have no idea what I'm doing. All right. Dealing with projections in spatial data analysis is similar to data munging. You don't like it. You're complaining about it. And I am the 10th, 12th, 15th person to complain about data munging today. But I'm doing it for spatial data, so I'm unique. Um, so what's actually going on? Essentially, one of these data structures has DC sort of in this, you know, it's like projected this way. And another one has DC projected this way. Your boundaries aren't matching up because your latitude and longitude are mapping to x and y differently. Latitude and longitude are 3D coordinates. And when you work with x, y, you have to push them down into like a flat surface. There are ways of doing this. And if you push them down in different ways and they don't align up, you can't make a map with both layers. So what we're actually going to do is straighten out and make sure that these two layers are the same. To do that, I'm going to check the projections. So the STCRS of the crime layer uses the 4326 EPSG code. And as we expected, the coordinate reference system for DC tracks uses the 4269 code. I think we have found our problem. We can make them the same. This is relatively straightforward. So I'm going to transform DC crime and DC tracks to be Maryland state plane projection. Now, generally, it's accepted that there are common projections for each state. Like there are some projections that are better for each state. Like map to an actual flat map better. I do not have these memorized. I have no idea that the Maryland state plane was 102285. I just looked up best projection for DC and found 102285 as my code. So I decided to use it. Now I'm going to check the projections again. DC crime. OK, now the EPSG is 102285. DC tracks. Now the EPSG is 102285. They look the same. I'm good to go. So if I try joining again, it works. Woo! So you can see that this data structure is still a simple feature collection. Now you can see that I've joined the census tracked onto the offense. So now I have the offense, and I have its associated census tracked on the left. As you can see, there are a number of things in this simple feature collection. It tells you that it's a point geometry. It tells you where the bounding box is, which is the rectangle that surrounds your data. And it also tells you the coordinate projection system. Or yes. So it worked. Yay. Whew. OK. Now I'm going to go ahead and map the crime counts. So these are just the number of crimes. 0 to 20, 20 to 40, for each 60. It looks like certain parts are a little bit more crime prone. This makes a cartographer sad. This map is not great. Why? We're mapping a spatially extensive variable. What does that mean? That means that the count is related to the area of the tract. So you can imagine that if the size of your square is bigger, the number of crimes that happen in it is probably going to be bigger, too. We're going to fix this, as you mentioned before, by dividing by population. Right here, I'm using the tidy census library, which is a wonderful, wonderful package that I just love. And it's so good for social science researchers and anyone who's interested in census data at all. It pulls from the census API. It gets both census 10-year census data and American community survey data, which is a sample. So I'm getting tract data. I'm looking for population. And I'm getting it for DC, which is not a state, but had to do that for the syntax. OK. And what does that return? A nice tibble with the GID. The census tract, the variable population, and the census tract. Again. As you can see, there's one row in here that doesn't have very much population. I'm inclined to think that this is the mall, but I'm really, really curious as to why there are eight people living in the mall. Who knows? OK. So now I'm mapping crime rates per capita. And this is a much better map. So this is telling me the crime per 1,000 people. And I took out the mall and I think another one that was just, like I think people don't live there. I think one of them was a park. Does someone know where that white square is? Anyone know? I think DC yours from here. OK. I took out the mall and something else. What is it? It might be the botanical garden. Oh, it might be the botanical garden. Yes, local knowledge. Very important. OK. So after I made these Coroplast maps, so the name for these maps with little squares that are colored in are called Coroplast maps. They are not Cloro. There's no L. It's just Coro. It stands for, oh no, I used to know the Greek. And now I don't. Coro, Greek, if someone knows what that means. And plus. Color? Color? Yeah, color. And then, plus is, I think, area, right? So color, area, Coroplast. OK. So I made this Coroplast map. Now I'm really curious in my exploratory spatial data analysis to see if crime rates and median income are related. So now I'm not mapping the rate of crime rates. Rate of crime. I'm looking at one variable and its relation to another. So I want to grab median income by track. This I get from the American Community Survey data. It is in the same package, tidy census. But it's a different variable. And now if I look at this data, head of DC income, I can now see that I have the variable and then an estimate of the income, median income, which is quite high. And the margin of error. Don't worry about the margin of error, because this is a sample. They have margins of error on their data. OK. So now I'm going to go ahead and map crimes and median income. By the way, I'm doing this not in Leaflet, but I'm doing it in T-map, which is my favorite and preferred mapping package. It was designed by a cartographer, and it really is very extensible. Just like base plotting and ggplot, you can do whatever type of mapping you want. You can do whatever type of plotting you want, as long as it gives you some sort of way to work fast with the data when you're looking at it. I use T-map because I know it well. You may use Leaflet. You may use ggplot even. You can do maps in ggplot. But in any case, so I've done a quick dive into spatial data analysis. I made a few maps, taken a look at a few things I might find interesting and look at rates. But there's a lot more that I could do. There might be other local demographic and socioeconomic factors that affect crime rates. So I would do this with spatial regression and modeling. Crime in one area might be linked to crime in another area. So I would look at crime rates and see if you have a neighboring track that has a lot of crime. What is the effect on you? Because there's probably some linkage there. So that is called spatial autocorrelation. And I might be interested in spots where crime is abnormally high or low. And that would be solved with some sort of spatial clustering, trying to identify hot or cold spots in the city. All right. So just a taste of what spatial data analysis can do for you. If you want to know more, feel free to talk to me. All right. Thanks. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you."}, {"Year": 2018, "Speaker": "Lizzy Huang", "Title": "Let's Git'R Marked Down: Streamline Model Development Using R Markdown", "Abstract": "Model development consists of data extraction, estimation implementation, analysis visualization and the documentation of results. Usually coding, visualization, and documentation are done by different tools in separate platforms. It is tough to keep track of any changes and improvements, and to ensure they are applied uniformly, especially when the whole process involves collaboration of many developers. In this talk, I will discuss an approach to streamline the development process by utilizing R Markdown under version control. This approach not only allows better control of the development code change, but also synchronizes results into documentation to ensure reproducibility and ease of auditing.", "VideoURL": "https://www.youtube.com/watch?v=GotBod025us", "id0": "2018_14", "transcript": "So, just a little bit about me and about me and arm markdown. So to be honest, I never touched R until last year March and what happened. So when I was in college, so I used to go in C and then I did some numerical analysis using MATLAB and then once I got into a grad program, got into my PhD program, I decided to do theoretical math. So, for about six years, the time that when I used my computer is when I need to lay text off. That's the only time that I need to code. And then it's about time for me to graduate, right? So I was like, okay, I need to look for jobs. And I first got into the academic jobs and you know how tough it is, especially for pure mathematician. And then eventually I realized, okay, I probably is not sure about for academia. It's definitely my problem, right? So finally, I saw that, okay, data scientist is the fastest job. And I need to make a living, then what happened? So I had to go back to review my statistics and then I realized, oh, everybody's using R. Then I had to learn it. So I started using my R while I'm taking some online courses from Coursera. And then I realized, well, it's not really how I imagined using R because think about I used to code in C. It's a low level language. There's no something called packages. And then I turned into MATLAB. And there's still not something called packages. And finally, when I used R, I had to figure out what packages I should use by Googling and reading things from Stack Overflow. That's just frustrating. But then how I got into R Markdown, I was taking classes and then all those classes would require me to do report and like assignments using R Markdown to code up your stuff and then also like explain the graphics, what you see from the data and things like this. And I realized, well, R Markdown has some features that I'm familiar with because I can lay text off. And moreover, I can still code things and then I can make it organized and consistent. So I got obsessed with R Markdown. And that's why today I'm not going to talk about my mathematical thesis. I'm going to talk about R Markdown. So my topic is let's get all Markdown. And thanks for my SaaS friend and actually he make up the title. And ironically, I'm not going to talk about SaaS and actually I'm trying to persuade people to get away from SaaS. Oh, it's... I feel like I'm going to get into trouble because they're actually recording this. So, I will talk about how to use R Markdown and also version control to streamline the model of turbulence. So let's take a look at the outline. So now I got a job in Freddie Mac. I'm in the modeling team and I'm a modeler. I need to do a lot of this modeling stuff, but moreover, I also need to do a lot of data analytics stuff. So from modeling like from end to end workflow, I will first talk about, you know, in general how we think about what would happen if we need to model something, right, from end to end. I mean, from pulling the data, processing the data, and to adventure, you got something to production. And then I will talk a little bit about what's the current development process and its challenges. And then I will talk about how we can improve this situation using R Markdown and version control. And finally, I will show you guys a demo if we have time and I'll make sure that it will be fast because we have a happy hour waiting for us. Okay, so the general model development work full. So you know, I'm a mathematician, so I like to just like organize things step by step, right? Your math teacher always tell you that step one is this, step to step. So starting from data extraction, and then you do some data analysis, feature engineering, of course you need to clean your data and then preprocess it. And then you will decide, okay, what will be the model algorithm I should use? And what kind of model that will fit into this situation? And then you will start implementing it. And then after that, you will take a look at what's the testing results. So that's post model analysis. You need to look at the model error and moreover for us in Freddie Mac, you need to be very careful with the model. And we need to do some sensitivity analysis, we need to do some scenario analysis. What would the home price suddenly draw? Right? You know what I mean by that. And then we also need to look at the impact analysis because that has something to do with our securities. And if that doesn't work, we need to go back. If it works, fine, then we let other team to do model validation and then also like user accept, accept and testing. And then finally the big thing, documentation. Every single step, we need to document that and tell people that why this model works and why we should use it. This is a very important thing in the corporate environment. And you can see that like from someone who comes from academia, this is just mind blowing and how much time that we actually spend on documentation. And then finally, when everything is fine, we let model go into production. So it seems it's okay, you know, this work for very clear, right? And also my company is a good company, so they provide a lot of tools. So currently what tools that I can use. So for programming tools, of course, SAS. Yeah. We need SAS because we need to deal with a lot of big data sets and then SAS provide a nice server. So we have a lot of legacy code written in SAS. You cannot avoid it. So SAS and then of course you need to connect to database SQL. And of course we also have like open source software, R and Python and so on. So fine. But we also need to do some data analysis stuff, right? We need to do reporting, we need to do visualization, we need to tell our users why this model is nice by showing them charts. You don't show them data sets, you don't show them numbers, you show them charts. So to do charts, we think for the Microsoft team, we have Excel, we have Word, and of course I forget that I'm using PowerPoint. And of course we also have internal GUI applications because we have lots of models that they will touch each other. They will just interact with each other. We use some of the outputs from other models. And then we, our output, generated by our models, will also get used by other models. So we need to take a look at if we change the model a little bit, what would be the impact? Or if other people change their model, what would be their impact to us? So that's something we need to do. Okay, so it seems everything is fine, but what reality is like this? So we have a lot of tools, a lot of platforms, but there's some issue like this, like here it's like some of my calls might be working in using SAS, SQL, and do some data extraction, some model estimation because we need the server for back data. But then some of us may prefer to use open source software, R and Python, and then we also can do some testing of the model there. And then whatever these numbers come out, we need to transfer to Excel, do visualization. And then we also need to put them into the internal GUI applications to locate this impact. And finally, every single result goes to Word because we need documentation. We need to document every single step. So speaking of this flowchart, to be honest, it's kind of messy. And most important thing is like, how can we collaborate? Because remember when you're in academia, you're mainly working probably like within a small team. But here you will get a lot of people involved. And every single step, you need to make sure you get it right. So that will process a lot of challenges if we are now working in such a process. So we'll be the main challenges that we have here. So the first thing is difficult to keep track of any changes that you have for your data and code. So when you're working by yourself, you totally know where you put your data, which version you're using, and why you use that. So what if you're working with other people and how you share with them? You need to go back and forth. And at some point, you probably just lose track of which data you're actually using, or which code, which version of the code that you're actually using. And then you call it my use the wrong data, my use of wrong code, the get the wrong results, and then you need to come back again and see what happened. So the second thing is, if it's hard to keep track of the data, of course, we will have less control of the data quality. And you don't want to fit in the wrong set of data to train your model, right? And then also, it's unable to standardize the estimation algorithms for each product. So think about in Freddie Mac, we do with fixed rates, mortgage 30 years, 50 years, 20 years, and then adjusted rate mortgages and all the other products that you guys might get in real life. And we want to hold a core model that will apply to every single product so that we don't need to write every single product, it's almost set of model code. But if we do it this way, then it's totally unable to just standardize this code because call A might be doing this, call B might be doing something different, and then they need to talk to each other. And it's just tough to keep track. And of course, it's impossible to automate the entire whole process. Then it takes time. And it's inconvenient for collaboration. So that's what I mentioned before, if you're working with a huge team or not to be a huge team, 10 people, then how are you going to make sure everybody's on the same track, right, on the same page? And then it's hard to reproduce your results. And then finally, it takes a long time for documentation. So we want to reduce the time to do documentation so that we can do more of modeling and research. But then if we need to do so many copy and paste, then also we need to find a piece everything into one documentation, it will just take a month. So to kind of reflect a little bit what we actually need here for this process. So first, of course, we need a programming platform that can standardize all our code. And then we need an interactive platform so that while we're coding, we can also see the graphs. And we can also input text. It's not just the comments for the call. But it's actually the explanation and all these things, you know, how you actually think about your model and then how the model behave. While you are testing your model or you're training your model, you can immediately just put down all these texts. And then you don't need to come back again while you're documenting it. And also we need a centralized platform so that you can reproduce your results. Everybody's doing it at the same place. But then remember that we're sharing because we're working in a team. So finally, we need a shared platform with version control so that everybody is using the same code, same version of data, and then that will be for our collaboration. So what will be the ideal approach here? We say first we need an interactive and centralized platform. This platform should be able to connect to a database because we need to do an end to end model process. So we first pull data, we'll be able to connect to a database. And then we will be able to implement code in different languages. So people might prefer R, might prefer Python, might prefer SQL, sorry, no SAS. And then also we hope that we can integrate with the graphics and also text so that we can, while we're doing all these model testing stuff, we can just generate our graphics that's needed for the documentation. We can just type in our test so that we can document everything while we're doing the stuff. And then finally, we should be able to hook up with the internal GUI API so that we can just push out our output into internal GUI and easy to see that will really impact if we change our model. And all of this, we need to document them. But underneath, most important thing is we need a shared platform and an underversion control so that everybody is on the same page. Okay, so let's talk about the solution. So if you're from academia, I feel like I'm just preaching to acquire. And even right now, I feel like I'm preaching to acquire now because everybody know that, okay, the solution, of course, you need to have something that's interactive coding platform and that you need a version control. So from my experience, excuse me, I started R since March last year and then I got used to R Markdown. And then finally, I had the opportunity to join a call server team to develop a course from Duke. And from that point, I was starting writing my first textbook for the course using book down. So that's a package on top of R Markdown. And at that point, after I joined Freddie Mac, I was like, well, can I actually use some of my past experience to help? So the first thing I could think of is, okay, I need R Markdown for interactive and centralized platform. So what I did at that point, my team was busy with documenting the model change. And then I proposed that, okay, what if I use R Markdown? And I can generate a template for everybody. So that everybody using the same template and I created the code. And then once you've just fit in the dataset, it will automatically generate all the table, all the explanation and all the graphics that are needed. That's the first step. And then finally, we can just combine the entire whole thing together into a whole document. And then we can send it out. Now in my company, we have the version control that's getting more mature. So we're thinking, okay, we want to automate the entire process. And we want to do it from the model development process, post model analysis, and also finally, we can synchronize the documentation process at the same time. But then we need to get to do the version control. And we need to control the code version while we can still enable the possibility for every teammate, they're doing their own thing. As long as when they make any changes, when they push anything, we know that, okay, which version we're actually using. So here, I want to show you guys a demo. So how I pretended I'm developing a model using Bookdown. So that's a package built on top of R Markdown. Let me see whether I can change it here. So a nice job from RStudio is very easy for you to just get a Bookdown project and then they automatically give you a template of it. So here I have a demo, how I'm going to streamline the model development. I can show you guys, this will be the final documentation or final PDF file that I will generate from this code. And you can also find this code from GitHub repository. So what I'm doing here is I'm trying to just make a model documentation process. So first, I will have an index file, which is easy. And good thing for Bookdown is that once you separate all these R Markdown using the lumbering and then they will automatically separate the chapter for you. So say for example, I want to, oh sorry, it's just a little bit messy here because I cannot just split to OPs. So in this structure, I have the index file and then you can see that I have some other R Markdown files layout over there. So once you lumbering your R Markdown file, they will automatically just split the chapter for you. So it's very easy to finally combine the entire whole thing as a PDF file. So I try to do like a small model to predict the results of an English Premier League game. So I'm a huge soccer fan, so that's why I picked this topic. So the first thing, well, then it's kind of tough for me because I want to switch from the other R Markdown file. So the first thing you need to do, of course, you need to do with the data. So my data is actually come from the Kaggle European soccer database. I process it a little bit so that we don't need to do with a huge database and I just pull out the data that I needed. It's a very small set of data. So first you will need to do some variable summary data analysis. Say for example, if I want to see like will be the number of counts of the shot-ons for each situation. I have draw and loss and also win. You see that the majority is actually in win. And you can kind of tell that this data is not a good dataset because if you actually look at the numbers of the game results, win. This class is way more than the other results like draw and loss. And you can kind of guess if I do a lot just the regression, multi-class log just the regression. And this is a balanced dataset and that model will not be good. So I'm not trying to tell you guys how to do a model. So please forget about it. Forgive me by doing a lousy model here. So then let's go to the model part. We'll usually describe the methodology of the model. We'll load our package. So say for example, here I have a package.r. Suppose that's my model implementation code and then it's written in the package. And then I will talk about what explanatory variables that I'm going to use and then also what method I'm going to use. So here I just use a very simple Lasso regularization model. And then I will try to use this model on different products. So the first case, I want to focus on the team. Manchester United. So I will use this model and then just like split the dataset that's only Manchester United and then train it and then I got some results and then I'll report what the model will tell us and all this kind of thing. And of course it's boring to see here. Eventually I will show you the entire PDF version. And then suppose like my another colleague, he or she is more interested in another team or another product or he or she is responsible for doing that. And then they can also do a separate product. So second case is Liverpool. And if you are a soccer fan, you probably understand why I picked these two teams. So suppose like you have another colleague who is responsible for another product and this colleague can just totally just like set up another arm-out-down file and then these two will not conflict with each other. So you can easily just like separate the workload, it's enabled more flexibility for collaboration. But then eventually when you commit your changes using Git, the version control, you will know that okay, you'll call it making changes here and then you're making change here. The two of you will not conflict with each other. But then finally you will see the final products. Let me go back to my PDF version. So here is my demo. I have my data and then this is my chapter two because chapter one is the introduction and then I have my variable summary, a nice table explaining what variable that I have in this dataset and then I will talk about like how I split the train data and then testing data, some of the input variable analysis and also the output variable analysis. And then finally when I got to the model part, so this is my next chapter and I first talk about methodology that will work for all different cases and then I first talk about Manchester United, the model results. Well it seems like it says if you want to Manchester United to make a draw, it only matters whether it's a home game or a wade game. So you see that the model is just terrible. And if you want to get a loss, it has something to do with whether the team has some crosses but then minus the opponent team's crosses, lumber of crosses. If you want to get a win, you should not fail so many. And then what's weird is you should not also pass so many, so the lumber of team crosses have a negative parameter in front of it and you should have more possession. So we probably know that why this season Manchester United is not winning that many because they are losing their possession. And then finally if you're in a wade game and you're probably not in a favor of winning and you see that right next to it, you can just piece these two together using book down and that will be the second case when you have Liverpool and then it tells you that under this model training there's no way for you to get a draw which just makes sense and if you want Liverpool to lose, probably Liverpool should not just like, it will be negatively correlated with the team crosses and also if they're in a wade game and then they will be more likely to lose a game and then for Liverpool to win, it's also negatively correlated to team crosses which I don't understand and then it will be partially correlated to the possession time of the team. So Liverpool right now is on the third I believe in this season so apparently they're doing basically the same thing here. So let's get back a little bit of my slides. So there I did not show one thing is how I coordinate the collaboration using Git because I just don't have the Wi-Fi so I don't want to show that part but usually what you can imagine is in the Git repository so suppose you have multiple colleagues working together for one project then you can put all your model code package and also a documentation template into the master branch and so then you can set up different branches like Manchester United and then you will have one colleague called, you know, Lizzie Huang working on that branch and then she can just like change whatever she like and feel like okay for this particular dataset, for this particular case, how I can change my model and then you will have another colleague working for another branch called Liverpool and then this Lizzie Huang will just like, you know, try to come up with a different set of parameters that will work particularly for Liverpool so now everybody know how to find me on Facebook and Gmail and please don't hack my accounts. Okay so I've talked too much and let's do a summary. So we say that in the current model development process we use multiple programming and reporting tools but they are not necessarily integrated so that will take us more time to actually do this redundant work. So the lack of interactive coding platform version controls creates difficulties and also challenges in tracking the code and data changes, how to standardize your code and how to reproduce the results and also like it's inconvenient for collaboration and of course it's impossible to automate this entire process. So our markdown together with Git will provide the ideal solution involving an interactive and centralized platform and with version control so that it's much easier and you can also syncretize the documentation process while you're actually doing your modeling. Okay so thank you very much."}, {"Year": 2018, "Speaker": "Tommy Jones", "Title": "Mining Text with textmineR", "Abstract": "textmineR introduces a framework for natural language processing (NLP) in R that improves upon current NLP frameworks available in R. Specifically, textmineR has a syntax that is more intuitive to experienced R users. It uses objects, methods, and functions that behave like regular R dense matrices. This lowers the barrier to beginning statistical analyses of language to statisticians and other data analytics professionals. textmineR also implements diagnostic and analysis methods for topic models.", "VideoURL": "https://www.youtube.com/watch?v=C39qwwQ7EJ8", "id0": "2018_15", "transcript": "Just so I can level set on my expectations of the audience here, who here has done some text mining in R? You can show a handful, a lot of you. Who here? We use tidy text. Okay. That's really helpful. Thank you. So in 2013, I had just finished my master's degree in math and statistics. I was working for a science policy research institute. And I had a group of our scientists come to me and say, hey Tommy, we need some stuff clustered and I was like, great, I learned clustering in grad school, like bring it on. And then they were like, here's your data set and it was a bunch of documents. And I was like, so I don't know if you guys know this, but I need numbers to cluster. And these are words. Little did I know that my career was taking a sharp left turn at that moment. So skip forward a few years. I'm a PhD student at Mason. My dissertation research is in topic modeling. So I guess I really just ran with that ball. But somewhere between the two, I started, I wrote a package called text minor, which is what I'm here to talk to you about today. So do you guys remember Jim's talk yesterday about Gmail? And he had that one slide that was using the TM package to try and download his Gmail records. And then he had to create some weird vcorpus object and it didn't print well. And he wasn't quite sure what he was doing. And he got really frustrated and abandoned it and found another tool. So back in 2013, that was not an option that was available to me. So text minor really started as a series of wrapper functions around the TM package and then also the LDA package that I had written both to, well, obviously to make my job easier. But really to enforce, I think, a more R way of doing things, which is text mining is actually just statistical modeling, just like the rest of what we do in R. So I released the first version or the first public version of text minor with a little fanfare almost four years ago. And so it's just sort of sat on crayon and been churning away. But in the intervening period, something big sort of happened in that ecosystem, which is tidy text. So Julia, Silgy, and her co-authors have created this package that is a unifying framework for text mining in R. It is a much more R way of doing things. So that really compels the question today, right? If they've built this framework and it's got wide adoption and it's awesome because it is, why might you still want to use text minor? So one, I think my way of thinking about things is still a pretty good way. So it's at least worth talking about. But it's also sort of evolved into a pretty darn good topic modeling workbench. So my dissertation research is in topic modeling. And sort of as I, to this last point, as I develop something and go through a proof of concept, and I'm like, hey, this works pretty good, text minor just becomes a landing pad for that. And so I'll throw it in. And it's available to anyone who would care to use it. So this is what we're going to be talking about today. I don't want one slide too far. This is what we're going to be talking about today. For the sake of time, I'm just going to get in it. So first, just a little bit of level setting and making sure we're sort of all on the same page of at least about how I think about these things. So one of two core mathematical structures you need to do text mining is something called a document term matrix. Every row is a document, however you're going to define that, whether it's a book, a research paper, a paragraph, a sentence. And every column is some sort of linguistic artifact, usually a word or a pair of words. We'll call them unigrams, biograms, and grams. And then inside your matrix is some sort of frequency measure. So if we're counting, it would be the number of times word J appeared in document I. The second core mathematical structure for text mining is something called a term co-occurrence matrix. So that's relating terms to terms. So it's a square matrix. It's not necessarily symmetric. This could be a count, you could be displaying a count of the number of times word I and word J co-occurred in the same document together. It could be a count of the number of times word I and J appeared within a fixed window of each other, so like within five words of each other. That's called the skip gram model if you guys ever hear about that. Or it could be something like the number of times word J appeared within five words after word I or something like that. And so basically these two matrices are what we're doing math and statistics on that basically lends itself to any manner of natural language processing applications. And so a basic pipeline for doing these sorts of analyses is you've got your raw data that are your documents. You need to curate them and clean them up and turn them into a matrix of data that you're then going to feed to a model or algorithm. You're going to look at the results of that model and decide if you're happy with where you are and you might iterate between one and three several times until you get something that you think stands up. And then hopefully if you have a good model, I don't have to report the results here, but you know, that's assumed. But hopefully if you have a good model it generalizes to some sort of population. And so you would then apply that model to new data that is drawn from that population. And if this sounds familiar to everyone it should because this is also just how we do statistical analyses. And so again it is just sort of like a soapbox thing for me because really I am a statistician, not like an NLP guy. This is just statistics. What I have found is that many NLP packages both in R and outside of R, they're sort of these little like cut off enclaves of their own weird data types that have this their own weird workflow and syntax. And it doesn't need to be that way because you're not actually doing anything that is meaningfully different than what you're doing for any other type of data that you would analyze. So my second soapbox issue here, so many models of text. So I'm going to pick on Word to Vek for a little bit. They get wrapped up as sort of this black box where they don't really differentiate how you're handling your data from the statistical or mathematical method that you're using. And that's really misleading because a lot of where the battles won or lost a lot of the different meeting and how you might interpret the results of the exact same mathematical model happens here. So you're making these sort of very fundamental decisions when you're going to set up this data matrix and that is where things are won or lost. So in a few slides I'm going to talk about, I'll just say it now, so topic models and text embeddings are actually the same thing. Just you just give a term co-occurrence matrix to one and a document term matrix to the other. I don't think that's terribly obvious from the way we currently think or talk about these. So I wanted something that sort of decoupled data curation from modeling so that was a little bit more obvious. So moving on to scalability concerns, right? So I do actually think that a reason that a lot of these NLP packages are hard to use isn't because the people who made them are dumb or they don't care about you. It's just that until very recently they were solving a scalability problem and the usability problem was second. Now only just now reached a point where we can just assume that things will scale and so now we can focus on utility. So a quick equation. Have you guys ever thought about how memory gets allocated and are when you create an object? Like how big is something? Where does that come from? So with a matrix you're going to have the number of rows times the number of columns because each entry in that matrix is just a number and that has to be given a memory address. If we're doing integers then that memory address is going to be 8 bytes and I think if it's float, I want to say it's like 46, 43, something like that, but let's just stick with integers. So I've got N rows times K columns times 8 bytes. That's how many bytes this thing should take up in memory and I want to divide by a billion if I want to see that it's in gigabytes. Now for most statistical analyses you have to have like a lot of that for it to amount to something. How big could these possibly get in text mining? So this is what happened when I tried to allocate 5 terabytes of memory on my MacBook Air. So 100,000 documents times 6 million unique words and that was after removing stop words mind you. Times 8 bytes divided by a billion is just shy of 5 terabytes there. So just another couple examples here. So these are more moderately sized corpora. You know 10,000 observations, that's not a lot in normal statistics land but you can end up using half your available memory on your machine right there if you're using a standard matrix. A big important thing in a lot of text mining and natural language processing applications is we need some sort of sparse matrix to be able to get memory savings so we can actually work on these things without needing a super computer or a big server. I'm going to come back to that in a bit. So briefly we're talking about the philosophy here. So when I created this package I wanted to honor three basic principles. I wanted to use data types to enforce maximum interoperability within ours ecosystem. I didn't want an enclave. I wanted something that could just integrate with the rest of R even if it had nothing to do with NLP. I wanted syntax that was idiomatic to R so if you're coming to text mining and you're pretty good at R I would like you to be able to sort of like have some intuition of what's going on. And then of course for reasons that we just discussed things need to be scalable in terms of object search and computation time. And so I'm going to give you an example of each one of those that I've done in text miner. So the first here deals with both interoperability and scalability so we need a sparse matrix type. So the TM package uses sparse matrices from the SLAM package. I think that stands for sparse lightweight arrays and matrices. And then text miner and text evac and I think maybe Juan Detta, a few others use sparse matrices from the matrix package. And I think one of these things is not like the other. So matrix actually ships with the crayon version of R. It's really well embedded in that ecosystem. In fact we actually saw it yesterday during Jared's talk when he was talking about sparse matrices I saw there was a little DGC matrix. And so it works with Glimnet, right? Nothing to do with NLP but you could just make a document term matrix, hand it out. If you're going to get it off to Glimnet maybe that will do something for you. Just so you guys don't think that I am confusing quantity with quality, here's the popularity of various text mining frameworks in R. Maybe you guys will like what you see and you can help me out with that. So idiomatic syntax. What does that mean? I'm not even sure how to define it. So instead of defining it I'm just going to show you some code here. So up there at the very top that's interoperability again. Your documents are just a character vector. You can store your documents with your metadata in a data frame or a list or something. I don't care. R has lots of base classes that work perfectly well for this. I don't know what a v-corpus is. I don't know why I should have to know what a v-corpus is and I don't know why you guys should have to know either. Character vectors work fine. Single function to create a document term matrix is a similar create TCM function that's there as well. And then you want to fit a topic model. You just hand your matrix off to it. So in three steps we've gone from raw documents down to fitted topic model. I've created an S3 class for all the topic models that text miner supports. So when you get new data and you want to get a prediction under that model you can just use a predict method. And so I don't spell out the arguments there but it's predict object comma new data just like all of the rest of the models that we have in R. And then finally speaking towards scalability I wanted text miner to use all of your cores by default although that's an option you can change. Because of the size and scale of language data we really need parallelism but my cell phone has multiple cores at this point. So this shouldn't be something that's relegated to somebody with a background in high performance computing. So I wrote this wrapper function around stuff from the parallel package to handle parallelization whether you're using a Unix system or whether you're using Windows. And then Jared pointed out to me the other day that do parallelism has actually done that since 2011. So maybe that's not quite so novel and maybe we shouldn't have statisticians writing parallel algorithms. So I'll look into that but meanwhile that's what's running in the back end. So very quickly in the last five or so minutes that I have. So my focus has been on topic modeling and so that's where a lot of the modeling components of text miner have come in. And so as I said before this is sort of my mental model for topic models and text embeddings. They're all basically matrix decomposition methods. So with latent semantic analysis you're using a single value decomposition you're factoring a matrix with latent Dirichlet allocation you're using some probabilistic decomposition right. But basically all of these well and then text embeddings you're just doing that with a term co-occurrence matrix. All of these result in these smaller dimensional matrices. So rather than having to use dozens of packages that represent them all differently instead I've written some wrappers that sort of enforced this. So I've got one matrix that is a distribution of topics over whatever the rows of my matrix were and then another one that is a distribution of words over those topics. And so looking at the output of an LDA object in text miner and just focusing in on those top two I know it's kind of small but we've got phi and theta. And so every topic model that I have either implemented or written a wrapper for you get at least those. So you can actually compare directly like do I want to use model A or model B instead of spending all of your time grappling with the syntax and things like that. That said I am but one man so I've only wrapped, I've implemented LDA over the summer that was a wrapper until very recently and then wrapped correlated topic models and then LSI. I've got a few more that I've experimented with but I hate to compare myself to Steve Jobs but defaults really matter. He talks about computers being the bicycle of the mind. So I have these other wrappers that are sitting around and I just don't want to release them until I know enough about the model to say that if you were just dumb enough to just be like here's my data give me something that that something is going to be like a reasonably good choice. But I would love help so if anyone else wants to give me a shout out I'd love to get more models in there. And then these are just a list of a subset of topic modeling helper functions that are available in TextMiner. So like I said it's turning into a pretty darn good topic modeling workbench. So this second to last little section here as I said I'm getting a PhD I'm focusing on topic modeling and so as I've developed certain things and decided I'm happy with them you know TextMiner has become a landing platform for them so anyone who wants to use them and finds them useful TextMiner is a great place anyone who wants to use them and tell me all the ways I got it wrong you can open up an issue on GitHub. So the first one is a couple years ago I derived a real R squared for topic models. It's not a pseudo R squared it's not something that just looks like R squared. It is in fact the proportion of variability in your data that has been explained by your model. I've got a whole other talk for this but just the punch line in like a couple sentences. You can look at the normal R squared that we use in linear regression as a ratio of sums of squared Euclidean distances in one space. So we can generalize that to end space and this is a picture of total sum of squares and residual sum of squares in two space. You can generalize it to end space and then you can get an R squared from a topic model because it's basically saying under the model how well does this compare to the document term matrix or term co-occurrence matrix that you that you handed me. In 2013 I developed a metric called probabilistic coherence. Coherence is a measure of topic quality and I wasn't satisfied with anything else that I found there so I used a little bit of probability theory. Developed this metric I thought it was great. I actually wanted to write it up into a small paper and realized I had zero baseline for objective comparison to say it was better than any other metric out there and then I was like oh god and that's what my dissertation is on but I'm not going to get into that now. I recently found out that somebody else had derived the exact same metric in 2011 and published it and it really just didn't get circulated widely. It took me many years to even discover that it was out there so that's one less paper I have to write so you know great but I like it and I think it works really well. I'm actually not going to get into this so much but needless to say a lot of my research has indicated a need for some more flexibility in an implementation of LDA and so that's what I wanted to do so the LDA implementation that TechSminer has has a lot more flexibility for setting priors although it still has pretty good defaults I think as well as a more traditionally Bayesian statistics sort of handling of this model. Alright so in the future I want to do a bunch of things. I think that update method is really important because anyone who's used a topic model and then said this is great now I want to add new data. It realizes that when you retrain it the thing that was topic 5 that you loved is not topic 5 anymore and you don't know where it went. That seems like a pretty big usability issue so I want to tackle that. TechSminer is on crayon and I have stickers they look like that they're in the back and I have a ton more so I'd be happy to give lots of stickers to people and if you want to get started thanks to my friend BJ who's sitting back there he pressured me into writing a whole bunch of vignettes and so they're available on the crayon distribution. Thank you."}, {"Year": 2018, "Speaker": "Max Richman", "Title": "SQL for Everyone", "Abstract": "In the spirit of Lander (R for Everyone book), Chen (Pandas for Everyone book), and Richman (R for Every Survey Analysis talk), this talk will focus on how you, yes you, can and should probably use SQL as part of your R data analysis workflow. This is a beginner-to-intermediate level introduction to fundamental data pipeline skills every analyst should have in their toolkit, explained in a way to make it easy to get started yourself afterwards. Topics include RODBC, SQL group by, SQL window functions, among other handy tools and tips.", "VideoURL": "https://www.youtube.com/watch?v=Nv4GFwoFZxk", "id0": "2018_16", "transcript": "So, hey everybody. My name is Max Richmond. I am a quantitative analyst at Arcadia Power and the song that you just heard is called Sequel by Harry Chapin. You guys might be surprised to know when Structured Query Language was developed I guess 44 years ago, Harry Chapin was instrumentally involved in that, hence the title of his song Sequel. Actually, no, that's not true at all. Harry was not involved with Sequel. Sequel is a story about actually eight years after he was like an attack. He has this attack story where he was riding along an attack scene. He found an old flame and a taxi. For those of you that don't know what a taxi is, it's like the worst Uber you've ever been in basically. So, yeah, so he's riding around this taxi, finds this old flame, and then eight years later writes a song called The Sequel, which is actually spelled S-E-Q-U-E-L, which is about getting another taxi with the same person again, Sue. And Sue is, let's say, someone that he has a lot of feelings for but doesn't actually get married to. He gets married to somebody called Sandy. And so, Sandy, before getting married to Harry Chapeman, had like two kids, I don't know, three kids with a letter J. They were like, you know, Jonathan and Jennifer and whatever. And then they had two more kids bringing it to five, and all of them had J-named as well, like Jeremy and Jebediah and I don't know, something like that. And so it got me thinking that I hope even though Jared starts with a J, he names all his kids starting with an R. So I'm thinking like Reginald or Rutherford or Rihanna or like some names that like start with R. So this is also a sequel in other ways because I spoke at the first New York Data Conference, New York R Conference back in 2015, where I channeled the spirit of R for everyone and as you might be able to see from this tiny little Virginia Tech hat here, this is our friend Dan Chen, who also wrote a book called Python for everyone. And I just love the spirit of those books and this conference of just like, sure, anyone can get started with this. Let's make it easy. So I decided that I would similarly buck the trend and not really talk about R, but talk about sequel. So this is a very gentle intro to sequel. A little bit about me. I based here in DC. Yes, I am actually wearing the same exact shirt that I'm wearing in that photo because I am kind of like a hobo or maybe a hippie or something. I don't know, something along those lines. So I've worn a few hats. I've done GIS in this building for like five years for USAID. I founded DataKind in DC, which is a great network also in New York for volunteering, your time for due data science for good. And today I work at Arcadia Power doing data analytics. So I'm hoping, unlike Harry Chapin, who tragically died after his sequel in a terrible car accident, that fate will not become me as I continue beyond the sequel. And I hope that at the next, our conference perhaps like on the moon or on Mars, I can do another photo where hopefully the video is getting like all of this and then I can have like infinite recursion of like the talks and the person. Anyway, so we'll see if we get there. Okay, so okay, enough preamble and whatnot. What am I actually gonna really talk about in this? I'm talking about nothing apparently. That's it, no, I guess it'll come back, but I'm gonna riff in the meantime. It's a pretty simple agenda. We've got, I'm gonna talk a little bit about the new renewable energy sector. And then I'm going to, I also made fun of someone yesterday for like they're deck dying and then my deck died. So, and I'm gonna talk about some intro level sequel and our characteristics. So imagine like a nice GIF, like of a spinning windmill right now. Cool, does anybody know what 1.5 C means? Anyone? 1.5 C? It's like a number are you guys know numbers? Does number mean anything? No, all right, well if you could imagine lots of GIFs on the screen here flailing around showing how once we reach the global temperature change of 1.5 degrees Celsius, we are screwed, right? So, we have unfortunately eclipsed that as the most recent report from the IPCC announced. Meaning that we are well on our way to a lot of terrible effects from climate change. But what can we do about this? It feels like we're kind of powerless. We can actually do quite a bit, at least here in the US, with our own activities and what we do. And we can also do some things with our home electricity usage. So the company that I work for, Arcadia Power, makes it easy for you to basically opt in to make your energy usage green and to save money. So the way that that then works is you would sign up on the website. And automatically we would find deals for you to save money, connect you to solar projects, use wind to offset what you're using and what not. And we're also hiring like a lot of people and data analytics are interested in kind of combining environmental science with data science come talk to me. I now have a charming picture of Hadley Wickham up here with the obligatory motivating Hadley Wickham quote, which in which he says, however, in the long run, I highly recommend that you at least learn the basics of SQL. It's a valuable skill for any data scientist. Hadley Wickham. In this next slide, I have a SQL query just sort of laid out in simple format. We have a select statement, which is the beginning of your query. We have an asterisk, which is representing what columns you want to select in your query. In this case, all of them. And we have a frums, a bit of syntax that then indicates which table or schema you want your data to come from and a nice little where clause. So it's really simple when you think about it in your mind. Next I have another motivating XKCD because of course every talk needs to be motivated by some XKCD. And in this one, we learned that the mom named, oh hey, hooray. Great. I'll still like detail this because it's still impossible to read it. But basically we have little Bobby Tables whose mom named him Robert, single quote, closed paren, semicolon, drop, table, students, which if you have a Hadley Wickham level of basic knowledge and SQL you would chuckle at because basically it results into the entire student table being dropped by the unsuspecting elementary school. So this is why it's important to have some baseline knowledge of SQL. So let me see if there's anything else we're showing. Oh yeah, here's the thing. So this thing, yeah. Hopefully this is somewhat familiar. If not, take a look at it. It's a nice way to query data and do stuff like that. And then finally, oh yeah, here's that nice picture. Oh yes, here is the death spiral. So as you can see, since the dawn of industrialization, we are unfortunately spiraling towards higher temperatures, but you can do something about it. So moving on, the main topic here I'm going to talk about are seven key areas where you can do something in R and you can do something in SQL. And even though maybe one you're faster than the other, it's kind of worth knowing how you can do both of them. And I'm going to talk about the end about why you would do one or the other. So at my job, we have, well, sorry, the first step is kind of getting set up with environments and installing databases and software and all of that. So at my work, we use something called Amazon Redshift, which is a data warehouse. There are probably other data warehouses that people use, but what's really nice is it's a columnar data storage like with a lot of speed. So it uses an older flavor of Postgres. So it's a little wonky. It doesn't have all the bells and whistles of latest Postgres, but you can like join tons of things together like pointlessly and you know, it's really great. So we like using that. We also pull in data with Stitch, which is like an API integration service or basically you can plug in your like Facebook advertising API. You can plug in your whatever third party vendor data sources, other groups and also seamlessly integrate that into Redshift to make just like all the data together. It's really fun. And then you can set up your ID. So of course, our studio is wonderful on the R side. And then on the ID side for SQL, there's SQL workbench and a lot of other great tools for that. And once you're set up with your databases and with your tools, then it's about bringing the data in by a show of hands who's used RODBC before or a variant of that. Oh, great. So good like half of the room or more. And the other half is probably asleep. But in case you weren't asleep and you really haven't used it that or how many people have used the DB plier package from the Hadleyverse. OK, about a third of people a little bit less. Basically the beauty of these two packages is that you can write SQL directly into R and query the underlying database that it's being pulled from. So in this case, using RODBC, you install a driver on your machine. I intentionally did not do the Java based one because otherwise you will go down to Java rabbit hole and you will never return. So raise your hand. That's happening. All right. So this is many people as if you had these database package, which is sad. OK, so yeah, once you get set up there, you put in all your credentials for your database. You can just like list out your tables and then you can run whatever queries you want to run. What kind of queries might you want to run? Well, just like an R, you might want to just like know what's in your data. So we have information about just like how many rows are in this data set. So select count from the table and the schema or an R. Just like tell me how many rows there are. And similarly, you might want to preview some of the data like how many columns are there? What are the names of them? What kind of data do they have inside of them? You can similarly do that and you can grab random samples and you can look at tiny bits of it both in R and in SQL. Descriptive stats as well. This is where actually sometimes SQL is kind of an advantage over it. So the table command is so simple. It's nice. It's lovely. You can just write table and you can look at some kind of vector and it's beautiful and it's great. But once you want to start combining a whole bunch of them, like you want to cross two, three, four, five, six, seven variables, then you're like, oh, I got to install packages or there's probably ways to do it. Then base R that I'm not familiar with. But these are like, it becomes non-trivial. Whereas in SQL, you just keep writing how many things you want to group things by. So for our utility company, I might want to see how many customers we have across all 50 states with given utility. And I can just say select the utility name. And then for my next column, have a count of it. So basically an aggregation count how many there are and then group it by that first one, utility name. And I get the count by each of the utility names of what I have. And if I want to add also by state, then I add state as a column. I say group by one and two. And if I want to also add, you know, what specifically, like if a utility is an investor owned utility or if it's small entity, I add that and you can just keep going and quickly just get nice cross tabs. Now where R is a little bit nicer is on just like the percentages. So SQL, you have to do some weird stuff where you kind of loop things over things to get your percentages going. But you know, on R it's nice. You just kind of throw a prop table up there. Similarly, you can add rows and columns. The basically the merge and the join are pretty much the same. You just kind of can dictate what you want to join to what. So you want to append some data more columns. You can do a left join. If you want to just limit it to the joins, the data between those two columns, you do interjoin, that kind of stuff. Let's see. So you can also attach more rows at the bottom. So kind of append things from the bottom in SQL that's union and R based R that's R bind. And there are other things within the plier, of course, that you can use to do a lot of this data manipulation. And let's say you want to create new data. So my previous talk was about R for every survey analysis. And similarly, when you're coding survey data, you're just constantly recreating new coded variables. So you can do that pretty easily in both of these tools as well. Here's where we start to get, I guess, a little bit more advanced, but not so advanced, is subsetting and filtering. In this example, I want to see, select all the mail customers from the last two years that have either wind or solar plans. So I go ahead and type in my query and have a bunch of filters there. Similarly in base R, you can do that with this or another way of doing it. And you can also use the deep plier package to do some filtering and things like that. Generally, it's quite performant in SQL as well. So then you have subqueries, which is kind of like this idea of nesting things. So you can do a query within a query, within a query, within a query. And you might do that until let's say aggregate something and then join it to something that's like not aggregated and things like that. And similarly, you can use the special character, Magrit, thing, pipe thing, non-pipe thing in R. I'm sure it has a better name that I've forgotten here. So yeah, so grouping and aggregating is the next big piece here. As I mentioned earlier, you can just keep adding these group buys, group buy every column that you have, and you can summarize your data quite quickly. This is similar to what you can do in deep plier. In fact, in deep plier, there's this neat thing called show query where you can be like, hey, deep plier, what would be the SQL equivalent of what I'm doing here? So if you like wrote something there, you could actually see what it looks like on the SQL side and things like that. And then another nifty little, I think, sort of, you know, all the other things I've been showing you, I just compiled over a month of just working and being like, oh, this is a thing that I did. This is the thing that I did. It is really this many things. And you can do a lot with data analytics analysis and just like aggregation. But another useful one is called a window function in SQL. This is probably one of the more like kind of complicated ones. Basically just ask a coworker for a copy of their code that's working that has a window function and then just like plug things in and out until it kind of works for you. But what it allows you to do is if for example, let's say we have 12 utility statements for customers and we want to sort them by date and we want to get the second newest utility statement. So here we have another one of those nested queries, query within a query, where we go ahead and we kind of rank over this statement date, ordering it descending from newest oldest as the statement rank. So basically every statement gets a rank of one, two, three, four, five all the way to 12. And then from that we select just the one that's the second one. So basically allows you to kind of create a coding and a ranking and a numerical ranking of things based on other columns in your data set. Sends it being pretty useful. And then finally with all things it's about reporting. So if you're reading this stuff into R, then you can use, you know, GD plot, R markdown, whatever your favorite ways of reporting things are. Another really useful thing of doing it in SQL is that if, let's say you have Tableau or some other BI tool that you like using, a lot of them can just ingest that SQL directly in. So you can go ahead and just like write your queries up and then take that same query, put in Tableau, and then you never have to do anything again, because the view will just be updated as your database is updated as well. And then finally, of course, the obligatory like Excel outputs as well, always needed for managers, for anyone who doesn't want to live in your R SQL universe. And I have three little tabs here that I would encourage you to do in your Excel outputs, you know, have a summary or high level takeaways, have your actual raw data in case somebody wants to filter it and play with it, and then actually put your code in there, put your SQL code. It's going to look kind of janky, but it's going to really improve reusability when you have to communicate with your future self six months from now, or it's going to really improve that if you have to share and collaborate with colleagues and others, or simply just rerun the analysis the next week when your team member asks you for it. So what are some differences kind of synthesizing what we've talked about? And I use the word often here, but you can also see sometimes or never. I mean, like, I don't know, like you can do all these things and all these things as Tommy was saying, like right now, just like with memory on computers, like these becomes less of a problem. But you know, there are huge data sets out there that you maybe want to combine together and do that before you bring it into R. So that's maybe a useful use case for some SQL stuff. Similarly, you know, if you have tons of columns and you want to run like a lot of these frequencies of like five or six, seven or 10 variables the same time, I find SQL is pretty useful for that. And then this fourth bullet about sort of ease of understanding is important on the SQL side. Often if you're like in a product company like me, you work a lot with engineers and product folks who can understand and think and breathe in SQL because it's kind of this lowest common denominator. So even though you might write out the best like prototype version in R of what you want ultimately to be in a production, engineer might not want to look at R or if they do, you know, you have to like really explain though, this is a weird package. It came from this place does this thing and you have all these dependencies and stuff. So sometimes just modeling what you want to do and handing it over in SQL is a way to kind of interface. And then I'm sure there's many more reasons here. I give them that I'm an R conference that R is better. I'm sorry, there's only two bullets, but like in your mind, just like add lots more bullets that I'm forgetting here. But R is often better for, you know, modeling, of course, more sophisticated clustering and other like data summarization, all the extensible packages that are there. And then reporting whether it's through our markdown or things like that. And then what are some really sweet similarities that bring it together? You know, they're both syntax and code to manipulate type of data stored in memory. You can do those seven common tasks that I listed before in both of them pretty easily. They have a lot of built in functions, data arithmetic, other things that make it useful and easy to work with. You can create new functions within both of them. You can like have as much white space as you want and lots of tabs and new lines and all that stuff to make it as parsimonious or as spaced out as you want. And then there are great IDEs that make it a lot better to transact with and to work with and see the highlighting and all that stuff. And then you can like with both of them export to variety formats to work with further. So that's it. Thanks a lot."}, {"Year": 2018, "Speaker": "David Smith", "Title": "Not Hotdog: Image Recognition with R and the Custom Vision API", "Abstract": "Building an application that can recognize a specific type of object from scratch is possible with tools like convolutional neural networks, but it's not easy: you may need many thousands of labelled representative and unrepresentative images, and training such a model may consume many expensive GPU core-hours. An simpler yet effective way is to use Transfer Learning: use a standard neural network already trained to recognize general objects, and use the features it has already learned to recognize a new set of objects. With this method, you need far fewer novel images, and the training process is much faster. In this talk, I'll use R in conjunction with the Microsoft Custom Vision API to train and use a custom vision recognizer. I'll use an example motivated by the TV series \"Silicon Valley\", and with just a couple of hundred images of food, create Shiny application that can detect whether or not a given image contains a hot dog.", "VideoURL": "https://www.youtube.com/watch?v=HEFEYgoAj2o", "id0": "2018_17", "transcript": "So as you heard it, my name is David Smith and I'm pressing the big. And Jarrod gave me a great introduction so you've probably heard everything you need to know about me. So with that I'm going to jump right into the talk. And yeah, so as you might have guessed from the entry music, this talk was inspired by an episode of Silicon Valley. I don't have anybody saw this episode from season four where Jinyang builds an app on his phone that he could point at food and it will tell you what the food was except that it only worked for hot dogs. Funny thing about this, this episode only aired last year, in May last year. And at that time, the whole concept that you could have, and by the way, the time is not going down here guys. The whole concept that you could have an app that you could point at anything and it would tell you what was in the picture was complete fantasy. Nonetheless, the producers did build exactly this app. They used TensorFlow and uploaded it to core ML and ran it on a device, actually I think was an Android device, now that I think about it. They have a really good blog post that explains the process that you use to actually build this model from scratch and train it and it was quite pioneering. And today, this kind of stuff is pretty routine. And in fact, what I'm going to show you in this talk today is how you can build exactly an app like this. An app that takes a picture, you point at something that may or may not be a hot dog and it will tell you if it is or is not a hot dog. And the way I'm going to do that is by starting with getting some images of hot dogs that I'm going to train a model with and I'm going to get those images from a place called ImageNet and I'll show you how to do that in a little while. Then we're going to train a recognizer using a process called transfer learning. And the really nice thing about this is you don't need to do that from scratch. I'm actually going to use an online service in the Azure Cloud to do that and drive all that computation using art. But the end result of this is we're going to get a shiny app that we can use, we can give it a picture or a URL of a picture. It was analyzed that picture and tell us whether or not it's a hot dog. So let's see how to build one of those. Before I do that though, I want to do a little bit of explanation about computer vision and I'm going to gloss over many, many, many details during this very brief explanation. But if you do want to go into the details of how this works and particularly the mathematics behind how computer vision works, I really recommend Brandon Roer's blog. He explains it in really clear and easy to understand ways to check that out. And in particular, I borrowed this particular image from Brandon's blog. But let's have a look at how computer vision works. The basic idea behind computer vision is we have a neural network, which is basically a big bunch of matrix operations. We start off with an image. Here's a picture of my bicycle. And that image, all the pixels in that image goes to this node right here. And what happens at that node is a filter is applied. A filter in exactly the same sense as a filter in Instagram or Snapchat. That filter is defined by a small number of weights that get applied to pixels, typically nine weights or four weights. But it's just a simple mathematical operation which transforms that picture, changes it somehow and makes it a little bit smaller in the process. Same thing here. We did the same kind of process with a different filter here, different filter here, different filter here. We get four new images. We combine the four images together and the new filter, which makes the image smaller again. A new filter, a new filter, a new filter, another layer in the neural network. Same process happens again. So as we apply all these pictures, these filters rather, and combine the pictures together, where we're inferring more and more information somehow about that picture. This particular neural network is a very small one by computer vision standards. It only has four hidden layers. But it's also only designed to detect four different classes of objects. It's designed to detect dogs, bicycles, apples and tennis balls. And you can see that as we take this picture of a bicycle and process it from filter to filter to filter, the image gets smaller and smaller and smaller until at the very last layer. We combine everything together and that entire picture has converted to one number, which has been z between zero and one, which you can think of as the probability or the confidence that the picture represents one of these four classes. And you can see that in this particular case, it's working well for this bicycle because that's the one that gets the highest score. And this neural network would classify this picture as a bicycle. Well, how do we get to this stage? Well, we train us with thousands and thousands, possibly even millions of images. And we choose the weights of all of those filters, such that pictures of bicycles get the highest score here, pictures of dogs get the highest score here and so on and so forth. How does that work? It doesn't matter. It's really just an optimization problem. It's just log likelihood if you like it. It's not log likelihood, but it's very, very similar. There are great tools out there that do this optimization process. You don't care anything about back propagation or really or anything like that. Just know that these weights are optimized such that this condition holds true. So that's the way basic computer vision works. What if we want to classify pictures that are not one of those classifications that the train neural network is designed to recognize? In our case, what if we want to recognize something other than dogs, apples, tennis balls, or bicycles? What if we want to identify hot dogs instead? Now, of course, we could go back to the drawing board, get millions of pictures of hot dogs, train one of those neural networks in the same way. That takes a lot of time, takes a lot of money, takes a lot of computation. But there is a shortcut that lets us do just as well with much less data and much less time and that's called transfer learning. We could take that same network that we just trained already to detect dogs and bicycles and so forth, strip out the last layer that does the classification. And instead take a new image, run it through that pre-train neural network, and then just grab the numbers that exist in that second to last layer. We don't know what those numbers really represent, but that we call them features. We get eight numbers for every image, F1, F2, F3, F4, F5, F6, F7, and F8. And then what we can do is take some pictures that represent the classification that we want to detect with transfer learning. Let's take hot dogs. Let's start with something that's not a hot dog. It's a picture of a burrito. We can run it through that network. That's going to give us eight numbers and a binary classification, zero or one. Let's do it again for a picture of a hot dog this time. Run it through that same network. We get eight numbers and a binary classification. And we can do that again and again and again for a couple of hundred pictures. So we get a couple of hundred sets of eight numbers, a couple of hundred binary classifications. What does that sound like to you? That's just logistic regression, right? We could build a pretty simple model which given this data predicts its classification and it turns out it works pretty well. We don't really know why except in the sense that this whole process of applying filters, when you get down to this last layer, you can imagine each of these data points here as representing something intrinsic about the image generally. We don't really know what, as I said, but this one might represent the greenness of an image. This one might represent the number of circles in the image. This one might represent, if it was working with faces, the number of eyes that it detects, those kinds of things. It's pretty difficult to figure out what they represent, but when you apply new images to them and do this reclassification process, it turns out it works pretty well. Especially when you use a network that's trained on images that are somewhat like what you're working with. So in my case, I'm actually going to work with a network that was trained on food items and it turns out well that it detects features of food pictures really well, that makes this classification work really well. All right. And you don't even have to code this up yourself. There's a nice service that is available in Microsoft Azure called Microsoft Cognitive Services Custom Vision. Let's let you just take a bunch of images that represent the classes that you want to detect. So in my case, I'm going to give it a bunch of images of hot dogs and a bunch of images of things that look kind of like hot dogs that are not, so it can discriminate a bit better. I can just upload them to this service and it will build one of these transfer learning models for me. It has a nice sort of web UI. Here's an example for this particular project that I'm working on here where I've got 61 pictures of hot dogs, 78 pictures of things that look like hot dogs, but are not. And you can do all this kind of stuff through the GUI, but that didn't work for me in this particular case because although you can just upload a bunch of pictures from a folder into that service, I don't have pictures in this case. Because I went to ImageNet to get pictures of hot dogs. If you're looking for a source of general images by classification, imageNet.org is a really useful service. It's a website that's been going for about 10 years to drive competitions in image analysis, which is actually what generated these pre-trained models that we're using today. You can regard a search for hot dog, but what you get out of that if you download things is just a list of URLs that represent the hot dogs on the web. And so it would have been a real pain for me to go to each of these and download them and then upload them into the service. So instead I used R and interfaced with the API for custom vision to do this. So to get my images to work with, I searched for hotdogs and image.net and also things that were related like Frankfurterbuns. That gave me about 2300 URLs to work with. Similarly for my contrast set, things that look like hot dogs, but are not actually hot dogs. I looked for tacos and hamburgers and got about 2600 images for that, but I don't need that many. It turns out you only need a couple of hundred images for transfer learning to work well in a pre-trained network that's related to your field. So I just sampled 100 from each of those lists using R. All of the code for this, by the way, I'll give you in a GitHub repository at the end of the talk. And then I just wrote a function in R to upload them to the custom vision service. The only thing you need to make this work is a key to access the service. You can get that for free from Azure, details in the blog post. But here's the function I use, upload URLs. I give it my vector of URLs and the tag I'm using, and it uploads them into the system, and you get what you saw in the previous slide. There is a bit of a problem with this, however, working with data with ImageNet at least. And that is when you search for hotdogs, you get things like this. As I said, that site's been going for about 10 years. Many of the images are on Flickr. And when images are deleted from Flickr, it's not kind enough to give you a 404. It just gives you a black screen like this. No, it gives you this little icon, which is an image, but it's not an image of a hotdog. So we need to figure out how to do with that. I've lost my monitor, there's my monitor as well. Okay. Here is another image of a hotdog that you get by searching for hotdogs. In ImageNet, it's a woman who is eating something that is not a hotdog, but it's apparently an ice cream sandwich. So we wouldn't want to have that in our image sets, the training. And here's another example. It's adorable. It's gone. It was adorable. But it's not a hotdog. So there is a decent amount of dandy cleaning that you would need to do to make this work. And of course, you could have a look at those URLs individually and just remove them from your vector and are before you upload. But actually, this is one of the things that the web interface is really convenient for. You can actually browse through the images, select ones that you want to retag or delete. And in this case, it's my little dog and we'll get rid of the dog by pressing delete. So that was how I could have filtered out or cleaned up my data before my transfer learning training. And after that process is complete, you can see we've got 61 images of hotdogs. There are all sorts of sort of categories, if you guess. Sometimes there's multiple hotdogs. Sometimes there's other things in the image, like a person or other bits of food. Sometimes it's a bit blurry. But this is the kind of data that we're using to do training, as we'll see it works pretty good. And then for things that might be mistaken for hotdogs, tacos, hamburgers and so forth, I've got those in there under another tag to do the training. So now when we want to do the training, we've got our images uploaded into Custom Version. We tag them as hotdog and not hotdog. When you do the training, you have to choose something called a domain, which is the pre-trained network that's used to do the transfer learning when they strip off the last set of categories. There are domains that have been trained rather with just all sorts of images. And that's generic. There's one that's been trained with landmarks. There's one that's been trained with photos that you might see in our online shopping catalogue for retail applications. There's one that's been trained with adult images, which is mainly used for content filtering when people upload bad stuff. The one I use is the one that's been trained on food items, which is why this works so well with so few images. You can just click a button in the website rather to launch the training process. But I wanted to kick it off from R, so I could automate this. And this is just an example of how we're interfacing with the API directly. There is a very nice API that's all documented online. And with the HTTP library, which is lovely, I can just interface directly with the endpoint that kicks off training, and then just check it for a few times until it's completed, because the training takes about 30 seconds and happens asynchronously. But once that is done, I have a trained model that I can pump an image into. Before that, it's worthwhile having a look at how well your model is doing. And you can have a look on the website to do this. It shows you these machine learning terms, precision and recall, which I can never remember what they mean. I just remember that this one is the false positives. Of all the images in our database, how many were incorrectly classified, and this is the impact of false negatives. Of the things that weren't the thing, when did it classify as the other thing? And of course, this is all dependent on our probability threshold. Remember that number that came up at the very end that was between zero and one? We can choose how high that number has to be before we classify it as either that thing or that other thing. And so how you choose that number depends on how the false positives, how many false positives and how many false negatives you get. Think about that carefully. It doesn't matter too much when I'm trying to classify hotdogs, but if you're trying to classify, say, cancer from an image x-ray, the impact of a false positive is a lot worse than the impact of a false negative. So how you choose that number is really, really important. Depending on the application. All right, next slide. Once we have trained the model, I've written a function in R called hotdog predict. I can just pass it in a URL and that threshold that determines the classification and the same kind of thing again. Just get to the endpoint, pass in the image URL and get the response as JSON from the API. And then using the JSON package, I can extract out the predictions for that particular classification. So I could do that from the R command line really easily. So here is the URL of some hotdogs on the grill. And it comes back to the hotdog classification is 99%. The not hotdog classification is less than 1%. So just about any threshold, this would classify it as a hotdog and not as a not hotdog. Another example, here is the burritos. Here is my picture, URL of pictures of burrito with rice from Wikimedia. Also a great source by the way of images searching in Wikimedia. And this one is classifying it as a hotdog at 10% and a not hotdog at about 80%. So with the threshold somewhere in the 50, 70% range, it would make that classification correctly. But it's not so much fun to do it just from the R command line. Let's actually build an app to do this. There are several ways you can encompass these predictions into an application. You can do it offline. So if you wanted to run it sort of on a server that wasn't connected to the internet or probably more realistically on a phone application, you can export it as core ML. So to run an iPhone, you can export it as TensorFlow. So to run an Android, you can also export it as OnX, which will run in lots of different platforms. This is a general open source framework for defining trained predictive models with deep learning. Or you can export it to a container which will run on anything like a Raspberry Pi, for example. When I think about that, it's fast. You don't need any internet connection to make it work. But you are limited in the size of the underlying neural network that you use just for performance reason on these small devices. And so there's something called the compact model in custom vision, which is a very small general model but works pretty well for these offline applications. Or you can do it online. If you connect it to the internet, you can connect through the REST API and you're sure me doing that from R a minute ago. And I'm gonna use those same functions to connect to a shiny app just through the endpoints that the service provides me. The code for the shining app is available on GitHub, but let me just show you how it works. First of all, let's start with an image that is not one of those classifications. This is my dog, EZ. Here's the URL, the picture of him from the blog. And you can see that even at the 50% threshold, he's very confidently classified as not hot dog. He is a hot dog, but he's not a hot dog. All right. There's, here's another example. This is a hot dog. I come from Chicago. This would never be classified as a hot dog in Chicago. Nonetheless, my app doesn't be classified as a hot dog. So it's interpreting this particular thing as a hot dog. Another example, these are some spring rolls. And you can see my app is classified and that is a hot dog. This is a good example of a misclassification here. It is dependent on the threshold. I've got a slider over here which you can manipulate in the shiny app. Have it bump that threshold up to... Up to... Up to... Next slide, please. There we go. You move the slider up and down and you can see that the threshold for that particular picture was 52%. But this is a good example. Like if you're finding things that misclassify that shouldn't be, this is a good prompt to you to put some more images into the training set and retrain so I can better discriminate these types of things. I would add some pictures of spring rolls to my training set and hopefully this kind of thing would be fixed. All right. It was really nice building this next slide, please. It was really nice building this as a shiny app. Really easy to do as well. And one of the really nice features of shiny is setting reactive inputs. And if you haven't played around with it, sort of the long story short is that when I call out to the API, it's cashes the values that it has previously requested. So that as I move that slider back and forth, it's not going back to the API any time. It just uses the categorization from the API and changes the output in the shiny app. That's really nice because it saves me money because it doesn't have to make a query back to the API every time. Although actually in this case, it doesn't matter because I'm using the free tier of custom vision, which is completely free up to two classifications per second, which is enough for an API, for a shiny app. But if you have a sort of production application, you can do it for 10 cents, or one cent for every 10 classifications. All right, so that's it. If you want to find, oops, next slide, please. Just to summarize, it's really easy to do these transfer, is transfer learning classifications using the custom vision.ai app. I was really surprised at how few images you need. Like I said, I just needed less than 200 so to build that app. It's really fast. It just takes seconds actually, rather than days you would require to build up a complete customer, computer vision network from scratch. And you can do it either offline or as a REST API, and you can drive it from R with the HTTP package. And so on the next slide, you'll be able to find the GitHub repository where you can get all the code for the classification and the app. If you want to try it, go to customvision.ai. We do need an Azure account, even though it's free, you can get a free account at this link here, which will also give you 200 credits if you're $200 in credit, if you're a new user to do anything you want on Azure. Thank you. Thank you. Thank you. Thank you."}, {"Year": 2018, "Speaker": "Max Kuhn", "Title": "Equivocals in Predictive Modeling", "Abstract": "When important quantities are being predicted by a model, it makes sense to avoid making predictions when there is significant uncertainty. In laboratory diagnostic tests, it is common (and often mandated) to use an equivocal zones for this purpose. We'll show examples from drug discovery and two different methods for labeling predictions as equivocal.", "VideoURL": "https://www.youtube.com/watch?v=6ZYVYBDEIpA", "id0": "2018_18", "transcript": "So I'm going to talk about something called Equipicles when we do predictive modeling. I thought I'd finish these slides and then I saw all the aboutmes. So statistician, I've worked in like the diagnostic industry which I'll talk briefly about pharmaceuticals. I work at RStudio now doing mostly modeling packages sort of in the order of the ones that I've released. I mean they're all here in a couple books. Not a hot dog. And yeah, that's my dog who you can see the treats lying next to her coming back from her walk. But she's I think I was making like a hamburger. And that's the look I got. So anyway. Alright, so I want to talk about some particular statistical methods you use in the diagnostic industry. And then how they might be applied in certain circumstances, not universally but in certain circumstances when you're doing just general modeling. So, you know, I had this job and one of the jobs that I did was the algorithms for instruments. Which means like you're like a really bad cough and you go into the doctor and you know you speed them sample or they like tickle your throat. Sorry Mar. You know you give them a sample and then they put it in some diagnostic instrument and it gets you a signal back. And the signal is you know high probably if you have some like let's say tuberculosis right. In low if you don't. And so you usually get data looks like this where the samples that are negatives have you know a fairly low signal distribution but a long tail. And then you have some positives here that on average are higher. And then like one of the final jobs is that essentially when doing that case would be what's my actual cutoff. Like above what number here do I say somebody's an actual positive. And so typically you would use what's called an RC curve or receiver operating characteristic curve analysis. And you know that what's what that lets you do is balance like how many false negatives versus how many false positives and that kind of thing. And you might come up with a cut off here. So the samples to the right or the negatives would be false positives and your false negatives are down here. But what FDA would require is to put something in their code and equivocal zone. Which means like you know if this cutoff is let's say 2.7 if you had a value of 2.7. They're like yeah maybe not a positive even though it's past your cutoff. And so we had to do all the sensitivity analysis and things like that based on other data sets and come up with what they call an equivocal zone. And the equivocal zone in this particular instance would just be like a buffer around your cutoff that you determined with data that would optimize really the performance of the assay. Or the lab test. And we tried, I mean you could make this performance really good by making it really wide. So we had to balance off samples we exclude. So we had what we call a reportable rate. Like you know anything that's inside this equivocal zone you would not actually report the value to the patient. You would say hey run the assay again or collect a new sample. You know it's so close we don't think we can make a call between positive and negative. So we had to balance what we call the reportable rate which is the number of patient samples that you actually give to people versus performance. And that was just something we had to do. And it seems like kind of like a dodgy thing if you're a statistician because you're saying hey take the ones like you're most likely to miss and just get rid of those in your analysis. But at the very end I have a link to an FDA guidance for that. So it's like a real thing that they would sort of make us do whether we wanted to do or not. But again these are the ones that you're most likely to miss and you actually when you do the analysis you would get rid of any samples that are within the equivocal zone. So the idea is you know when do we especially when we're predicting something very important there are situations where we might not want to make a prediction. Right I mean we can take whatever data we have and funnel it through our model and get a number but then we have to question like wait should I give that number to somebody is there any criteria in which I shouldn't do that. And that's kind of what the previous equivocal zone is doing is we think that there's these samples are problematic not because of what their true answer is but because of the value we're getting before we see the true answer. Okay so like an unsupervised rule that would make you exclude data you think are not good enough for predictions. And so you know and this comes into play in these things called applicability demand. So I used to work a lot in computational chemistry and this was really big there like if you build in pharma a lot of times in direct discovery we would build these models to make predictions on new potential drugs. Right what we call compounds. And these compounds a lot of times would be something that a chemist would design on their computer and would not actually exist you know IRL and then we would take those formulas that they come up with and make predictions on whether it's toxic or whether it's permeable and all these things that chemists want to know. Right so if you have a model you build for let's say toxicity and it involves like a thousand data points a thousand compounds and then you have a new compound that looks nothing like the ones that you've built your model on you might flag that is not being applicable. Like it's so different from the data that was used to build your model you might not want to make a prediction on that no matter what the actual value of the prediction is. Okay and so these are situations again if you're predicting something like patient response like you want to make sure you have a really definitive understanding of what the result should be from the model maybe before you get into the model. Maybe before you give that result to like a patient because the consequences are potentially dire. So what I'm going to do is I'm going to show an example of drug discovery and all you need to know about it is there's a thing called mutagenicity which is when you take a drug and it actually or you ingest anything really and it damages your DNA which I don't think I need to explain is like not a good thing right. You won't grow like a second hat or anything like that. Almost never. But mutagenicity is something we want to avoid. There's this thing called a lab test called the Ames test and it gives you back like a yes or no answer and so we have like a standard data set that we use that has about about 4,300 data points and then there's a lot of predictors there's actually about 1500 predictors but they're mostly extremely correlated and we have some ways of willing that down to about you know 80 some predictors. So what we want to do is we want to build a model to make predictions on compounds as to whether they're toxic or not this particular kind of toxicity and then how could we use an equivocal zone to maybe filter out some predictions that we're not really quite sure about that we should give to the chemist. And in this particular case like you know it's hard to give bad news to people. I mean if you say hey you know I think there's like a 90% chance that your compounds going to like hurt somebody you know they're very conscientious but they're like prove it. So this is a situation where like if we call it something equivocal then what we would say is hey well let's look at this particular structure and see if it's really related to existing structures or compounds that we know are dangerous and things like that. So what we would do in a equivocal situation here from a model would be tell the chemists to go back and just gather more data or run it in a more definitive lab screen or something more to get better data instead of accepting what this particular model would say. So what I'm going to do is I'm going to fit a logistic regression which you probably are all familiar with but I'm going to do a Bayesian logistic regression and if you're not really familiar with Bayes methods basically what we do is we take the beta parameters in our logistic regression and before we do any data analysis from modeling we put what we call prior distributions which is a statistical distribution saying what we think you know absent of any data what we think those parameters should be. And what's pretty typical in a model like this is if you think about like the predictors being centered and scaled we put a really really wide normal distribution on these parameters like a standard deviation of 10 and a mean of zero. Right super super wide prior distribution and what Bayes rule does is it mixes the prior and the data that you have in your data set come up with something called the posterior. And the posterior mostly looks like the prior a lot of times when your data is very small or weak but if you have a lot of data and very let's say precise data then the output of your Bayesian analysis this posterior distribution tends to look a lot more like the empirical data that you have. So Bayes rule is really good at coming up with a mixture of your prior distribution in your observed data to create this thing called a posterior. Right so what we do is when we get we fit our model in this particular case and we get predictions like we would for many other logistic regression like here's the probability that this particular compound is toxic. I keep on and say not a hot dog. Thanks David. So we're going to get it like a class probability out of this and just remind you if you remember back from like your old school statistic days when you had to actually look at binomial distributions that the rates parameter of a binomial distribution has a variance that's if the rate was P the variance is P times one minus P. Right so just keep that in mind that relationship there because we'll come back to it. So one cool thing about Bayesian analysis is even though my prediction is an actual number like a probability of it being toxic. We actually what we do is that posterior distribution is actually a distribution around the probability. So we have a probability distribution on a probability estimate basically. And so what that allows us to do is to get a measure of uncertainty. So if I tell you a compound had a probability of being toxic is like 93.4 plus or minus 0.001 that's a very precise prediction but if it's much wider than that we can get that estimate in a Bayesian analysis which we can't really get an ordinarily dispersion easily at least. Okay so what I did is I used a tenfold cross-voutation and the plots I'll show you the predictions in a minute are the hold out samples from tenfold cross-voutation. Right so I'm not involving a test center anything right now. And one last little note about this is there are a lot of other models besides like some fancy Bayesian model that you can get these types of interval estimates for predictions. So like there's a recent paper for let's say random forest where it shows how to take a random forest prediction and then get a what we call a it's kind of like a confidence interval for the predicted value. So it's not the only way you could have known this. So this plot on the x-axis is the probability of being toxic and the y-axis is our uncertainty around that prediction which I've just called a standard error here. So all I did is each one of these data points has a posterior distribution around its prediction. The points are on the x-axis are the mean of that distribution and the points on the y-axis are the standard deviation of that posterior. So it's sort of like our standard deviation of our predicted value. Okay and in the dotted line going from left to right that's basically a multiple of p times one minus p. And so that gives us a sense of like our average level of uncertainty is for each of these compounds. And you can see some of them are below the line which means that they're more precise than what we'd expect on average. But then we have some that are actually pretty flagrantly high, you know like here and up here. And so one thing about this is hopefully the color comes across well and you can see thankfully more of our red is to the right of 0.5. That means the models you know working because they're being predicted as a toxic compound. But you see some high flyers here and there. And so we see even when we come down to those things that we're very confidently predicting not to be toxic. You see a couple of toxic ones here that have a much higher variation like up here than others. So even when you get a very confident prediction it can have a large amount of uncertainty around it. Okay. So let's take an example. Let's look at something that was predicted to be really, really very strong probability of metoxic. We have one that's way up here in terms of its variation. And then I'm going to take one that's sort of in the mass here that has very low variation. Let's look at like, well wait, how does that happen? So let's look at the posterior distribution of those two samples. And so they're both predicted probability of about 0.94, 0.3, 0.4. They come from different folds and cross-vautation although that's not the particular reason that they're different. But if you look at the posteriors over here, what you see is you have this one number 25.09 that has a pretty tight distribution around 94.3. And then you see this other one was at 128 that is really, really confident that it's toxic, except when it's not really, and it feels really moderately confident that it's not toxic. So you have this like this really polarized bathtub-shaped distribution of your posterior. And it's like a very bipolar prediction. And so the uncertainty, this is why the uncertainty estimate is so high because it really does think it's toxic except when it doesn't. And this to me is like a problematic prediction. The variation around this prediction is so high that maybe we don't report this one. This would be a reason that we would go back to our chemists and say, yeah, I know what the probability says, but I just don't feel right about this prediction. Let's not use this value. Or in fact, I think we probably not even give them the probability number. We would just say, yeah, this probability prediction wasn't very good. So how can we label these as being equivocal? Okay, so we need to set up some rules to take the output of our classification model and decide which ones we're going to actually give the consumers of this model, which ones we're not going to. Now, one reason this could happen is extrapolation. So if this compound represented by the blue histograms here was something that was really, really different from the day that we used in the model, like if you think about like your old linear regression class where we showed like confidence intervals across a prediction, remember how they like sort of flare out at the ends? Right, that's uncertainty and that's uncertainty due to extrapolation. And my feeling is for this particular example, this compound is very much an extrapolation compared to what was used to build a model and that's why it's supposed to be your distribution. So, janky is the technical term. It's not. Don't use that when you're talking to your boss. Yeah, it's really janky. Yeah, anyway. So what we're going to do is we're going to come up with basically two rules. The first rule is going to be sort of like I showed at the very beginning where if we have things that are around 0.5, but it's maybe not report them because they're sort of equivocal. We can't really decide if they are close enough to the line to call it a toxic compound or not. And the other thing we're going to do is we're going to try to find a rule that will flag the compounds that have really high variation, right? But high variation means something different on this end of the scale as it is this end of the scale. So we have to have something that says, you know, for a probability around 0.3, the variation can't be above a certain value. And that value would be different than if the probability was like 0.001. Okay. So the way I've chosen to solve this here is to come up with basically, you know, a plus or minus around 0.5. And then basically we estimate the standard error, basically using this line here, and come up with a multiplier there. So if you're, let's say, and I just eyeball this, to be honest with you, if you're like about 75% more variable than the average compound, we think that's probably two variable for me to report the actual value. So if you take this plot and apply that rule, we come up with what I, on Twitter referred to as the, um, the buck tooth mustache plot. And that looks like this. All right. So like, so, okay. So let me explain this. The red line is kind of the rule. So if we have things around plus or minus 0.5, we don't want to report those. So they're above the line. So these would be called equivocal for that, for that level of buffer, right? And then things, you know, down here, these would be reported equivocal, even though they have very strong probability values, then certainty is far enough, then far enough larger than other samples around its value for me to feel safe in making predictions. So then the question is like, well, wait, Max, how did you come up with these numbers? And so these would be basically like tuning parameters for the model, where you want to optimize your, uh, equivocal zones such that it gives you a good, uh, better performance, but not so much that you're excluding almost all predictions. So you probably want to, before you do this, come up with a number that says, you know, we never want to exclude more than 5% of the values that come out of this model. Right. So, so what I'm going to do is I'm going to show you how that works. I'm going to leave the, the, the mustache part of this plot alone for a little bit and then just look at the, I don't believe I'm saying this, the buck tooth part. Okay. I'm the same guy who made juice in our, uh, in our functions. So I think that's what it's worth. But, uh, I'm going to see, you know, as we vary the size of the buck tooth here, um, how that affects performance. So if I, if I start with zero, that means like I'm not excluding anything because there's no buffer on point five, and then the numbers on the next plot basically show on the x-axis how wide that, that part of the equivocal zone is getting and how performance changes. And I use the air into the RC curve to, um, to do this. So if you start off with a, um, with basically no buffer, but with the, um, the, the mustache part of the equivocal zone, um, we start off with, um, 100. So on this panel is the number that you report out. So we report out almost 100% of our samples with no additional, um, equivocal zone. And our performance goes immediately from, um, point eight five to around, uh, I'm sorry, point eight three five to around eight four, two, eight four three. So we get, you know, an immediate jump in our performance without losing many samples to being equivocal. And that is simply because that's the ones above the, the mustache part of the plot. Okay. And so as we increase the width of that buck tooth part, you can see the reportable rate is going down. So around here, we're only reporting about 95% of results. And then you can see it like linearly decreases, which makes sense. And then you can see performance sort of at a different rate increases in terms of its air into the RC curve. Right? And of course, you can keep winding this or not making it wide enough. And you would probably choose how wide you want to make it probably more based on the reportable rate, um, by how many you don't want to make predictions for than probably the air into the RC curve. Because every time I've done this, I've almost always had a linear increase in performance. So it's not like it really tops out at somewhere. Um, so anyway, this is what I typically see. And again, this is a tuning parameter. So if you wanted to do this, you'd want to have some optimality criteria by saying like, give me the best air into the RC curve under the condition that I always report at least 90% of my data or something like that. Okay. And just so you know, like when I did this in diagnostics, we would actually have a separate R&D trial that collected like a thousand data points only to do this part of the analysis. Okay. So if you are going to use data to make a decision about what this should be in, either have a separate data set to do it or include it inside of your resampling procedure. All right. So, so how can you do this? Um, have a package that I sent to crayon. It's not yet approved. It's been about a week called parsnip. And that helps with the modeling part here. And then we have a package. I hope to put on crayon before the end of the year called probably, um, which I think the sticker is going to be me. Um, and probably has the bits of the equivocal zone. And so what parsnip would do is it would enable you to fit this like beige and logistic regression or. Curis or, you know, glimnet or whatever kind of logistic regression you want to use. Um, uh, with a minimal syntax. It's kind of like the unified interface part of care, but more tidy. And so if you wanted to do that, you would load the parsnip package. You tell it you want to do logistic regression. And then the engine is like the package or the, the, the computational engine you want to use. So if you wanted to use stand, you set the computational engine to stand. And then if there's any stand specific criteria or arguments you want to use, you can add them here. So, so those posterior distributions were based off of, say, did four chains each at 2000, but you cut out half of them. So it's about 4000 data points around those posterior distributions. And those come from me saying the number of chains here. Right. And then fit just fits that model. Right. So the syntax here is very concise and it's almost identical if you were using glimnet or Keras or GLM or whatever code you want to use to fit the logistic regression. And then the little bit to make the equical zones here is what we did is we, um, the predict method will give you your values. And then, um, what we have is we have a, um, a data structure call, uh, I think it's called a class under our pred object, which is basically like a factor. Um, but it, what it allows you to do is inside the factor we've marked which elements of that, uh, vector should be called a quivicle. So it doesn't add like another layer to the factor. It just, it's an additional attribute that we keep along. And when you convert that class pred object to a factor, an actual factor, like when you want to calculate accuracy or whatever, it actually takes all the equivocal values and marks them as NA. Right. So you don't actually, even though they get stored in the data, when you convert it back to a factor to do your analysis, it marks the equivocal as this NA. Okay. And if you had NA going in, that's just one more NA that was maybe not equivocal that gets added to the list of NA's when you convert to factor. Right. So, so it handles the equivocal's in a sensible way and very consistent with what you would do and are. And it also means like if you want to do your analysis on any of these data, you don't have to have like a special like accuracy or precision recall function to handle these, uh, equivocal samples. It just all sort of happens for you. And this syntax you might expect is very deep player, uh, consistent. So, um, so yeah, this is all stuff we're building and hopefully it'll be done by, or at least on cram by the end of the year, um, uh, depending on some other packages that are being released. Um, so in summary, you know, there are times in a model where you don't want to make predictions based not on what the predicted value is itself maybe, but the quality of the prediction. So maybe it's again too close to the cutoff, or maybe there's something else about the, um, uh, prediction, like it's amount of extrapolation or something like that that would make you say, yeah, let's not give somebody a answer. Yeah, let's not give somebody a number in those instances. And you need to balance, you know, how you make something equivocal, you need to balance the performance and what we call the reportable rate, just to make sure you're not excluding all but the most obvious samples from your model. Um, and then we have packages that are in process for this. Um, here's a link to the FDA guidance. One of the many FDA guidance that talks about the Quokal zones. And here's a link for the paper and they have their data in a supplemental file, although we have an R package with it. And that is it."}, {"Year": 2018, "Speaker": "Kelly O'Briant", "Title": "Tinkering with Serverless Computing and R in the Cloud", "Abstract": "Serverless compute is lauded as a solution to freeing developers from the arduous task of managing (cloud) infrastructure. I would argue that it can also be a good entry point into learning or getting more comfortable with cloud infrastructure services and offerings. In this talk I'll talk about deploying R projects with App Engine - the Google Cloud Platform offering for building fully managed serverless and scalable web apps and microservices. In the process, I hope to provide resources and a getting-started path for people interested in learning new skills related to analytic administration in R.\\n\\nKey Topics:\\n- What is an analytic admin, what do they do for R and data science teams?\\n- What is serverless compute, why is it fun to tinker with?\\n- Steps to creating and managing a microservice on Google App Engine with R code.", "VideoURL": "https://www.youtube.com/watch?v=gPZ4wHXOpUY", "id0": "2018_19", "transcript": "Okay, so today I'm here to talk about how I like to tinker in the cloud with R and how I like to, through my job at RStudio, support other people doing the same thing and becoming more familiar with this concept that we call analytic administration. So I was listening to a podcast and very bad, I don't remember where this came from or what podcast or who said it, but I like the idea of lowering the cost of turning our ideas that we as data scientists produce into reality. So I really liked when I heard this on a podcast that if you have something of value and a PayPal account, you can start an online business in minutes. At RStudio, I, you are goal in developing these tools, sharing applications, our markdown documents as doing kind of the same thing, but for data science. So yesterday we heard a lot of great talks that talked about Shiny and Emily talked about developing these tools that help her do her job better, help other people do their jobs better. And that's a mission that I really like drew me to to apply to work for RStudio and the reason that I feel so passionate about working in this space together. So, so if you have a data science, just a data analysis of value and you have the proper data product tool chain, you should theoretically be able to share that analysis with everyone you know in minutes. But as you might have noticed, or maybe you didn't notice yesterday in a lot of the talks, they would get to the point where they're like, we built this cool thing. And then what comes next? Like where are they in their organizations where they have the ability to then distribute that data product out to everyone in their organization and really like spread the things that they're building out to everyone. A lot of the times I think in smaller organizations or ones that have not embraced R as an analytic standard, we get to this point where like, sure, on my local machine, I can build something really cool. But then I'm kind of trapped like, how do I get this out to other people? Like, I don't want to ever be in the position where I have this cool burning idea and I don't have the tools or the means to distribute it, execute it in the manner that I dreamed of. But the internet is a beautiful, wonderful place and there are lots of tools for getting this stuff out there. So as you progress and you're very creative and using the internet, you can do a number of things basically for free to get your ideas out to the world. You could post a GIF or a screenshot to Twitter. You could write a blog post about it. You could create a GitHub repository and share your code through that. You can also use these publishing platforms that are hosted like rpubs or shinyapps.io which has a free tier. But at a certain point, like, especially if you're in a situation where you're trying to do this at work and you don't necessarily want to put these things out on the public internet, you start to run into some blockers. So the next two steps can be a little bit more challenging. They involve like using servers, dealing with IT, dragons, fire, Linux administration, and running that space. So at our studio, we are fully committed to this idea that all organizations should have someone inside of them who is a data scientist who's decided to get like very uppity and start doing what they need to do to make our legitimate analytic standard within their business. And that involves like learning a little bit of Linux administration, onboarding new tools, deploying these solutions, supporting the data science team and the existing standards that your IT department has placed on you. We'd like them to work closely with IT if that's at all possible and help them understand why R is so cool and so important and why you love it so much. So how do you become this person if you're interested in taking on this role? It's a little bit intimidating, but there are two main things that I'd like to share today. One is that you need to get yourself a sandbox, a place to play around in. And this is like what I call my Tinker space. And whether this is something that you do at home on the weekends in your spare time, which is not a healthy work life balance that I like to promote as an our ladies organizer, or if your organization is very cool and it supports you in this mission, maybe you can get a server sandbox from them. Either way, like taking the initiative to ask for this thing so that you can learn the ropes of Linux and DevOps, practice with open source tools, and then build out these proof of concepts to develop a sense for yourself of what the best practices in this area are. Those are all important things that will then lead you to being able to support your IT department in creating a secure analytic infrastructure for your entire team. So at our studio, I have kind of seen my job as twofold. I really like to promote best practices and doing smart things with the tools that we have, our professional products. But I also really like to say, you know, like if you're motivated by practical need and burden curiosity, tinkering is great with one caveat. And that's what I alluded to in the previous slide where we talk about developing a sense for best practices. So when you have a sandbox, you can do a lot of cool stuff with it. And I've learned over time that there are things that you can do and there are things that you should do. And those aren't the same all the time. So today I want to run through like a tinkering project that I did last summer for usar. And I want to talk about a couple of cool cloud technologies involving serverless and R. But just bear in mind that I am not telling you that this is the best practice. So I want to talk about plumber APIs, which is a really cool package, package plumber developed by Jeff Allen who works on the R studio connect team. And he gave a webinar about plumber back in July. It's now available on demand on our website. So do check it out if you're interested. Mara just the other day yesterday or something tweeted this. She doesn't even remember. And it just shows that people are getting excited about using plumber APIs. And this person in particular wrote a really, according to Mara fun and well explained guide to using Docker to deploy plumber APIs. And this code on the side kind of shows you the concept here. You're basically getting for very little, just adding declarations to your code, turning functions that you've written in R into rest APIs. The catch is you need somewhere to serve these APIs in order for anyone outside of you working with curl in your local machine to use them at all. So I've seen a lot of good examples like the one I just threw up that mark tweeted about people with used Docker to serve these plumber APIs. But I thought it would be cool to take it another step further and use Docker in a serverless cloud computing environment. Now, if you haven't heard of serverless before, it's a little bit of a confusing term. It's not magic. We're not like doing things without servers. It's basically a term that means you're going to hand off the responsibility of managing this infrastructure to someone else, be it Amazon or Azure or Google Cloud. And the particular serverless application platform that I'm interested in using for this project is Google App Engine. Now, Google App Engine came out in 2008 and I was in college at the time and I was like, this looks awesome. What does it do? I don't know. And I never figured it out. Ten years later, I read a press release from them that they now had custom run times. So I thought thinking to myself, hey, I wonder if I can put R in there. Again, sandbox, tinkering, not best practice, necessarily. I think one of the basic ideas is that you bring your code R otherwise and you have Google Cloud platform, manage all the infrastructure for you. And then, theoretically, you just pay for what you use. As I said, R is not included as a runtime by default. They have this popular crowd of run times that they support. R is not one of them, but with the new custom run times, you can bring whatever you want inside of a Docker container. So that's what I wanted to set up. I have two little app engines here. And one has plumber inside of Docker and it's serving this REST API. And then because I wanted to document it and give people a place to go to learn about my REST endpoints, I also served up a static HTML generated from our markdown that was then being served by Nginx. Oh, I want to pause here to say I do talk to a lot of people in my position who get very excited about Docker. And I think it's an awesome, very cool technology. My opinion is that it is awesome and cool when used in the right situations. And so you don't always need to Docker all the things, but in this particular case, we do because that's what it requires. So the first part of this project is that you create a Google Cloud project. So you go to the GCP console, you sign in, you create an account, and then you create a project. You should also at this point download install the Google Cloud SDK so that you can do things from your console. You don't have to be doing it from their console. There are basically three parts to a little Google App Engine. You need your Docker file. You need your app.yaml file. And this is literally the entire contents of the app.yaml file. Runtime custom environment flex. That's all you need. And then you need your assets. This would be like the plumber.r script or the HTML generated from our markdown. Those things. And this is the Docker prep that I did for the plumber app. This is just reading in Jeff Allen's plumber, the Docker image that he has hosted for free on Docker Hub. And then what I've done is install an extra package in there, GDU plot two, because my REST API produces beautiful plots of Iris data so that people in Python can use them. And then I copy over the plumber R file into the Docker container. And then the final process here is just exposing port 8080, because that's what Google App Engine wants to listen on. The Docker prep for Engine X is really super simple as well. I just read in the official repository for Engine X, Docker image. And then essentially the only thing I do after that is add the static content. So then we're basically at the final step here. You've prepped your Docker files, created this app.yaml file, gathered all of your R assets together. Final two stages are running these two commands, GCloud app create, and GCloud app deploy. It's at this stage that I usually forget that I haven't attached my billing account. And like David Smith was just saying, a lot of these Cloud providers provide free credit to use and to tinker with. So Azure provides $200, GCP provides $300, and you do have to usually give them your own credit card information when you attach these accounts. And that can be very scary. But you do get through GCP a lot of good visibility into what's burning down this free credit and how fast it's disappearing. So you can say, oh, stop, burn it all down so that you can stop getting charged if you need to be. I also want to mention that Google App Engines are not an instantaneous deployment process by any means. Leave yourself at least 15 minutes at least to get your app up and running on Google Cloud. If you need something with fast push button deployment, I work for our studio. We have products. Just kidding. Sorry, dude. So once you have this app running and serving arrest API, there are some other fun things that you can do to prove to yourself that this is running and functional and doing what you expected to in a serverless environment. So what I recommend is really fun. So you can create these siege servers using the siege utility. And you can say, like, I want this number of users. I want to hit this API for this number of seconds and you can add delay. But literally every time I run one of these, I keep. I can't help in my mind's like yelling, it's just a lot of fun. You can see it running in real time and they're like waiting to see. And then you can see how many failures or successes there were hitting your API. Like I said, I did this original project for usar over the summer. So I have been tinkering with extensions to it. Once you get them into exploring these managed services that these cloud providers have available, you'll learn that there are like sloughs of them. And they all do very interesting, different. And some of them are in beta and some of them die off. But this one in particular has caught my eye recently, this Google Cloud Composer, because it is a managed Apache Airflow service. You haven't heard of Apache Airflow. It's basically like if you were to set up CRON, but get all of the information you actually wanted to about job scheduling. So here's what I'd like to build next. In my next sandbox, I'd like to explore this Google Cloud Composer using Apache Airflow so that I can kind of track a workflow in our running off of REST API endpoints. So this would kind of be like a multi-step process for training a model or running a series of R scripts on a schedule that you wanted high visibility into. So you want to know like what's running now, how long is it taking, I want a visual diagram of where my job is, over time, I want to know if it fails, I want to get an email, I want to know if something suddenly starts running very slow. And all of this can be accomplished through Apache Airflow, which I think is really cool. So this is my next project. It just so happens that it also runs inside of a Google App Engine. So as you can see, I'm slowly building up these skills, learning all of these managed services, fitting them together, and cool new configurations that'll have something to do with R, and I'm getting a lot of joy out of it. Finally, you want to burn whatever you've built in the sandbox down to the ground. So it wouldn't be in our studio talk if I didn't talk about burning things. And this is how you do it. Very explicitly, go to settings and utilities, project settings, shutdowns. You want to stop getting charged when you're done with your sandbox. Lastly, I'm really interested in building out these resources for people who are interested in learning analytic administration. I'd really like to see an analytic administrator become a legitimate job title someday. So if you'd like to connect with me, I've recently written a little bit about learning analytic administration through a sandbox on our R views blog. You can check that out. And I also am trying to get this radmins hashtag going. So wish me luck with that. Thanks guys. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you."}, {"Year": 2018, "Speaker": "Jonathan Hers", "Title": "ow to Start a Data Science Insurrection at an Organization that Would Prefer You Not", "Abstract": "Disruptive transformations are all the rage in Silicon Valley, and even traditional organizations are starting to discuss machine learning and artificial intelligence. However, new methods from data science can create internal frictions within organizations that alter power dynamics. The extent to which any project scales can often be determined by institutional support, even if the project is successful. Moreover, 'move fast and break things' isn't a reliable model in organizations where the cost to failure are high or catastrophic. Discussing his experience within institutions such as the World Bank and Inter-American Development Bank, this talk will discuss strategies to help data scientists respectfully advocate for data science solutions at more traditional institutions. In the process, we will learn about research on using satellites to estimate bombing damage in Syria, and how 3G cell phones are caused thousands of accidents per year.", "VideoURL": "https://www.youtube.com/watch?v=NPBvlge1ONY", "id0": "2018_20", "transcript": "Wow. So thanks so much. It's really a pleasure to be here. I always, four tips. I think there's one behind, oh, it's on auto. Okay. So let's try to go back. Yes, it's really a pleasure to be here. You know, every time I come here, I feel like I learned so much. I'm really honored to be here because I'm probably the worst R programmer in the entire room. You know, I thought about what would be useful for people in this room. And I wanted to present something. So I want to present something about what would be useful for me. So I thought about the people who maybe work at organizations in this area. And I thought about back when I was doing my PhD and I worked for the World Bank here. And I thought about, you know, what we were trying to do with the World Bank. And I wanted to translate some of the things that we learned there. But I also wanted to tell about some of my research. So, you know, you can think about this as how to start a, this is a very like sexy title. I wanted to get you in the room. But really, I'm just going to tell you a bit about my research and also give you some tips on doing machine learning projects that can scale within an organization. And just by way of introduction, I'm an assistant professor of management science and economics at Chapman University. If you've seen the TV show, the OC, that is like not too far from like the student population that we have there. And I, you know, I feel like, so, so I always want to talk at my university because it has a really great reputation on the West Coast. So we get a choice of California undergrads. But you know, these are kids coming in off of like with surfboards and skateboards. And I can say that we've had explosive growth within interest in machine learning. So I teach machine learning courses to undergrads and MBAs. And we've had nearly 100% year over year growth. So now we teach four sections a semester of, of machine learning. We're opening up and expanding kind of every business schools doing this, like trying to expand into more analytics classes. So it's a great place to be if you're interested. You know, it's just a great time for, for machine learning. So the thing I want to talk about is, you know, when you think about machine learning, you often think about these companies on the left. So you think about Facebook, Google, these are so-called digital native companies. So these are companies that kind of from top down, they pretty much have a machine learning or a coding mindset. And you know, you know, there's a lot of organizations here. You know, think about the World Bank, think about the Department of Census, think about the Consumer Financial Protection Bureau. These are a lot of the organizations that are represented maybe in this room. And you pretty much can't expect that people top down have a digital native mindset, right? And yet we're often the people in those rooms, maybe we're the guy or the girl who's trying to implement these solutions. And anyone who's done this in these organizations realized how tough this is, like incredibly difficult. And yet if you think of the mission statement for these organizations, like, if we can implement some of the machine learning solutions at these organizations, just incredibly powerful. And I would say that the low-hanging fruit for what is possible is much greater here than it is over there. No knock if you happen to work over here, you probably make more money if you work over here. But like a lot of the people over here, you know, this is where I think there's a lot of impact that's still left to be made. So, okay, so, but a lot of the conflict comes from this aspect, which is that like, you know, who within an org chart actually understands machine learning, right? So, you pretty much maybe if you work in like marketing or R&D, you understand R, but how about, you know, to get any project that's going to scale, you're going to have to like somehow communicate what you're doing up to up the hierarchy. And so, you know, do the rest of the people understand R? Do they understand machine learning? And how about the people above them really like, there's just like no way it is happening. So, and yet the problem is, the problem is that you eventually need to like your, what thing you're doing that transformative project, which everyone in this room has capability of making some transformative project, you eventually need to make this its way upwards. And so, the advice I want to give here, before I talk about my research, I want to talk about how do you scale that thing upwards within an organization. And, you know, there's a lot of like constraints to an organization for implementing these things. And so, kind of the first piece of advice I want to give you is, you need to be able to explain how your model works in language, not just how you understand it or how your boss understands it, but how your boss's boss understands it. So, you can't just, you can't just throw around buzzwords, you can't just say things like CNN, R&Ns, do we even understand what these mean, but you have to be able to explain them in ways that you're, that, you know, they often say like, you know, the grandmother test, like, like explain this to ways that your grandmother would understand something. And I think that's a little patronizing, but, you know, think about this, it's not just about like not using acronyms, it's also about any culture has its own way of doing things and has its own way of explaining things. And so, you have to find a way to kind of bridge in that machine learning topic and you have to explain that using their own language. So, find out like how they communicate within an organization, find out how you can explain that machine learning model and then communicate with those two. And I think that's really hard because then you have to understand actually what you're doing, right? You have to understand it on so many levels, on mathematical level, a programming level, and also kind of an intuitive level. And that's why, you know, I'm going to give a shout out to business school students who learn machine learning because that's one of the things we focus on, like, from the get go. It's not just about do you understand this mathematically, do you understand how to program it, but it's really like how do you communicate within a team. So, maybe hire some people from who have a business school background. So, another thing, so let's say like you're able to communicate, you got this great memo report, there might be another constraint here. It's like, if you think about it, like how do these people really get to the top, right? And, okay, so let's say you're coming in and you're like the new, you got all the new cool tools, right? So, you're coming in there, you're all checked out. But think about your boss, like your boss, your boss came in there using the old new tools, right? So, maybe your boss learned SAS. I mean, think about this, like your boss rose that org chart, about what was popular maybe 10 or 20 years ago. So, maybe your boss was like, was the best SAS program in there and he rose to the top. And I'm not even if your boss is interested in learning new things, like there's this natural conflict where it's like something's trying to take food off your plate, you're just going to resist that kind of change. It's a natural human instinct. And so, the best quote that I have for this, I think, is by Upton Sinclair. So, it's difficult to get a man to understand something when his salary depends on his not understanding it. Like, especially at these organizations, which are so risk-averse, that it's just, it's really hard to get somebody, especially your boss, when it's like, hey, we're going to do this new thing, it does this other thing much better than your old method. That's going to make you obsolete. Like, we're here to disrupt you. Like, that's a very, very cautious approach with that. And kind of like, that's why I think these type of meetups where you're explaining people of different backgrounds, that helps it out because it's not like, it's not like an exclusive thing. It's like, we're inclusive, we're all going to learn this process together. But I think it's naturally how most people learn. Okay, so, so, tip two, I think the way that you actually motivate this is you say, you know, show the current methods and show where they break. Like, explain them, so here's what's lacking with the current methods. Like, I'm kind of a methods person. Like, I think, like, this is a really cool method. We're going to employ this, it's going to be really fun. But if you're the type who, you know, got famous or gotten renowned on the old methods, you're just going to be naturally resistant to the new stuff. And there's a bit of like a generational gradient within an organization about who's willing to implement new stuff. So the way that you do this, it just, just explain why the current methods are not working. And so, all right, let's try that. All right, so, so in my own work, I often work with satellite images. And I try to use these to fill in what are called data gaps. So data gaps are where we just don't know enough information about certain areas. So my dissertation work, I took satellite images and I use computer vision technologies, a lot of the stuff that, like, we saw earlier with hot dog, not hot dog, I was using pretty much the same models, except I was extracting out buildings, not necessarily hot dogs. I was using to show how those can be correlated with things like poverty. And why that's so important is because this is how most surveys are gathered. So we have to go to somebody's house, we have to interview them for a series of hours. And this is incredibly labor intensive, incredibly expensive. And so the way that I showed at the World Bank that this was broken, or at least I didn't, I kind of like highlighted this result, which is that 57 countries had zero or only one poverty estimate between 2002 and 2011. And so, you know, my boss or my boss's boss, they got renowned on the basis of being the best survey methodologist. And that works well in some cases, but clearly there's a gap. And so the way in which that gap exists, that invites some new methodologies and some new possibilities. All right, so another tip here is, need to be honest about the limitations of your method. So you want to show what your method does well. And if it's, you know, like, don't be honest about these things because in the real world things are never quite as simple as they are in your desktop. So, so I like to show, you know, we're just going to keep watching this. Okay, so, so, you know, I showed that the satellite imagery is, they can be useful for predicting poverty, but I was very cautious about what, what are the explanatory power. And so it turns out that we can explain somewhere between about 0.4 and 0.7 of predicted poverty using satellite images. Now that's really good if you have no poverty estimates for say the last like 15 or 20 years, but it's obviously not going to replace a really good survey. So, you know, we're very cautious, like, understand where these new technologies can be useful and where they, where they're not necessarily going to be useful. All right, but you really don't want to, you know, you want to show the upside potential. You don't want to undersell your own work. Like, it's not the case that you want to say like, look, look, like, like this stuff is only going to work in certain situations. Like, you want to say like, what is the potential of the stuff if it really scales, right? Like, that's the incredible thing about these organizations that they're so big that we, they can like start implementing things at scale. And once they do, like, then you have a lot of possibilities. And so you need to show that possibility so that, that person high on the org chart says, like, ah, if I'm the person who grabs ahold of this, then I'm going to get renowned for this, right? All right, so you're going to like spark their imagination about this. So, one thing I always like to show and these, this is something I'm experimenting with in my own work is using these, they have these like micro satellites by companies like Planet. And so, they're about the size of a bread box. They cost about $30,000. And so, right now Planet has more satellites in orbit than any other organization. And in fact, they have somewhere like 200 or 200 satellites that are constantly scanning the Earth's globe. So, we have daily satellite images of all the Earth's land surface. Some of you are nodding your head because you know about this data. So, you know, think about like how, but you know, maybe, maybe right now, like, this is just like a small child, but like, think, imagine what happens when the small child grows up. So, I always try to like, you know, show what is, what are the capabilities of this. So, this is, this is a, we've done an image classification over land use. And so, this is an image of every road in Mexico trained off of that Planet imagery. So, if you wanted to, you could capture what are every road in Mexico at the daily rate. So, you could see like expansion of roads, expansion, you know, at the monthly or some arbitrary frequency. So, you just got to like, you know, this is sexy, you got to sell it. So, okay. So, so, Jared said that he wanted me to do something technical. He's like, you can't just come up here and do a bunch of memes. And the problem is that, like, as a researcher, you're, you know, these things take a long time. And so, like, it was only six months since my last talk. So, I'm going to talk about the things that I've done since. And so, shifting gears a bit, I'm going to talk about some research where we're trying to answer the question, do the internet enabled mobile phones increase traffic accidents? I'll let you think about in your own lives of whether it's like a danger for you for helping you out. And this is joint work with Matt and Brie Lang at UC Riverside. And so, the working paper we just released is car accidents and 3G coverage, new evidence using cell phone towers. And you can kind of think about how we're weaving in those principles. Even though we're like, now we're like academics in the wild doing whatever we want. Like, you can think about how this could be something that you could implement and maybe the Department of Transportation. There's nothing here that we're doing that is different than you couldn't do on your own. So, think about those principles and how we're, how we're weaving those in. All right. So, okay. So, we're going to look at the growth of 3G technology only within California. We're looking at the growth of cell towers, as well as the growth of just internet enabled, like at the road level. So, our unit of observation is going to be a road. And we're going to measure between 2001 and 2016. When did that get access to, we're going to measure all the cellular towers. The nice feature is that the government makes any cell tower register. So, we have all this rich data on cell towers. And so, we're just going to measure, and this is the growth at the road level of which roads get access to 3G coverage. And this is a great event study, right? Like, if every road is getting access to at different times, you can think that's exogenous to a bunch of other stuff, then we can measure the impact on here. So, here's the problem. And this is where machine learning is going to come in, is that we only measure 3G coverage at one time period in 2016. And, but we know all this other rich data about the closest tower characteristics for these roads between the entire sample. And the solution here is that we're going to fill in the data gap. So, we're going to build a random forest or a machine learning model to predict 3G coverage through this time period. So, that's going to tell us we're going to get predicted coverage for 3G access at the road level. And we're going to do this based on like a rich data set on tower characteristics, like tower elevation, tower height, kind of the elevation around the area when the tower was built. And of course, we're going to do this in carrot, because carrots are a great package for using this stuff. Shout out to carrot, shout out to max. Okay. All right. So, let's see how this, I'll walk you through how we do this. Oh, that was too funny. Okay. All right. So, the first thing we want to do is once we've loaded our data, we certainly want to cross validate, select our parameters. And the carrot has really nice sampling features. So, this is where we're cross-valuting over MTRI. So, that's a number of variables, a sample of every split. And it looks like five is the optimal for all these different models. These three different models are different assumptions about the introduction. So, when was actually 3G access introduced? Because we don't know that starting date. We have some ideas about, we have reports about when 3G technology was introduced, but it's kind of like staggered introduction. So, we either assume that it was introduced in 2003, 2004, 2005. We pretty much know by 2006, there's 3G access. And, you know, the 3G access is a thing that's really going to distract you. That's a thing like, you know, having Facebook, all these other things that are going to distract you while you're driving. And it looks like the 2005 assumption of introduction of 3G access is a thing that really, that does the best. So, all right. So, another thing for us that suggests that we should use 2005 as the introduction of 3G access is that if we plot, like in red, we see the random force prediction for the fraction of roads that have access to 3G. And then we see the fraction of like mobile broadband subscribers per 100 inhabitants. We see that these two things like match each other really well. So, this red is the prediction we get from the random force. This blue is actual data. They match pretty well. So, we know that our predictive model is doing something real here. All right. So, now we're going to do some model calibration. This is something I always like to do with any model that I do. What we're going to do is we're going to take all those predicted probabilities that we have. So, now we have just from 0 to 1 for every road, what's the probability that this road has 3G access at any time period? We're going to bunch up, we're going to do bins of those predicted probabilities. So, the one between 0 and 0.1, 0.1 and 0.2. And the so-called well calibrated model is one in which if we predict it to have 10% probability of having a 3G or some event, we should observe that, you know, one out of 10 of those actually has the event. And so, what we want is we want like these dots to line up perfectly on the 45 degree line. And okay, so it's not perfect. So, we're kind of like under calibrated initially, and then we're over calibrated. But this is about as good as like a model's I've ever built. I don't know. This looks okay. It doesn't look like too bad. Yeah, it kind of goes up, sinusoidal, I don't know. Mine tend to look like that. But it doesn't look like totally whack, right? That's also a technical term. So. Now, of course, we're going to do some auto sample accuracy plots. So, this is in the test sample. Like when you first do an RSC plot like this, you're like, well, I obviously overfit something. But this is in the test set. So, I don't know what's happening. This is better than any model I've ever built. I don't know what's going on. Maybe there was some type of information leak between the test and training set, right? It looks like that, but it's not. I promise you. I don't know. I just, I don't know. It's a really, it's maybe it's a very easy prediction problem because the area under the curve is super high. But when you look at something like this, you're like, wow, this is really overfit. But it's not. This is in the test set. So, good on me. Not bad. Thanks, Obama. Thanks, Obama. All right. So, that's all the machine learning stuff. We're going to embed this machine learning technology within an event study. An event study is going to measure what is the probability of an accident right when we get access to a, to 3G technology on that road. We're going to do a fixed effect Poisson model. Poisson model, we're going to use that as opposed to regression because it's account data problem. You can't have fractions of accidents. You can't have negative accidents. And so, we're going to estimate this model. The coefficient of interest here is this data. That's just going to measure what happens before and what happens after the timing of the introduction of 3G. Like, so once this road gets access, we're going to line up all the roads right when they get access to 3G and figure out what happened before and what happens after. And you might say that some roads just are more dangerous. So, we're going to control for the average road traffic. And we're actually going to have road fixed effects so that if one road is just like, no, also janky or has some sort of like, just has like a higher probability of accidents, we're going to control for that. And we're going to control time fixed effects. So, maybe in December, there's more accidents for whatever reason. All right. So, let's look. So, let's see whether actually, like, getting access to mobile enabled cell phones caused more accidents. The suspense. Okay. So, we're plotting the probability of an accident on the y-axis. And then on the x-axis, we're prodding the years relative to that change, whether you whether you get 3G access or not. And so, it looks at, you know, pretty much, you know, two years after getting access to 3G, remember this 3G is predicted, looks like we have an increase in probability of accidents. And then three or four years out, we get a much greater increase in probability of accidents. And we can do this heterogeneous based on the age of the driver because we actually know a lot of information on accidents. So, it looks like not surprisingly, younger drivers more susceptible to looking at their phones when they should be driving. So, this first plot up here to the left is for accidents for people 29 and younger. See, like, much greater increase in probability of accident. And, but pretty much, you know, the people between 30 and 40 and 50 and 64 also are tempted by using their cell phones. So, they also have like a higher propensity of getting in traffic accidents following this road having 3G access. It's only the people who are 65 and older that have no temptation whatsoever. People are not tempted by looking at their phones. So, sorry, sorry, mom and dad, you guys are bad luck. All right, so to conclude, we see that accident rates increased by 1.1 percent when a road gets access to 3G coverage. And internet, this is, so if you look at this, an aggregate looks like internet connected mobile phones cause over 3000 accidents in California. It's kind of crazy to me. And I think one thing that's really cool is this is just further evidence that you can embed machine learning predictions within a causal model, right? People often say that, oh, machine learning, it's only for prediction. That's all you can do. But, no, no, this is causal. We've just embedded within a causal model here. Okay. All right. So, comment suggestions. Please email me. I'm always happy to talk about this. If you want to sponsor my research or you want to make me aware of any grants, I would love to talk to you. And like I said, we're always expanding in the number of classes that we're offering. So, if you happen to be in this other California area and you want to teach a class, like I handle a lot of the curriculum there. So, I'd love to talk to you about that. So, I know we're always hiring like full-time non-tenure track people. So, if you want to talk about teaching class there, we'd love to have you. Okay. All right. Thanks."}, {"Year": 2018, "Speaker": "Mara Averick", "Title": "How I Found Your Answer", "Abstract": "Equal parts history and mystery, noted *data sciolist*, Mara Averick, will take you on a whirlwind tour through the lighter side of learning and communicating (data) science. From the 19th-century pages of Science Gossip to modern-day social media, you\u2019ll find out how the nature of scientific communities has and hasn\u2019t changed over the past 200 years, and discover how \u201clearning out loud\u201d can help you navigate an ever-increasing amount of information, or (at the very least) keep you entertained while so doing.", "VideoURL": "https://www.youtube.com/watch?v=AD2Bnjru6g0", "id0": "2018_21", "transcript": "I came up with the title to this talk and spent most of my time making the photo slide look like how I met your mother, which I have never seen. Anyway, so I've seen like half of an episode. So my name is Mara Avrik. I am the Tidyverse Developer Advocate at our studio, which we're not sure what that means. Yes, so how I found your answer. Let's talk about that by running your code. So that's all I have. Fine, I crack myself up. All right, so please note this is not the scale. Something I was thinking of the other week in a Tidyverse meeting when somebody put up a slide and the board was like, oh, sorry, I was just running that code in my head. And I was like, oh, I do all the time. Not at all. I don't know. Yeah, so when you have a question, here's the universe of people who can help you fix your code. Don't hear people who can run code in their head. All the way over there is people who will answer your question without running the code first. And that's something that it turns out, people who are new to programming don't necessarily begin. If you don't believe me or think I know anything because I'm not cool, even Jenny Brian is like caveat, I did not run your code. People run your code when you ask them a question because they want to make sure it's right. And I started talking about this stuff about a year ago. And this slide and the one before is, first of all, this is actually true. That is, I have fielded a GitHub issue that said it doesn't work and also had our template for an issue, which was really helpful. So you've got a problem and it doesn't work except no one's going to be able to find your answer based on that. So you're like, oh, they're going to need more information. But there's this newcomer's paradox, right? So like when you're new to something, you don't have the words necessarily to solve your problem. And then somebody's like, oh, like, just do foo bar and best. And you're like, I don't know what that means. And I ran question mark foo. And it's not even helping me in the function reference. So these past two slides, I've been, that was also a joke. It's not a function. So these two slides have been thinking about for a while and kind of between them and now what changed for me is that I've read, actually, I looked at this yesterday since I made this slide two weeks ago. It's now 29.2,000 questions on the RStudio community. So it turns out there are a lot of things that you kind of have to think about. You have to tell somebody when you want an answer that apparently not everyone knows and that I didn't necessarily know. And the thing is that the context in which you're asking your question matters, right? So like different environments have different norms. Also I like Archer. So let's talk about like three different areas in which you can ask for help. All of which are good. Like Twitter is like hashtag R stats. Like does this package exist or like not like an in-depth coding question? Or if you just want to know, like who might know about this, go for it. Stack Overflow has a really specific model that is incredibly helpful. But again, it's really specific. So the goal of Stack Overflow, right, is to have basically unique good question and answer pairs on the internet. And this was a flow chart that was in meta Stack Overflow saying like how do you respond to RTFM questions, which for those of you who don't know is read the flipping manual. And it's kind of like can it be found with a simple Google search? But basically there are like three outcomes here. Like you can answer the question, you can improve it, or you can close it. The thing is when you're a beginner, you don't always know exactly what your question is to start out with. And as somebody who isn't a beginner, you might be able to help somebody, but you don't necessarily know the entire answer for a self-contained, that's like the goal of a Stack Overflow question is that it be self-contained, and you don't have any resources, but you might be able to put them in the right direction. So our studio community has been around for about a year. It uses a platform called Discourse, which was made by Jeff Atwood, who also did Stack Overflow. And it's a civilized discussion for your community. The platform itself looks like this, and it lends itself to a different kind of discussion. Certainly a lot of question and answer, but also things are a bit more kind of free form. And there's, I spend a lot of time kind of helping people to write a good question so that they can get the answer. And what I've found in the past year or so of doing that is that it's actually really beneficial. So what's in it for me as in you? So not me. If you're asking yourself this question, you're asking what's in it for me. And I'm recommending that you help people answer questions, which feels kind of Mr. Miyagi getting his car waxed. Don't worry, I took out the Mike Tyson slide. But the thing that you end up discovering is that you end up getting better at your own troubleshooting skills by helping other people, which is valuable. And there's a lot of advice on how to ask a question. This is from a paper that was published in a year that I don't know. Ten simple rules for getting help from online scientific communities. You don't have to read this whole thing, but number one is like, don't be afraid to ask a question. And the subtext of two through ten is like, be slightly afraid about asking your question. Granted, like I am on the other side of this. Like it can be really hard, but you also have to have like a certain amount of empathy for the user. So let's think, I'm going to tell you guys some good things about asking questions and then some thoughts I have on, you know, how do people help people how to ask their questions. So it turns out actually like the anatomy of doing this isn't really unique to programming. So for a GitHub issue, we talk about like, all right, the problem description, the expected behavior, and a minimal reproducible example. And that is like GitHub saying that. So it's official. But it turns out this is like basically the anatomy of figuring shit out. So like in my mom's a psychologist and like in behavior therapy, they call this the ABCs. So you're looking at the antecedents, the behaviors and the consequences. An example of this that I found online that I just love this kid, which is like, all right, what was the antecedent circle time, singing days of the week, behavior, kicks other student, consequence time out, possible function, escape, circle time, singing days of the week, kicks two students. Like if we're doing a linear model, I've got some guesses about what's going to, but like the point of writing this, I really got to respect the hell out of that kid. You know, like the point here is that like you're breaking down an issue and it's easy to get caught up in the technical elements of that being like, I don't understand this. I don't understand that. But really like think about the information that you would need to like open your car door or do another normal human thing to sit through the days of the week song. And why were you trying to do it? This is like a huge thing that people often forget to describe, like, oh, this is why I was trying to do this thing. When you say it doesn't work, it's very huffful if you say why you were trying to do that thing. So, you know, like how does this work out? If we give people that anatomy, does it all go PT-Kine? This is a very real issue that was filed. Steps taken so far, I tried and tried. And the thing is they're not wrong. You know, like, technically speaking, but that's also not the information that's going to help me or anyone find your answer. I like this quote by Carl Popper, like, it's impossible to speak in such a way that you cannot be misunderstood. That question was probably just asked to illustrate this quotation. Turns out writing prose about code is really hard. And if you think about what's going on here, like, dude over there, Chad, like, wrote some code. Now he's thinking about it. He thinks about it. He's like, this is what I just did. And then he writes a long thing describing what he did. Like, and then I had this for lunch and my boss yelled at me and like, I wrote one of those squiggly dots. And like, you're like, oh, and then like, Marna, my alter ego, over here is like interpreting that. And I'm like, oh, it's a squiggle dot a semicolon. Is it a snake? Whatever. And then I'm writing an answer backing code being like, and then press the third button to, like, if you think about this from a computational standpoint, there's a lot of like, interpretation evaluation and then like, there's a lot of room for error or something there. So sometimes people are asking a question. This is kind of like, this is what it feels like, you know, like, they're asking me for a question. I'm asking them for more information and saying, like, no, I'm withholding it. Look at me. Got to go with the reality is it's really more like, I don't understand the question and I won't respond to it. It's no one's being withholding. There's just only so much you can do when you're given limited information. So enter magic of rep racks. So it stands for reproducible example. And before I tell you what it is, let me tell you, like, the raison d'etre is help me help you. I think I've seen a lot of people feel like they're like, everyone here is just asking me to make a rep rack. So I already had a problem and now I have a rep rack problem. The reality is like, it is to help us help you. So what I found when helping people, this is like, the meta helpfulness is that like, I go with basically a three pronged strategy approach, is like, all right, what I'm asking you to do, I'm asking you to make a reproducible example, why I'm asking you to do it and how you can do the thing. So let's talk about the thing, the keys to reprexilence. I'm into a good portmanteau. So what you need is code that actually runs, code that doesn't have to be run and code that can be easily run. Which, hence my intro song, I naturally thought of this to the tune of wrap God. You guys should really hear Mr. Boombastic heteroscedastic if that's another talk. Yeah, so like, for reprex you need your packages, you need your objects. And I'm asking for this for the flipping context. I didn't actually mean for that to run as much as it did. So what's the point of doing all of this? Well, what reprex does is basically runs our mark down render on your code, if you have everything there. And then if I'm reading your code, like, I can see your error message. I can see what code you ran, I can see what you were using, and then I can take it and run it myself and I can help you. Which was what you were aiming for, if I hope. And I did not play that gift before. So now you can see that in action. Even cooler, in my humble opinion, is the fact that reprex also is amazing for troubleshooting issues that involve plotting. Because it automatically, literally, this is on your clipboard, you've cut and pasted it in. I can see everything you did. This issue was written by Klaus Volke when we had an issue with the line height. And you can see he did a very good job of keeping it minimal. And depending on where you're at in your art journey, you may be able to see like, you can see exactly what is happening there and you don't need to do any of this kind of like, push and pull. When I'm answering your question, I'm not Hadley, but like, I try and leave like, a little bit of a treasure trail of like, how I found the information. You know, so Hadley here saying like, what did I do? I looked in the S book, I went to the R source mirror, I found the relevant part, I saw the history, and then I sent an email. The use of that is that, then next time somebody has a question, or the next time you have a question, has anyone else here had a question and then read their own answer? Yeah. I'm always like, who is that smart cookie or moron? And I'm like, it was me. Yikes. So that's one thing that you're kind of guiding people through this. Like, it may be, it may feel obvious to you, but if you're a newcomer, you don't know where things are. So, or if you're yourself in a year, reading error messages. That was something I did not do for a long time. I think I just read an error message as being like, super obnoxious and scary and that I broke it. And this one is an excellent example of there being like, super useful information. This literally says like, configuration failed because this, like, this is what you have to do. But if you are in like, error messages or super scary mode, you probably didn't read it. So what I end up doing sometimes is like, per the error message, which sounds a little bit obnoxious. But like, this might be that person's first time ever seeing a useful error message. Like, I don't know if, I don't know if you guys have deleted your fortran compilers by accident, but I sure have. And like, the errors that you get are not usually, it's like, again, damn squiggly marks. Some circling back here to RTFM. Like, seriously guys, read a book. When you write a package and you write documentation and all of that, and you know where it is, it's really easy to feel this way. You're like, I just wrote this out. And I have seen many times, like the response, like, it's in writing our extensions. Well, here's the freaking manual. Oh, just read that. A step away, helpfully, Colin Fay formatted all of the R manuals as book down, which is step in the right direction. You could say, go read this slightly prettier manual. Or you can really guide people through the docs. And that's, this is something that I try and do, in part, just as a way of modeling, like, what is helpful behavior. Being like, here's where I found a thing. And some other things. And this kind of model of helping people ask good questions through responding in a certain way with something that there's a great paper written called, we don't do that here. Yeah. And it was, they did a test pilot from Stack Overflow where essentially they had like a room that helped people to ask better questions. And what the mentors helped people improve on was like, you know, phrase your question title better, being like, my boss is going to kill me. SOS. I don't even know what your question is. All I see is those characters. And that might be all you literally see. Formatting. So when code and text are interspersed, hard to read. Community triage. This is very legit. Like, if you're filing, like, I, like, I've been working on this and this isn't going that well on GitHub, probably not the right place. But being like, here is the right place. Or like, I do it on Twitter all the time. If somebody asked me a code question, like, oh, can you come ask me this on community? Because I only have 280 characters to answer it. And code doesn't fit in there. Framing the question. And then, like, for Stack Overflow, like, FYI, like, kind of the culture of asking is like, we don't say, like, thank you. Or, you know, like, because that signal noise. But you've got a sense for that. Quick tips. I have, like, avoid a major data dump. Like, what I think I'm doing is being like, oh, look, look at all the packages and manuals we have. What they think I'm doing is like, great, I should read everything. And I do try to use our trigger. I mean, I do try to slowly guide people, ultimately, to source code. Which you don't always have to get to. But in about a year of reading a lot of source code, I can tell you there is really useful stuff in there. And taking out bits and pieces and showing people how you got to the point where you could find their answer is a really powerful thing. There's part of this that's about giving back. Excellent piece of literature here. Thankfully, maintaining open source software. Doing it out of the goodness of your heart or something. But I am super into being selfish. Because I think that is where, like, the free and open source software happy place resides. So sometimes I'm on Twitter and I tweet things. And I totally do that because those are the things I'm reading and I want to be able to keep track of them. It turns out that's been helpful for other people. But that's, like, a side benefit. So I think that answering questions and asking questions are both really valuable. And I think that if you take an approach to it where you're breaking things down, you're going to be able to solve your own problems better. And you're going to be able to help other people solve their problems. Everyone has kind of come up and talked about amazing things today that I want to go try out. And I'm probably going to break something while doing it. And I hope that somebody will help me out. So kind of try and feed back into the community so we can all do. More cool things because holy shit, eukics are badass. That's it."}, {"Year": 2018, "Speaker": "Michael Garris", "Title": "The Story of the MNIST Dataset", "Abstract": "It has been said the MNIST handprinted character image dataset is the \u201cHello World\u201d implementation for machine learning, and the dataset is used as a world-wide machine learning benchmark. But where did it come from, how was it created, and for what purpose? A clue is found within its name. This talk will demystify the renown dataset by telling the story from its inception to its impactful journey.", "VideoURL": "https://www.youtube.com/watch?v=oKzNUGz21JM", "id0": "2018_22", "transcript": "I want to thank Jared for the opportunity to be here and to talk about this specific data set. Who has actually worked with MNIST? All right, cool, very good. And if you go to Wikipedia, you can kind of find out all about it, right? You go down way in the footnotes with an asterisk and you might be able to trace down the kind of information I'm going to tell you today. Which is how did this data set even come about? And in telling the story, I'd like to get a feel for just how costly creating data sets are. All right, so rewind the tape. So I started in this in 1986, roll back to the late 80s. What do we have going on at that point in time? Improvements in computation in algorithms. And there's rapid advancement in pattern recognition technologies. This is driving signal processing and image processing. And a key approach that's just caught fire is this thing called a multi-layer perceptron, an MLP, also known as a neural network. Does that sound familiar today? It really does, doesn't it? It's kind of eerie. Now at that time, this wasn't lost in the US government. Department of Defense was investing a ton of money and resources into this automated target recognition. But on the civilian side, you're a census. There was a shout out earlier to that. That was great. The postal service and, yes, the IRS. They're all looking at how to use handwriting to automate things like reading zip codes so we can get the mail to its proper destination faster. All right, so back in the late 80s, these are the type of neural networks that we were dealing with. They often had one hidden layer. So you got this input layer of your data. You got the output layer of your activation answers in the middle. You do this combinatorial, nonlinear feed forward and get some sort of prediction. We did a lot of work on not feeding the actual raw data into this network at the input layer. We would do classical numerical methods to create feature vectors that then we would put in to do the machine learning. All right, and oh, by the way, around 1989 or so, Jan Lecun showed up at one of the zip code conferences and said, hey, I've added a fourth layer to the bottom. It's a convolutional layer and it'll take the raw pixels in and start to do something with it. 1989. So fast forward about 2010. AlexNet comes out and oh my gosh, we've discovered convolutional neural nets. Well, not necessarily, but you can see that something's gone on here, right? Now these last three layers, they're pretty much the same technology in the prior slide. But dealing with the raw information and pixels to self-derived image features, that's what's all going on in this deep network. All right, so that just gives you an idea. There's a lot that isn't new and there's some things that are new. And what's really driving this is more about the data and the computational capacity that we have today. So why NIST? Okay, so back then, we had this key question, what is the accuracy of hand printing? The character recognition. Vendors were selling this to the government. They were running their own benchmarks on their own data using their own metrics. So how do you know which numbers to trust? And that's where NIST comes in. We help bring objective measurement to technology and emerging technology. So what do you need to do some sort of a benchmark? You need some sort of calibrated instrument, calibrated artifact, in this case ground truth data, right? And so that's why this character data set actually came into existence. Real quick in my story, I'd like to talk to you about the timeline. It starts in 1990 with the release of something called special database one. We go out to 1988, oh, but I thought MNIST was new, no, 1998. MNIST is introduced and there's all this NIST work that got us there. So I'd like to talk to you a little bit about that. And then, oh, by the way, there's this data point out here, real recent, called EM-NIST data set, and we'll get there at the end. All right, so the MNIST data set is born through a carefully designed data collection using this type of handwriting sample form. In the end, there's about 100 templates of this form in the original data set. And it was carefully designed. You can see the digits zero through nine written out three times in order. But then you get this set of variable length fields with a balanced but randomized distribution of those digits, that handled co-location effects and things like that. There's a field down here of lower case alphabet and the upper case alphabet. And for some free form text, which was just kind of pushing everything in the day, we had people right up by hand, again, ask them to print the preamble to the US Constitution. Now quick story, we sent out thousands of these forms through business return envelopes. They come back to NIST. We start getting tens, 20s, hundreds. We're like, oh my gosh, we've got to open all these envelopes, right? So we borrow a letter opener, an automated letter opener from our mailroom, and a very overly eager technician grabs a stack of these envelopes and goes, bah, okay, they blow open the first envelope. Those forms were bifolded, put into the envelope. So not only did it rip off the end of the envelope, it also severed the forms in two. So anyway, always be careful. And don't entrust your valuable data to inexperienced people. Things got a delay that I'm not used to. Come on, there we go. All right, so 1990, we ended up receiving back 2,100 of these completed forms. They were filled out by census employees. They were field data takers all across the United States. So we had handwriting representation samples across the lower 48 anyway. This data set was distributed on something, do you know what that is? A compact disk read-only memory, right? CD-ROM, right? So this was new technology in the data. So if we put that data out, we could get what a gigabyte of data on there or something. So the forms were on there, the full scanned images, and also the cropped out fields of the handwriting. We scanned this in at 300 pixels per inch, and we did it in bi-level, black and white. And just to get to this point, we had to do that data collection design. We then had to do the form scanning. We had to determine an image format and compression because images, they would fill up capacity of disk in that day very quickly. We had to create something called form registration so we could look at the image, find the boxes on the form to find the characters in the boxes, and then do this field isolation. So that was 1990. Then a couple years ago, we're still working really, really hard, and we really special database three. This is a collection of that labeled segmented and labeled characters that were on all those handwriting forms. And now we had this collection of individual character images, a digits lower case. Each character centered in 128 by 128 pixel image. To get to this point, the milestones needed were, well, we wrote a character recognizer. So we used a lot of prints, or blob coloring, a component analysis, and then we had to reconstruct things like when the top of a five that's not attached or not a attached has to be put back together. So a lot of work in character segmentation. And then before we could send this out, we needed the labels, we needed the ground truth. Well, we were pretty clever about how we designed the form. Remember on top of the form, we told the writer what to write. Well, maybe they didn't write what we asked them to write. And then you add automatic character segmentation to the whole bit. You now have error. And so we had to human adjudicate all those characters. So here's a quick story. How did we do that? Right? We have 100,000, 100,000 of these characters. Well, we had a lab technician, didn't have a grad student. Sorry. We had a lab technician who was prone to wander and become distracted and was put under a performance improvement plan of the government. And he had the wonderful measured task to sit in a very cold machine room on a sun micro system display. And what we did is we created a thumbnail montage of 1,024 character images of the same class, all the zeros at once, all the ones at once. And the visual aspects of our human brain, you can sit there and go, oh my gosh, that is not a zero, right? And so you just pluck them out. You could reassign it. And we could comb the data that way. And anyway, we won't talk about the guy's performance plan review. Okay. So in 1992, we used that data to run a test. Remember NIST objective measurements? Let's figure out what the accuracy is for the benefit of the government to run procurements, know how to put in our RFPs the proper error rate requirements and things like that. We also processed another 500 forms of characters in order to do a blind, sequester test set called SD27. That was filled out by some high school, some students. And then fast forward to 1995, we repeated this whole process and scaled it up. And my colleague, Patrick Grover, gets a lot of credit to get us to this milestone. And we have something called special database 19. These special databases were put out by NIST in the public domain free to be used. We didn't put any traceability on it. We knew when people ordered CDs, but you could put CD data up and you'd share it. And we didn't have any protocols on how to use the data. But in the end, we had over 800,000 segment of label characters. And to get to this place, we also introduced, remember that preamble paragraph at the bottom? I wrote code that could then take the segmentation in further reading order, use a dictionary to actually clean up the classification with a neural network classifier. And so that is the special database 19 that Jan Lecun picked up for MNIST, this data base, this data set is available today. Now, all that technology in order to create this, I put together a package called HSF, handwriting sample form system, SIS, HSF, SIS. And this package is written in C, so you can probably get it, I'm sorry, it's not NAR. But you can get it, you could probably port the code with some work to the modern compiler. But in there is form registration, field isolation, form removal. Because we would get the characters and we would segment them out of the boxes, but a lot of times the characters overlapped with the box information itself. How do you remove and clean that up? All that's in there, neural network based classification, that classifier code actually contains approaches that are being used extensively still today with the deep learning. And then, fast forward from 95 to 98, when MNIST actually is first introduced. Out of all those characters, Jan Lecun, Karina Cortes and Chris Burgess, they carved out 70,000 of the hand printed digits. Balance sets, 60,000 for training, 10,000 for testing. But remember I said it was bi level, black and white image originally. MNIST is in gray, well that's not real gray, all right? It's a pseudo gray created through a bicubic interpolation of the binary images and scaling them down to 28 by 28, okay? And so, that is MNIST and now there's something called EMIST. And also that's why MNIST is even in the name. So extended MNIST. So SD19, 800, 1400,000 characters, only 70,000 were actually put in the MNIST benchmark. Particularly with the advent of deep learning, convolutional neural nets at scale. The air rates, as many of you know, are very, very low now on that MNIST benchmark. And it really is time to raise the bar. We're just not seeing enough air, we're not pushing the algorithms enough to really be comparatively useful. So, let's see, Gregory Cohen, Said, Afshar, Jonathan Tapsen and Andre Van Schijk, they get credit for going back to the SD19 and kind of carving out some new sets. You'll see at the bottom of this table, the MNIST set, preserved for backward reference and compatibility. But if you go three lines up to digits, there's a new reference set out there that has a total of 280,000. So that the community can then retrain and start to also generate benchmark accuracy air rates and compare each other to this. All right. I think I blew right through this. So, what I'd like to leave you with is some lessons learned. And I'll be around after the break to talk about any of this. First thing, producing high quality, ground truth labeled data sets is costly. How many of you have done labeling of data and created data sets to run your R experimentation? Yeah. It's just a pain in the butt. But there are things that can help. And as you saw demonstrated here, starting out with a careful data collection design can be very helpful. It can save you a lot down the road, developing a robust tool set in the domain that you're working in so that you can use automated techniques to do labeling, a first-passet labeling, and then use human adjudication through visualization or some other secondary means in order to try to clean this up. We were able to estimate that through our validation or authentication, adjudication process, we got these labeled characters at that scale down to less than one error in a thousand. Then another thing I'd like to leave you with is hang in there. This is what, 2018, fiscal year 2019, if you're fed. So we're talking about this stuff was 1980, late 80s, early 90s. Hang in there because you never know what's going to stick. My career took me off into panel recognition and biometrics, fingerprint, face iris recognition, things like that. I had no idea there was a community that was swelling around some of this SD19 data. I mean, NIST found out very late. In fact, where we saw the use of SD19 in the early 2000s was, I am not a bot where you have to go in and validate that you're really someone, the characters they show, companies realized they could get handprint data for free. So that helped feed that for a while. So hang in there and I can say that the odds of having something stick increases when your work, in this case, data and source code is made public. All right. And then the last thing I'd like to say is pay it forward because what goes around comes around. And I know I'm preaching to the choir here because I think this is definitely the spirit of this community. So Jared, thank you so much. It's really the first time that I know that the real story of emnis has actually been told. Thanks for listening. You've been the guinea pigs or the unfortunate first time listeners. But again, I'll be around at the break. Thank you. Thank you."}, {"Year": 2018, "Speaker": "Abhijit Dasgupta", "Title": "Activity Monitoring Using Sensors and R", "Abstract": "We increasingly use sensors to monitor our sleep and activity on a real-time basis, using ubuquitous wearable devices like the FitBit, furniture-linked sensors like Beddit, as well as the time-tested heart rate monitor and GPS. Zansors adds to this environment by developing a wearable breathing sensor to monitor activity and sleep. In this presentation, I\u2019ll describe some of the characteristics of the data we see, how it can be munged using the tidyverse, analyzed using different R packages and displayed using htmlwidgets, to enable interpretation of different activities as well as sleep patterns. To this end, I will describe available implementations of moving averages, as well as linked dynamic graphs that are invaluable to intepreting the data.", "VideoURL": "https://www.youtube.com/watch?v=9o5Mo5aWiqI", "id0": "2018_23", "transcript": "So, a little bit about me, and you heard a little tidbit, I'll talk about it in a minute. I have a relationship with almost all the speakers' topics at some level. I did genomics for 10 years after my Biostat PhD, and then I got sick of it. Simply because the way the field was going was going in one direction, I didn't like the direction. Right now I work at NIAMS. Anyone here from NIH or HHS? Good, so I don't have to give you the test of what NIAMS is. NIH is a 27-inch institute that you're supposed to know the acronyms for most of them. This is the Earth HIDIS group, so obviously, rheumatology. I co-founded a startup called Xanswers, which we do sensors for sleep apnea screening, and a few other things I'll talk about some of the work I'm doing there today. So, NIH is north 15 miles, Xanswers is in Arlington, who's right across the river. I'm a Biostatistician, but as I mentioned, I actually don't do clinical trials except as a reviewer. So, I'm a data safety monitoring board member for mental health trials for NIH, and I also am a reviewer for SBIR trials in neurobiology. So, I'm all over the place, so if my talk is scattered, you'll understand why. I teach practical R, which is the name of the course at the NIH graduate school, which is their basic R class that's taught to all the graduate students in the Biotatistic Science program. Most of you have met Martin at some point, he teaches the basic Python class there. So, we sort of partner in our colleagues and arms there. And my personal interest in this area is in statistical machine learning, data visualization, which I'll talk about today, and how do we pass this knowledge on efficiently to the next generation of whatever you want to call yourself, be it data scientist, ask station, operations researcher, or whatnot. On the community side, we've only mentioned, I've survived the grind of the data community. I was at the first meetup in DC, literally eight years ago. I'm a board member of data community in DC, and somehow Jared mentioned this earlier about books and meetups, and the like, I actually managed to go out through a book with some other people who were in the community a few years ago. Wouldn't be here without my family. So this is from Diwali, like three days ago, literally. My wife and daughters made the Rangoli, which is on the floor. And so I did my duty as a father and a husband, just appreciated their creativity, because I have, it's no way I'm doing that. My wife is not working on her third master, so she's far more qualified than me. And both my daughters are classically trained in Indian dance, and they're also unadvanced choruses of their school. So you think there's a lot of song and dance at home, but nothing to do with Bollywood, which is really nice. My older one is practicing to play Ursula in the Little Mermaid this spring, so very different kind of song. And speaking of Aikido, we talked about it a little bit. But the second longest relationship I have, after my parents, it predates my marriage, and is the reason I maintain sanity, I stuck around long enough. This is proof that if you stick around long enough in something, you'll actually get somewhere. It's like the bad person that doesn't go away, even though he's throwing him. Finally got a four degree blag about a few years ago. I teach about 15 miles north of here in a place called New Market. For those of you who know Frederick, it's like the next town over. If you've never seen Aikido, these are my friends from Rochester, and they're actually husband and wife, so this is how they get rid of their marital angst. They're wonderful people. They're also spits. They actually make iron stuff. So this is if you've never seen Aikido, and the guy standing in the back, you can see it much better on the screens than on this screen, which is a little fuzzy. That's my instructor from Japan. In this, you'll believe it or not, he is 80 years old in this picture. He's now 81. This is from last year. He can still kick my butt. So it's pretty interesting. For a guy who's 122 pounds. So that's what I do when I want to get his love team. So regardless, I'm going to, I actually modified, just added this slide after listening to all the talks in this meetup, because it got me thinking. So someone was talking to Dave Smith that he's started before R was R. I'm almost there. I remember starting in 1997. So it's been 20 years, I think version 0.3. So you know, Roger said yesterday version 0.6, he doesn't have gray hair yet. You have to get gray hair to get way back there. But the thing that I want to say about this is that in those days we were dealing with base art and base plotting and stuff that people sort of don't use anymore in some ways. But it took a good 10 years of working on that and developing that. A lot of the infrastructure came from S plus before, but cran was tiny. Cran was in two digits. And then, so we did that for 10 years. And then, thank God, had we become God of PhD. And created GD plot. And then created plier and created reshape and reshape two and deep plier and now become the tidy verse and all the R studio supported stuff that's out there now. And that, so someone was mentioning something about a hockey stick shaped graph. Well, at least the corner of that graph in my opinion, in terms of fire, things happen in terms of growth, right? But the nice thing that's happened now and is sort of reflected in my talk and a lot of the talks that have happened over the last few days or in the last few hours, essentially last 48 hours, is that ours become a glue language in many ways for data science. So we've gotten the core parts of our pretty well set, right, with tidy verse and a lot of the packages that know what they're doing. And with bio conductor, which has been fantastic for the biological sciences and genomics and all the omics stuff that those of us who are NIH or in pharma or biotech use all the time. Which, by the way, historically started up the street as well at Hopkins. So Rafael Zari, who was at Hopkins at that time, was crucial in starting all of that. So a lot of stuff has happened here. But now it's become a glue language, right? I mean, today we're getting wrappers around things that allow us to ingest all sorts of data. So heaven is a package of you all the time to grab SaaS data. There's XML, there's all sorts of things that basically if there's data out there, are can ingest it now to the efforts of a lot of people. And on the back end of this, we've gotten the point where we've wrapped these JavaScript, beautiful JavaScript visualization libraries from our through HTML widgets or other platforms, right? So ours and not just there, who here has tried out the RStudio preview version, the version 1.2 that's on the preview? I know you're using it, so as am I. If you go down the list of, so on the left corner when you hit that plus button and said, you know, our script or our markdown or text or whatever, the new version includes Python, SQL, Stan, and there's some other language that's in there as well. So our studio and ours becoming the glue language, who here's used to articulate? Or those of the package articulate? So particularly does it play on Python, right? It's another kind of Python or the nanaconda. It connects R to Python or Python to R. There's always been R pi two which connected R to Python. So the point is you don't have to be a monoglaut anymore in data science. You can actually interact with the use different things and R seems to be one of the good glues around that. So great time to be in R and come to an R conference and use it because it's really, really rich now. What's sticking around for 20 years like the other stuff, right? Now it's finally fun. It's been fun for a while actually. So what I'm going to talk about today, I mean, I sort of have to talk about what I put in my abstract, some extent. So the answer is about five years old. We've been doing these sort of small patchiness in real time on a model in a couple of slides. It's spucky, big. Six on your lapel, six on your neck. And the primary purpose that we had of this was to try and detect sleep apnea in a comfortable situation. Anyone who has ever taken a sleep study, polysomnography, lucky you. Sleep apnea testing requires you to be overnight in a hospital with 16 leads on you, including a chest strap, a head cap, a cannula, and all sorts of wires. So yeah, you're getting real great sleep on a sleep lab. One of the big problems we found talking with people around this is that you have to do these children sometimes, and children hate it, especially the cannula. So trying to think of new ways. So this is in process. The after this needs to deal with FDA and stuff like that. But what we realized is we can actually leverage this technology to work with activity modeling, activity monitoring through breath work, right? Breath rate is about to heart rate. And so when we went there, we realized that we sort of have to compare this and figure out sort of what ground truth is. We had a talk from NIST about talking about ground truth and how to compare things. So there's not really ground truth here unless you go to an x-ray's lab and do stuff. And we've done some of that. But also just field testing, right? Runners are enormously tech savvy, right? They're wired with garments and whatnot. Today, a lot of us are wearing a smartwatch or a Fitbit or whatever. So we have sensors on us that collect activity data. And so what I'm going to talk about is how you can sort of grab that and play with it. So the data I'm going to talk about today is actually GPS data from a Garmin 4Runner, which one of our crash test dummies used on a few runs. And he also had an additional heart rate strap, so we got heart rate through that as well. Now, I hadn't played with this data before, so I went out and looked sort of the kind of stuff that Mara just talked about. And I found out that essentially you can get all this data in something called GPX format, which is the GPS exchange format. It turns out it's all it is is XML. So it's not rocket science, thankfully. It's an open format that you can check and the DOM is well known and you know how to do it. And so there are packages people have written for this. So in plot KML, there is a read GPX function and map tools as a read GPS function. But since it's just an XML file, I went bare metalized and used the XML package, right? And so, in Gory code, we're not going to go through the details of it, but the point ends up being that you can extract each of the tracks that you would see displayed on a GP on a like Strava or runtastic or whatever your favorite software is. By grabbing essentially those four lines, we just grab out the tagged data from the XML file. So it's really easy. So what we're trying to do here, so you can tell how small this thing is. It's literally on the guys lapel. We're detecting breathing and motion. So there's an accelerometer in here. There's a microphone. And we distill the raw signals into respiration rates and cadence, which is how many times you're stepping per minute and you can because of the accelerometer. I won't give you the details of how I do that for obvious reasons. But I will talk about how to sort of use that and sort of create some visualizations with it. So we've seen a lot of leaflet, this conference, right? Very easy. So this is an example of one run. So I took the longitude and latitude from the GPS and just plotted it. And this person was going, for those of you who know this area, went sort of from Central Arlington across the river and went to DuPont Circle. So leaflet is great, but we've got other tracks, right? So we're not just dealing with where they went. We're talking about things like elevation, things like heart rate that they have, things, the breathing rate that we're picking up. So thinking more in R, so first of all, I'm going to go to GGplot plus, sort of leveraging what Mark talked about yesterday. And the first thing I'm going to show is GG map, one of the ones that Mark didn't talk about. So GG map makes it very easy to do maps. So this is off a Google map and you can tell there's different kinds of maps you can do. This one is the map type is road map, you can do a terrain map, there's all sorts of things you can do. And then the syntax is very GGplot, right? The one little twinkle that has happened in the last year is Google changed their policy, you now need an API key to actually get the maps. Which means they want your credit card, right? As some of you have probably figured out who used maps. Open Street map is an alternative, but they were more cludgy in terms of this connection for whatever reason. So yeah, so I can make a nice static map of this route. I can even use things like how plot or grid arrange or. So there's several packages in the GGplot extension world that will create multiple independent GGplot objects and put them together, right? I tend to prefer cow plot. I have no idea why the man named it cow plot. Because it's a personal package that he released, it's this lab package he released. But it's really, really nice. And so what I'm seeing here is the run on one side and then the right side I'm seeing his heart rate. Over time and then there's this weird dip. Right? Suspicious. The first part I get is he's starting his run from static, right? But there's this weird dip. But the problem is I can't, well the static graphs connect these two. I didn't know which part of the run this happened in. And so there's really, really nice to, so Jared briefly mentioned crosstalk at some point. I know. So crosstalk is this really nice package that has been written to share data across visualizations. And not all visualizations are crosstalk ready, so caveat emptor. Leaflet is and D3 scatter is. The one that you normally use for this kind of data is digraphs. If you go to the HTML, we just website and start mining it. Digraphs is the time series one. It's not crosstalk ready. And I actually went on their GitHub issues page. It's still after three years is still not crosstalk ready. They're still discussing it. So these three scatter work pretty well. And so the idea here is actually very, you know, elegant in some ways. So at the very beginning, you create a shared data set and then you share it with each of these different visualizations. And you arrange it to this BS calls function and so you just say it up in columns. So it's very, very, I find it very elegant to work with it. The interesting thing with this, of course, is the syntax that I see in leaflet and D3 scatter reminds me of GGViz. With the tilde notations with the one sided formulas. Yeah, Jared's laughing this. He actually gave a talk with GGViz and he remembered it. And it's dead, basically now. It's dead in the water. Not his talk and not there, but GGViz is dead in the water. But the notations stay stuck around for some reason. So this reminded me of that at the very bottom. I have a filter slider. So the reason that crosstalk works does two things well. It is meant to do what's called link to brushing. So if you have two pictures and you select a part of it, it'll automatically select the corresponding points in the other one. Which is great unless you have a lot of points and you can't tell the difference between what's selected and what's not. So I added the filter slider to just kill a bunch of points when I want to drill in. And then I can actually do the link brushing. So that's the reason for that. And you'll see in a minute. So that's why that filter slider happened. So these are actually linked. So now I have leaflet on the left and I have the D3 scatter on the right. Same data. So you notice this will actually be live. So if I actually select that part, it is selected, but it's not lightning enough on this side for me to find out where that's being selected. Right? That's why I needed the filter. So let's get out of that. So this is roughly a 10 minutes into the run. So let's take this slider and go, I'm going to bring you to about a little earlier. So let's go to eight minutes. And let's bring this side down to. You want to move with me today? Thank you. Please. Pretty please. It doesn't like me today. I'm sorry. The current feeding of sliders has not happened. I'm sorry. But I know the answer, so I'm going to actually drill down to it. So this is the very beginning of this. And I've actually seen where the drop is. This is at the beginning. So both sides got cut off, right? So if I move this slider back a little, you see that both graphs are growing back out. So if I bring it back down to eight, I'm at the beginning of this drop. So I'm way down here. So let's go in there. Move it up a little bit so people can see. And so if I just see where this drop happened, now this thing will actually, because I have fewer points. I can see exactly what happens. This little dark is going to look much better on the LCDs if you're back there. The drop happened because he was waiting at a crosswalk. For about two minutes, which is a really long crosswalk stop. But his heart rate fell. But this is part of the power of these linked dynamic visualizations. I can drill down from the graph and go, there's an explanation here, because this is weird. And so being able to find that is actually really, really nice. I'm going to add respiration to that panel. So same data set, same structure, except I'm doing two panels, one with respiration, one with heart rate. And hopefully that will work. There we go. So it's the same panel. I just squished them. So the top one is heart rate. The bottom one is respiration. And we saw and the nice validation for us was that when the heart rate went out, so did the respiration. So see the story. So he is heart rate went down. He has to get it back up to his training level. So he works very hard to get it up there really quickly. So his breathing goes up. And then stabilizes. And then there's a little bit of elevation here that I'm not showing today, which is fine. And then there's this really weird piece right here. This piece that I'm. If it actually selects selecting on my screen, but it's not selecting very well there. That piece, which is really weird. So why are you breathing so hard for about five minutes of reading that fast? So same deal. Let's go to minute 35. Hopefully this will slide over because it seems to behave. There we go. And then let's see if this left side right side actually wants to move the right side. Just doesn't want to move for me today. It's OK. So what you realize is this beginning of this was when he was crossing the key bridge. Has anyone from DC actually walked across the key, the key bridge? You realize how windy it is. That's what happened there. The mic didn't have a windscreen on it. And so we got a lot of noise on that screen and that created an artifact, which is great for it. This is actually one of our early experiments that helped us figure stuff out here. But it's stuff like so this was really part of what I was doing when I was analyzing this because you can't squint at the data. So you have to really look at it and tools like this were really, really invaluable for it. So what would you notice? So I have created dashboards. I'm not showing it to you. You can create a shiny app. It's all this fun stuff that everyone is now doing. If you haven't used flash dashboard, I'm actually, that's my new love affair because it's really, really nice. And there's actually a really nice article as to when you should use flex dashboard, when you should use shiny. I forget who wrote it. It might be David or it might have been. Hadley, someone wrote it. Basically, flash dashboard does not have a shiny server underneath it. So it's just native JavaScript connections. But it's really nice. Or you can just have some fun. So there's another sort of nice package that Thomas Peterson has updated recently called GGAnimate, which was originally a David Robinson package and it's no longer. Actually, it's really funny going on to stack or on the Google because some people have the old API that David wrote and some people use the new API that Thomas wrote and they don't tell you which one they're using. So suddenly, so you're going GGAnimate P and you realize, no, no, that's David's. That's not Thomas's. So it's very interesting. So by or be aware when you're looking online for GGAnimate advice. And so you can do literally with two additional lines of code from what I did from my GG plot. I can actually track what you did. So it's very, very easy now, which is, which is, which is sweet. So I'm actually on time. So Jared can, can thank me later. The presentation is on GitHub and I have a blog and stuff on Netlify, you know, following the eWe path of blog down and so on. I'd like to end with one thing is almost for the end of the to the conference and it's been a great conference. I'd like to ask for a round of applause for Jared and Mark and all the Lander crew who've done a wonderful job and all the sponsors. I know there's only two left, but great job guys. Hopefully we'll see you again next year. Okay. Thank you very much."}, {"Year": 2018, "Speaker": "Ami Gates", "Title": "Association Rule Mining With Tweets: Thinking Outside the Basket", "Abstract": "With the increasing and continued interest is text mining, and the potential for relationships between words or items, association rule mining has become a more popular technique. The classic example for association rule mining is to investigate \u201cbaskets\u201d of items originating from transactions. The most notable such example is the \u201cmarket basket\u201d, where foods appear within transactions with greater or lower joint probabilities. However, collections of items, or baskets, are not the only application for association rule mining. Applying association rule mining to Twitter data (Tweet Text) using R offers interesting insight into words that are highly associated or correlated in a given set of Tweets. By thinking of each Tweet as a transaction, one can collect Tweets, reformat them into basket-style .csv data, and use R to apply association rule mining to discover relationships.", "VideoURL": "https://www.youtube.com/watch?v=eOOhn9CX2qU", "id0": "2018_24", "transcript": "So I'm going to do pre-thank yous and then I'll tell you guys who I am. I want to thank Jared first for letting me come up here and speak in front of people. That's always a risk. And just as a side note, and I'm not picking on anybody, but my song was the most awesome. Putting it out there. And I want to thank Mark for making me aware of DCR, making me aware of this awesome community, which from now on I will definitely be a part of. And it also brings me joy to realize that everyone in this room likes R. That's excellent. Okay, so, I mean, unusual. So a little bit about me. Things I love. My husband, you know, obviously. My coffee, very important. And my mountains. That's, so that's me in Ireland, that's me in Switzerland, that's me. When we went slot canyoning and that is an incident that we will not discuss in this note. Okay, that's a long story, a long story. So what do I do when I'm not doing this? I am the director of the analytics program at Georgetown University. Go Georgetown. Go Hoys. And if anyone has any questions about that, of course, please come talk to me at any point. I'm also a professor at the university. So I teach data analytics, data visualization. And I teach R and Python, which is fine. My background, it's up there. Okay, good stuff. So when I got started thinking about doing this talk, yeah, I cheated and I said to Jared, you know, what do you think? Because there's so many people in so many different talks. And I do something that's repetitive. And he said, well, how about association, rule mining? And I was like, huh, I never would have thought of doing a talk on that. So yeah, sure. And what was fun about that is I'd only actually done a lot of that in Python because I code a lot in Python and I came to R a little later in the game. So my background is computer engineering. And I was around when people were still coding in Fortran. That was actually my first language. Thank you very much. And so when I first started with R, I hated it. True story. It was like this is ridiculous. And now I love it. And I feel like that that's just a path you have to travel through with R. And so when I teach and I tell my students, I have a slide called misery and suffering. Because that's really important. And if you don't realize that you will suffer and you will be miserable and you will spend 10 hours finding something that you then realize is really easy. And someone else who knows you, who you didn't ask, was like, oh, I could have told you that. And then you hate them forever. OK, so anyway, let's do that. And the direction we're going to go in today is normally, and so how many people use do or remember vaguely association rule mining? Just like in see what I'm dealing with. OK, good. Almost none of you. Awesome. That's great. You won't even know if I'm saying something stupid. So association rule mining is an unsupervised learning method. It allows us to do discovery. And from my point of view, I love discovery because it lacks prejudice, which as you can imagine being the director. And when I tell people they're like, really? So it's nice for a moment to not have that reaction with something. And we're also going to go in the direction of thinking outside the basket, if you will. So association rule mining has a lot to do with transaction data. In fact, it has everything to do with transaction data. So as a beginning, we're going to go ahead and just do a very quick review, the classic review to bring everybody on board with me. So this is from Kumar's book, Introduction to Data Mining. A transaction is kind of what it sounds like. It's a collection of items. And when you have a data set of transactions, each row, if you have it in basket format, represents a set of items. And so association rule mining is actually based on set theory, which is nice if you come from a stats background and not as nice if you don't. And so what you can do with this data set is you can create what are called rules. And so I have just a few of the most common rules here. Like if somebody goes in, this is not a gender bias comment, by the way, but it's going to be. Okay. If a male, not a male, someone goes into a store and they put diapers in their cart as it turns out there's a very high probability that they're also going to put beer in their shopping cart. I didn't make that up. That's a classic example. Any gender bias example as it turns out. And so this is what we start with when we talk about association rule mining. But the real question is what things are associated and why do we care? And from the beginning of that story, you think, well, okay, if you're a supermarket and someone's always going to buy diapers and beer, what are you going to do? What would you guys do? So the women said, put them far away. And yes, that's what you're going to do. But if you're a male and a manager of the store, you're going to put them close together. So you're going to go with that. Now, either way, you have a plan based on information, based on data. That's what we do. That's what we care about. So we can take this a step further and we can then have measures that answer questions for us like, oh, no. Okay, good. I have a force field in technology. It doesn't matter. So there are three common measures that we use for association rule mining. Support which is just based on the probability of both of the sets occurring together. And just to back up again, what are sets? What do I mean? So any rule that you can generate is made up of a set of items, call it the set X. It can be empty. And it goes to the set of items Y, which can also be empty. So the question is, if you have X, what's the probability you're going to have Y? That's the big association rule mining question. So the support is the probability that they're going to occur together. That's a good measure and it's helpful. But it's not enough because what if one of those items is something that everybody buys all the time if you're dealing with with items. Then it's going to be with everything. And so its measure is going to be maybe a little off. And then what if you have items that come together all the time but are just not super popular in general? So one of the items is support. The next measure is called confidence. This is basically just conditional probability. They're saying, what's the probability that these items occur together given that the person has the set of items X in their card? So that's an interesting and different measure to look at. And then the last measure we'll look at is called lift. And if you look closely at this and you remember your stats, if the numerator and the denominator are the same, what is that? What's that mean? Independence. Thank you. It's my students. He's awesome. Okay. Thank you for knowing that. So if the numerator and denominator are the same, that's independence, meaning that these items in A and B, they don't really go together. So if the lift is between zero, which would be disjointness and one, which would be independence, we don't care about that rule. And that's actually important because you'll get a lot of rules that actually sometimes will have a high support and other things and you'll think, oh, hey, this is cool. And you realize the lift is one or something and then you don't care about it as much. So these three measures are what are called, if you read this on CRAN or whatnot, they call it interestingness, rules that are interesting and rules that are not interesting. And these are three different measures that help you determine which of these rules are maybe more or less interesting to you. Okay. So I'm not going to read you guys math just because that would be mean, especially at the end of the day here. But if you wanted to go through and do the calculations, let's just for a second look at beer and diapers. Beer and diapers are in transaction three and four. They occur together twice out of a transaction set of five transactions. So they have a 40% support. They have a two thirds confidence and their lift is greater than one. So this is an interesting rule. That's what would be considered interesting. Okay. So in R, there's an also in Python, there are packages that allow you to perform what's called the a priori algorithm. Who remembers a priori? Who's ever heard of a priori? Okay. That's great. Thank you. My students are back there. Okay. So what this basically does, you can imagine that if you have an enormous set of items and you want to look at every possible combination of every item, how humongous that tree is going to be. This is a graph of just five items. And look at the number of combinations there. So what the a priori does is it prunes based on super sets. So it looks at what is some kind of threshold or measure of an item set. Let's say the set A and the set B. And remember, A can be multiple items and B can be multiple depending on how you set it up. We're looking at this as individual items though. So vitamin item B. If that turns out that they're infrequent, that they just don't occur much at all, we're going to prune everything that's a super set of that because by definition they will also not occur a lot. And that's how the a priori algorithm works and that's why it's faster because otherwise if you had to go through this graph, it would take forever and ever. So now the other side of the coin is how do you get this to work in R? So in theory it sounds great. We like it. We want to see what words are interesting. We're interested in correlation. I'll always put this out as a reminder, especially when I'm working with medical doctors. No offense, no offense. But correlation does not mean cause. That's right. Just putting that out there. So this is a measure of correlation. Words that occur together, associations, that's the name that's where it comes from. Okay, so there are three different formats that you'll generally see this in. The first one all the way to the side here is called basket. The one in the middle is called single. And the one over to the right is actually sparse matrix. What's interesting about the one to the left is that even though you can store it in a CSV file and that's exactly what I'm going to do, it's not really record data. And that matters. You don't want it to be record data in this case. And the one over to the right is record data, but the big challenge of using a sparse matrix is you have to know every single item you have at all moments. So if you add a room of an item, you've got to go back and update that. Whereas you don't have to do that if you use a basket format or a single format. So with R, and I had to update the data set because honestly Kumar's data set is disgusting. The food items and the choices. So I went ahead and made my own healthy sensible data set. Yes, these are the items that you should be eating. Now, you'll notice that the person, the people shopping. So I did two things here. One, I removed the ID number because you don't actually need one. And I wanted to make that something that is notable. Also I put quinoa in every single person's cart. Quinoa give away that day, free quinoa. So everybody had quinoa in their cart, which is good. You have to soak in and wash it, just FYI. Okay. Now, what's the issue here? Like, what's going to happen is the support, the confidence. Everything's going to be related to quinoa, right? So I did this purposely as a teaching example so people could feel what the measures mean. And instances where things come out oddly. Look at also the association between the coffee and chocolate. It's purposely done as well. They occur together a lot. And so when you run this, when you run this in R, here's what it looks like. So the library in R is called A-Rules. You will have difficulty with A-Rules. And if you don't, you know, email me and tell me how. So you'll install it. I have some other. If you have it, just come ask me. Anyway, what's going on here is I'm pulling in the transactions. And I'm going to use the basket format. I can inspect my transactions. I can build a rules object using the a priori algorithm. And I can sort those rules and then I can look at them. So that's what's going on here with the R code. Okay. And this is what we get. And this is fun because it basically gives you a set of rules. And it gives you all the measures, the support, the confidence, the lift, and the actual physical count of how many times those items or objects ended up in the same transaction. Okay. And you can control this too. So you can sort by confidence. You can sort by support. You can sort by lift. You can control the min length, the max length. You can even have items that have zero in a set and so on to just see how many times things actually occur. So that's the story behind association rule mining. Here's the code for you wanted to use single versus basket just so that you don't have to go searching around for how that works. So now we get to go to the interesting bit. That was the review of association rule mining. So what I thought to myself is, yeah, okay. So everyone does this for transactions, usually for items and how they're associated. Then it'd be fun and interesting to do this for Twitter data and see what I get. You know, see if there are specific words. And then the next question is, what's the format of the data going to look like? That's arguably one of the most important questions we answer when you data science. Data cleaning, data formatting, data processing, right? So I don't want this to be a bag of words. If I grab in a whole bunch of tweets and normally when we do NLP or we do text mining, even when we compare documents, we do any of that stuff. We grab in, we make a bag of words, we make a big CSV files, we compare things. We're not keeping basically context, if you will. What I'm trying to do here is make sure that I retain each tweet as its own transaction. That's the goal. And so in order to make that happen, and I played with a whole bunch of stuff so I left the libraries up there and I left the things that I ran into and how to fix them up there just in case you are wondering. But the ones highlighted are the ones I'm using for this talk, this FYI. So number one, and I'm going to go through this quickly because I think most of you know this part, is in order to do anything with Twitter, you have to get a developer's account. You have to make sure that you get all their keys and passwords. And then you have to put them into R. So there's the code for that. At the very top, if you're wondering what the heck is that because I blocked some of it out because those are my actual tokens, there are many ways to do this. What I did here is I created just a text file with the variable names and then under them the actual values of my tokens so that I can easily read that data in and I can easily update it and use it in multiple files. So instead of hard coding it in, it lives in a file and that's what you see going on here. At the very bottom, what you see is me pulling this into Twitter. So basically saying to Twitter, I'm who I am. And then I use Twitter and I used a method called search Twitter. Now when I was at home doing this, I actually did it on political data because that's probably what you're thinking already that was really, really fun but it took me forever to try to pull all the profanity out of it. So I was like, I don't know, I'm not going to do that. So I did something completely safe though it also turned out interesting which was I love chocolate, hopefully no one's offended by that. So when we go through the, it's okay, it's part of my retardism. When you take a look and see what your object looks like, if you're familiar with Twitter at all, it's going to look very familiar to you. And I've read this into a data frame because I only want to access the text of these tweets. But you don't have to do that. You can look at the geo code, you can look at latitude, longitude, I just read a paper the other day where people used a method very similar to this to measure happiness around the United States based on what people were tweeting and where they were located. Apparently one of the happiest places is Boulder, Colorado and there are miserable places too and I'm not going to list those. So the key here is that each tweet has to be a transaction and you can't lose that and that you want to be able to access that. So this actually took a little bit of doing, I thought it was simple, it was not simple, it was so free to steal my code because it actually took the longest of everything I'm telling you here, this one slide. So what I did to make this happen is I created a file first and I put the very first tweet as a transaction in that file, separated as CSV. I had to use Squish, I had to use Unlist, I had to use a number of these fun methods. So later if you want to play with this, I recommend it because it's not simple. So it's an interesting problem. Once you do that, this is kind of what you've got. Every row is a tweet so I didn't lose that context, I still have that in there. And so my question is first, how can I clean it and then second, what's it telling me? So to clean it, I went through a bunch of just your basic steps. This is where I look at, I summarize it, I take a sample and take a look at that, I looked at it visually, I noticed I'm going to have to take out all the HTTPs, all the HTTPSs, something called RT, I got to take these things away so that they don't destroy my association. So I actually did quite a bit of cleaning on this data, I'm going to fast forward to that slide, I'm running out of time a little bit. And so essentially with my cleaning, I used a combination of just getting rid of stuff by hand, it's not cheating, you've got to do this sometimes, it has to happen. And then I took away things like digits, I used grep L for that and what I ended up doing was creating a logical list and then applying that logical list to the data and deleting things that fell under the logical, true or false. So I could explain that a lot better but not in one minute and 41 seconds. So this is an extension of that idea where I just took things away that were garbage for lack of a better word. Okay, then I ended up with a cleaner data set and then I was finally in a position to take a look at the rules. So I inspected them, I sorted them and this is what I got. And this was very interesting to me and you get what you expect a little bit. So you get things like yummy, you get things like gift box and it depends on when you do it, like what day it is and whether it's Halloween, you get different stuff at different times. And so it's interesting as just a general result. I also discovered that there's some really strange stuff going on with chocolate. There's a community that associates chocolate with their food porn. You can see it better here in this A-Rules-Viz graphic. And so this is actually interactive and there's an even more fun one. But A-Rules-Viz creates just your basic plots and it gives you some nice visualization. When you pull it up on your computer, you can move things around and the size is based on support, the colors based on confidence where darker is the higher confidence. And because it's a network, it immediately gives you the first X. I chose 10 associations here. And you can do it based on confidence, support or lift as well. Which gives you a nice idea of what you have. And so in the end we discovered that chocolate is happy. It is a dessert. It happens on the weekends. It is associated with food pornography and sometimes it comes in gift boxes. And these are the things people talk about. That's right. That's zero. Bam. That's just timing."}, {"Year": 2018, "Speaker": "Vivian Peng", "Title": "Ethics of Data Storytelling", "Abstract": "Data informs decision making from the individual to the system at play. What are the considerations to be mindful of when telling stories with data, particularly in the media? This talk explores how to choose what story to tell, find the right medium, and prepare your data for the media, keeping in mind that with great powers come great responsibilities.", "VideoURL": "https://www.youtube.com/watch?v=CgYDsDBQAwU", "id0": "2018_25", "transcript": "Thank you so much for staying to the end of this conference. It's been so great so far and I'm really, really happy to be here. I want to start by saying a disclaimer which a lot of people have said in this conference too is that views are my own. And so let me tell you a little bit about who I am. I used to work for Doctors Without Borders for the past six years doing communications and advocacy. So I was originally based in the New York office. I was running a campaign getting Pfizer to literally the price of a pneumonia vaccine. And after they lowered the price, I switched over to field contracts. So I did a mission in Capella, Kenya, and then last summer. And beginning of this year, I was based in Myanmar and I finished my mission earlier this year. So at Doctors Without Borders, we put out a lot of reports about public health issues, right? And as a communications manager, my job is to tell stories, to get attention and get the right kind of media attention around these issues. So I care a lot about how our work is perceived because it has a real impact on the populations that we work in and our operations. And I noticed these days that our role is more and more polarized. And I'm not just talking about the US elections. I think the issue really started with the birth of social media. Because of how things get optimized on social media, we like headline news, we like things in photo format, and we like small sound bites. And over time, that's really changed our behavior. It shrank our attention span. And it really changes how we consume content. And as a result, a lot of information gets taken out of context. And I think that's why a lot of our news feels really sensational and we feel really divided as a human race. So today I want to talk about what considerations we have to keep in mind when creating data stories in this environment. And let me clarify what I mean by data storytelling. So I'm not here to talk about the ethics of data science or how to put together a good data analysis. I'm just going to go ahead and assume that everyone in this room cares a lot about the integrity of your data work. But what happens when your analysis is done and you're ready to publish it either as a business report or go to the media or just something that you push online, what happens to it then? What considerations do we have to keep in mind? And how do we essentially protect the integrity of our analysis when it's ready for the world to see? I think I want to start by giving an example of that illustrates kind of the challenge that we're working in. This is an article that was published by the Guardian earlier this year. And the headline says, Revealed. Facebook hate speech exploded in Myanmar during Rohingya crisis. It's very dramatic. It's very sensational. And reading the headline alone gives the impression that everyone in Myanmar is posting hateful things about the Rohingya population on Facebook. But if you read a couple of paragraphs down to the article, it gives a source of the data. And this study pulled comments from one Facebook group called the Mabata. And the Mabata is a hardline nationalist Buddhist organization with a following around 55,000 people at the time the state of it was pulled. So that is inherently not representative of the whole country. And really what this study found was that hate speech exploded in a hardline national group during the Rohingya crisis. And it's irresponsible to imply that the views of this one group as representative of the entire country, which has many ethnicities, each with their own identity and their values. So I reached out to the researcher. His name is Ray Serato. Because he published a really well-documented GitHub post that shows the data, its limitations, and his way of thinking when he was processing this information. And I wondered how all that context got lost in the published article. And we talked about how a lot of times this is just out of the control of researchers and even communications managers. Because when we hand off our data analysis to the media, even with briefing, even with context, we also hand over control of how this story is told. So that's one of the challenges that we have to deal with is once your data story goes out into the world, it can grow and take on a life of its own. I also noticed another kind of interesting response is that in the US on social media, there was so much outrage over Facebook regarding this article. And I fundamentally believe in holding tech companies accountable for how their platforms are used to spread hateful speech. And I really, really hope that they invest more in their detection technology. But I wondered why we don't have that same outrage towards YouTube for how ISIS uses their platform to spread hateful ideology. And Ray and I got to talking and we acknowledge that the time period when this article came out really impacted people's perceptions. This article was published in April this year and that was right at the height of the Facebook, Cambridge, and a little cuz scandal. So people were already outraged at Facebook and this just gave them more evidence to not trust Facebook. So this is the next challenge that we have to keep in mind is that the news ecosystem has a way of impacting the way, impacts the way people absorb our data. Right, we are affected by what we see in the news and that affects how we respond to this kind of information. So what considerations do we have to keep in mind to protect the integrity of our data story? The first consideration is can my data be misinterpreted? And to help you assess this question, I will recommend two filters. The first is what I'll call a mean filter, right? So whether we like it or not, the world in which we live in optimizes everything online for quick reactions and quick views, right? And I remember publishing a Facebook campaign a couple years ago and when you're setting up the ad, it runs it through this tool that calculates the ratio between image and text. And if we had too much text, we had to cut it out. And so it's optimized that everything comes into mean format, right? Can you imagine if instead of calculating image to text, Facebook built a tool that validated truth instead? Right, so the idea of a mean filter is breaking down your data analysis into little bite-sized chunks, right? And imagine if one of those data points was pulled out, plugged out and became a story on its own, would it tell the same story message as your entire report? Okay, so here's an example. We published a report on the right shot, bringing down barriers to affordable and adaptive vaccines. It is 124 pages of really good data, but the thing that made news and made headlines was that it's 68 times more expensive to vaccinate a child in 2014 compared to 2001, right? And so, you know, essentially that it's the same story message. Vaccine prices are too high, kids are not getting vaccinated. That passes, right? And if it doesn't pass a mean filter, then you have to consider what other crucial information that this has to be paired with to give the overall picture. So the next filter I would pass this through is a news filter, right? Earlier I just gave an example of how the news impacts our perception. And so you can be really strategic about this, you can bring in some communications professionals, and we can put together a media report for you. Or you can be really technical, you know, scrape some comments off the news website and do some sentiment analysis, like what are people saying online? Or you can just do a gut check and log into social media and just kind of see what people are saying online. But whatever way or however way you do it, just take time to get a pulse of what's going on in the news. So the next consideration is how might this impact the people involved? And by people involved, I mean the people in your study, the people paying you to do the study, the people who are going to read your study and any other kind of dynamics that could be impacted. And I think it's really easy to remember this when you're working with humanitarian data. But it's equally important to think of this when we're working with data that seems, you know, that is not too sensitive. And I'll give an example. So the peer research center put our report about Asian Americans that we are very diverse and growing population. It's great. I love this story message. But then I noticed something that I couldn't ignore. I identify as Taiwanese American and we are not listed as one of the Asians in the US, right? So I looked a little bit further into the data and saw that here we are in a footnote. Chinese includes those identifying as Taiwanese, right? And so my whole existence is kind of reduced down into a footnote. And what this tells me is that, you know, I do not matter enough to be included. This tells all the marketers or researchers who look at this study that it's okay to group in this way. And you bet that this impacts my life every day to the point where people, you know, whenever they ask, where are you from? From California. No, no, no, where are you really from? And I say, my family's from Taiwan. Their answer is usually, oh, do you mean Thailand? Or, oh, so you mean Chinese, right? And so now I have a stranger explaining my identity to me. And so I'm not going to get into the politics of this because that's not my point here, you know, but the point that I'm trying to make and what I really want to get across is that, you know, all this data that we look out, all the insights we gather and the modeling and forecasting, you know, what makes big data big. And I think what makes big data big is that fundamentally each of these data points represents one human. And it has a potential to impact an individual, and that is a big responsibility, right? So ultimately, what we want to really try to do is to be able to pull out the human in our data, right? We heard earlier yesterday from Rogers talk how these tools have really transformed the lives of, you know, millions of people. It has that potential. But he also mentioned that these tools have the limitations, right? They only answer kind of the broader blunt questions and not the more subtle context. So my suggestion of how to deal with this dilemma is whenever you can to try to interview and speak to someone who is part of your study population, right? Every time I meet our patients and they share their most traumatic experiences with me, you know, it is a privilege that I get to hear their stories, and it is my moral responsibility to make sure I communicate it in a way that is complete with integrity, right? And, you know, we're not alone in trying to like figure this out. You know, I think about this stuff all the time because it was my job too, right? But we all have played a role in, you know, how our data stories get told. And traditionally, this is the flow of telling data stories, right? It starts out with the analysts, you know, you look at the data. Once you get some insights, you pass it over to the advocacy folks. So, you know, what kind of ask can we make? What kind of decisions can we be informed by this? And then once it's ready, it gets passed down to communications and say, we're ready for dissemination, right? And I really want to encourage a different kind of workflow. You know, one where advocacy and communications are brought in right at the beginning, and we work hand-in-hand and coordinated in the design process of this analysis. You know, this allows us to, from the get-go, get a good, you know, scope of what our objective is. And at each stage of the development, we can really keep in mind the ethics of data storytelling. And ultimately, it makes us all drive better, right? You know, we can really design an experience that tells the story that you want to tell. And I will end by showing one example, my favorite thing that I've done, is, you know, turning a data point into performance art. So, the data point was that, you know, 2,500 kids die from pneumonia each day, right? But how do you visualize that? I can just tell you that, but 2,500, like, what does that feel like, you know? It feels like a lot in some contexts. It doesn't feel like a lot in other contexts, right? So, what we chose to do was we bought 2,500 flowers, and we did a procession from Grand Central Station in New York, all the way to Pfizer's headquarters in New York, which is about two avenues of walk. And, you know, we had people hold these flowers and march slowly and silently in, like, this bustling New York city vibe. They walked all the way to the headquarters, and once we got to the front doors, there is this crib, brand new crib, and we had people place flowers one by one, very slowly. You know, and the impact is that when you're participating in this activity, you know, you're walking with these flowers, it ended up being around, like, 20 flowers per person, and you realize that each of these flowers represents a baby who won't go home with their parents tonight. You know, and that kind of impact really, like, drives in the message for the people experiencing it, and the final kind of imagery was this, like, pristine crib, overfilling with flowers, you know, 2,500 flowers on the floor. And it also had an impact on Pfizer, you know, and now they have this, like, giant crib that they had to move indoors, and it sparked a lot of conversations, you know, in their office. And then next day, we attended their shareholder meeting, and the CEO reacted to this performance, you know, this protest. And as a result of that, you know, we were able to have a meeting with the CEO, and months and months and months later of negotiation, more campaigning, they finally dropped the price. And as a result, you know, now we can vaccinate kids in places that we haven't been able to before. You know, now we're using this vaccine to vaccinate refugee children in Syria. You know, so when we work together, and we, you know, this might look really, like, abstract and artsy, but we really work together and design the right kind of experience, you know, it can make a really, really big impact. You know, so I really want to encourage that we continue to have these conversations, right? Because, you know, I brought up a lot of considerations that won't always have answers, but we have to keep asking them because it will push our work and it will push the impact that we want to deliver. Thank you. Thank you."}]